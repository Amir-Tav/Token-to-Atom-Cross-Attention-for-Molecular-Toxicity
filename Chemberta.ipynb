{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d76729de",
   "metadata": {},
   "source": [
    "# ChemBERTA model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c17e85",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b88cf18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, json, math, time, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup\n",
    "\n",
    "try:\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedShuffleSplit\n",
    "except Exception:\n",
    "    MultilabelStratifiedShuffleSplit = None\n",
    "    print(\"iterstrat not available; will fall back to random split.\")\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6905c1d",
   "metadata": {},
   "source": [
    "## dataset paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86d9f3cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_lightgb_pipeline\\Data_v6\\processed\\tox21.csv\n",
      "Saving models to: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_chembera_pipeline\\models\\chemberta_v1\n",
      "Saving outputs to: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_chembera_pipeline\\outputs\n"
     ]
    }
   ],
   "source": [
    "# Prefer LightGBM CSV so we share labels/splits\n",
    "LGB_CSV = Path(\"tox21_lightgb_pipeline/Data_v6/processed/tox21.csv\")\n",
    "CHEMBERTA_CSV = Path(\"tox21_chembera_pipeline/Data_v6/processed/tox21.csv\")\n",
    "DATA_CSV = LGB_CSV if LGB_CSV.exists() else CHEMBERTA_CSV\n",
    "\n",
    "ROOT = Path(\"tox21_chembera_pipeline\")\n",
    "MODELS_DIR = ROOT / \"models\" / \"chemberta_v1\"\n",
    "SPLITS_DIR = ROOT / \"Data_v6\" / \"splits\"\n",
    "OUT_DIR = ROOT / \"outputs\"\n",
    "\n",
    "for p in [MODELS_DIR, SPLITS_DIR, OUT_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Using data:\", DATA_CSV.resolve())\n",
    "print(\"Saving models to:\", MODELS_DIR.resolve())\n",
    "print(\"Saving outputs to:\", OUT_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33336ae8",
   "metadata": {},
   "source": [
    "## loading CSV, Labels/meta columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2149754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NR-AR</th>\n",
       "      <th>NR-AR-LBD</th>\n",
       "      <th>NR-AhR</th>\n",
       "      <th>NR-Aromatase</th>\n",
       "      <th>NR-ER</th>\n",
       "      <th>NR-ER-LBD</th>\n",
       "      <th>NR-PPAR-gamma</th>\n",
       "      <th>SR-ARE</th>\n",
       "      <th>SR-ATAD5</th>\n",
       "      <th>SR-HSE</th>\n",
       "      <th>SR-MMP</th>\n",
       "      <th>SR-p53</th>\n",
       "      <th>mol_id</th>\n",
       "      <th>smiles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOX3021</td>\n",
       "      <td>CCOc1ccc2nc(S(N)(=O)=O)sc2c1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOX3020</td>\n",
       "      <td>CCN1C(=O)NC(c2ccccc2)C1=O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TOX3024</td>\n",
       "      <td>CC[C@]1(O)CC[C@H]2[C@@H]3CCC4=CCCC[C@@H]4[C@H]...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOX3027</td>\n",
       "      <td>CCCN(CC)C(CC)C(=O)Nc1c(C)cccc1C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>TOX20800</td>\n",
       "      <td>CC(O)(P(=O)(O)O)P(=O)(O)O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NR-AR  NR-AR-LBD  NR-AhR  NR-Aromatase  NR-ER  NR-ER-LBD  NR-PPAR-gamma  \\\n",
       "0    0.0        0.0     1.0           NaN    NaN        0.0            0.0   \n",
       "1    0.0        0.0     0.0           0.0    0.0        0.0            0.0   \n",
       "2    NaN        NaN     NaN           NaN    NaN        NaN            NaN   \n",
       "3    0.0        0.0     0.0           0.0    0.0        0.0            0.0   \n",
       "4    0.0        0.0     0.0           0.0    0.0        0.0            0.0   \n",
       "\n",
       "   SR-ARE  SR-ATAD5  SR-HSE  SR-MMP  SR-p53    mol_id  \\\n",
       "0     1.0       0.0     0.0     0.0     0.0   TOX3021   \n",
       "1     NaN       0.0     NaN     0.0     0.0   TOX3020   \n",
       "2     0.0       NaN     0.0     NaN     NaN   TOX3024   \n",
       "3     NaN       0.0     NaN     0.0     0.0   TOX3027   \n",
       "4     0.0       0.0     0.0     0.0     0.0  TOX20800   \n",
       "\n",
       "                                              smiles  \n",
       "0                       CCOc1ccc2nc(S(N)(=O)=O)sc2c1  \n",
       "1                          CCN1C(=O)NC(c2ccccc2)C1=O  \n",
       "2  CC[C@]1(O)CC[C@H]2[C@@H]3CCC4=CCCC[C@@H]4[C@H]...  \n",
       "3                    CCCN(CC)C(CC)C(=O)Nc1c(C)cccc1C  \n",
       "4                          CC(O)(P(=O)(O)O)P(=O)(O)O  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label columns: ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n",
      "Meta columns: ['mol_id', 'smiles']\n",
      "NaNs detected in label columns (filling with 0):\n",
      "NR-AR             566\n",
      "NR-AR-LBD        1073\n",
      "NR-AhR           1282\n",
      "NR-Aromatase     2010\n",
      "NR-ER            1638\n",
      "NR-ER-LBD         876\n",
      "NR-PPAR-gamma    1381\n",
      "SR-ARE           1999\n",
      "SR-ATAD5          759\n",
      "SR-HSE           1364\n",
      "SR-MMP           2021\n",
      "SR-p53           1057\n",
      "dtype: int64\n",
      "Unique values per label (should be [0,1]):\n",
      "{'NR-AR': [0, 1], 'NR-AR-LBD': [0, 1], 'NR-AhR': [0, 1], 'NR-Aromatase': [0, 1], 'NR-ER': [0, 1], 'NR-ER-LBD': [0, 1], 'NR-PPAR-gamma': [0, 1], 'SR-ARE': [0, 1], 'SR-ATAD5': [0, 1], 'SR-HSE': [0, 1], 'SR-MMP': [0, 1], 'SR-p53': [0, 1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7831, (7831, 12))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_CSV)\n",
    "display(df.head())\n",
    "\n",
    "# Assumption: first 12 columns are labels, then 'mol_id','smiles'\n",
    "all_cols = df.columns.tolist()\n",
    "assert 'smiles' in all_cols, \"CSV must contain a 'smiles' column.\"\n",
    "\n",
    "label_cols = all_cols[:12]\n",
    "meta_cols = all_cols[12:]\n",
    "print(\"Label columns:\", label_cols)\n",
    "print(\"Meta columns:\", meta_cols)\n",
    "\n",
    "# --- Clean labels: coerce to numeric, replace inf with NaN, then fill NaN with 0 and cast to int ---\n",
    "for c in label_cols:\n",
    "    df[c] = pd.to_numeric(df[c], errors='coerce')  # convert strings like 'nan' to NaN\n",
    "# replace +/-inf -> NaN\n",
    "df[label_cols] = df[label_cols].replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# report NaNs before filling\n",
    "nan_counts = df[label_cols].isna().sum()\n",
    "if nan_counts.sum() > 0:\n",
    "    print(\"NaNs detected in label columns (filling with 0):\")\n",
    "    print(nan_counts[nan_counts > 0])\n",
    "\n",
    "# fill NaNs with 0 (common practice for Tox21 preprocessing); clip to [0,1] and cast to int\n",
    "df[label_cols] = df[label_cols].fillna(0.0).clip(lower=0.0, upper=1.0).astype(int)\n",
    "\n",
    "# sanity check\n",
    "unique_vals = {c: sorted(df[c].unique().tolist()) for c in label_cols}\n",
    "print(\"Unique values per label (should be [0,1]):\")\n",
    "print(unique_vals)\n",
    "\n",
    "Y = df[label_cols].values.astype(np.float32)\n",
    "len(df), Y.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e9950",
   "metadata": {},
   "source": [
    "## creating our data folds, similar to the LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c22fea73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded splits: train=5481, val=1175, test=1175\n"
     ]
    }
   ],
   "source": [
    "train_idx_path = SPLITS_DIR / \"train_idx.npy\"\n",
    "val_idx_path   = SPLITS_DIR / \"val_idx.npy\"\n",
    "test_idx_path  = SPLITS_DIR / \"test_idx.npy\"\n",
    "\n",
    "if train_idx_path.exists():\n",
    "    train_idx = np.load(train_idx_path)\n",
    "    val_idx = np.load(val_idx_path)\n",
    "    test_idx = np.load(test_idx_path)\n",
    "    print(f\"Loaded splits: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n",
    "else:\n",
    "    n = len(df)\n",
    "    test_size = 0.15\n",
    "    val_size = 0.15\n",
    "\n",
    "    indices = np.arange(n)\n",
    "    if MultilabelStratifiedShuffleSplit is not None:\n",
    "        # Split off test\n",
    "        msss = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=test_size, random_state=SEED)\n",
    "        train_val_idx, test_idx = next(msss.split(indices, Y))\n",
    "        # Split train/val\n",
    "        rel_val = val_size / (1.0 - test_size)\n",
    "        msss2 = MultilabelStratifiedShuffleSplit(n_splits=1, test_size=rel_val, random_state=SEED)\n",
    "        train_idx, val_idx = next(msss2.split(train_val_idx, Y[train_val_idx]))\n",
    "    else:\n",
    "        rng = np.random.default_rng(SEED)\n",
    "        rng.shuffle(indices)\n",
    "        tcount = int(test_size * n)\n",
    "        vcount = int(val_size * n)\n",
    "        test_idx = indices[:tcount]\n",
    "        val_idx  = indices[tcount:tcount+vcount]\n",
    "        train_idx = indices[tcount+vcount:]\n",
    "\n",
    "    np.save(train_idx_path, train_idx); np.save(val_idx_path, val_idx); np.save(test_idx_path, test_idx)\n",
    "    print(f\"Saved new splits: train={len(train_idx)}, val={len(val_idx)}, test={len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba33487",
   "metadata": {},
   "source": [
    "## tokenizer and dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07f4ba1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Config (you can tweak) ===\n",
    "MODEL_NAME   = \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "MAX_LEN      = 256          # 128 is faster; bump to 256 if you see truncation issues\n",
    "AUG_PROB     = 0.25         # train-time SMILES randomization probability\n",
    "CANONICALIZE = True         # canonicalize base SMILES once on dataset build\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    padding_side=\"right\",\n",
    "    truncation_side=\"right\",\n",
    "    use_fast=True\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "\n",
    "# --- RDKit helpers (optional but recommended) ---\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "except Exception as e:\n",
    "    Chem = None\n",
    "    print(\"RDKit not available; SMILES validation/augmentation will be skipped.\")\n",
    "\n",
    "def canonicalize_smiles(s: str) -> str:\n",
    "    if Chem is None:\n",
    "        return s\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    # Canonical SMILES with stereochemistry kept\n",
    "    return Chem.MolToSmiles(mol, canonical=True)\n",
    "\n",
    "def randomize_smiles(s: str) -> str:\n",
    "    \"\"\"Return a randomized (but equivalent) SMILES; fallback to original if RDKit missing/fails.\"\"\"\n",
    "    if Chem is None:\n",
    "        return s\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None:\n",
    "        return s\n",
    "    # doRandom=True creates a randomized traversal (augmentation)\n",
    "    return Chem.MolToSmiles(mol, canonical=False, doRandom=True)\n",
    "\n",
    "class SmilesDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dynamic-padding ready: we DO NOT pad in __getitem__; the DataCollator will pad per batch.\n",
    "    Augmentation (randomized SMILES) occurs only in train mode with probability AUG_PROB.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, indices, label_cols, tokenizer, max_len=128,\n",
    "                 mode=\"train\", augment_prob=0.25, do_canonicalize=True):\n",
    "        self.df = df.iloc[indices].reset_index(drop=True).copy()\n",
    "        smiles = self.df[\"smiles\"].astype(str).tolist()\n",
    "\n",
    "        # Validate/canonicalize once up-front (drop invalids)\n",
    "        if do_canonicalize and Chem is not None:\n",
    "            keep = []\n",
    "            canon = []\n",
    "            for s in smiles:\n",
    "                cs = canonicalize_smiles(s)\n",
    "                if cs is None:\n",
    "                    keep.append(False)\n",
    "                    canon.append(s)\n",
    "                else:\n",
    "                    keep.append(True)\n",
    "                    canon.append(cs)\n",
    "            if not all(keep):\n",
    "                dropped = int(np.sum(np.logical_not(keep)))\n",
    "                print(f\"[SmilesDataset] Dropping {dropped} invalid SMILES.\")\n",
    "                self.df = self.df.loc[np.where(keep)[0]].reset_index(drop=True)\n",
    "                smiles = [canon[i] for i in np.where(keep)[0]]\n",
    "            else:\n",
    "                smiles = canon\n",
    "\n",
    "        self.smiles = smiles\n",
    "        self.labels = self.df[label_cols].values.astype(np.float32)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "        self.mode = mode\n",
    "        self.augment_prob = augment_prob if mode == \"train\" else 0.0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.smiles)\n",
    "\n",
    "    def _maybe_augment(self, s: str) -> str:\n",
    "        if self.augment_prob > 0 and np.random.rand() < self.augment_prob:\n",
    "            rs = randomize_smiles(s)\n",
    "            return rs if isinstance(rs, str) and len(rs) > 0 else s\n",
    "        return s\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.smiles[idx]\n",
    "        s = self._maybe_augment(s)\n",
    "\n",
    "        # NOTE: no padding here; let the collator handle it per-batch.\n",
    "        enc = self.tokenizer(\n",
    "            s,\n",
    "            padding=False,\n",
    "            truncation=True,\n",
    "            max_length=self.max_len,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        item = {k: v.squeeze(0) for k, v in enc.items()}\n",
    "        item[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.float32)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b078463",
   "metadata": {},
   "source": [
    "## dataloaders with class imbalance weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b907526",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:06:31] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaders OK — Train/Val/Test sizes: 5481 1175 1175\n",
      "pos_weight (rounded): [24.26 31.43  9.36 26.82  9.04 23.47 45.85  7.33 28.47 20.75  7.88 17.71]\n",
      "Weighted sampler enabled. Example weights stats: min=0.30  max=13.71  mean=1.00\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "from transformers import DataCollatorWithPadding\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# --- Config ---\n",
    "BATCH_SIZE   = 64          # increased from 32\n",
    "NUM_WORKERS  = 0           # safe on Windows\n",
    "PIN_MEMORY   = False\n",
    "USE_WEIGHTED_SAMPLER = True   # set False if you see overfitting/instability\n",
    "\n",
    "# --- Data collator: dynamic padding to the longest in the batch (amp-friendly) ---\n",
    "collator = DataCollatorWithPadding(tokenizer, pad_to_multiple_of=8)\n",
    "\n",
    "# --- Build datasets (augmentation only on train) ---\n",
    "train_ds = SmilesDataset(\n",
    "    df, train_idx, label_cols, tokenizer, max_len=MAX_LEN,\n",
    "    mode=\"train\", augment_prob=0.25, do_canonicalize=True\n",
    ")\n",
    "val_ds = SmilesDataset(\n",
    "    df, val_idx, label_cols, tokenizer, max_len=MAX_LEN,\n",
    "    mode=\"val\", augment_prob=0.0, do_canonicalize=True\n",
    ")\n",
    "test_ds = SmilesDataset(\n",
    "    df, test_idx, label_cols, tokenizer, max_len=MAX_LEN,\n",
    "    mode=\"test\", augment_prob=0.0, do_canonicalize=True\n",
    ")\n",
    "\n",
    "# --- Class imbalance handling in the LOSS: pos_weight = (#neg / #pos) on TRAIN ONLY ---\n",
    "y_train = train_ds.labels  # (N_train, C)\n",
    "pos_counts = y_train.sum(axis=0)\n",
    "neg_counts = y_train.shape[0] - pos_counts\n",
    "pos_counts_safe = np.clip(pos_counts, 1, None)\n",
    "pos_weight = torch.tensor(neg_counts / pos_counts_safe, dtype=torch.float32, device=device)\n",
    "\n",
    "# --- OPTIONAL: imbalance-aware sampler (multi-label) ---\n",
    "# Per-sample weight = sum_c inv_freq[c] * y_ic; if no positives, give a small baseline weight\n",
    "if USE_WEIGHTED_SAMPLER:\n",
    "    N = y_train.shape[0]\n",
    "    pos_freq = pos_counts / N\n",
    "    inv_freq = 1.0 / np.clip(pos_freq, 1e-8, None)      # inverse prevalence\n",
    "    inv_freq = inv_freq / inv_freq.mean()                # normalize (mean ~1)\n",
    "\n",
    "    sample_weights = (y_train * inv_freq).sum(axis=1)    # higher if sample has rare positives\n",
    "    # Give some weight to all-negative rows so we still sample negatives:\n",
    "    sample_weights = np.where(sample_weights > 0, sample_weights, 0.2)\n",
    "    # Normalize for stability (not strictly necessary):\n",
    "    sample_weights = sample_weights / sample_weights.mean()\n",
    "\n",
    "    sampler = WeightedRandomSampler(\n",
    "        weights=torch.tensor(sample_weights, dtype=torch.double),\n",
    "        num_samples=len(train_ds),\n",
    "        replacement=True\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, sampler=sampler,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collator\n",
    "    )\n",
    "else:\n",
    "    train_loader = DataLoader(\n",
    "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collator\n",
    "    )\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collator\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collator\n",
    ")\n",
    "\n",
    "print(\"Loaders OK — Train/Val/Test sizes:\", len(train_ds), len(val_ds), len(test_ds))\n",
    "print(\"pos_weight (rounded):\", pos_weight.detach().cpu().numpy().round(2))\n",
    "if USE_WEIGHTED_SAMPLER:\n",
    "    print(\"Weighted sampler enabled. Example weights stats:\",\n",
    "          f\"min={sample_weights.min():.2f}  max={sample_weights.max():.2f}  mean={sample_weights.mean():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ad1a40",
   "metadata": {},
   "source": [
    "## ChemBERTa Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3c8166d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at seyonec/ChemBERTa-zinc-base-v1 and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params: 44,113,164 | Trainable: 44,113,164\n",
      "Dropout — hidden: 0.15  attn: 0.15\n",
      "Gradient checkpointing: enabled\n",
      "First 0 encoder layer(s) frozen.\n"
     ]
    }
   ],
   "source": [
    "# MODEL BUILD — eager mode (no torch.compile), safer on Windows\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "\n",
    "num_labels = len(label_cols)\n",
    "\n",
    "# Config with slightly higher dropout (regularisation)\n",
    "config = AutoConfig.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    num_labels=num_labels,\n",
    "    problem_type=\"multi_label_classification\",\n",
    ")\n",
    "if hasattr(config, \"hidden_dropout_prob\"):\n",
    "    config.hidden_dropout_prob = 0.15\n",
    "if hasattr(config, \"attention_probs_dropout_prob\"):\n",
    "    config.attention_probs_dropout_prob = 0.15\n",
    "\n",
    "# Build model (no torch.compile)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    config=config,\n",
    ").to(device)\n",
    "\n",
    "# Training-time flags\n",
    "if hasattr(model.config, \"use_cache\"):\n",
    "    model.config.use_cache = False\n",
    "if hasattr(model, \"gradient_checkpointing_enable\"):\n",
    "    model.gradient_checkpointing_enable()  # reduces VRAM, OK to keep\n",
    "\n",
    "# Re-init classification head (better than default)\n",
    "if hasattr(model, \"classifier\"):\n",
    "    if hasattr(model.classifier, \"dense\"):\n",
    "        torch.nn.init.xavier_uniform_(model.classifier.dense.weight)\n",
    "        if model.classifier.dense.bias is not None:\n",
    "            torch.nn.init.zeros_(model.classifier.dense.bias)\n",
    "    if hasattr(model.classifier, \"out_proj\"):\n",
    "        torch.nn.init.xavier_uniform_(model.classifier.out_proj.weight)\n",
    "        if model.classifier.out_proj.bias is not None:\n",
    "            torch.nn.init.zeros_(model.classifier.out_proj.bias)\n",
    "\n",
    "# (Optional) freeze first N encoder layers for warm-up\n",
    "FREEZE_N_LAYERS = 0  # set to 2–4 if you want\n",
    "if FREEZE_N_LAYERS > 0 and hasattr(model, \"roberta\"):\n",
    "    for i, layer in enumerate(model.roberta.encoder.layer):\n",
    "        if i < FREEZE_N_LAYERS:\n",
    "            for p in layer.parameters():\n",
    "                p.requires_grad = False\n",
    "\n",
    "# Report\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "drop_hidden = getattr(model.config, \"hidden_dropout_prob\", None)\n",
    "drop_attn = getattr(model.config, \"attention_probs_dropout_prob\", None)\n",
    "print(f\"Total params: {total_params:,} | Trainable: {trainable_params:,}\")\n",
    "print(f\"Dropout — hidden: {drop_hidden}  attn: {drop_attn}\")\n",
    "print(\"Gradient checkpointing:\", \"on\" if hasattr(model, \"gradient_checkpointing\") else \"enabled\")\n",
    "print(\"First\", FREEZE_N_LAYERS, \"encoder layer(s) frozen.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e00610",
   "metadata": {},
   "source": [
    "## Training Setup — ASL + LLRD + EMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7fdd424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ASL] Using Asymmetric Loss (gamma_pos=0.5, gamma_neg=3.0, clip=0.05) + clamped pos_weight.\n",
      "Total params: 44,113,164 | Trainable: 599,820\n",
      "Dropout — hidden: 0.15  attn: 0.15\n",
      "[LLRD] max_layer=6 | decay=0.85 | HEAD_LR=3.0e-04 | BASE_LR=1.5e-05\n",
      "[EMA]  decay=0.999\n",
      "Training steps: 1720, warmup steps: 172\n",
      "AMP: on | GradAccum: 2 | GradClip: 1.0\n",
      "Backbone warmup epochs: 1 (classifier-only training if >0)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Training setup: ASL + LLRD + EMA\n",
    "# (Place this cell RIGHT AFTER your model is built and moved to `device`)\n",
    "# =========================\n",
    "\n",
    "import math, numpy as np, torch\n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n",
    "\n",
    "# ---------- Tuned hyperparameters (safe defaults) ----------\n",
    "EPOCHS                 = 40\n",
    "BASE_LR                = 1.5e-5      # backbone base lr (decayed by depth)\n",
    "HEAD_LR                = 3e-4        # classifier head lr\n",
    "WEIGHT_DECAY           = 0.01\n",
    "WARMUP_RATIO           = 0.10        # 10% warmup\n",
    "PATIENCE               = 8\n",
    "GRAD_CLIP_NORM         = 1.0\n",
    "USE_COSINE_SCHED       = True\n",
    "GRAD_ACCUM_STEPS       = 2\n",
    "USE_AMP                = True\n",
    "BACKBONE_WARMUP_EPOCHS = 1\n",
    "LLRD_DECAY             = 0.85        # layer-wise lr decay per depth step\n",
    "EMA_DECAY              = 0.999\n",
    "\n",
    "# ---------- Loss: Asymmetric Loss (ASL) with clamped pos_weight ----------\n",
    "class AsymmetricLossMultiLabel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    ASL for multilabel classification with optional pos_weight.\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma_pos=0.5, gamma_neg=3.0, clip=0.05, eps=1e-8, reduction='mean', pos_weight=None):\n",
    "        super().__init__()\n",
    "        self.gamma_pos  = gamma_pos\n",
    "        self.gamma_neg  = gamma_neg\n",
    "        self.clip       = clip\n",
    "        self.eps        = eps\n",
    "        self.reduction  = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits, targets: (B, C)\n",
    "        xs_pos = torch.sigmoid(logits)\n",
    "        xs_neg = 1.0 - xs_pos\n",
    "\n",
    "        # optional clipping for negatives\n",
    "        if self.clip is not None and self.clip > 0:\n",
    "            xs_neg = torch.clamp(xs_neg + self.clip, max=1)\n",
    "\n",
    "        log_pos = torch.log(xs_pos.clamp(min=self.eps))\n",
    "        log_neg = torch.log(xs_neg.clamp(min=self.eps))\n",
    "\n",
    "        loss_pos = targets * log_pos\n",
    "        loss_neg = (1 - targets) * log_neg\n",
    "\n",
    "        # focal modulation\n",
    "        if self.gamma_pos > 0 or self.gamma_neg > 0:\n",
    "            pt_pos = xs_pos * targets + (1 - targets)\n",
    "            pt_neg = xs_neg * (1 - targets) + targets\n",
    "            loss_pos *= (1 - pt_pos) ** self.gamma_pos\n",
    "            loss_neg *= (1 - pt_neg) ** self.gamma_neg\n",
    "\n",
    "        loss = -(loss_pos + loss_neg)\n",
    "\n",
    "        # optional per-class weighting\n",
    "        if self.pos_weight is not None:\n",
    "            loss = loss * (targets * (self.pos_weight - 1) + 1)\n",
    "\n",
    "        if self.reduction == 'mean': return loss.mean()\n",
    "        if self.reduction == 'sum':  return loss.sum()\n",
    "        return loss\n",
    "\n",
    "# Compute pos_weight from TRAIN ONLY and clamp extremes for stability\n",
    "y_train = train_ds.labels  # (N_train, C) — from your earlier dataset construction\n",
    "pos_counts = y_train.sum(axis=0)\n",
    "neg_counts = y_train.shape[0] - pos_counts\n",
    "pos_counts_safe = np.clip(pos_counts, 1, None)\n",
    "\n",
    "pos_weight = torch.tensor(neg_counts / pos_counts_safe, dtype=torch.float32, device=device)\n",
    "pos_weight = torch.clamp(pos_weight, 1.0, 50.0)  # clamp to avoid huge gradients\n",
    "\n",
    "asl_loss = AsymmetricLossMultiLabel(\n",
    "    gamma_pos=0.5, gamma_neg=3.0, clip=0.05, reduction='mean',\n",
    "    pos_weight=pos_weight.to(device)\n",
    ")\n",
    "print(f\"[ASL] Using Asymmetric Loss (gamma_pos=0.5, gamma_neg=3.0, clip=0.05) + clamped pos_weight.\")\n",
    "\n",
    "# ---------- LLRD: Layer-wise LR Decay parameter groups ----------\n",
    "decay_exclusions = (\"bias\", \"LayerNorm.weight\", \"LayerNorm.bias\")\n",
    "def is_decay_param(n): return not any(nd in n for nd in decay_exclusions)\n",
    "\n",
    "# Map parameter name -> layer id (embeddings=0, encoder.layers=1..L, classifier=1000)\n",
    "def get_layer_id(name: str):\n",
    "    if name.startswith(\"classifier\") or name.startswith(\"lm_head\"):\n",
    "        return 1000\n",
    "    if name.startswith(\"roberta.embeddings\"):\n",
    "        return 0\n",
    "    if name.startswith(\"roberta.encoder.layer.\"):\n",
    "        try:\n",
    "            idx = int(name.split(\"roberta.encoder.layer.\")[1].split(\".\")[0])\n",
    "            return idx + 1  # 1..L\n",
    "        except Exception:\n",
    "            return 0\n",
    "    # fallback for other backbones (bert, etc.)\n",
    "    if name.startswith(\"bert.embeddings\"): return 0\n",
    "    if name.startswith(\"bert.encoder.layer.\"):\n",
    "        try:\n",
    "            idx = int(name.split(\"bert.encoder.layer.\")[1].split(\".\")[0])\n",
    "            return idx + 1\n",
    "        except Exception:\n",
    "            return 0\n",
    "    return 0\n",
    "\n",
    "# Determine max encoder layer for decay computation\n",
    "max_layer = 0\n",
    "for n, p in model.named_parameters():\n",
    "    if not p.requires_grad: continue\n",
    "    lid = get_layer_id(n)\n",
    "    if lid != 1000:\n",
    "        max_layer = max(max_layer, lid)\n",
    "\n",
    "# Build param groups\n",
    "optimizer_grouped_parameters = []\n",
    "for n, p in model.named_parameters():\n",
    "    if not p.requires_grad:\n",
    "        continue\n",
    "    if n.startswith(\"classifier\") or n.startswith(\"lm_head\"):\n",
    "        lr = HEAD_LR\n",
    "        wd = WEIGHT_DECAY if is_decay_param(n) else 0.0\n",
    "        optimizer_grouped_parameters.append({\"params\": [p], \"lr\": lr, \"weight_decay\": wd})\n",
    "    else:\n",
    "        lid = get_layer_id(n)  # 0..max_layer\n",
    "        lr = BASE_LR * (LLRD_DECAY ** (max_layer - lid))\n",
    "        wd = WEIGHT_DECAY if is_decay_param(n) else 0.0\n",
    "        optimizer_grouped_parameters.append({\"params\": [p], \"lr\": lr, \"weight_decay\": wd})\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters)\n",
    "\n",
    "# Optional 1-epoch head-only warmup\n",
    "def set_backbone_trainable(trainable: bool):\n",
    "    for n, p in model.named_parameters():\n",
    "        if n.startswith(\"classifier\") or n.startswith(\"lm_head\"):\n",
    "            continue  # head always trainable\n",
    "        p.requires_grad = trainable\n",
    "\n",
    "set_backbone_trainable(BACKBONE_WARMUP_EPOCHS <= 0)\n",
    "\n",
    "# ---------- Scheduler, AMP scaler ----------\n",
    "num_update_steps_per_epoch = math.ceil(len(train_loader) / max(1, GRAD_ACCUM_STEPS))\n",
    "num_training_steps = EPOCHS * num_update_steps_per_epoch\n",
    "num_warmup_steps   = int(WARMUP_RATIO * num_training_steps)\n",
    "\n",
    "scheduler = (\n",
    "    get_cosine_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    "    if USE_COSINE_SCHED else\n",
    "    get_linear_schedule_with_warmup(optimizer, num_warmup_steps, num_training_steps)\n",
    ")\n",
    "\n",
    "scaler = torch.amp.GradScaler('cuda', enabled=USE_AMP)\n",
    "\n",
    "# ---------- EMA (Exponential Moving Average) ----------\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        for name, param in model.state_dict().items():\n",
    "            if param.dtype.is_floating_point:\n",
    "                self.shadow[name] = param.detach().clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model):\n",
    "        for name, param in model.state_dict().items():\n",
    "            if name in self.shadow and param.dtype.is_floating_point:\n",
    "                self.shadow[name].mul_(self.decay).add_(param.detach(), alpha=(1.0 - self.decay))\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def apply_shadow(self, model):\n",
    "        self.backup = {}\n",
    "        for name, param in model.state_dict().items():\n",
    "            if name in self.shadow and param.dtype.is_floating_point:\n",
    "                self.backup[name] = param.detach().clone()\n",
    "                param.data.copy_(self.shadow[name].data)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def restore(self, model):\n",
    "        for name, param in model.state_dict().items():\n",
    "            if hasattr(self, \"backup\") and name in self.backup:\n",
    "                param.data.copy_(self.backup[name].data)\n",
    "        self.backup = {}\n",
    "\n",
    "ema = EMA(model, decay=EMA_DECAY)\n",
    "\n",
    "# ---------- Report ----------\n",
    "total_params    = sum(p.numel() for p in model.parameters())\n",
    "trainable_params= sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "drop_hidden = getattr(model.config, \"hidden_dropout_prob\", None)\n",
    "drop_attn   = getattr(model.config, \"attention_probs_dropout_prob\", None)\n",
    "\n",
    "print(f\"Total params: {total_params:,} | Trainable: {trainable_params:,}\")\n",
    "print(f\"Dropout — hidden: {drop_hidden}  attn: {drop_attn}\")\n",
    "print(f\"[LLRD] max_layer={max_layer} | decay={LLRD_DECAY:.2f} | HEAD_LR={HEAD_LR:.1e} | BASE_LR={BASE_LR:.1e}\")\n",
    "print(f\"[EMA]  decay={EMA_DECAY}\")\n",
    "print(f\"Training steps: {num_training_steps}, warmup steps: {num_warmup_steps}\")\n",
    "print(f\"AMP: {'on' if USE_AMP else 'off'} | GradAccum: {GRAD_ACCUM_STEPS} | GradClip: {GRAD_CLIP_NORM}\")\n",
    "print(f\"Backbone warmup epochs: {BACKBONE_WARMUP_EPOCHS} (classifier-only training if >0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15460ed1",
   "metadata": {},
   "source": [
    "## metrics (macro AUROC/AP, macro F1 at thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d60b00c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step    50/   86 | loss 0.3773 | 2.4s\n",
      "Epoch 01 | train_loss 0.3287 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 4.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step    50/   86 | loss 0.2616 | 2.1s\n",
      "Epoch 02 | train_loss 0.3323 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 9.7s\n",
      " step    50/   86 | loss 0.3610 | 9.1s\n",
      "Epoch 03 | train_loss 0.3305 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 27.0s\n",
      " step    50/   86 | loss 0.3154 | 7.0s\n",
      "Epoch 04 | train_loss 0.3189 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 14.6s\n",
      " step    50/   86 | loss 0.3705 | 8.4s\n",
      "Epoch 05 | train_loss 0.3252 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 16.6s\n",
      " step    50/   86 | loss 0.2567 | 9.8s\n",
      "Epoch 06 | train_loss 0.3231 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 751.5s\n",
      " step    50/   86 | loss 0.3116 | 9.3s\n",
      "Epoch 07 | train_loss 0.3257 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 18.2s\n",
      " step    50/   86 | loss 0.2643 | 10.2s\n",
      "Epoch 08 | train_loss 0.3190 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 19.5s\n",
      " step    50/   86 | loss 0.3472 | 8.7s\n",
      "Epoch 09 | train_loss 0.3306 | val_loss 0.7409 | VAL(EMA) macro_AUROC 0.7742 | macro_AP 0.3450 | time 17.9s\n",
      "Early stopping (macro-AP) after 9 epochs.\n",
      "\n",
      "==== SUMMARY ====\n",
      "VAL  macro_AUROC: 0.7742 | macro_AP: 0.3450 | macro_F1@thr: 0.3764\n",
      "TEST macro_AUROC: 0.8540 | macro_AP: 0.4571 | macro_F1@thr: 0.4684\n",
      "✅ Saved predictions and summary to tox21_chembera_pipeline\\outputs\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Metrics + Train/Validate loop (macro-AP early stopping)\n",
    "# =========================\n",
    "import time, numpy as np, torch\n",
    "from copy import deepcopy\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# --- Metrics (your existing function; unchanged) ---\n",
    "def multilabel_metrics(y_true, y_prob, thresholds=None):\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    N, C = y_true.shape\n",
    "    per_auc, per_ap = [], []\n",
    "    for c in range(C):\n",
    "        yt = y_true[:, c]; yp = y_prob[:, c]\n",
    "        if len(np.unique(yt)) < 2:\n",
    "            per_auc.append(np.nan); per_ap.append(np.nan); continue\n",
    "        try:\n",
    "            from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "            per_auc.append(roc_auc_score(yt, yp))\n",
    "        except ValueError: per_auc.append(np.nan)\n",
    "        try:\n",
    "            per_ap.append(average_precision_score(yt, yp))\n",
    "        except ValueError: per_ap.append(np.nan)\n",
    "    macro_auc = np.nanmean(per_auc); macro_ap = np.nanmean(per_ap)\n",
    "\n",
    "    micro_auc = np.nan; micro_ap = np.nan\n",
    "    if np.unique(y_true).size > 1:\n",
    "        try:\n",
    "            from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "            micro_auc = roc_auc_score(y_true.ravel(), y_prob.ravel())\n",
    "            micro_ap  = average_precision_score(y_true.ravel(), y_prob.ravel())\n",
    "        except Exception: pass\n",
    "\n",
    "    macro_f1 = np.nan\n",
    "    if thresholds is not None:\n",
    "        thr = np.asarray(thresholds)\n",
    "        y_pred = (y_prob >= thr).astype(int)\n",
    "        per_f1 = []\n",
    "        for c in range(y_true.shape[1]):\n",
    "            yt = y_true[:, c]; yp = y_pred[:, c]\n",
    "            if len(np.unique(yt)) < 2:\n",
    "                per_f1.append(np.nan); continue\n",
    "            per_f1.append(f1_score(yt, yp, zero_division=0))\n",
    "        macro_f1 = np.nanmean(per_f1)\n",
    "\n",
    "    pos_counts = y_true.sum(axis=0)\n",
    "    prevalence = (pos_counts / N).astype(float)\n",
    "\n",
    "    return {\n",
    "        \"per_auc\": per_auc, \"per_ap\": per_ap,\n",
    "        \"macro_auc\": macro_auc, \"macro_ap\": macro_ap,\n",
    "        \"micro_auc\": micro_auc, \"micro_ap\": micro_ap,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"label_stats\": {\"pos_counts\": pos_counts, \"prevalence\": prevalence}\n",
    "    }\n",
    "\n",
    "# --- Utilities ---\n",
    "def sigmoid_np(x): return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_tta(model, base_ds, n_tta=8, p_aug=0.8, batch_size=64):\n",
    "    \"\"\"Average probs across randomized SMILES. Flip to 'train' mode for augmentation path in dataset.\"\"\"\n",
    "    from copy import deepcopy\n",
    "    ds = deepcopy(base_ds)\n",
    "    ds.mode = \"train\"; ds.augment_prob = p_aug\n",
    "    loader = DataLoader(ds, batch_size=batch_size, shuffle=False,\n",
    "                        num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collator)\n",
    "    all_probs = []\n",
    "    model.eval()\n",
    "    for _ in range(n_tta):\n",
    "        _, logits, _, _ = run_epoch(loader, train=False, log_every=10_000)\n",
    "        all_probs.append(sigmoid_np(logits))\n",
    "    return np.mean(all_probs, axis=0)\n",
    "\n",
    "def calibrate_thresholds(y_true, y_prob, grid=np.linspace(0.05, 0.95, 19)):\n",
    "    \"\"\"Choose per-label threshold by max F1 on VAL.\"\"\"\n",
    "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob)\n",
    "    C = y_true.shape[1]; thr = np.zeros(C, dtype=float)\n",
    "    for c in range(C):\n",
    "        yt = y_true[:, c]; yp = y_prob[:, c]\n",
    "        if len(np.unique(yt)) < 2: thr[c] = 0.5; continue\n",
    "        best, best_f1 = 0.5, -1\n",
    "        for t in grid:\n",
    "            f1 = f1_score(yt, (yp >= t).astype(int), zero_division=0)\n",
    "            if f1 > best_f1: best, best_f1 = t, f1\n",
    "        thr[c] = best\n",
    "    return thr\n",
    "\n",
    "# --- Core epoch runner (uses your globals: model, asl_loss, optimizer, scheduler, scaler, ema, GRAD_* etc.) ---\n",
    "def run_epoch(dataloader, train=False, log_every=50):\n",
    "    model.train() if train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    logits_list, labels_list = [], []\n",
    "    accum = max(1, GRAD_ACCUM_STEPS)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    step_in_accum = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for step, batch in enumerate(dataloader, 1):\n",
    "        input_ids = batch[\"input_ids\"].to(device, non_blocking=True)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device, non_blocking=True)\n",
    "        labels = batch[\"labels\"].to(device, non_blocking=True)\n",
    "\n",
    "        if train and USE_AMP:\n",
    "            with torch.amp.autocast('cuda'):\n",
    "                logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "                loss   = asl_loss(logits, labels) / accum\n",
    "        else:\n",
    "            logits = model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "            loss   = asl_loss(logits, labels) / accum\n",
    "\n",
    "        if train:\n",
    "            if USE_AMP: scaler.scale(loss).backward()\n",
    "            else:       loss.backward()\n",
    "            step_in_accum += 1\n",
    "\n",
    "            if step_in_accum >= accum:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP_NORM)\n",
    "                if USE_AMP:\n",
    "                    scaler.step(optimizer); scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                ema.update(model)          # EMA after each step\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                step_in_accum = 0\n",
    "\n",
    "        total_loss += (loss.item() * accum) * input_ids.size(0)\n",
    "        logits_list.append(logits.detach().cpu().numpy())\n",
    "        labels_list.append(labels.detach().cpu().numpy())\n",
    "\n",
    "        if train and (step % log_every == 0):\n",
    "            elapsed = time.time() - t0\n",
    "            print(f\" step {step:5d}/{len(dataloader):5d} | loss {loss.item()*accum:.4f} | {elapsed:.1f}s\")\n",
    "            t0 = time.time()\n",
    "\n",
    "    avg_loss   = total_loss / len(dataloader.dataset)\n",
    "    logits_all = np.concatenate(logits_list, axis=0)\n",
    "    labels_all = np.concatenate(labels_list, axis=0)\n",
    "    probs_all  = sigmoid_np(logits_all)\n",
    "    return avg_loss, logits_all, probs_all, labels_all\n",
    "\n",
    "# --- Train/validate with early stopping on macro-AP (EMA weights on VAL) ---\n",
    "best_state_ema = None\n",
    "best_val_ap    = -np.inf\n",
    "no_improve     = 0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    t_epoch = time.time()\n",
    "\n",
    "    # optional head-only warmup\n",
    "    if epoch == 1 and BACKBONE_WARMUP_EPOCHS > 0:\n",
    "        set_backbone_trainable(False)\n",
    "\n",
    "    tr_loss, _, _, _ = run_epoch(train_loader, train=True, log_every=50)\n",
    "\n",
    "    # unfreeze after warmup\n",
    "    if BACKBONE_WARMUP_EPOCHS > 0 and epoch == BACKBONE_WARMUP_EPOCHS + 1:\n",
    "        set_backbone_trainable(True)\n",
    "\n",
    "    # Validate with EMA shadow\n",
    "    ema.apply_shadow(model)\n",
    "    va_loss, va_logits, va_prob, va_y = run_epoch(val_loader, train=False, log_every=10_000)\n",
    "    ema.restore(model)\n",
    "\n",
    "    va_metrics = multilabel_metrics(va_y, va_prob)\n",
    "    print(f\"Epoch {epoch:02d} | train_loss {tr_loss:.4f} | val_loss {va_loss:.4f} | \"\n",
    "          f\"VAL(EMA) macro_AUROC {va_metrics['macro_auc']:.4f} | macro_AP {va_metrics['macro_ap']:.4f} | \"\n",
    "          f\"time {time.time()-t_epoch:.1f}s\", flush=True)\n",
    "\n",
    "    # Early stopping by macro-AP\n",
    "    if va_metrics['macro_ap'] > best_val_ap + 1e-4:\n",
    "        best_val_ap = va_metrics['macro_ap']\n",
    "        ema.apply_shadow(model)\n",
    "        best_state_ema = deepcopy(model.state_dict())\n",
    "        ema.restore(model)\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve >= PATIENCE:\n",
    "        print(f\"Early stopping (macro-AP) after {epoch} epochs.\")\n",
    "        break\n",
    "\n",
    "# Restore best EMA checkpoint\n",
    "if best_state_ema is not None:\n",
    "    model.load_state_dict(best_state_ema)\n",
    "\n",
    "# --- Threshold calibration on VAL (optional, for reporting macro-F1) ---\n",
    "val_thr = calibrate_thresholds(va_y, va_prob, grid=np.linspace(0.05, 0.95, 19))\n",
    "val_metrics_thr = multilabel_metrics(va_y, va_prob, thresholds=val_thr)\n",
    "\n",
    "# --- FINAL TEST EVAL (with TTA for better ranking metrics) ---\n",
    "# Note: requires test_ds/test_loader from your earlier setup\n",
    "te_prob = predict_with_tta(model, test_ds, n_tta=8, p_aug=0.8, batch_size=BATCH_SIZE)\n",
    "te_y    = test_ds.labels\n",
    "test_metrics      = multilabel_metrics(te_y, te_prob)\n",
    "test_metrics_thr  = multilabel_metrics(te_y, te_prob, thresholds=val_thr)\n",
    "\n",
    "print(\"\\n==== SUMMARY ====\")\n",
    "print(f\"VAL  macro_AUROC: {va_metrics['macro_auc']:.4f} | macro_AP: {va_metrics['macro_ap']:.4f} | macro_F1@thr: {val_metrics_thr['macro_f1']:.4f}\")\n",
    "print(f\"TEST macro_AUROC: {test_metrics['macro_auc']:.4f} | macro_AP: {test_metrics['macro_ap']:.4f} | macro_F1@thr: {test_metrics_thr['macro_f1']:.4f}\")\n",
    "\n",
    "# --- Optional: build a per-label table like yours and save JSON summary ---\n",
    "per_label = []\n",
    "for i, lab in enumerate(label_cols):\n",
    "    va_auc = va_metrics[\"per_auc\"][i]; va_ap = va_metrics[\"per_ap\"][i]\n",
    "    te_auc = test_metrics[\"per_auc\"][i]; te_ap = test_metrics[\"per_ap\"][i]\n",
    "    per_label.append({\n",
    "        \"label\": lab,\n",
    "        \"val_AUROC\": None if np.isnan(va_auc) else round(va_auc, 4),\n",
    "        \"val_AP\":    None if np.isnan(va_ap)  else round(va_ap,  4),\n",
    "        \"test_AUROC\":None if np.isnan(te_auc) else round(te_auc, 4),\n",
    "        \"test_AP\":   None if np.isnan(te_ap)  else round(te_ap,  4),\n",
    "    })\n",
    "\n",
    "summary = {\n",
    "    \"VAL\":  {\"macro_AUROC\": float(va_metrics['macro_auc']),  \"macro_AP\": float(va_metrics['macro_ap']),  \"macro_F1_thr\": float(val_metrics_thr['macro_f1'])},\n",
    "    \"TEST\": {\"macro_AUROC\": float(test_metrics['macro_auc']),\"macro_AP\": float(test_metrics['macro_ap']),\"macro_F1_thr\": float(test_metrics_thr['macro_f1'])},\n",
    "    \"thresholds\": {label_cols[i]: float(val_thr[i]) for i in range(len(label_cols))},\n",
    "    \"per_label\": per_label,\n",
    "}\n",
    "(OUT_DIR / \"metrics\").mkdir(parents=True, exist_ok=True)\n",
    "np.save(OUT_DIR / \"metrics\" / \"val_prob.npy\",  va_prob)\n",
    "np.save(OUT_DIR / \"metrics\" / \"test_prob.npy\", te_prob)\n",
    "with open(OUT_DIR / \"summary.json\",\"w\") as f:\n",
    "    import json; json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved predictions and summary to\", OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6895370",
   "metadata": {},
   "source": [
    "## validation & test metrics (pre-threshold) + per-label table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b8b40b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAL — macro AUROC: 0.7742 | macro AP: 0.345\n",
      "TEST — macro AUROC: 0.8651 | macro AP: 0.5397\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>val_AUROC</th>\n",
       "      <th>val_AP</th>\n",
       "      <th>test_AUROC</th>\n",
       "      <th>test_AP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NR-AR</td>\n",
       "      <td>0.7049</td>\n",
       "      <td>0.3264</td>\n",
       "      <td>0.8140</td>\n",
       "      <td>0.5261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NR-AR-LBD</td>\n",
       "      <td>0.8516</td>\n",
       "      <td>0.5073</td>\n",
       "      <td>0.9547</td>\n",
       "      <td>0.6608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NR-AhR</td>\n",
       "      <td>0.8287</td>\n",
       "      <td>0.4252</td>\n",
       "      <td>0.8763</td>\n",
       "      <td>0.5431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NR-Aromatase</td>\n",
       "      <td>0.7625</td>\n",
       "      <td>0.1989</td>\n",
       "      <td>0.8561</td>\n",
       "      <td>0.3669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NR-ER</td>\n",
       "      <td>0.7050</td>\n",
       "      <td>0.4047</td>\n",
       "      <td>0.7710</td>\n",
       "      <td>0.4654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NR-ER-LBD</td>\n",
       "      <td>0.7401</td>\n",
       "      <td>0.2957</td>\n",
       "      <td>0.8778</td>\n",
       "      <td>0.6023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NR-PPAR-gamma</td>\n",
       "      <td>0.7334</td>\n",
       "      <td>0.2272</td>\n",
       "      <td>0.8702</td>\n",
       "      <td>0.5850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SR-ARE</td>\n",
       "      <td>0.7361</td>\n",
       "      <td>0.3458</td>\n",
       "      <td>0.8266</td>\n",
       "      <td>0.4572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SR-ATAD5</td>\n",
       "      <td>0.8160</td>\n",
       "      <td>0.2775</td>\n",
       "      <td>0.8888</td>\n",
       "      <td>0.6902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SR-HSE</td>\n",
       "      <td>0.7815</td>\n",
       "      <td>0.3393</td>\n",
       "      <td>0.8787</td>\n",
       "      <td>0.5797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SR-MMP</td>\n",
       "      <td>0.8415</td>\n",
       "      <td>0.5227</td>\n",
       "      <td>0.8838</td>\n",
       "      <td>0.5722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>SR-p53</td>\n",
       "      <td>0.7889</td>\n",
       "      <td>0.2693</td>\n",
       "      <td>0.8828</td>\n",
       "      <td>0.4269</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            label  val_AUROC  val_AP  test_AUROC  test_AP\n",
       "0           NR-AR     0.7049  0.3264      0.8140   0.5261\n",
       "1       NR-AR-LBD     0.8516  0.5073      0.9547   0.6608\n",
       "2          NR-AhR     0.8287  0.4252      0.8763   0.5431\n",
       "3    NR-Aromatase     0.7625  0.1989      0.8561   0.3669\n",
       "4           NR-ER     0.7050  0.4047      0.7710   0.4654\n",
       "5       NR-ER-LBD     0.7401  0.2957      0.8778   0.6023\n",
       "6   NR-PPAR-gamma     0.7334  0.2272      0.8702   0.5850\n",
       "7          SR-ARE     0.7361  0.3458      0.8266   0.4572\n",
       "8        SR-ATAD5     0.8160  0.2775      0.8888   0.6902\n",
       "9          SR-HSE     0.7815  0.3393      0.8787   0.5797\n",
       "10         SR-MMP     0.8415  0.5227      0.8838   0.5722\n",
       "11         SR-p53     0.7889  0.2693      0.8828   0.4269"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse run_epoch() and multilabel_metrics() from earlier cells\n",
    "\n",
    "# --- Validation ---\n",
    "val_loss, val_logits, val_prob, val_y = run_epoch(val_loader, train=False)\n",
    "val_metrics = multilabel_metrics(val_y, val_prob)\n",
    "\n",
    "print(\"VAL — macro AUROC:\", round(val_metrics['macro_auc'], 4),\n",
    "      \"| macro AP:\", round(val_metrics['macro_ap'], 4))\n",
    "\n",
    "# --- Test ---\n",
    "test_loss, test_logits, test_prob, test_y = run_epoch(test_loader, train=False)\n",
    "test_metrics = multilabel_metrics(test_y, test_prob)\n",
    "\n",
    "print(\"TEST — macro AUROC:\", round(test_metrics['macro_auc'], 4),\n",
    "      \"| macro AP:\", round(test_metrics['macro_ap'], 4))\n",
    "\n",
    "# --- Per-label table (AUROC/AP) for both VAL and TEST ---\n",
    "per_label = []\n",
    "for i, lab in enumerate(label_cols):\n",
    "    va_auc = val_metrics[\"per_auc\"][i]\n",
    "    va_ap  = val_metrics[\"per_ap\"][i]\n",
    "    te_auc = test_metrics[\"per_auc\"][i]\n",
    "    te_ap  = test_metrics[\"per_ap\"][i]\n",
    "    per_label.append({\n",
    "        \"label\": lab,\n",
    "        \"val_AUROC\": None if np.isnan(va_auc) else round(va_auc, 4),\n",
    "        \"val_AP\":    None if np.isnan(va_ap)  else round(va_ap, 4),\n",
    "        \"test_AUROC\":None if np.isnan(te_auc) else round(te_auc, 4),\n",
    "        \"test_AP\":   None if np.isnan(te_ap)  else round(te_ap, 4),\n",
    "    })\n",
    "\n",
    "per_label_df = pd.DataFrame(per_label)\n",
    "display(per_label_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c78b618",
   "metadata": {},
   "source": [
    "## Model save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74ac4760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ChemBERTa model + tokenizer saved to tox21_chembera_pipeline\\models\\chemberta_v1\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# === Save ChemBERTa model & tokenizer ===\n",
    "save_dir = Path(\"tox21_chembera_pipeline/models/chemberta_v1\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# save best EMA weights into model before saving\n",
    "if best_state_ema is not None:\n",
    "    model.load_state_dict(best_state_ema)\n",
    "\n",
    "# Hugging Face format\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "\n",
    "print(f\"✅ ChemBERTa model + tokenizer saved to {save_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb78a6d2",
   "metadata": {},
   "source": [
    "## threshold calibration (per label) + F1 on VAL/TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "70ab4404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-label VAL F1 and chosen thresholds:\n",
      "NR-AR         F1= 0.378  thr=0.95\n",
      "NR-AR-LBD     F1= 0.436  thr=0.95\n",
      "NR-AhR        F1= 0.436  thr=0.90\n",
      "NR-Aromatase  F1= 0.228  thr=0.95\n",
      "NR-ER         F1= 0.423  thr=0.95\n",
      "NR-ER-LBD     F1= 0.346  thr=0.95\n",
      "NR-PPAR-gamma  F1= 0.235  thr=0.95\n",
      "SR-ARE        F1= 0.374  thr=0.90\n",
      "SR-ATAD5      F1= 0.323  thr=0.95\n",
      "SR-HSE        F1= 0.460  thr=0.95\n",
      "SR-MMP        F1= 0.511  thr=0.90\n",
      "SR-p53        F1= 0.366  thr=0.95\n",
      "\n",
      "VAL — macro F1 (thr-calibrated):  0.3764\n",
      "TEST — macro F1 (thr-calibrated): 0.5231\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def calibrate_thresholds(y_true, y_prob, grid=np.linspace(0.05, 0.95, 19)):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      thresholds: (C,) best threshold per label (by F1 on VAL)\n",
    "      f1s:        (C,) best F1 per label (on VAL)\n",
    "    \"\"\"\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_prob = np.asarray(y_prob)\n",
    "    C = y_true.shape[1]\n",
    "    thresholds = np.zeros(C, dtype=float)\n",
    "    f1s = np.zeros(C, dtype=float)\n",
    "\n",
    "    for c in range(C):\n",
    "        yt = y_true[:, c]\n",
    "        yp = y_prob[:, c]\n",
    "        if len(np.unique(yt)) < 2:\n",
    "            thresholds[c] = 0.5\n",
    "            f1s[c] = np.nan\n",
    "            continue\n",
    "        best_f1 = -1.0\n",
    "        best_thr = 0.5\n",
    "        for thr in grid:\n",
    "            ypred = (yp >= thr).astype(int)\n",
    "            f1 = f1_score(yt, ypred, zero_division=0)\n",
    "            if f1 > best_f1:\n",
    "                best_f1 = f1\n",
    "                best_thr = thr\n",
    "        thresholds[c] = best_thr\n",
    "        f1s[c] = best_f1\n",
    "    return thresholds, f1s\n",
    "\n",
    "# Calibrate on VAL\n",
    "thr_vec, f1s_val = calibrate_thresholds(val_y, val_prob)\n",
    "print(\"Per-label VAL F1 and chosen thresholds:\")\n",
    "for lab, f1v, thr in zip(label_cols, f1s_val, thr_vec):\n",
    "    f1_show = \"nan\" if np.isnan(f1v) else f\"{f1v:.3f}\"\n",
    "    print(f\"{lab:12s}  F1={f1_show:>6}  thr={thr:.2f}\")\n",
    "\n",
    "# Evaluate macro-F1 on VAL/TEST with calibrated thresholds\n",
    "val_metrics_thr  = multilabel_metrics(val_y,  val_prob,  thresholds=thr_vec)\n",
    "test_metrics_thr = multilabel_metrics(test_y, test_prob, thresholds=thr_vec)\n",
    "\n",
    "print(\"\\nVAL — macro F1 (thr-calibrated): \", round(val_metrics_thr['macro_f1'], 4))\n",
    "print(\"TEST — macro F1 (thr-calibrated):\", round(test_metrics_thr['macro_f1'], 4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4fb13a",
   "metadata": {},
   "source": [
    "## saving the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4abfde5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved model, tokenizer, thresholds, labels, and metadata to tox21_chembera_pipeline\\models\\chemberta_v1\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Save HuggingFace model + tokenizer\n",
    "model.save_pretrained(MODELS_DIR)\n",
    "tokenizer.save_pretrained(MODELS_DIR)\n",
    "\n",
    "# Save thresholds & label names\n",
    "with open(MODELS_DIR / \"thresholds.json\", \"w\") as f:\n",
    "    json.dump({lab: float(thr) for lab, thr in zip(label_cols, thr_vec)}, f, indent=2)\n",
    "\n",
    "with open(MODELS_DIR / \"labels.json\", \"w\") as f:\n",
    "    json.dump(list(label_cols), f, indent=2)\n",
    "\n",
    "# Metadata about training\n",
    "metadata = {\n",
    "    \"model_name\": MODEL_NAME,\n",
    "    \"max_len\": MAX_LEN,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"base_lr\": BASE_LR,\n",
    "    \"head_lr\": HEAD_LR,\n",
    "    \"weight_decay\": WEIGHT_DECAY,\n",
    "    \"warmup_ratio\": WARMUP_RATIO,\n",
    "    \"best_val_macro_auc\": float(val_metrics[\"macro_auc\"]),\n",
    "    \"best_val_macro_ap\": float(val_metrics[\"macro_ap\"]),\n",
    "    \"val_macro_f1_thr\": float(val_metrics_thr[\"macro_f1\"]),\n",
    "    \"test_macro_auc\": float(test_metrics[\"macro_auc\"]),\n",
    "    \"test_macro_ap\": float(test_metrics[\"macro_ap\"]),\n",
    "    \"test_macro_f1_thr\": float(test_metrics_thr[\"macro_f1\"]),\n",
    "}\n",
    "with open(MODELS_DIR / \"metadata.json\", \"w\") as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved model, tokenizer, thresholds, labels, and metadata to\", MODELS_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3095d6d",
   "metadata": {},
   "source": [
    "## prediction & summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b871886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved predictions and summary metrics to tox21_chembera_pipeline\\outputs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'VAL': {'macro_AUROC': 0.774190344876498,\n",
       "  'macro_AP': 0.3450011528294685,\n",
       "  'macro_F1_thr': 0.3763729999290725},\n",
       " 'TEST': {'macro_AUROC': 0.8650696516173912,\n",
       "  'macro_AP': 0.539660679964537,\n",
       "  'macro_F1_thr': 0.5230949784582123}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save arrays for reproducibility\n",
    "np.save(OUT_DIR / \"val_logits.npy\", val_logits)\n",
    "np.save(OUT_DIR / \"val_prob.npy\", val_prob)\n",
    "np.save(OUT_DIR / \"val_y.npy\", val_y)\n",
    "\n",
    "np.save(OUT_DIR / \"test_logits.npy\", test_logits)\n",
    "np.save(OUT_DIR / \"test_prob.npy\", test_prob)\n",
    "np.save(OUT_DIR / \"test_y.npy\", test_y)\n",
    "\n",
    "# Summarize key results\n",
    "summary = {\n",
    "    \"VAL\": {\n",
    "        \"macro_AUROC\": float(val_metrics['macro_auc']),\n",
    "        \"macro_AP\": float(val_metrics['macro_ap']),\n",
    "        \"macro_F1_thr\": float(val_metrics_thr['macro_f1'])\n",
    "    },\n",
    "    \"TEST\": {\n",
    "        \"macro_AUROC\": float(test_metrics['macro_auc']),\n",
    "        \"macro_AP\": float(test_metrics['macro_ap']),\n",
    "        \"macro_F1_thr\": float(test_metrics_thr['macro_f1'])\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(OUT_DIR / \"summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"✅ Saved predictions and summary metrics to\", OUT_DIR)\n",
    "summary\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lord",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
