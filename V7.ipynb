{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "123540a8",
   "metadata": {},
   "source": [
    "# V7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce15353",
   "metadata": {},
   "source": [
    "## Phase 1 (Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ece5de",
   "metadata": {},
   "source": [
    "### Load & Inspect the Dataset (V7 Initial Checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07146771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded dataset with shape: (7831, 14)\n",
      "ðŸ”Ž Running sanity checks...\n",
      "ðŸ” Duplicates â†’ mol_id: 0, smiles: 0\n",
      "âœ… Sanity checks passed. Metadata saved to:\n",
      "  â€¢ Preview: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\meta\\preview.csv\n",
      "  â€¢ Label stats: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\meta\\label_stats.csv\n",
      "  â€¢ Schema: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\meta\\schema.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Setup paths\n",
    "# ------------------------------\n",
    "DATA_PATH = Path(\"v7/data/tox21.csv\").resolve()\n",
    "META_DIR = DATA_PATH.parent / \"meta\"\n",
    "META_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Load the dataset\n",
    "# ------------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"âœ… Loaded dataset with shape: {df.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Infer column roles\n",
    "# ------------------------------\n",
    "# Assume 12 labels, then mol_id and smiles\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# Confirm labels exist\n",
    "missing_labels = [c for c in expected_labels if c not in df.columns]\n",
    "assert not missing_labels, f\"âŒ Missing label columns: {missing_labels}\"\n",
    "\n",
    "# Detect mol_id and smiles\n",
    "assert 'mol_id' in df.columns, \"âŒ Missing 'mol_id' column\"\n",
    "assert 'smiles' in df.columns, \"âŒ Missing 'smiles' column\"\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Sanity checks\n",
    "# ------------------------------\n",
    "print(\"ðŸ”Ž Running sanity checks...\")\n",
    "\n",
    "# Check for missing or empty SMILES\n",
    "num_missing_smiles = df['smiles'].isna().sum()\n",
    "num_empty_smiles = (df['smiles'].astype(str).str.strip() == \"\").sum()\n",
    "assert num_missing_smiles == 0, f\"âŒ {num_missing_smiles} missing SMILES\"\n",
    "assert num_empty_smiles == 0, f\"âŒ {num_empty_smiles} empty SMILES\"\n",
    "\n",
    "# Check label values are 0, 1, or NaN\n",
    "bad_values = {}\n",
    "for col in expected_labels:\n",
    "    unique_vals = df[col].dropna().unique()\n",
    "    bad_vals = [v for v in unique_vals if v not in [0, 1, 0.0, 1.0]]\n",
    "    if bad_vals:\n",
    "        bad_values[col] = bad_vals\n",
    "\n",
    "assert not bad_values, f\"âŒ Invalid label values detected: {bad_values}\"\n",
    "\n",
    "# Check duplicates\n",
    "num_dup_mol = df['mol_id'].duplicated().sum()\n",
    "num_dup_smiles = df['smiles'].duplicated().sum()\n",
    "print(f\"ðŸ” Duplicates â†’ mol_id: {num_dup_mol}, smiles: {num_dup_smiles}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Save metadata outputs\n",
    "# ------------------------------\n",
    "# Preview CSV\n",
    "df.head(5).to_csv(META_DIR / \"preview.csv\", index=False)\n",
    "\n",
    "# Label stats\n",
    "label_stats = []\n",
    "for col in expected_labels:\n",
    "    total = df[col].notna().sum()\n",
    "    pos = int((df[col] == 1).sum())\n",
    "    neg = int((df[col] == 0).sum())\n",
    "    missing = int(df[col].isna().sum())\n",
    "    prevalence = pos / total if total > 0 else 0\n",
    "    label_stats.append({\n",
    "        \"label\": col,\n",
    "        \"n_samples\": total,\n",
    "        \"n_positive\": pos,\n",
    "        \"n_negative\": neg,\n",
    "        \"n_missing\": missing,\n",
    "        \"positive_rate\": round(prevalence, 5)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(label_stats).to_csv(META_DIR / \"label_stats.csv\", index=False)\n",
    "\n",
    "# Schema summary\n",
    "schema = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"n_rows\": len(df),\n",
    "    \"n_cols\": df.shape[1],\n",
    "    \"mol_id_column\": \"mol_id\",\n",
    "    \"smiles_column\": \"smiles\",\n",
    "    \"label_columns\": expected_labels,\n",
    "    \"has_duplicates\": {\n",
    "        \"mol_id\": bool(num_dup_mol),\n",
    "        \"smiles\": bool(num_dup_smiles)\n",
    "    }\n",
    "}\n",
    "with open(META_DIR / \"schema.json\", \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "print(\"âœ… Sanity checks passed. Metadata saved to:\")\n",
    "print(f\"  â€¢ Preview: {META_DIR / 'preview.csv'}\")\n",
    "print(f\"  â€¢ Label stats: {META_DIR / 'label_stats.csv'}\")\n",
    "print(f\"  â€¢ Schema: {META_DIR / 'schema.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a899373",
   "metadata": {},
   "source": [
    "### 2: RDKit Descriptor Generation and Cleaning\n",
    "\n",
    "This cell computes RDKit descriptors (~300 per molecule) from SMILES strings, then:\n",
    "- Drops molecules where descriptor generation fails\n",
    "- Replaces infinite/extreme values with NaN\n",
    "- Applies median imputation and StandardScaler\n",
    "- Saves:\n",
    "  - Cleaned descriptors â†’ `X_rdkit.npy`\n",
    "  - Binary labels â†’ `Y.npy`\n",
    "  - Metadata: `smiles.npy`, `mol_ids.npy`\n",
    "  - Feature names â†’ `feature_names.txt`\n",
    "  - Fitted imputer and scaler (for reuse in training/inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bd8269",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:41:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:41:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:41:09] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RDKit descriptors computed. Failures: 0\n",
      "ðŸ” Total NaNs after sanitization: 2966\n",
      "âœ… RDKit descriptors: Imputed and scaled\n",
      "ðŸ§© Descriptor shape: (7831, 208)\n",
      "ðŸ§¬ Label shape: (7831, 12)\n",
      "\n",
      "ðŸ“ Saved:\n",
      "â€¢ Features       â†’ D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\X_rdkit.npy\n",
      "â€¢ Labels         â†’ D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\Y.npy\n",
      "â€¢ SMILES         â†’ D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\smiles.npy\n",
      "â€¢ mol_ids        â†’ D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\mol_ids.npy\n",
      "â€¢ Feature names  â†’ D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\feature_names.txt\n",
      "â€¢ Scaler         â†’ D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\scaler.joblib\n",
      "â€¢ Imputer        â†’ D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\imputer.joblib\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "DATA_PATH = Path(\"v7/data/tox21.csv\").resolve()\n",
    "META_DIR = DATA_PATH.parent / \"meta\"\n",
    "DESC_DIR = DATA_PATH.parent / \"descriptors\"\n",
    "DESC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Load the dataset\n",
    "# -------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Setup RDKit descriptor calculator (~200â€“300 descriptors)\n",
    "# -------------------------\n",
    "desc_names = [desc[0] for desc in Descriptors._descList]\n",
    "desc_calculator = MoleculeDescriptors.MolecularDescriptorCalculator(desc_names)\n",
    "\n",
    "# -------------------------\n",
    "# Compute descriptors from SMILES\n",
    "# -------------------------\n",
    "features, labels = [], []\n",
    "smiles_list, mol_ids = [], []\n",
    "failed_count = 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    smi = row['smiles']\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    try:\n",
    "        desc = desc_calculator.CalcDescriptors(mol)\n",
    "        features.append(desc)\n",
    "        labels.append(row[expected_labels].values)\n",
    "        mol_ids.append(row['mol_id'])\n",
    "        smiles_list.append(smi)\n",
    "    except:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"âœ… RDKit descriptors computed. Failures: {failed_count}\")\n",
    "\n",
    "X = np.array(features, dtype=np.float64)\n",
    "Y = np.array(labels).astype(float)\n",
    "mol_ids = np.array(mol_ids)\n",
    "smiles_list = np.array(smiles_list)\n",
    "\n",
    "# -------------------------\n",
    "# Clean descriptor matrix: replace inf/extreme with NaN\n",
    "# -------------------------\n",
    "X[~np.isfinite(X)] = np.nan\n",
    "X[np.abs(X) > 1e6] = np.nan\n",
    "\n",
    "n_total_nan = np.isnan(X).sum()\n",
    "print(f\"ðŸ” Total NaNs after sanitization: {n_total_nan}\")\n",
    "\n",
    "# -------------------------\n",
    "# Impute and Scale\n",
    "# -------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "print(\"âœ… RDKit descriptors: Imputed and scaled\")\n",
    "print(f\"ðŸ§© Descriptor shape: {X_scaled.shape}\")\n",
    "print(f\"ðŸ§¬ Label shape: {Y.shape}\")\n",
    "\n",
    "# -------------------------\n",
    "# Save outputs\n",
    "# -------------------------\n",
    "np.save(DESC_DIR / \"X_rdkit.npy\", X_scaled)\n",
    "np.save(DESC_DIR / \"Y.npy\", Y)\n",
    "np.save(DESC_DIR / \"smiles.npy\", smiles_list)\n",
    "np.save(DESC_DIR / \"mol_ids.npy\", mol_ids)\n",
    "\n",
    "with open(DESC_DIR / \"feature_names.txt\", \"w\") as f:\n",
    "    for name in desc_names:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "joblib.dump(imputer, DESC_DIR / \"imputer.joblib\")\n",
    "joblib.dump(scaler, DESC_DIR / \"scaler.joblib\")\n",
    "\n",
    "print(\"\\nðŸ“ Saved:\")\n",
    "print(f\"â€¢ Features       â†’ {DESC_DIR / 'X_rdkit.npy'}\")\n",
    "print(f\"â€¢ Labels         â†’ {DESC_DIR / 'Y.npy'}\")\n",
    "print(f\"â€¢ SMILES         â†’ {DESC_DIR / 'smiles.npy'}\")\n",
    "print(f\"â€¢ mol_ids        â†’ {DESC_DIR / 'mol_ids.npy'}\")\n",
    "print(f\"â€¢ Feature names  â†’ {DESC_DIR / 'feature_names.txt'}\")\n",
    "print(f\"â€¢ Scaler         â†’ {DESC_DIR / 'scaler.joblib'}\")\n",
    "print(f\"â€¢ Imputer        â†’ {DESC_DIR / 'imputer.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479e2080",
   "metadata": {},
   "source": [
    "#### 2b) Save *sanitized raw* RDKit descriptors (no scaling)\n",
    "\n",
    "Recompute RDKit descriptors and only sanitize (replace inf/Â±inf/extremes with NaN).  \n",
    "This gives us `X_rdkit_raw.npy` so we can fit imputer/scaler **on the train split only** in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5860ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:41:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:41:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:41:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… RDKit descriptors recomputed. Failures: 0\n",
      "ðŸ” NaNs after sanitization: 2966 | Shape: (7831, 208)\n",
      "ðŸ“ Saved sanitized raw â†’ D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\X_rdkit_raw.npy\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"v7/data/tox21.csv\").resolve()\n",
    "DESC_DIR = DATA_PATH.parent / \"descriptors\"\n",
    "DESC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# RDKit descriptor setup\n",
    "desc_names = [d[0] for d in Descriptors._descList]\n",
    "calc = MoleculeDescriptors.MolecularDescriptorCalculator(desc_names)\n",
    "\n",
    "features, labels, smiles_list, mol_ids = [], [], [], []\n",
    "fail = 0\n",
    "for _, row in df.iterrows():\n",
    "    smi = row[\"smiles\"]\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        fail += 1\n",
    "        continue\n",
    "    try:\n",
    "        desc = calc.CalcDescriptors(mol)\n",
    "        features.append(desc)\n",
    "        labels.append(row[expected_labels].values)\n",
    "        smiles_list.append(smi)\n",
    "        mol_ids.append(row[\"mol_id\"])\n",
    "    except:\n",
    "        fail += 1\n",
    "\n",
    "print(f\"âœ… RDKit descriptors recomputed. Failures: {fail}\")\n",
    "\n",
    "X_raw = np.array(features, dtype=np.float64)\n",
    "Y = np.array(labels, dtype=np.float64)\n",
    "smiles_arr = np.array(smiles_list)\n",
    "mol_ids_arr = np.array(mol_ids)\n",
    "\n",
    "# Sanitize only (no impute/scale)\n",
    "X_raw[~np.isfinite(X_raw)] = np.nan\n",
    "X_raw[np.abs(X_raw) > 1e6] = np.nan\n",
    "print(f\"ðŸ” NaNs after sanitization: {np.isnan(X_raw).sum()} | Shape: {X_raw.shape}\")\n",
    "\n",
    "# Save sanitized raw\n",
    "np.save(DESC_DIR / \"X_rdkit_raw.npy\", X_raw)\n",
    "np.save(DESC_DIR / \"Y.npy\", Y)  # overwrite same labels to keep in sync\n",
    "np.save(DESC_DIR / \"smiles.npy\", smiles_arr)\n",
    "np.save(DESC_DIR / \"mol_ids.npy\", mol_ids_arr)\n",
    "print(\"ðŸ“ Saved sanitized raw â†’\", DESC_DIR / \"X_rdkit_raw.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d68f38",
   "metadata": {},
   "source": [
    "### 3: Deterministic Scaffold-Based Train/Val/Test Split (80/10/10)\n",
    "\n",
    "This cell:\n",
    "- Computes RDKit **Bemisâ€“Murcko scaffolds** for each SMILES.\n",
    "- **Fallback:** If a scaffold is empty (acyclic molecules), uses the moleculeâ€™s **canonical SMILES** as a pseudo-scaffold so every molecule is assigned.\n",
    "- Groups molecules by scaffold and performs an **80/10/10** split **by scaffold**, deterministic and balanced by group size.\n",
    "- Saves:\n",
    "  - Index masks: `v7/data/splits/train.npy`, `val.npy`, `test.npy`\n",
    "  - Metadata: `v7/data/splits/scaffold_split.csv` (mol_id, smiles, scaffold, split)\n",
    "  - Label distribution per split: `v7/data/splits/label_distribution.csv`\n",
    "  - Split summary: `v7/data/splits/split_summary.json`\n",
    "\n",
    "Sanity checks ensure:\n",
    "- All molecules are assigned and covered exactly once\n",
    "- Split sizes match the target proportions (Â±1 due to rounding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b458357",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Split complete â†’ Train: 6265, Val: 783, Test: 783\n",
      "ðŸŽ¯ Targets       â†’ Train: 6265, Val: 783, Test: 783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:28] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ Saved:\n",
      "â€¢ Index masks       â†’ v7\\data\\splits\n",
      "â€¢ Scaffold metadata â†’ v7\\data\\splits\\scaffold_split.csv\n",
      "â€¢ Label distributionâ†’ v7\\data\\splits\\label_distribution.csv\n",
      "â€¢ Summary JSON      â†’ v7\\data\\splits\\split_summary.json\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, OrderedDict\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ------------------------\n",
    "# Paths & constants\n",
    "# ------------------------\n",
    "DATA_DIR = Path(\"v7/data\")\n",
    "DESC_DIR = DATA_DIR / \"descriptors\"\n",
    "SPLIT_DIR = DATA_DIR / \"splits\"\n",
    "SPLIT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "mol_ids = np.load(DESC_DIR / \"mol_ids.npy\", allow_pickle=True)\n",
    "smiles = np.load(DESC_DIR / \"smiles.npy\", allow_pickle=True)\n",
    "Y = np.load(DESC_DIR / \"Y.npy\")  # shape: [N, 12]\n",
    "\n",
    "# For reporting label-wise stats with names\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# Split ratios and seed\n",
    "train_frac, val_frac, test_frac = 0.80, 0.10, 0.10\n",
    "rng_seed = 42\n",
    "random.seed(rng_seed)\n",
    "\n",
    "# ------------------------\n",
    "# Helper: compute scaffold or fallback to canonical SMILES\n",
    "# ------------------------\n",
    "def scaffold_key_from_smiles(smi: str) -> str:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return \"__INVALID__\"\n",
    "    scaf = MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=False)\n",
    "    if not scaf or scaf.strip() == \"\":\n",
    "        # Fallback: canonical SMILES as pseudo-scaffold for acyclic molecules\n",
    "        scaf = Chem.MolToSmiles(mol, isomericSmiles=False)\n",
    "    return scaf\n",
    "\n",
    "# ------------------------\n",
    "# Group indices by scaffold\n",
    "# ------------------------\n",
    "scaffold_to_indices = defaultdict(list)\n",
    "for idx, smi in enumerate(smiles):\n",
    "    key = scaffold_key_from_smiles(str(smi))\n",
    "    scaffold_to_indices[key].append(idx)\n",
    "\n",
    "# Shuffle groups of equal size in a deterministic way\n",
    "size_to_groups = defaultdict(list)\n",
    "for scaf, idxs in scaffold_to_indices.items():\n",
    "    size_to_groups[len(idxs)].append(idxs)\n",
    "\n",
    "for size in size_to_groups:\n",
    "    random.shuffle(size_to_groups[size])  # deterministic shuffle due to rng_seed\n",
    "\n",
    "# Build ordered list of groups: large -> small, with shuffled ties\n",
    "ordered_groups = []\n",
    "for size in sorted(size_to_groups.keys(), reverse=True):\n",
    "    ordered_groups.extend(size_to_groups[size])\n",
    "\n",
    "# ------------------------\n",
    "# Allocate groups to splits (80/10/10) by total assigned molecules\n",
    "# ------------------------\n",
    "N = len(smiles)\n",
    "N_grouped = sum(len(g) for g in ordered_groups)\n",
    "assert N_grouped == N, f\"Grouping mismatch: grouped {N_grouped} != total {N}\"\n",
    "\n",
    "target_train = int(round(train_frac * N))\n",
    "target_val = int(round(val_frac * N))\n",
    "target_test = N - target_train - target_val  # ensure sums to N\n",
    "\n",
    "train_idx, val_idx, test_idx = [], [], []\n",
    "cnt_train = cnt_val = cnt_test = 0\n",
    "\n",
    "for group in ordered_groups:\n",
    "    gsize = len(group)\n",
    "\n",
    "    # Greedy fill towards targets\n",
    "    if cnt_train + gsize <= target_train:\n",
    "        train_idx.extend(group); cnt_train += gsize\n",
    "    elif cnt_val + gsize <= target_val:\n",
    "        val_idx.extend(group); cnt_val += gsize\n",
    "    else:\n",
    "        test_idx.extend(group); cnt_test += gsize\n",
    "\n",
    "# If due to rounding we still have leftovers to meet exact target sizes, rebalance\n",
    "remaining = set(range(N)) - set(train_idx) - set(val_idx) - set(test_idx)\n",
    "if remaining:\n",
    "    # Fill val, then test, then train (in that order) to hit targets\n",
    "    for idx in list(remaining):\n",
    "        if cnt_val < target_val:\n",
    "            val_idx.append(idx); cnt_val += 1\n",
    "        elif cnt_test < target_test:\n",
    "            test_idx.append(idx); cnt_test += 1\n",
    "        else:\n",
    "            train_idx.append(idx); cnt_train += 1\n",
    "\n",
    "# Final sanity checks\n",
    "all_assigned = set(train_idx) | set(val_idx) | set(test_idx)\n",
    "assert len(all_assigned) == N, \"Not all molecules assigned to a split.\"\n",
    "assert len(set(train_idx) & set(val_idx)) == 0, \"Overlap between train and val.\"\n",
    "assert len(set(train_idx) & set(test_idx)) == 0, \"Overlap between train and test.\"\n",
    "assert len(set(val_idx) & set(test_idx)) == 0, \"Overlap between val and test.\"\n",
    "\n",
    "# Sort indices for neatness\n",
    "train_idx = np.array(sorted(train_idx))\n",
    "val_idx = np.array(sorted(val_idx))\n",
    "test_idx = np.array(sorted(test_idx))\n",
    "\n",
    "print(f\"âœ… Split complete â†’ Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "print(f\"ðŸŽ¯ Targets       â†’ Train: {target_train}, Val: {target_val}, Test: {target_test}\")\n",
    "\n",
    "# ------------------------\n",
    "# Save index masks\n",
    "# ------------------------\n",
    "np.save(SPLIT_DIR / \"train.npy\", train_idx)\n",
    "np.save(SPLIT_DIR / \"val.npy\", val_idx)\n",
    "np.save(SPLIT_DIR / \"test.npy\", test_idx)\n",
    "\n",
    "# ------------------------\n",
    "# Save metadata CSV\n",
    "# ------------------------\n",
    "def compute_scaffolds_for_indices(idxs):\n",
    "    return [scaffold_key_from_smiles(str(smiles[i])) for i in idxs]\n",
    "\n",
    "meta_df = pd.DataFrame({\n",
    "    \"idx\": np.concatenate([train_idx, val_idx, test_idx]),\n",
    "    \"mol_id\": mol_ids[np.concatenate([train_idx, val_idx, test_idx])],\n",
    "    \"smiles\": smiles[np.concatenate([train_idx, val_idx, test_idx])],\n",
    "    \"scaffold\": compute_scaffolds_for_indices(np.concatenate([train_idx, val_idx, test_idx])),\n",
    "    \"split\": ([\"train\"] * len(train_idx)) + ([\"val\"] * len(val_idx)) + ([\"test\"] * len(test_idx)),\n",
    "})\n",
    "meta_df.to_csv(SPLIT_DIR / \"scaffold_split.csv\", index=False)\n",
    "\n",
    "# ------------------------\n",
    "# Label distribution per split (positives and non-missing)\n",
    "# ------------------------\n",
    "def split_label_stats(Y, idxs, split_name):\n",
    "    sub = Y[idxs]\n",
    "    pos = np.nansum(sub == 1, axis=0).astype(int)\n",
    "    non_missing = np.sum(~np.isnan(sub), axis=0).astype(int)\n",
    "    prev = np.divide(pos, np.maximum(non_missing, 1))  # avoid div0\n",
    "    return pd.DataFrame({\n",
    "        \"split\": [split_name] * len(expected_labels),\n",
    "        \"label\": expected_labels,\n",
    "        \"n_non_missing\": non_missing,\n",
    "        \"n_positive\": pos,\n",
    "        \"prevalence\": np.round(prev, 5),\n",
    "    })\n",
    "\n",
    "stats_df = pd.concat([\n",
    "    split_label_stats(Y, train_idx, \"train\"),\n",
    "    split_label_stats(Y, val_idx, \"val\"),\n",
    "    split_label_stats(Y, test_idx, \"test\"),\n",
    "], ignore_index=True)\n",
    "stats_df.to_csv(SPLIT_DIR / \"label_distribution.csv\", index=False)\n",
    "\n",
    "# ------------------------\n",
    "# Split summary JSON\n",
    "# ------------------------\n",
    "summary = {\n",
    "    \"seed\": rng_seed,\n",
    "    \"N_total\": int(N),\n",
    "    \"sizes\": {\n",
    "        \"train\": int(len(train_idx)),\n",
    "        \"val\": int(len(val_idx)),\n",
    "        \"test\": int(len(test_idx)),\n",
    "    },\n",
    "    \"targets\": {\n",
    "        \"train\": int(target_train),\n",
    "        \"val\": int(target_val),\n",
    "        \"test\": int(target_test),\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"train_idx\": str(SPLIT_DIR / \"train.npy\"),\n",
    "        \"val_idx\": str(SPLIT_DIR / \"val.npy\"),\n",
    "        \"test_idx\": str(SPLIT_DIR / \"test.npy\"),\n",
    "        \"scaffold_meta\": str(SPLIT_DIR / \"scaffold_split.csv\"),\n",
    "        \"label_distribution\": str(SPLIT_DIR / \"label_distribution.csv\"),\n",
    "    }\n",
    "}\n",
    "(Path(SPLIT_DIR) / \"split_summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"ðŸ“ Saved:\")\n",
    "print(f\"â€¢ Index masks       â†’ {SPLIT_DIR}\")\n",
    "print(f\"â€¢ Scaffold metadata â†’ {SPLIT_DIR / 'scaffold_split.csv'}\")\n",
    "print(f\"â€¢ Label distributionâ†’ {SPLIT_DIR / 'label_distribution.csv'}\")\n",
    "print(f\"â€¢ Summary JSON      â†’ {SPLIT_DIR / 'split_summary.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66adef1a",
   "metadata": {},
   "source": [
    "### 4: Train-only impute/scale â†’ package train/val/test\n",
    "\n",
    "- Load `X_rdkit_raw.npy` (sanitized, unscaled)\n",
    "- Fit `SimpleImputer(median)` and `StandardScaler` **on train only**\n",
    "- Transform train/val/test\n",
    "- Save:\n",
    "  - NPZ bundles: `v7/data/prepared/{train,val,test}.npz`\n",
    "  - Train-fitted artifacts: `imputer_train.joblib`, `scaler_train.joblib`\n",
    "  - `dataset_manifest.json`\n",
    "- Sanity checks: coverage, overlap, finiteness, label validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1eeda1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Train-only impute/scale complete.\n",
      "Shapes â†’ train (6265, 208) val (783, 208) test (783, 208)\n",
      "ðŸ“ Saved bundles & manifest â†’ v7\\data\\prepared\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATA_DIR = Path(\"v7/data\")\n",
    "DESC_DIR = DATA_DIR / \"descriptors\"\n",
    "SPLIT_DIR = DATA_DIR / \"splits\"\n",
    "PREP_DIR  = DATA_DIR / \"prepared\"\n",
    "PREP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load sanitized RAW descriptors and metadata\n",
    "X_raw   = np.load(DESC_DIR / \"X_rdkit_raw.npy\")   # sanitized, may contain NaNs\n",
    "Y       = np.load(DESC_DIR / \"Y.npy\")\n",
    "smiles  = np.load(DESC_DIR / \"smiles.npy\", allow_pickle=True)\n",
    "mol_ids = np.load(DESC_DIR / \"mol_ids.npy\", allow_pickle=True)\n",
    "\n",
    "train_idx = np.load(SPLIT_DIR / \"train.npy\")\n",
    "val_idx   = np.load(SPLIT_DIR / \"val.npy\")\n",
    "test_idx  = np.load(SPLIT_DIR / \"test.npy\")\n",
    "\n",
    "N, F = X_raw.shape\n",
    "assert Y.shape[0] == N == smiles.shape[0] == mol_ids.shape[0], \"Row mismatch across arrays.\"\n",
    "\n",
    "# --- Sanity: coverage & overlap\n",
    "assigned = set(train_idx.tolist() + val_idx.tolist() + test_idx.tolist())\n",
    "assert len(assigned) == N, f\"Not all rows assigned: {len(assigned)} != {N}\"\n",
    "assert not (set(train_idx) & set(val_idx)),  \"Overlap train/val\"\n",
    "assert not (set(train_idx) & set(test_idx)), \"Overlap train/test\"\n",
    "assert not (set(val_idx) & set(test_idx)),   \"Overlap val/test\"\n",
    "\n",
    "# --- Fit imputer & scaler on TRAIN ONLY\n",
    "imputer_tr = SimpleImputer(strategy=\"median\")\n",
    "scaler_tr  = StandardScaler()\n",
    "\n",
    "X_tr_imp = imputer_tr.fit_transform(X_raw[train_idx])\n",
    "X_tr     = scaler_tr.fit_transform(X_tr_imp)\n",
    "\n",
    "def transform_split(indices: np.ndarray, name: str):\n",
    "    X_imp = imputer_tr.transform(X_raw[indices])\n",
    "    X_s   = scaler_tr.transform(X_imp)\n",
    "    Y_s   = Y[indices].astype(np.float32, copy=False)\n",
    "    s_s   = smiles[indices]\n",
    "    m_s   = mol_ids[indices]\n",
    "    y_missing = np.isnan(Y_s)\n",
    "    # checks\n",
    "    assert np.isfinite(X_s).all(), f\"Non-finite features in {name} split after transform.\"\n",
    "    return X_s.astype(np.float32), Y_s, s_s, m_s, y_missing\n",
    "\n",
    "X_train, Y_train, smi_train, mid_train, miss_train = transform_split(train_idx, \"train\")\n",
    "X_val,   Y_val,   smi_val,   mid_val,   miss_val   = transform_split(val_idx,   \"val\")\n",
    "X_test,  Y_test,  smi_test,  mid_test,  miss_test  = transform_split(test_idx,  \"test\")\n",
    "\n",
    "print(\"âœ… Train-only impute/scale complete.\")\n",
    "print(\"Shapes â†’\",\n",
    "      \"train\", X_train.shape, \n",
    "      \"val\",   X_val.shape, \n",
    "      \"test\",  X_test.shape)\n",
    "\n",
    "# --- Save bundles\n",
    "def save_npz(name, Xs, Ys, smi, mids, idxs, miss):\n",
    "    out = PREP_DIR / f\"{name}.npz\"\n",
    "    np.savez_compressed(out, X=Xs, Y=Ys, smiles=smi, mol_id=mids, indices=idxs, y_missing_mask=miss)\n",
    "    return str(out)\n",
    "\n",
    "p_train = save_npz(\"train\", X_train, Y_train, smi_train, mid_train, train_idx, miss_train)\n",
    "p_val   = save_npz(\"val\",   X_val,   Y_val,   smi_val,   mid_val,   val_idx,   miss_val)\n",
    "p_test  = save_npz(\"test\",  X_test,  Y_test,  smi_test,  mid_test,  test_idx,  miss_test)\n",
    "\n",
    "# --- Save train-fitted artifacts\n",
    "joblib.dump(imputer_tr, DESC_DIR / \"imputer_train.joblib\")\n",
    "joblib.dump(scaler_tr,  DESC_DIR / \"scaler_train.joblib\")\n",
    "\n",
    "# --- Manifest\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "manifest = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"n_total\": int(N),\n",
    "    \"n_features\": int(F),\n",
    "    \"labels\": expected_labels,\n",
    "    \"artifacts\": {\n",
    "        \"prepared_dir\": str(PREP_DIR),\n",
    "        \"train_npz\": p_train, \"val_npz\": p_val, \"test_npz\": p_test,\n",
    "        \"imputer_train\": str(DESC_DIR / \"imputer_train.joblib\"),\n",
    "        \"scaler_train\": str(DESC_DIR / \"scaler_train.joblib\"),\n",
    "        \"splits_dir\": str(SPLIT_DIR),\n",
    "        \"descriptors_dir\": str(DESC_DIR),\n",
    "        \"raw_descriptors\": str(DESC_DIR / \"X_rdkit_raw.npy\"),\n",
    "    },\n",
    "    \"splits\": {\n",
    "        \"train\": {\"size\": int(len(train_idx))},\n",
    "        \"val\":   {\"size\": int(len(val_idx))},\n",
    "        \"test\":  {\"size\": int(len(test_idx))}\n",
    "    }\n",
    "}\n",
    "(PREP_DIR / \"dataset_manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "print(\"ðŸ“ Saved bundles & manifest â†’\", PREP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc454806",
   "metadata": {},
   "source": [
    "## Phase 2 (Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04fb19c",
   "metadata": {},
   "source": [
    "### 1 : Cross-Attention Fusion Core + Sanity Test\n",
    "\n",
    "This defines the **novel fusion** block:\n",
    "- SMILES token embeddings (queries) attend over graph node embeddings (keys/values)\n",
    "- Residual + LayerNorm\n",
    "- Masked mean pooling for text and graph streams (fixed broadcasting)\n",
    "- Descriptor MLP to align RDKit features with the model dimension\n",
    "- Classifier head â†’ 12 toxicity logits\n",
    "\n",
    "A synthetic sanity test checks tensor shapes and masks end-to-end (no encoders yet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c5ec13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest. N_LABELS=12, DESC_IN_DIM=208\n",
      "[Sanity] logits shape: (4, 12) (expected: 4 x 12)\n",
      "âœ… Fusion core defined & sanity-checked.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------\n",
    "# Repro & device\n",
    "# -----------------------------\n",
    "def seed_everything(seed: int = 42):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load descriptor dim & labels from manifest\n",
    "# -----------------------------\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "manifest_path = PREP_DIR / \"dataset_manifest.json\"\n",
    "assert manifest_path.exists(), f\"Missing manifest at {manifest_path}. Run Phase 1, Cell 4 first.\"\n",
    "\n",
    "with open(manifest_path) as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "LABEL_NAMES = manifest[\"labels\"]\n",
    "N_LABELS = len(LABEL_NAMES)\n",
    "DESC_IN_DIM = manifest[\"n_features\"]  # RDKit feature count (e.g., 208)\n",
    "\n",
    "print(f\"Loaded manifest. N_LABELS={N_LABELS}, DESC_IN_DIM={DESC_IN_DIM}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Mask helpers (fixed broadcasting)\n",
    "# -----------------------------\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x:    (B, L, D)\n",
    "    mask: (B, L) with 1 for valid, 0 for pad\n",
    "    returns: (B, D)\n",
    "    \"\"\"\n",
    "    # ensure same device/dtype\n",
    "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "    # keepdim=True to make denom shape (B,1), so it broadcasts over D\n",
    "    denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)  # (B,1)\n",
    "    num = (x * mask.unsqueeze(-1)).sum(dim=dim)              # (B,D)\n",
    "    return num / denom                                       # (B,D)\n",
    "\n",
    "def lengths_from_mask(mask: torch.Tensor) -> torch.Tensor:\n",
    "    return mask.long().sum(dim=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Modules\n",
    "# -----------------------------\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int = 256, p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (B, in_dim)\n",
    "        return self.net(x) # (B, out_dim)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single cross-attention layer: text queries attend to graph keys/values.\n",
    "    Uses PyTorch MultiheadAttention. Expects masks for graph nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n_heads: int = 4, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, dropout=p, batch_first=False)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "        \"\"\"\n",
    "        text_tokens: (B, L, D)\n",
    "        text_mask:   (B, L)  1=valid, 0=pad\n",
    "        graph_nodes: (B, N, D)\n",
    "        graph_mask:  (B, N)  1=valid, 0=pad\n",
    "        Returns:\n",
    "          text_out: (B, L, D) after cross-attn + residual + LN\n",
    "        \"\"\"\n",
    "        B, L, D = text_tokens.shape\n",
    "        N = graph_nodes.size(1)\n",
    "\n",
    "        # Convert to (S, B, D) for MHA\n",
    "        Q = text_tokens.transpose(0, 1)   # (L, B, D)\n",
    "        K = graph_nodes.transpose(0, 1)   # (N, B, D)\n",
    "        V = graph_nodes.transpose(0, 1)   # (N, B, D)\n",
    "\n",
    "        # key_padding_mask: (B, N) with True for positions to ignore\n",
    "        key_padding_mask = (graph_mask == 0)  # bool\n",
    "        attn_out, _ = self.mha(Q, K, V, key_padding_mask=key_padding_mask)  # (L, B, D)\n",
    "\n",
    "        # Residual + LN\n",
    "        attn_out = attn_out.transpose(0, 1)   # (B, L, D)\n",
    "        text_out = self.ln(text_tokens + self.dropout(attn_out))\n",
    "        return text_out\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Pools attended text and graph streams, fuses with descriptor embedding, predicts 12 labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n_labels: int, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim * 3, dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(dim * 2, n_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask, desc_embed):\n",
    "        # Masked mean pools (fixed)\n",
    "        text_pool = masked_mean(text_tokens, text_mask, dim=1)   # (B, D)\n",
    "        graph_pool = masked_mean(graph_nodes, graph_mask, dim=1) # (B, D)\n",
    "\n",
    "        fused = torch.cat([text_pool, graph_pool, desc_embed], dim=-1)  # (B, 3D)\n",
    "        fused = self.dropout(fused)\n",
    "        logits = self.mlp(fused)  # (B, n_labels)\n",
    "        return logits\n",
    "\n",
    "class V7FusionCore(nn.Module):\n",
    "    \"\"\"\n",
    "    Novel fusion core (no encoders here).\n",
    "    Expects:\n",
    "      - text_tokens: (B, L, D)\n",
    "      - text_mask:   (B, L) 1/0 (valid/pad)\n",
    "      - graph_nodes: (B, N, D)\n",
    "      - graph_mask:  (B, N) 1/0\n",
    "      - desc_feats:  (B, DESC_IN_DIM)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 256, n_heads: int = 4, n_labels: int = 12,\n",
    "                 desc_in_dim: int = DESC_IN_DIM, desc_hidden: int = 256,\n",
    "                 p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.cross = CrossAttentionBlock(dim=dim, n_heads=n_heads, p=p)\n",
    "        self.desc_mlp = DescriptorMLP(in_dim=desc_in_dim, out_dim=dim, hidden=desc_hidden, p=p)\n",
    "        self.classifier = FusionClassifier(dim=dim, n_labels=n_labels, p=p)\n",
    "\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask, desc_feats):\n",
    "        text_attn = self.cross(text_tokens, text_mask, graph_nodes, graph_mask)  # (B,L,D)\n",
    "        desc_embed = self.desc_mlp(desc_feats)                                   # (B,D)\n",
    "        logits = self.classifier(text_attn, text_mask, graph_nodes, graph_mask, desc_embed)  # (B, n_labels)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ”Ž Sanity test with synthetic tensors (no encoders yet)\n",
    "# -----------------------------\n",
    "def _sanity_test():\n",
    "    B, L, N, D = 4, 64, 48, 256\n",
    "    desc_in = DESC_IN_DIM\n",
    "\n",
    "    text_tokens = torch.randn(B, L, D, device=device)\n",
    "    graph_nodes = torch.randn(B, N, D, device=device)\n",
    "\n",
    "    # Build masks with at least 1 valid token/node per item\n",
    "    text_mask = (torch.rand(B, L, device=device) > 0.1).int()\n",
    "    graph_mask = (torch.rand(B, N, device=device) > 0.1).int()\n",
    "    for b in range(B):\n",
    "        if text_mask[b].sum() == 0:\n",
    "            text_mask[b, 0] = 1\n",
    "        if graph_mask[b].sum() == 0:\n",
    "            graph_mask[b, 0] = 1\n",
    "\n",
    "    desc_feats = torch.randn(B, desc_in, device=device)\n",
    "\n",
    "    model = V7FusionCore(dim=256, n_heads=4, n_labels=N_LABELS, desc_in_dim=desc_in, desc_hidden=256, p=0.1).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_tokens, text_mask, graph_nodes, graph_mask, desc_feats)\n",
    "    print(f\"[Sanity] logits shape: {tuple(logits.shape)} (expected: {B} x {N_LABELS})\")\n",
    "\n",
    "_sanity_test()\n",
    "print(\"âœ… Fusion core defined & sanity-checked.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91641f6",
   "metadata": {},
   "source": [
    "### 2: ChemBERTa Text Encoder Wrapper (+ config saved under `v7/model`)\n",
    "\n",
    "This cell defines a lightweight wrapper around a ChemBERTa checkpoint to produce\n",
    "token-level embeddings aligned to the fusion dimension (default **256**).\n",
    "\n",
    "**What it does**\n",
    "- Loads a SMILES-aware tokenizer & model (ChemBERTa: `seyonec/ChemBERTa-zinc-base-v1`)\n",
    "- Projects hidden size â†’ `fusion_dim` (256) for compatibility with the fusion core\n",
    "- Returns:\n",
    "  - `text_tokens`: `(B, L, 256)` token embeddings\n",
    "  - `text_mask`: `(B, L)` with 1=valid, 0=pad (compatible with fusion core)\n",
    "- Utilities:\n",
    "  - `freeze_backbone(n_unfrozen_layers=0)` for staged fine-tuning\n",
    "  - Optional gradient checkpointing toggle\n",
    "- Saves a minimal **encoder manifest** to `v7/model/text_encoder/config.json`\n",
    "- Runs a **small sanity test** using a few SMILES from your prepared train split\n",
    "\n",
    "> Notes:\n",
    "> - Default max length = **256 tokens**; adjust via `max_length` when calling.\n",
    "> - Ensure `transformers` is installed (>=4.30 recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3682214e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ðŸ“ Saved text encoder manifest â†’ v7\\model\\text_encoder\\config.json\n",
      "Sanity:\n",
      "  input batch: 4\n",
      "  tokens: (4, 45, 256) (B, L, 256)\n",
      "  mask:   (4, 45) (B, L), 1=valid, 0=pad\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Prefer the \"seyonec\" ChemBERTa (stable & widely used)\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & constants\n",
    "# -----------------------------\n",
    "MODEL_DIR = Path(\"v7/model\")\n",
    "TEXT_DIR  = MODEL_DIR / \"text_encoder\"\n",
    "TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "train_npz = PREP_DIR / \"train.npz\"\n",
    "assert train_npz.exists(), \"Missing train split. Please run Phase 1, Cells 3â€“4.\"\n",
    "\n",
    "# -----------------------------\n",
    "# Config you can tweak\n",
    "# -----------------------------\n",
    "CHEMBERTA_CKPT = \"seyonec/ChemBERTa-zinc-base-v1\"   # or: \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "FUSION_DIM     = 256\n",
    "DROPOUT_PROB   = 0.1\n",
    "MAX_SEQ_LEN    = 256    # default when encoding batches\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Text encoder wrapper\n",
    "# -----------------------------\n",
    "class ChemBERTaEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a SMILES-aware ChemBERTa to produce token embeddings aligned to fusion dim.\n",
    "    Returns (text_tokens, text_mask):\n",
    "      - text_tokens: (B, L, FUSION_DIM)\n",
    "      - text_mask:   (B, L) int {0,1}\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ckpt_name: str = CHEMBERTA_CKPT,\n",
    "        fusion_dim: int = FUSION_DIM,\n",
    "        dropout_p: float = DROPOUT_PROB,\n",
    "        gradient_checkpointing: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ckpt_name = ckpt_name\n",
    "\n",
    "        # Tokenizer/Model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "        self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size, fusion_dim),\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(fusion_dim)\n",
    "\n",
    "        if gradient_checkpointing and hasattr(self.backbone, \"gradient_checkpointing_enable\"):\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "\n",
    "        # Keep mask semantics explicit: 1=valid, 0=pad\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        if self.pad_token_id is None:\n",
    "            # Some Roberta tokenizers don't have pad by default; set to eos\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(\n",
    "        self,\n",
    "        smiles_list: List[str],\n",
    "        max_length: int = MAX_SEQ_LEN,\n",
    "        add_special_tokens: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Fast no-grad encode â†’ (text_tokens, text_mask)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        return self.forward(smiles_list, max_length, add_special_tokens)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        smiles_list: List[str],\n",
    "        max_length: int = MAX_SEQ_LEN,\n",
    "        add_special_tokens: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass â†’ (text_tokens, text_mask)\n",
    "        \"\"\"\n",
    "        enc = self.tokenizer(\n",
    "            list(smiles_list),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids      = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)  # 1=valid, 0=pad\n",
    "\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state  # (B, L, H)\n",
    "\n",
    "        tokens = self.proj(last_hidden)          # (B, L, fusion_dim)\n",
    "        tokens = self.ln(tokens)                 # (B, L, fusion_dim)\n",
    "\n",
    "        # Return mask as int {0,1} to match fusion core expectations\n",
    "        mask = attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "        return tokens, mask\n",
    "\n",
    "    def freeze_backbone(self, n_unfrozen_layers: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Freeze full backbone; optionally unfreeze the last `n_unfrozen_layers` transformer blocks.\n",
    "        \"\"\"\n",
    "        # Freeze all backbone parameters\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        if n_unfrozen_layers > 0:\n",
    "            assert hasattr(self.backbone, \"encoder\") or hasattr(self.backbone, \"roberta\"), \\\n",
    "                \"Unexpected backbone structure; adjust unfreezing logic.\"\n",
    "            # For RoBERTa-like models in HF, layers live under .encoder.layer (or .roberta.encoder.layer)\n",
    "            encoder = getattr(self.backbone, \"encoder\", None)\n",
    "            if encoder is None and hasattr(self.backbone, \"roberta\"):\n",
    "                encoder = self.backbone.roberta.encoder\n",
    "\n",
    "            if encoder is not None and hasattr(encoder, \"layer\"):\n",
    "                L = len(encoder.layer)\n",
    "                for idx in range(L - n_unfrozen_layers, L):\n",
    "                    for p in encoder.layer[idx].parameters():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        # Always keep projection & layernorm trainable\n",
    "        for p in self.proj.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.ln.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "# -----------------------------\n",
    "# Build & save a minimal manifest\n",
    "# -----------------------------\n",
    "text_encoder = ChemBERTaEncoder(\n",
    "    ckpt_name=CHEMBERTA_CKPT,\n",
    "    fusion_dim=FUSION_DIM,\n",
    "    dropout_p=DROPOUT_PROB,\n",
    "    gradient_checkpointing=False,  # set True if you need memory savings\n",
    ").to(device)\n",
    "\n",
    "manifest = {\n",
    "    \"checkpoint\": CHEMBERTA_CKPT,\n",
    "    \"fusion_dim\": FUSION_DIM,\n",
    "    \"dropout_p\": DROPOUT_PROB,\n",
    "    \"max_seq_len_default\": MAX_SEQ_LEN,\n",
    "    \"pad_token_id\": text_encoder.pad_token_id,\n",
    "    \"hidden_size\": int(text_encoder.backbone.config.hidden_size),\n",
    "    \"device\": str(device),\n",
    "}\n",
    "(TEXT_DIR / \"config.json\").write_text(json.dumps(manifest, indent=2))\n",
    "print(\"ðŸ“ Saved text encoder manifest â†’\", TEXT_DIR / \"config.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ”Ž Quick sanity test on real SMILES from train split\n",
    "# -----------------------------\n",
    "batch = np.load(train_npz, allow_pickle=True)\n",
    "sample_smiles = [str(s) for s in batch[\"smiles\"][:4].tolist()]  # small batch of real strings\n",
    "\n",
    "with torch.no_grad():\n",
    "    toks, mask = text_encoder.encode(sample_smiles, max_length=MAX_SEQ_LEN)\n",
    "\n",
    "print(\"Sanity:\")\n",
    "print(\"  input batch:\", len(sample_smiles))\n",
    "print(\"  tokens:\", tuple(toks.shape), \"(B, L, 256)\")\n",
    "print(\"  mask:  \", tuple(mask.shape), \"(B, L), 1=valid, 0=pad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d33ee33",
   "metadata": {},
   "source": [
    "### 3: Graph Encoder (RDKit â†’ GIN)\n",
    "\n",
    "This cell builds a lightweight GIN graph encoder without external GNN libs:\n",
    "- RDKit featurisation â†’ padded tensors\n",
    "- Pure-PyTorch GIN layers (sum aggregation + MLP + residual + LayerNorm)\n",
    "- Outputs per-node embeddings (dim=256) + masks, ready for fusion\n",
    "\n",
    "Saves:\n",
    "- `v7/model/graph_encoder/config.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4149a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ðŸ“ Saved graph encoder manifest â†’ v7\\model\\graph_encoder\\config.json\n",
      "Sanity:\n",
      "  input batch: 4\n",
      "  nodes: (4, 21, 256) (B, N, 256)\n",
      "  mask:  (4, 21) (B, N), 1=valid, 0=pad\n",
      "  valid node counts: [16, 15, 21, 20]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & constants\n",
    "# -----------------------------\n",
    "MODEL_DIR = Path(\"v7/model\")\n",
    "GRAPH_DIR = MODEL_DIR / \"graph_encoder\"\n",
    "GRAPH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "train_npz = PREP_DIR / \"train.npz\"\n",
    "assert train_npz.exists(), \"Missing train split. Please run Phase 1 Cells 3â€“4.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "FUSION_DIM   = 256\n",
    "MAX_NODES    = 128   # cap for very large molecules (rare in Tox21)\n",
    "DROPOUT_PROB = 0.1\n",
    "GIN_LAYERS   = 4\n",
    "GIN_HIDDEN   = 256   # keep equal to fusion dim\n",
    "\n",
    "# -----------------------------\n",
    "# Atom featurisation\n",
    "# -----------------------------\n",
    "# Common organic set; everything else -> \"other\"\n",
    "ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "HYB_LIST  = [\n",
    "    Chem.rdchem.HybridizationType.S,\n",
    "    Chem.rdchem.HybridizationType.SP,\n",
    "    Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3,\n",
    "    Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2,\n",
    "]\n",
    "CHIRAL_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER,\n",
    "]\n",
    "\n",
    "def one_hot(value, choices):\n",
    "    vec = [0]*len(choices)\n",
    "    if value in choices:\n",
    "        vec[choices.index(value)] = 1\n",
    "    return vec\n",
    "\n",
    "def clamp_one_hot_int(value: int, lo: int, hi: int) -> List[int]:\n",
    "    \"\"\"One-hot for integer value clamped to [lo, hi]; extra bucket if outside range.\"\"\"\n",
    "    # buckets: lo..hi and an \"other\"\n",
    "    buckets = list(range(lo, hi+1))\n",
    "    if value < lo or value > hi:\n",
    "        return [0]*(len(buckets)) + [1]\n",
    "    out = [0]*(len(buckets)+1)\n",
    "    out[value - lo] = 1\n",
    "    return out\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> List[float]:\n",
    "    sym = atom.GetSymbol()\n",
    "    atom_type = one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST + [\"other\"])\n",
    "\n",
    "    degree = clamp_one_hot_int(atom.GetDegree(), 0, 5)              # 7 dims (0..5 + other)\n",
    "    formal = clamp_one_hot_int(atom.GetFormalCharge(), -2, 2)       # 6 dims (-2..2 + other)\n",
    "    hyb    = one_hot(atom.GetHybridization(), HYB_LIST) + [0]       # +1 \"other\"\n",
    "    aromatic = [1 if atom.GetIsAromatic() else 0]\n",
    "    in_ring  = [1 if atom.IsInRing() else 0]\n",
    "    chiral   = one_hot(atom.GetChiralTag(), CHIRAL_LIST)\n",
    "\n",
    "    total_h  = clamp_one_hot_int(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)  # 6 dims\n",
    "    valence  = clamp_one_hot_int(atom.GetTotalValence(), 0, 5)                     # 7 dims\n",
    "    mass     = [atom.GetMass() / 200.0]  # scale roughly into [0, 1]\n",
    "\n",
    "    feat = atom_type + degree + formal + hyb + aromatic + in_ring + chiral + total_h + valence + mass\n",
    "    return feat\n",
    "\n",
    "def smiles_to_graph(smi: str, max_nodes: int = MAX_NODES) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x:   (N, F_node)\n",
    "      adj: (N, N) binary adjacency (no self loops)\n",
    "    Truncates to max_nodes if needed.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None or mol.GetNumAtoms() == 0:\n",
    "        return np.zeros((0,0), dtype=np.float32), np.zeros((0,0), dtype=np.float32)\n",
    "\n",
    "    # Atom features\n",
    "    feats = [atom_features(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "    x = np.asarray(feats, dtype=np.float32)\n",
    "\n",
    "    # Adjacency (no self-loops here; weâ€™ll handle in GIN)\n",
    "    N = mol.GetNumAtoms()\n",
    "    adj = np.zeros((N, N), dtype=np.float32)\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        adj[i, j] = 1.0\n",
    "        adj[j, i] = 1.0\n",
    "\n",
    "    # Truncate if needed\n",
    "    if N > max_nodes:\n",
    "        x = x[:max_nodes]\n",
    "        adj = adj[:max_nodes, :max_nodes]\n",
    "\n",
    "    return x, adj\n",
    "\n",
    "def collate_graphs(smiles_list: List[str], max_nodes: int = MAX_NODES) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Build a padded batch:\n",
    "      X:     (B, N_max, F_node)\n",
    "      A:     (B, N_max, N_max)\n",
    "      mask:  (B, N_max) 1=valid, 0=pad\n",
    "    \"\"\"\n",
    "    graphs = [smiles_to_graph(s, max_nodes) for s in smiles_list]\n",
    "    N_max = max([g[0].shape[0] for g in graphs] + [1])\n",
    "\n",
    "    # Node feature dim\n",
    "    F_node = graphs[0][0].shape[1] if graphs[0][0].size > 0 else len(atom_features(Chem.MolFromSmiles(\"C\").GetAtomWithIdx(0)))\n",
    "\n",
    "    B = len(graphs)\n",
    "    X = np.zeros((B, N_max, F_node), dtype=np.float32)\n",
    "    A = np.zeros((B, N_max, N_max), dtype=np.float32)\n",
    "    M = np.zeros((B, N_max), dtype=np.int64)\n",
    "\n",
    "    for i, (x, adj) in enumerate(graphs):\n",
    "        n = x.shape[0]\n",
    "        if n == 0:\n",
    "            continue\n",
    "        X[i, :n, :] = x\n",
    "        A[i, :n, :n] = adj\n",
    "        M[i, :n] = 1\n",
    "\n",
    "    # to tensors\n",
    "    X = torch.from_numpy(X)\n",
    "    A = torch.from_numpy(A)\n",
    "    M = torch.from_numpy(M)\n",
    "    return X, A, M\n",
    "\n",
    "# -----------------------------\n",
    "# Lightweight GIN (pure torch)\n",
    "# -----------------------------\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, eps_init: float = 0.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(eps_init, dtype=torch.float32))\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:    (B, N, D)\n",
    "        adj:  (B, N, N)  binary adjacency (no self loops)\n",
    "        mask: (B, N)     1=valid, 0=pad\n",
    "        \"\"\"\n",
    "        # Add self term explicitly: (1+eps) * x  +  sum_neighbors(x)\n",
    "        neigh = torch.matmul(adj, x)  # (B, N, D)\n",
    "        out = (1.0 + self.eps) * x + neigh\n",
    "        out = self.mlp(out)\n",
    "\n",
    "        # Zero-out padded nodes\n",
    "        out = out * mask.unsqueeze(-1).to(out.dtype)\n",
    "        return out\n",
    "\n",
    "class GraphGINEncoder(nn.Module):\n",
    "    def __init__(self, node_in_dim: int, hidden_dim: int = GIN_HIDDEN, n_layers: int = GIN_LAYERS, dropout: float = DROPOUT_PROB):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(\n",
    "            nn.Linear(node_in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden_dim, eps_init=0.0, dropout=dropout) for _ in range(n_layers)])\n",
    "        self.out_ln = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, smiles_list: List[str], max_nodes: int = MAX_NODES) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        self.eval()\n",
    "        return self.forward(smiles_list, max_nodes)\n",
    "\n",
    "    def forward(self, smiles_list: List[str], max_nodes: int = MAX_NODES) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          node_embeddings: (B, N, hidden_dim)\n",
    "          mask:            (B, N) int {0,1}\n",
    "        \"\"\"\n",
    "        X, A, M = collate_graphs(smiles_list, max_nodes)  # (B,N,F), (B,N,N), (B,N)\n",
    "        X = X.to(device)\n",
    "        A = A.to(device)\n",
    "        M = M.to(device)\n",
    "\n",
    "        h = self.inp(X)  # (B,N,D)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, A, M)\n",
    "        h = self.out_ln(h)\n",
    "\n",
    "        # final mask as int {0,1}\n",
    "        mask = M.to(dtype=torch.int32)\n",
    "        return h, mask\n",
    "\n",
    "# -----------------------------\n",
    "# Build encoder & save manifest\n",
    "# -----------------------------\n",
    "# Infer node feature dim from a simple atom (or compute from a real SMILES)\n",
    "probe_x, _ = smiles_to_graph(\"CCO\")\n",
    "node_in_dim = int(probe_x.shape[1]) if probe_x.size > 0 else 64  # fallback\n",
    "\n",
    "graph_encoder = GraphGINEncoder(node_in_dim=node_in_dim, hidden_dim=FUSION_DIM, n_layers=GIN_LAYERS, dropout=DROPOUT_PROB).to(device)\n",
    "\n",
    "manifest = {\n",
    "    \"fusion_dim\": FUSION_DIM,\n",
    "    \"hidden_dim\": FUSION_DIM,\n",
    "    \"n_layers\": GIN_LAYERS,\n",
    "    \"dropout_p\": DROPOUT_PROB,\n",
    "    \"max_nodes\": MAX_NODES,\n",
    "    \"node_in_dim\": node_in_dim,\n",
    "    \"atom_types\": ATOM_LIST + [\"other\"],\n",
    "    \"hybridizations\": [str(h) for h in HYB_LIST] + [\"other\"],\n",
    "    \"chiral_types\": [int(c) for c in CHIRAL_LIST],\n",
    "    \"device\": str(device),\n",
    "}\n",
    "(GRAPH_DIR / \"config.json\").write_text(json.dumps(manifest, indent=2))\n",
    "print(\"ðŸ“ Saved graph encoder manifest â†’\", GRAPH_DIR / \"config.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ”Ž Sanity test: 4 real SMILES from train split\n",
    "# -----------------------------\n",
    "batch = np.load(train_npz, allow_pickle=True)\n",
    "sample_smiles = [str(s) for s in batch[\"smiles\"][:4].tolist()]\n",
    "\n",
    "with torch.no_grad():\n",
    "    nodes, mask = graph_encoder.encode(sample_smiles, max_nodes=MAX_NODES)\n",
    "\n",
    "print(\"Sanity:\")\n",
    "print(\"  input batch:\", len(sample_smiles))\n",
    "print(\"  nodes:\", tuple(nodes.shape), \"(B, N, 256)\")\n",
    "print(\"  mask: \", tuple(mask.shape), \"(B, N), 1=valid, 0=pad\")\n",
    "print(\"  valid node counts:\", mask.sum(dim=1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f6366d",
   "metadata": {},
   "source": [
    "### 4: Full V7 Fusion Model + Label-Specialist Heads\n",
    "\n",
    "This cell assembles the full V7 model:\n",
    "- Text encoder: ChemBERTa (from Cell 2)\n",
    "- Graph encoder: lightweight GIN (from Cell 3)\n",
    "- Descriptors branch: MLP (inside fusion)\n",
    "- Fusion: **cross-attention** (SMILES tokens query graph nodes), masked pooling\n",
    "- Heads:\n",
    "  - **Shared multi-label head** (12 logits)\n",
    "  - **Optionally**: 12 **label-specialist** heads (one-vs-rest heads) for ensembling\n",
    "\n",
    "What it saves/creates:\n",
    "- Model manifest: `v7/model/v7_fusion/config.json`\n",
    "- Ensemble folders (empty, ready for training): `v7/model/ensembles/<LABEL>/`\n",
    "\n",
    "Sanity test (no training):\n",
    "- Takes a tiny batch from `v7/data/prepared/train.npz`\n",
    "- Runs both **shared** and **specialist** forward passes\n",
    "- Prints tensor shapes and checks for NaNs\n",
    "\n",
    "> You can freeze/unfreeze encoders via helper methods for staged training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767d86bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ðŸ“ Saved fusion manifest â†’ v7\\model\\v7_fusion\\config.json\n",
      "ðŸ“ Ensemble label folders ready under: v7\\model\\ensembles\n",
      "Sanity:\n",
      "  shared logits:      (4, 12)  (B, 12)\n",
      "  specialist logits:  (4, 12)  (B, 12)\n",
      "  fused vector shape: (4, 768)  (B, 768)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Reuse modules from Phase 2 â€” Cell 1 and Cell 2/3\n",
    "# Expect these classes/functions already defined in your kernel:\n",
    "# - ChemBERTaEncoder (Cell 2)\n",
    "# - GraphGINEncoder  (Cell 3)\n",
    "# - CrossAttentionBlock, DescriptorMLP, FusionClassifier, masked_mean (Cell 1)\n",
    "# If you restarted, re-run those cells first.\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & constants\n",
    "# -----------------------------\n",
    "MODEL_DIR = Path(\"v7/model\")\n",
    "FUSION_DIR = MODEL_DIR / \"v7_fusion\"\n",
    "FUSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ENSEMBLE_DIR = MODEL_DIR / \"ensembles\"\n",
    "ENSEMBLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEXT_DIR = MODEL_DIR / \"text_encoder\"\n",
    "GRAPH_DIR = MODEL_DIR / \"graph_encoder\"\n",
    "\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "train_npz_path = PREP_DIR / \"train.npz\"\n",
    "manifest_path = PREP_DIR / \"dataset_manifest.json\"\n",
    "assert train_npz_path.exists() and manifest_path.exists(), \"Missing prepared data or manifest.\"\n",
    "\n",
    "with open(manifest_path) as f:\n",
    "    ds_manifest = json.load(f)\n",
    "LABEL_NAMES = ds_manifest[\"labels\"]\n",
    "N_LABELS = len(LABEL_NAMES)\n",
    "DESC_IN_DIM = ds_manifest[\"n_features\"]    # 208 features from RDKit\n",
    "FUSION_DIM = 256\n",
    "MAX_SEQ_LEN = 256\n",
    "MAX_NODES = 128\n",
    "DROPOUT = 0.1\n",
    "N_HEADS = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Label-specialist head\n",
    "# -----------------------------\n",
    "class LabelHead(nn.Module):\n",
    "    \"\"\"Small MLP head for a single label (binary logit). Input: fused (3*D).\"\"\"\n",
    "    def __init__(self, fused_dim: int, hidden: int = 256, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(fused_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: (B, fused_dim) -> (B, 1)\n",
    "        return self.net(z)\n",
    "\n",
    "# -----------------------------\n",
    "# Full Fusion Model\n",
    "# -----------------------------\n",
    "class V7FusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model that composes:\n",
    "      - text_encoder: ChemBERTaEncoder\n",
    "      - graph_encoder: GraphGINEncoder\n",
    "      - cross-attn + desc MLP + pooling to produce fused vector\n",
    "      - either shared multi-label head OR 12 specialist heads\n",
    "\n",
    "    Modes:\n",
    "      specialist=False (default): shared multi-label head â†’ (B,12)\n",
    "      specialist=True:  12 label heads (one-vs-rest) â†’ concat â†’ (B,12)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder: \"ChemBERTaEncoder\",\n",
    "        graph_encoder: \"GraphGINEncoder\",\n",
    "        desc_in_dim: int = DESC_IN_DIM,\n",
    "        dim: int = FUSION_DIM,\n",
    "        n_heads: int = N_HEADS,\n",
    "        n_labels: int = N_LABELS,\n",
    "        dropout: float = DROPOUT,\n",
    "        specialist: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.graph_encoder = graph_encoder\n",
    "        self.dim = dim\n",
    "        self.n_labels = n_labels\n",
    "        self.specialist = specialist\n",
    "\n",
    "        # Fusion components (reusing the exact modules from Cell 1 design)\n",
    "        self.cross = CrossAttentionBlock(dim=dim, n_heads=n_heads, p=dropout)\n",
    "        self.desc_mlp = DescriptorMLP(in_dim=desc_in_dim, out_dim=dim, hidden=256, p=dropout)\n",
    "\n",
    "        fused_dim = dim * 3  # [text_pool ; graph_pool ; desc_embed]\n",
    "\n",
    "        if specialist:\n",
    "            # 12 label-wise heads\n",
    "            self.label_heads = nn.ModuleList([LabelHead(fused_dim=fused_dim, hidden=dim, p=dropout) for _ in range(n_labels)])\n",
    "            self.shared_head = None\n",
    "        else:\n",
    "            # Shared multi-label head\n",
    "            self.shared_head = FusionClassifier(dim=dim, n_labels=n_labels, p=dropout)\n",
    "            self.label_heads = None\n",
    "\n",
    "    def freeze_text_backbone(self, n_unfrozen_layers: int = 0):\n",
    "        \"\"\"Freeze ChemBERTa backbone; keep proj/LN trainable; optionally unfreeze last N layers.\"\"\"\n",
    "        self.text_encoder.freeze_backbone(n_unfrozen_layers=n_unfrozen_layers)\n",
    "\n",
    "    def freeze_graph(self, freeze: bool = True):\n",
    "        for p in self.graph_encoder.parameters():\n",
    "            p.requires_grad = not freeze\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        smiles_list: List[str],\n",
    "        desc_feats: torch.Tensor,\n",
    "        max_seq_len: int = MAX_SEQ_LEN,\n",
    "        max_nodes: int = MAX_NODES,\n",
    "        return_intermediates: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[dict]]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          smiles_list: list[str] length B\n",
    "          desc_feats:  (B, DESC_IN_DIM) float tensor (already imputed/scaled)\n",
    "        Returns:\n",
    "          logits: (B, 12)\n",
    "          intermediates (optional dict)\n",
    "        \"\"\"\n",
    "        # Encode text & graph\n",
    "        text_tokens, text_mask = self.text_encoder(smiles_list, max_length=max_seq_len)  # (B,L,D), (B,L)\n",
    "        graph_nodes, graph_mask = self.graph_encoder(smiles_list, max_nodes=max_nodes)    # (B,N,D), (B,N)\n",
    "\n",
    "        # Ensure tensors on same device\n",
    "        text_tokens = text_tokens.to(device)\n",
    "        text_mask   = text_mask.to(device)\n",
    "        graph_nodes = graph_nodes.to(device)\n",
    "        graph_mask  = graph_mask.to(device)\n",
    "        desc_feats  = desc_feats.to(device)\n",
    "\n",
    "        # Cross-attention update of text tokens with graph context\n",
    "        text_attn = self.cross(text_tokens, text_mask, graph_nodes, graph_mask)  # (B,L,D)\n",
    "\n",
    "        # Descriptor embedding\n",
    "        desc_embed = self.desc_mlp(desc_feats)  # (B,D)\n",
    "\n",
    "        # Masked pools\n",
    "        text_pool  = masked_mean(text_attn,   text_mask,  dim=1)  # (B,D)\n",
    "        graph_pool = masked_mean(graph_nodes, graph_mask, dim=1)  # (B,D)\n",
    "        fused = torch.cat([text_pool, graph_pool, desc_embed], dim=-1)  # (B, 3D)\n",
    "\n",
    "        # Heads\n",
    "        if self.specialist:\n",
    "            logits_list = [head(fused) for head in self.label_heads]  # list of (B,1)\n",
    "            logits = torch.cat(logits_list, dim=1)                    # (B,12)\n",
    "        else:\n",
    "            logits = self.shared_head(text_attn, text_mask, graph_nodes, graph_mask, desc_embed)  # (B,12)\n",
    "\n",
    "        aux = None\n",
    "        if return_intermediates:\n",
    "            aux = {\n",
    "                \"text_tokens\": text_tokens,\n",
    "                \"text_attended\": text_attn,\n",
    "                \"graph_nodes\": graph_nodes,\n",
    "                \"desc_embed\": desc_embed,\n",
    "                \"text_pool\": text_pool,\n",
    "                \"graph_pool\": graph_pool,\n",
    "                \"fused\": fused,\n",
    "            }\n",
    "        return logits, aux\n",
    "\n",
    "# -----------------------------\n",
    "# Build text/graph encoders from previous cells\n",
    "# -----------------------------\n",
    "# These objects should exist if you ran Cell 2 and Cell 3; otherwise re-instantiate:\n",
    "try:\n",
    "    text_encoder\n",
    "except NameError:\n",
    "    # fallback: rebuild with defaults\n",
    "    text_encoder = ChemBERTaEncoder().to(device)\n",
    "\n",
    "try:\n",
    "    graph_encoder\n",
    "except NameError:\n",
    "    # probe node_in_dim like in Cell 3 if needed\n",
    "    from rdkit import Chem\n",
    "    def _probe_node_in_dim():\n",
    "        from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "        mol = Chem.MolFromSmiles(\"CCO\")\n",
    "        from math import isfinite\n",
    "        return 51  # fallback from previous cell config\n",
    "    graph_encoder = GraphGINEncoder(node_in_dim=_probe_node_in_dim(), hidden_dim=FUSION_DIM, n_layers=4, dropout=0.1).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Instantiate both variants (shared & specialist)\n",
    "# -----------------------------\n",
    "v7_shared = V7FusionModel(\n",
    "    text_encoder=text_encoder,\n",
    "    graph_encoder=graph_encoder,\n",
    "    desc_in_dim=DESC_IN_DIM,\n",
    "    dim=FUSION_DIM,\n",
    "    n_heads=N_HEADS,\n",
    "    n_labels=N_LABELS,\n",
    "    dropout=DROPOUT,\n",
    "    specialist=False,\n",
    ").to(device)\n",
    "\n",
    "v7_specialist = V7FusionModel(\n",
    "    text_encoder=text_encoder,\n",
    "    graph_encoder=graph_encoder,\n",
    "    desc_in_dim=DESC_IN_DIM,\n",
    "    dim=FUSION_DIM,\n",
    "    n_heads=N_HEADS,\n",
    "    n_labels=N_LABELS,\n",
    "    dropout=DROPOUT,\n",
    "    specialist=True,\n",
    ").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Save fusion config & create ensemble folders\n",
    "# -----------------------------\n",
    "fusion_manifest = {\n",
    "    \"labels\": LABEL_NAMES,\n",
    "    \"n_labels\": N_LABELS,\n",
    "    \"desc_in_dim\": DESC_IN_DIM,\n",
    "    \"fusion_dim\": FUSION_DIM,\n",
    "    \"n_heads\": N_HEADS,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"max_seq_len\": MAX_SEQ_LEN,\n",
    "    \"max_nodes\": MAX_NODES,\n",
    "    \"modes\": [\"shared\", \"specialist\"],\n",
    "    \"paths\": {\n",
    "        \"text_encoder_config\": str(TEXT_DIR / \"config.json\"),\n",
    "        \"graph_encoder_config\": str(GRAPH_DIR / \"config.json\"),\n",
    "        \"ensemble_root\": str(ENSEMBLE_DIR),\n",
    "    }\n",
    "}\n",
    "(FUSION_DIR / \"config.json\").write_text(json.dumps(fusion_manifest, indent=2))\n",
    "print(\"ðŸ“ Saved fusion manifest â†’\", FUSION_DIR / \"config.json\")\n",
    "\n",
    "# Create per-label ensemble directories (empty for now)\n",
    "for label in LABEL_NAMES:\n",
    "    (ENSEMBLE_DIR / label).mkdir(parents=True, exist_ok=True)\n",
    "print(\"ðŸ“ Ensemble label folders ready under:\", ENSEMBLE_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# ðŸ”Ž Sanity: small forward on real data (no training)\n",
    "# -----------------------------\n",
    "batch = np.load(train_npz_path, allow_pickle=True)\n",
    "smiles_batch = [str(s) for s in batch[\"smiles\"][:4].tolist()]\n",
    "desc_batch = torch.tensor(batch[\"X\"][:4], dtype=torch.float32, device=device)  # imputed+scaled\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Shared head\n",
    "    logits_shared, aux_shared = v7_shared(smiles_batch, desc_batch, return_intermediates=True)\n",
    "    # Specialist heads\n",
    "    logits_spec,  aux_spec   = v7_specialist(smiles_batch, desc_batch, return_intermediates=False)\n",
    "\n",
    "print(\"Sanity:\")\n",
    "print(\"  shared logits:     \", tuple(logits_shared.shape), \" (B, 12)\")\n",
    "print(\"  specialist logits: \", tuple(logits_spec.shape),   \" (B, 12)\")\n",
    "assert torch.isfinite(logits_shared).all() and torch.isfinite(logits_spec).all(), \"Found non-finite logits.\"\n",
    "\n",
    "# Optional peek at fused vector shape (for debugging/ablations)\n",
    "print(\"  fused vector shape:\", tuple(aux_shared[\"fused\"].shape), \" (B, 768)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f89a7c",
   "metadata": {},
   "source": [
    "## Phase 3 (Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c478f383",
   "metadata": {},
   "source": [
    "### 0: Hardware & Throughput Probe (fp32 vs AMP)\n",
    "\n",
    "This cell profiles:\n",
    "- GPU name/VRAM/CUDA/torch versions\n",
    "- Full-model forward throughput at batch sizes `[8, 16, 24, 32]`\n",
    "- Mixed precision (AMP) vs fp32\n",
    "- A quick fwd+backward step (1 iter) to estimate step time\n",
    "- Saves results:\n",
    "  - `v7/results/meta/hw_probe.json`\n",
    "  - `v7/results/meta/throughput_probe.csv`\n",
    "\n",
    "**Output:** A recommended batch size & precision mode for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3db2e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW: {\n",
      "  \"torch_version\": \"2.6.0+cu124\",\n",
      "  \"cuda_available\": true,\n",
      "  \"cuda_version\": \"12.4\",\n",
      "  \"device_name\": \"NVIDIA GeForce RTX 4070 Ti\",\n",
      "  \"total_vram_gb\": 11.99\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_2928\\3958611543.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else nullcontext()\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_2928\\3958611543.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_2928\\3958611543.py:220: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else nullcontext()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BS=8 | fwd fp32 0.0060s | fwd amp 0.0068s | step fp32 0.0507s | step amp 0.0336s | peak 0.87 GB\n",
      "BS=16 | fwd fp32 0.0100s | fwd amp 0.0095s | step fp32 0.0349s | step amp 0.0291s | peak 0.88 GB\n",
      "BS=24 | fwd fp32 0.0134s | fwd amp 0.0125s | step fp32 0.0439s | step amp 0.0400s | peak 0.98 GB\n",
      "BS=32 | fwd fp32 0.0167s | fwd amp 0.0133s | step fp32 0.0529s | step amp 0.0462s | peak 1.24 GB\n",
      "\n",
      "=== Probe Summary ===\n",
      "{\n",
      "  \"batch_size\": 32,\n",
      "  \"precision\": \"amp\",\n",
      "  \"note\": \"Use grad accumulation to reach higher effective batch if needed.\"\n",
      "}\n",
      "Results saved to: v7\\results\\meta\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Expect these to be defined from Phase 2:\n",
    "# - v7_shared (full model w/ shared head)\n",
    "# - text_encoder, graph_encoder\n",
    "# If not (e.g., after a restart), we'll try to rebuild minimal defaults.\n",
    "try:\n",
    "    v7_shared\n",
    "except NameError:\n",
    "    print(\"âš ï¸ v7_shared not found in memory â€” rebuilding minimal defaults.\")\n",
    "    # Minimal rebuild (assumes Phase 2 cells are available; else raise)\n",
    "    try:\n",
    "        text_encoder\n",
    "    except NameError:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        class ChemBERTaEncoder(nn.Module):\n",
    "            def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "                super().__init__()\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "                self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "                self.pad_token_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "                self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "                self.ln = nn.LayerNorm(fusion_dim)\n",
    "            def forward(self, smiles_list, max_length=256, add_special_tokens=True):\n",
    "                enc = self.tokenizer(list(smiles_list), padding=True, truncation=True, max_length=max_length, add_special_tokens=add_special_tokens, return_tensors=\"pt\")\n",
    "                input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "                out = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "                toks = self.ln(self.proj(out))\n",
    "                return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "    try:\n",
    "        graph_encoder\n",
    "    except NameError:\n",
    "        from rdkit import Chem\n",
    "        # Minimal graph encoder using same interfaces as before\n",
    "        ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "        def one_hot(v, choices): \n",
    "            z=[0]*len(choices); \n",
    "            if v in choices: z[choices.index(v)]=1\n",
    "            return z\n",
    "        def clamp_oh(v, lo, hi):\n",
    "            buckets=list(range(lo,hi+1))\n",
    "            if v<lo or v>hi: return [0]*len(buckets)+[1]\n",
    "            o=[0]*(len(buckets)+1); o[v-lo]=1; return o\n",
    "        def atom_features(atom):\n",
    "            hybs=[Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "            chir=[Chem.rdchem.ChiralType.CHI_UNSPECIFIED, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW, Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "            sym = atom.GetSymbol()\n",
    "            feat = one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST+[\"other\"])\n",
    "            feat += clamp_oh(atom.GetDegree(),0,5)\n",
    "            feat += clamp_oh(atom.GetFormalCharge(),-2,2)\n",
    "            feat += (one_hot(atom.GetHybridization(), hybs)+[0])\n",
    "            feat += [int(atom.GetIsAromatic())]\n",
    "            feat += [int(atom.IsInRing())]\n",
    "            feat += one_hot(atom.GetChiralTag(), chir)\n",
    "            feat += clamp_oh(atom.GetTotalNumHs(includeNeighbors=True),0,4)\n",
    "            feat += clamp_oh(atom.GetTotalValence(),0,5)\n",
    "            feat += [atom.GetMass()/200.0]\n",
    "            return feat\n",
    "        def smiles_to_graph(smi, max_nodes=128):\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None or mol.GetNumAtoms()==0:\n",
    "                return np.zeros((0,0),dtype=np.float32), np.zeros((0,0),dtype=np.float32)\n",
    "            feats = [atom_features(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "            x = np.asarray(feats, dtype=np.float32)\n",
    "            N = mol.GetNumAtoms()\n",
    "            adj = np.zeros((N,N), dtype=np.float32)\n",
    "            for b in mol.GetBonds():\n",
    "                i,j=b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "                adj[i,j]=1.0; adj[j,i]=1.0\n",
    "            if N>128: x=x[:128]; adj=adj[:128,:128]\n",
    "            return x, adj\n",
    "        def collate_graphs(smiles_batch):\n",
    "            graphs=[smiles_to_graph(s) for s in smiles_batch]\n",
    "            Nmax=max([g[0].shape[0] for g in graphs] + [1])\n",
    "            Fnode=graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "            B=len(graphs)\n",
    "            X=np.zeros((B,Nmax,Fnode),dtype=np.float32)\n",
    "            A=np.zeros((B,Nmax,Nmax),dtype=np.float32)\n",
    "            M=np.zeros((B,Nmax),dtype=np.int64)\n",
    "            for i,(x,a) in enumerate(graphs):\n",
    "                n=x.shape[0]; \n",
    "                if n==0: continue\n",
    "                X[i,:n,:]=x; A[i,:n,:n]=a; M[i,:n]=1\n",
    "            return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "        class GINLayer(nn.Module):\n",
    "            def __init__(self, h=256, p=0.1):\n",
    "                super().__init__()\n",
    "                self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "                self.mlp = nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "            def forward(self, x, adj, mask):\n",
    "                out=(1.0+self.eps)*x + torch.matmul(adj,x)\n",
    "                out=self.mlp(out)\n",
    "                return out*mask.unsqueeze(-1).to(out.dtype)\n",
    "        class GraphGINEncoder(nn.Module):\n",
    "            def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "                super().__init__()\n",
    "                self.inp=nn.Sequential(nn.Linear(node_in_dim,hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "                self.layers=nn.ModuleList([GINLayer(hidden_dim,p) for _ in range(n_layers)])\n",
    "                self.ln=nn.LayerNorm(hidden_dim)\n",
    "            def forward(self, smiles_list, max_nodes=128):\n",
    "                X,A,M=collate_graphs(smiles_list)\n",
    "                h=self.inp(X)\n",
    "                for layer in self.layers: h=layer(h,A,M)\n",
    "                return self.ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "    # Fusion pieces (masked_mean) minimal copy\n",
    "    def masked_mean(x, mask, dim=1):\n",
    "        mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "        denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "        return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "            self.ln  = nn.LayerNorm(dim)\n",
    "            self.do  = nn.Dropout(p)\n",
    "        def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "            Q=text_tokens.transpose(0,1); K=graph_nodes.transpose(0,1); V=graph_nodes.transpose(0,1)\n",
    "            kpm=(graph_mask==0)\n",
    "            attn,_=self.mha(Q,K,V, key_padding_mask=kpm)\n",
    "            attn=attn.transpose(0,1)\n",
    "            return self.ln(text_tokens + self.do(attn))\n",
    "\n",
    "    class DescriptorMLP(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.net=nn.Sequential(nn.Linear(in_dim,hidden), nn.GELU(), nn.Dropout(p), nn.Linear(hidden,out_dim), nn.GELU(), nn.Dropout(p))\n",
    "        def forward(self,x): return self.net(x)\n",
    "\n",
    "    class FusionClassifier(nn.Module):\n",
    "        def __init__(self, dim=256, n_labels=12, p=0.1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p), nn.Linear(dim*2, n_labels))\n",
    "        def forward(self, text_tokens, text_mask, graph_nodes, graph_mask, desc_embed):\n",
    "            text_pool = masked_mean(text_tokens, text_mask, 1)\n",
    "            graph_pool= masked_mean(graph_nodes, graph_mask, 1)\n",
    "            return self.net(torch.cat([text_pool, graph_pool, desc_embed], dim=-1))\n",
    "\n",
    "    class V7FusionModel(nn.Module):\n",
    "        def __init__(self, text_encoder, graph_encoder, desc_in_dim=208, dim=256, n_labels=12, n_heads=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.text_encoder=text_encoder\n",
    "            self.graph_encoder=graph_encoder\n",
    "            self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "            self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "            self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "        def forward(self, smiles_list, desc_feats, max_seq_len=256, max_nodes=128):\n",
    "            tt, tm = self.text_encoder(smiles_list, max_length=max_seq_len)\n",
    "            gn, gm = self.graph_encoder(smiles_list, max_nodes=max_nodes)\n",
    "            tt, tm, gn, gm, desc_feats = tt.to(device), tm.to(device), gn.to(device), gm.to(device), desc_feats.to(device)\n",
    "            tta = self.cross(tt, tm, gn, gm)\n",
    "            de  = self.desc_mlp(desc_feats)\n",
    "            logits = self.shared_head(tta, tm, gn, gm, de)\n",
    "            return logits\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    text_encoder = ChemBERTaEncoder().to(device)\n",
    "    graph_encoder= GraphGINEncoder().to(device)\n",
    "    v7_shared    = V7FusionModel(text_encoder, graph_encoder).to(device)\n",
    "\n",
    "# ---------------- Paths & data ----------------\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "RES_DIR  = Path(\"v7/results/meta\"); RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "train_npz = np.load(PREP_DIR / \"train.npz\", allow_pickle=True)\n",
    "\n",
    "smiles_train = [str(s) for s in train_npz[\"smiles\"].tolist()]\n",
    "X_train = torch.tensor(train_npz[\"X\"], dtype=torch.float32)\n",
    "\n",
    "# ---------------- HW info --------------------\n",
    "hw = {\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"cuda_version\": torch.version.cuda,\n",
    "    \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "    \"total_vram_gb\": round(torch.cuda.get_device_properties(0).total_memory/1024**3,2) if torch.cuda.is_available() else None,\n",
    "}\n",
    "(Path(RES_DIR / \"hw_probe.json\")).write_text(json.dumps(hw, indent=2))\n",
    "print(\"HW:\", json.dumps(hw, indent=2))\n",
    "\n",
    "# --------------- benchmark helpers -----------\n",
    "# --- Patch: unwrap model outputs to logits ---\n",
    "def _to_logits(out):\n",
    "    return out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "def bench_forward(model, smiles_batch, desc_batch, amp=False, iters=10, warmup=3):\n",
    "    from contextlib import nullcontext\n",
    "    model.eval()\n",
    "    use_amp = amp and torch.cuda.is_available()\n",
    "    ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else nullcontext()\n",
    "\n",
    "    with torch.no_grad(), ctx:\n",
    "        for _ in range(warmup):\n",
    "            _ = _to_logits(model(smiles_batch, desc_batch))\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad(), ctx:\n",
    "        for _ in range(iters):\n",
    "            _ = _to_logits(model(smiles_batch, desc_batch))\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "    return (time.time() - t0) / iters\n",
    "\n",
    "def bench_step(model, smiles_batch, desc_batch, amp=False):\n",
    "    from contextlib import nullcontext\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    use_amp = amp and torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else nullcontext()\n",
    "\n",
    "    # dummy targets for timing only\n",
    "    y = torch.rand((desc_batch.size(0), 12), device=desc_batch.device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    with ctx:\n",
    "        logits = _to_logits(model(smiles_batch, desc_batch))\n",
    "        loss = loss_fn(logits, y)\n",
    "    if scaler.is_enabled():\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt); scaler.update()\n",
    "    else:\n",
    "        loss.backward(); opt.step()\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "    return float(loss.item()), (time.time() - t0)\n",
    "\n",
    "# --- Re-run the driver loop & save artifacts again ---\n",
    "batch_sizes = [8, 16, 24, 32]\n",
    "results = []\n",
    "max_nodes_seen = 0\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    try:\n",
    "        smiles_bs = smiles_train[:bs]\n",
    "        X_bs = X_train[:bs].to(next(v7_shared.parameters()).device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gn, gm = graph_encoder(smiles_bs, max_nodes=128)\n",
    "            max_nodes_seen = max(max_nodes_seen, int(gm.sum(dim=1).max().item()))\n",
    "\n",
    "        t_f32 = bench_forward(v7_shared, smiles_bs, X_bs, amp=False, iters=10, warmup=3)\n",
    "        t_amp = bench_forward(v7_shared, smiles_bs, X_bs, amp=True,  iters=10, warmup=3)\n",
    "\n",
    "        loss_f32, step_f32 = bench_step(v7_shared, smiles_bs, X_bs, amp=False)\n",
    "        loss_amp,  step_amp = bench_step(v7_shared, smiles_bs, X_bs, amp=True)\n",
    "\n",
    "        mem_alloc = torch.cuda.max_memory_allocated()/1024**3 if torch.cuda.is_available() else None\n",
    "        if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        results.append({\n",
    "            \"batch_size\": bs,\n",
    "            \"forward_fp32_s_per_batch\": round(t_f32, 4),\n",
    "            \"forward_amp_s_per_batch\":  round(t_amp, 4),\n",
    "            \"trainstep_fp32_s\": round(step_f32, 4),\n",
    "            \"trainstep_amp_s\":  round(step_amp, 4),\n",
    "            \"approx_samples_per_s_fp32\": round(bs / t_f32, 2),\n",
    "            \"approx_samples_per_s_amp\":  round(bs / t_amp, 2),\n",
    "            \"peak_mem_gb\": round(mem_alloc, 2) if mem_alloc is not None else None,\n",
    "        })\n",
    "        print(f\"BS={bs} | fwd fp32 {t_f32:.4f}s | fwd amp {t_amp:.4f}s | step fp32 {step_f32:.4f}s | step amp {step_amp:.4f}s | peak {results[-1]['peak_mem_gb']} GB\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"OOM at batch size {bs}.\")\n",
    "            results.append({\"batch_size\": bs, \"error\": \"OOM\"})\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Save again\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RES_DIR / \"throughput_probe.csv\", index=False)\n",
    "(Path(RES_DIR / \"throughput_probe.json\")).write_text(json.dumps(results, indent=2))\n",
    "\n",
    "valid = [r for r in results if r.get(\"error\") is None]\n",
    "if valid:\n",
    "    succ_amp = [r for r in valid if r.get(\"forward_amp_s_per_batch\") is not None]\n",
    "    if succ_amp:\n",
    "        rec_bs = max(succ_amp, key=lambda r: r[\"batch_size\"])[\"batch_size\"]\n",
    "        rec_prec = \"amp\"\n",
    "    else:\n",
    "        rec_bs = max(valid, key=lambda r: r[\"batch_size\"])[\"batch_size\"]\n",
    "        rec_prec = \"fp32\"\n",
    "else:\n",
    "    rec_bs, rec_prec = 8, \"amp\"\n",
    "\n",
    "summary = {\n",
    "    \"hw\": hw,\n",
    "    \"max_nodes_seen\": int(max_nodes_seen),\n",
    "    \"results\": results,\n",
    "    \"recommendation\": {\n",
    "        \"batch_size\": int(rec_bs),\n",
    "        \"precision\": rec_prec,\n",
    "        \"note\": \"Use grad accumulation to reach higher effective batch if needed.\"\n",
    "    }\n",
    "}\n",
    "(Path(RES_DIR / \"hw_probe_summary.json\")).write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"\\n=== Probe Summary ===\")\n",
    "print(json.dumps(summary[\"recommendation\"], indent=2))\n",
    "print(f\"Results saved to: {RES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac28706",
   "metadata": {},
   "source": [
    "### 1: Shared-Head Training Loop\n",
    "\n",
    "This cell trains the **shared multi-label V7 fusion model** with strong settings:\n",
    "\n",
    "- Windows-safe DataLoaders (`num_workers=0`)\n",
    "- Batch size 32, **grad accumulation 4** (effective 128)\n",
    "- **ASL** (gamma_neg=5.0, gamma_pos=1.0) + **class-balanced per-label weights**\n",
    "- **AMP** mixed precision, **EMA**, cosine LR with warmup\n",
    "- **Stage A** (8 epochs): ChemBERTa backbone **frozen**\n",
    "- **Stage B** (20 epochs): unfreeze **last 2** transformer blocks\n",
    "- Early stopping on **val macro PR-AUC**\n",
    "- Checkpoints:\n",
    "  - best â†’ `v7/model/checkpoints/shared/best.pt`\n",
    "  - per-epoch â†’ `v7/model/checkpoints/shared/epoch_{stage}{epoch:02d}.pt`\n",
    "- Logs:\n",
    "  - training rows â†’ `v7/results/logs/train_log.jsonl`\n",
    "  - val summaries â†’ `v7/results/artifacts/val_metrics.jsonl`\n",
    "  - run config â†’ `v7/results/meta/train_run_config.json`\n",
    "\n",
    "> Run this cell to start training. Weâ€™ll add plots + specialist heads next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584ddb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Train batches: 195, Val batches: 25\n",
      "Train label prevalence: [0.047 0.038 0.128 0.051 0.135 0.055 0.027 0.165 0.037 0.056 0.167 0.061]\n",
      "Class-balanced alpha: [1.077 1.385 0.53  1.232 0.53  0.985 1.995 0.48  1.367 1.022 0.479 0.919]\n",
      "\n",
      "=== Stage A: Warm-up (text backbone frozen) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 1, 'train_loss': 0.035231313520135026, 'train_macro_pr_auc': 0.15394772991859337, 'train_macro_roc_auc': 0.6320955267390821, 'val_loss': 0.16247534304857253, 'val_macro_pr_auc': 0.060688107343346155, 'val_macro_roc_auc': 0.4460254985837288, 'epoch_time_s': 5.02}\n",
      "  âœ… New best (Stage A) macro PR-AUC: 0.0607 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:39] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 2, 'train_loss': 0.03287650308547876, 'train_macro_pr_auc': 0.2207926681933469, 'train_macro_roc_auc': 0.7010044653688422, 'val_loss': 0.1518526303768158, 'val_macro_pr_auc': 0.07123903216471082, 'val_macro_roc_auc': 0.48648823084936604, 'epoch_time_s': 4.77}\n",
      "  âœ… New best (Stage A) macro PR-AUC: 0.0712 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:47] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 3, 'train_loss': 0.03286819191506276, 'train_macro_pr_auc': 0.2186634655024724, 'train_macro_roc_auc': 0.7048009936671273, 'val_loss': 0.14330618798732758, 'val_macro_pr_auc': 0.0828460714633456, 'val_macro_roc_auc': 0.5262641855361049, 'epoch_time_s': 4.74}\n",
      "  âœ… New best (Stage A) macro PR-AUC: 0.0828 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:49] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 4, 'train_loss': 0.0327842725870701, 'train_macro_pr_auc': 0.21544627600788266, 'train_macro_roc_auc': 0.7084898348195994, 'val_loss': 0.13847267180681228, 'val_macro_pr_auc': 0.10282287714227774, 'val_macro_roc_auc': 0.5629917021669618, 'epoch_time_s': 4.75}\n",
      "  âœ… New best (Stage A) macro PR-AUC: 0.1028 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 5, 'train_loss': 0.032721771142230585, 'train_macro_pr_auc': 0.217635243463428, 'train_macro_roc_auc': 0.7083933660695082, 'val_loss': 0.13518619030714035, 'val_macro_pr_auc': 0.12165443303340044, 'val_macro_roc_auc': 0.5874946897864975, 'epoch_time_s': 4.8}\n",
      "  âœ… New best (Stage A) macro PR-AUC: 0.1217 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:00] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 6, 'train_loss': 0.03289635310379358, 'train_macro_pr_auc': 0.20782836636565902, 'train_macro_roc_auc': 0.7021579445824658, 'val_loss': 0.13288636565208434, 'val_macro_pr_auc': 0.13197449463464273, 'val_macro_roc_auc': 0.6008043797614259, 'epoch_time_s': 4.82}\n",
      "  âœ… New best (Stage A) macro PR-AUC: 0.1320 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:05] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 7, 'train_loss': 0.033106887522034154, 'train_macro_pr_auc': 0.20516994149759463, 'train_macro_roc_auc': 0.6974444627266895, 'val_loss': 0.13136810392141343, 'val_macro_pr_auc': 0.13953489453248116, 'val_macro_roc_auc': 0.6104861465660406, 'epoch_time_s': 4.73}\n",
      "  âœ… New best (Stage A) macro PR-AUC: 0.1395 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:11] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 8, 'train_loss': 0.033446282559098345, 'train_macro_pr_auc': 0.19800331001972962, 'train_macro_roc_auc': 0.6793394336970406, 'val_loss': 0.13041243970394134, 'val_macro_pr_auc': 0.14403884369407427, 'val_macro_roc_auc': 0.6171125914864409, 'epoch_time_s': 4.75}\n",
      "  âœ… New best (Stage A) macro PR-AUC: 0.1440 â†’ checkpoint saved.\n",
      "\n",
      "=== Stage B: Finetune (unfreeze last 2 ChemBERTa layers) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:19] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 1, 'train_loss': 0.033351336682263096, 'train_macro_pr_auc': 0.20295440301231638, 'train_macro_roc_auc': 0.6865177545804902, 'val_loss': 0.12948915004730224, 'val_macro_pr_auc': 0.14860819775385747, 'val_macro_roc_auc': 0.6244653691925603, 'epoch_time_s': 5.99}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.1486 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:25] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 2, 'train_loss': 0.0320538672976769, 'train_macro_pr_auc': 0.2409792810558551, 'train_macro_roc_auc': 0.7315953583704417, 'val_loss': 0.12786235362291337, 'val_macro_pr_auc': 0.15892553431027442, 'val_macro_roc_auc': 0.638336387427298, 'epoch_time_s': 5.89}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.1589 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:27] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 3, 'train_loss': 0.031573516235519676, 'train_macro_pr_auc': 0.25877013362413176, 'train_macro_roc_auc': 0.7455078833992309, 'val_loss': 0.1264248350262642, 'val_macro_pr_auc': 0.16992076406025358, 'val_macro_roc_auc': 0.6545090711061684, 'epoch_time_s': 5.95}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.1699 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:33] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 4, 'train_loss': 0.03169219841559728, 'train_macro_pr_auc': 0.2535870353864961, 'train_macro_roc_auc': 0.7403325204304396, 'val_loss': 0.12525496780872344, 'val_macro_pr_auc': 0.17670751058022716, 'val_macro_roc_auc': 0.664954602299176, 'epoch_time_s': 5.94}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.1767 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:44] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 5, 'train_loss': 0.0309824329872544, 'train_macro_pr_auc': 0.2783762235540493, 'train_macro_roc_auc': 0.7612776006180176, 'val_loss': 0.12415830075740814, 'val_macro_pr_auc': 0.18558657990269578, 'val_macro_roc_auc': 0.6738228522983359, 'epoch_time_s': 6.02}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.1856 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 6, 'train_loss': 0.031206133656012706, 'train_macro_pr_auc': 0.27829100621751796, 'train_macro_roc_auc': 0.755299053293554, 'val_loss': 0.1231931608915329, 'val_macro_pr_auc': 0.19293831224054891, 'val_macro_roc_auc': 0.6810440461639281, 'epoch_time_s': 5.92}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.1929 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 7, 'train_loss': 0.031034588914078017, 'train_macro_pr_auc': 0.2810264936485967, 'train_macro_roc_auc': 0.7623333892543859, 'val_loss': 0.1224514576792717, 'val_macro_pr_auc': 0.1996031092407037, 'val_macro_roc_auc': 0.6881710678940766, 'epoch_time_s': 5.98}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.1996 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:03] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 8, 'train_loss': 0.030679168958121384, 'train_macro_pr_auc': 0.29096576640307903, 'train_macro_roc_auc': 0.7672944482343568, 'val_loss': 0.12180023938417435, 'val_macro_pr_auc': 0.20525084567553417, 'val_macro_roc_auc': 0.6943129844375328, 'epoch_time_s': 5.98}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2053 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:03] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 9, 'train_loss': 0.030749009086344488, 'train_macro_pr_auc': 0.286702373022353, 'train_macro_roc_auc': 0.769002218430392, 'val_loss': 0.12127423912286758, 'val_macro_pr_auc': 0.21246464164262072, 'val_macro_roc_auc': 0.6987706221166011, 'epoch_time_s': 5.92}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2125 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:13] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 10, 'train_loss': 0.03075902950591766, 'train_macro_pr_auc': 0.2865881383643247, 'train_macro_roc_auc': 0.7706060849873775, 'val_loss': 0.12079172015190125, 'val_macro_pr_auc': 0.21125299214080603, 'val_macro_roc_auc': 0.7026318061256759, 'epoch_time_s': 6.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:17] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 11, 'train_loss': 0.03074433825050409, 'train_macro_pr_auc': 0.2866102209753219, 'train_macro_roc_auc': 0.767944986693523, 'val_loss': 0.12046253174543381, 'val_macro_pr_auc': 0.2160076280215174, 'val_macro_roc_auc': 0.7052656884511049, 'epoch_time_s': 5.99}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2160 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:24] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 12, 'train_loss': 0.03064479235177621, 'train_macro_pr_auc': 0.29587938734080016, 'train_macro_roc_auc': 0.7747545667274607, 'val_loss': 0.1201474142074585, 'val_macro_pr_auc': 0.21815972659022878, 'val_macro_roc_auc': 0.7084575461077836, 'epoch_time_s': 6.23}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2182 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:29] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 13, 'train_loss': 0.03051889530645731, 'train_macro_pr_auc': 0.3014124548450356, 'train_macro_roc_auc': 0.7754224122799486, 'val_loss': 0.11987470299005508, 'val_macro_pr_auc': 0.22021944101515215, 'val_macro_roc_auc': 0.7103294667602594, 'epoch_time_s': 6.24}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2202 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 14, 'train_loss': 0.03055528616771484, 'train_macro_pr_auc': 0.30036792174015225, 'train_macro_roc_auc': 0.7770674850742396, 'val_loss': 0.11969551861286164, 'val_macro_pr_auc': 0.2204572735624223, 'val_macro_roc_auc': 0.7117377024199903, 'epoch_time_s': 6.02}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2205 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:43] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 15, 'train_loss': 0.030464942605258564, 'train_macro_pr_auc': 0.3004765438036446, 'train_macro_roc_auc': 0.777889729862784, 'val_loss': 0.11952996402978897, 'val_macro_pr_auc': 0.22635373887372232, 'val_macro_roc_auc': 0.7136480190734761, 'epoch_time_s': 5.93}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2264 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 16, 'train_loss': 0.030600831323327162, 'train_macro_pr_auc': 0.2948815446342145, 'train_macro_roc_auc': 0.7766768345697753, 'val_loss': 0.11940244868397713, 'val_macro_pr_auc': 0.22757343295334945, 'val_macro_roc_auc': 0.7147992546108844, 'epoch_time_s': 5.95}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2276 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 17, 'train_loss': 0.0306711228707662, 'train_macro_pr_auc': 0.29773487707514557, 'train_macro_roc_auc': 0.7771101181372019, 'val_loss': 0.11931319043040275, 'val_macro_pr_auc': 0.2284273391126127, 'val_macro_roc_auc': 0.7155680955816607, 'epoch_time_s': 5.92}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2284 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:45:01] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 18, 'train_loss': 0.030714609569463973, 'train_macro_pr_auc': 0.29627989863976345, 'train_macro_roc_auc': 0.7806201270680816, 'val_loss': 0.11927412450313568, 'val_macro_pr_auc': 0.23048184628064763, 'val_macro_roc_auc': 0.7158172353803324, 'epoch_time_s': 5.94}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2305 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:45:10] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 19, 'train_loss': 0.030844925281902153, 'train_macro_pr_auc': 0.29332838681471207, 'train_macro_roc_auc': 0.7770113536906883, 'val_loss': 0.11922193139791488, 'val_macro_pr_auc': 0.23035495448753893, 'val_macro_roc_auc': 0.7166446565031673, 'epoch_time_s': 5.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:45:16] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 20, 'train_loss': 0.030895793051100694, 'train_macro_pr_auc': 0.28553649395747516, 'train_macro_roc_auc': 0.7788041242291293, 'val_loss': 0.11917937502264976, 'val_macro_pr_auc': 0.2308804674395504, 'val_macro_roc_auc': 0.7172780654968326, 'epoch_time_s': 6.15}\n",
      "  âœ… New best (Stage B) macro PR-AUC: 0.2309 â†’ checkpoint saved.\n",
      "\n",
      "ðŸŽ¯ Training complete. Best macro PR-AUC: 0.2309 | Best ckpt: v7\\model\\checkpoints\\shared\\best.pt\n",
      "Logs â†’ v7\\results\\logs\\train_log.jsonl\n",
      "Val metrics â†’ v7\\results\\artifacts\\val_metrics.jsonl\n",
      "Checkpoints â†’ v7\\model\\checkpoints\\shared\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time, random, platform\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "# ---------------------------\n",
    "# Env & reproducibility\n",
    "# ---------------------------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # quieter HF tokenizer\n",
    "def seed_everything(seed=42):\n",
    "    import numpy as _np, random as _r, torch as _t\n",
    "    _r.seed(seed); _np.random.seed(seed)\n",
    "    _t.manual_seed(seed); _t.cuda.manual_seed_all(seed)\n",
    "    _t.backends.cudnn.deterministic = False\n",
    "    _t.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# Paths & setup\n",
    "# ---------------------------\n",
    "BASE_DIR   = Path(\"v7\")\n",
    "DATA_PREP  = BASE_DIR / \"data\" / \"prepared\"\n",
    "RESULTS_DIR= BASE_DIR / \"results\"\n",
    "LOGS_DIR   = RESULTS_DIR / \"logs\"\n",
    "ARTIF_DIR  = RESULTS_DIR / \"artifacts\"\n",
    "META_DIR   = RESULTS_DIR / \"meta\"\n",
    "CKPT_DIR   = BASE_DIR / \"model\" / \"checkpoints\" / \"shared\"\n",
    "for d in [RESULTS_DIR, LOGS_DIR, ARTIF_DIR, META_DIR, CKPT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(DATA_PREP / \"dataset_manifest.json\") as f:\n",
    "    ds_manifest = json.load(f)\n",
    "LABEL_NAMES  = ds_manifest[\"labels\"]\n",
    "N_LABELS     = len(LABEL_NAMES)\n",
    "DESC_IN_DIM  = ds_manifest[\"n_features\"]\n",
    "\n",
    "# Expect v7_shared in memory (from Phase 2 â€” Cell 4)\n",
    "try:\n",
    "    v7_shared\n",
    "except NameError:\n",
    "    raise RuntimeError(\"v7_shared model not found. Please re-run Phase 2 (Cells 1â€“4) in this kernel.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Data: Dataset & Loader (Windows-safe)\n",
    "# ---------------------------\n",
    "class Tox21NPZDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, npz_path: Path):\n",
    "        b = np.load(npz_path, allow_pickle=True)\n",
    "        self.X = b[\"X\"].astype(np.float32)             # (N, F)\n",
    "        self.Y = b[\"Y\"].astype(np.float32)             # (N, 12), may contain NaN\n",
    "        self.mask_y = b[\"y_missing_mask\"].astype(bool) # True where NaN\n",
    "        self.smiles = b[\"smiles\"].tolist()\n",
    "        self.mol_ids= b[\"mol_id\"].tolist()\n",
    "        assert self.X.shape[0] == self.Y.shape[0] == len(self.smiles)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"smiles\": self.smiles[i],\n",
    "            \"desc\": torch.from_numpy(self.X[i]),\n",
    "            \"y\": torch.from_numpy(self.Y[i]),\n",
    "            \"y_mask\": torch.from_numpy(self.mask_y[i]),\n",
    "            \"mol_id\": self.mol_ids[i],\n",
    "        }\n",
    "\n",
    "def collate(batch):\n",
    "    smiles = [b[\"smiles\"] for b in batch]\n",
    "    desc   = torch.stack([b[\"desc\"] for b in batch], dim=0)\n",
    "    y      = torch.stack([b[\"y\"] for b in batch], dim=0)\n",
    "    y_mask = torch.stack([b[\"y_mask\"] for b in batch], dim=0)\n",
    "    mol_id = [b[\"mol_id\"] for b in batch]\n",
    "    return {\"smiles\": smiles, \"desc\": desc, \"y\": y, \"y_mask\": y_mask, \"mol_id\": mol_id}\n",
    "\n",
    "train_ds = Tox21NPZDataset(DATA_PREP / \"train.npz\")\n",
    "val_ds   = Tox21NPZDataset(DATA_PREP / \"val.npz\")\n",
    "\n",
    "IS_WINDOWS = platform.system() == \"Windows\"\n",
    "NUM_WORKERS = 0 if IS_WINDOWS else max(0, min(4, (os.cpu_count() or 2)//2))\n",
    "PERSISTENT = False\n",
    "\n",
    "# Batch & precision (from your probe)\n",
    "BATCH_SIZE  = 32\n",
    "GRAD_ACCUM  = 4      # effective batch 128\n",
    "USE_AMP     = True\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"),\n",
    "    collate_fn=collate, drop_last=True,\n",
    "    persistent_workers=(PERSISTENT if NUM_WORKERS>0 else False),\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"),\n",
    "    collate_fn=collate,\n",
    "    persistent_workers=(PERSISTENT if NUM_WORKERS>0 else False),\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Label stats & weights\n",
    "# ---------------------------\n",
    "def compute_label_stats(y: np.ndarray, mask_missing: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    valid = ~mask_missing\n",
    "    pos = np.nansum((y == 1) & valid, axis=0)\n",
    "    neg = np.nansum((y == 0) & valid, axis=0)\n",
    "    total = pos + neg\n",
    "    prevalence = np.divide(pos, np.maximum(total, 1))\n",
    "    return {\"pos\": pos.astype(int), \"neg\": neg.astype(int), \"total\": total.astype(int), \"prevalence\": prevalence}\n",
    "\n",
    "train_blob = np.load(DATA_PREP / \"train.npz\", allow_pickle=True)\n",
    "train_stats = compute_label_stats(train_blob[\"Y\"], train_blob[\"y_missing_mask\"])\n",
    "print(\"Train label prevalence:\", np.round(train_stats[\"prevalence\"], 3))\n",
    "\n",
    "def effective_number_weights(pos_counts: np.ndarray, beta: float = 0.999) -> np.ndarray:\n",
    "    eps = 1e-8\n",
    "    eff_num = (1 - np.power(beta, pos_counts + eps)) / (1 - beta)\n",
    "    alpha = 1.0 / np.maximum(eff_num, eps)\n",
    "    alpha = alpha / (np.mean(alpha) + eps)\n",
    "    return alpha.astype(np.float32)\n",
    "\n",
    "alpha_cb = effective_number_weights(train_stats[\"pos\"])\n",
    "print(\"Class-balanced alpha:\", np.round(alpha_cb, 3))\n",
    "alpha_cb_t = torch.tensor(alpha_cb, device=device)\n",
    "\n",
    "# ---------------------------\n",
    "# Loss: Asymmetric Loss (stronger)\n",
    "# ---------------------------\n",
    "class AsymmetricLossCB(nn.Module):\n",
    "    def __init__(self, gamma_neg=5.0, gamma_pos=1.0, clip=0.05, alpha: Optional[torch.Tensor]=None):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.alpha = alpha  # (n_labels,) on device\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor, missing_mask: torch.Tensor):\n",
    "        # mask out missing labels\n",
    "        valid = ~missing_mask\n",
    "        if not valid.any():\n",
    "            return logits.new_tensor(0.0)\n",
    "\n",
    "        logits_v  = logits[valid]\n",
    "        targets_v = targets[valid]\n",
    "\n",
    "        pred = torch.sigmoid(logits_v)\n",
    "        if self.clip:\n",
    "            pred = torch.clamp(pred, self.clip, 1 - self.clip)\n",
    "\n",
    "        anti_targets = 1 - targets_v\n",
    "        pt = pred * targets_v + (1 - pred) * anti_targets\n",
    "        one_sided_gamma = self.gamma_pos * targets_v + self.gamma_neg * anti_targets\n",
    "        focal_weight = torch.pow(1 - pt, one_sided_gamma)\n",
    "\n",
    "        loss = - (targets_v * torch.log(pred) + (1 - targets_v) * torch.log(1 - pred))\n",
    "        loss = loss * focal_weight\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            n_labels = logits.size(1)\n",
    "            label_idx_full = torch.arange(n_labels, device=logits.device).unsqueeze(0).expand_as(missing_mask)\n",
    "            label_idx_valid = label_idx_full[valid]\n",
    "            alpha_vec = self.alpha[label_idx_valid]\n",
    "            loss = loss * alpha_vec\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "criterion = AsymmetricLossCB(gamma_neg=5.0, gamma_pos=1.0, clip=0.05, alpha=alpha_cb_t)\n",
    "\n",
    "# ---------------------------\n",
    "# Optimizer, EMA, Scheduler\n",
    "# ---------------------------\n",
    "def build_optimizer(model, lr=3e-4, wd=1e-2):\n",
    "    return optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=lr, weight_decay=wd)\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[n] = p.detach().clone()\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
    "    @torch.no_grad()\n",
    "    def copy_to(self, model: nn.Module):\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                p.data.copy_(self.shadow[n])\n",
    "\n",
    "def build_warmup_cosine(total_steps, warmup_ratio=0.1, min_lr_scale=0.1):\n",
    "    def lr_lambda(step):\n",
    "        warm = int(total_steps * warmup_ratio)\n",
    "        if step < warm:\n",
    "            return float(step) / max(1, warm)\n",
    "        progress = (step - warm) / max(1, total_steps - warm)\n",
    "        cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return min_lr_scale + (1 - min_lr_scale) * cosine\n",
    "    return lr_lambda\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics (macro PR-AUC primary)\n",
    "# ---------------------------\n",
    "def eval_metrics(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
    "    pr_aucs, roc_aucs = [], []\n",
    "    for j in range(y_true.shape[1]):\n",
    "        mask = ~np.isnan(y_true[:, j])\n",
    "        if mask.sum() < 2 or len(np.unique(y_true[mask, j])) < 2:\n",
    "            pr_aucs.append(np.nan); roc_aucs.append(np.nan); continue\n",
    "        try:\n",
    "            pr_aucs.append(average_precision_score(y_true[mask, j], y_prob[mask, j]))\n",
    "        except Exception:\n",
    "            pr_aucs.append(np.nan)\n",
    "        try:\n",
    "            roc_aucs.append(roc_auc_score(y_true[mask, j], y_prob[mask, j]))\n",
    "        except Exception:\n",
    "            roc_aucs.append(np.nan)\n",
    "    return {\"macro_pr_auc\": float(np.nanmean(pr_aucs)),\n",
    "            \"macro_roc_auc\": float(np.nanmean(roc_aucs))}\n",
    "\n",
    "# ---------------------------\n",
    "# Stage settings (stronger)\n",
    "# ---------------------------\n",
    "MAX_EPOCHS_STAGE_A = 8\n",
    "MAX_EPOCHS_STAGE_B = 20\n",
    "UNFREEZE_LAST_N    = 2\n",
    "EARLY_STOP_PATIENCE= 5\n",
    "EMA_DECAY          = 0.999\n",
    "LR                 = 3e-4\n",
    "WEIGHT_DECAY       = 1e-2\n",
    "WARMUP_RATIO       = 0.1\n",
    "CLIP_NORM          = 1.0\n",
    "\n",
    "# Freeze text backbone for Stage A\n",
    "v7_shared.freeze_text_backbone(n_unfrozen_layers=0)\n",
    "v7_shared.freeze_graph(freeze=False)  # keep graph trainable\n",
    "\n",
    "# Save run config for reproducibility\n",
    "run_cfg = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"grad_accum\": GRAD_ACCUM,\n",
    "    \"use_amp\": USE_AMP,\n",
    "    \"max_epochs\": {\"stage_a\": MAX_EPOCHS_STAGE_A, \"stage_b\": MAX_EPOCHS_STAGE_B},\n",
    "    \"optimizer\": {\"lr\": LR, \"weight_decay\": WEIGHT_DECAY},\n",
    "    \"scheduler\": {\"type\": \"warmup_cosine\", \"warmup_ratio\": WARMUP_RATIO},\n",
    "    \"asl\": {\"gamma_neg\": 5.0, \"gamma_pos\": 1.0, \"clip\": 0.05},\n",
    "    \"ema_decay\": EMA_DECAY,\n",
    "    \"unfreeze_last_n\": UNFREEZE_LAST_N,\n",
    "    \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "    \"clip_norm\": CLIP_NORM,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "}\n",
    "(META_DIR / \"train_run_config.json\").write_text(json.dumps(run_cfg, indent=2))\n",
    "\n",
    "# ---------------------------\n",
    "# AMP scaler\n",
    "# ---------------------------\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(USE_AMP and device.type==\"cuda\"))\n",
    "\n",
    "# ---------------------------\n",
    "# One epoch\n",
    "# ---------------------------\n",
    "def run_epoch(model: nn.Module, loader, optimizer, scheduler, ema: Optional[EMA], stage_name=\"train\"):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(mode=is_train)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    steps = 0\n",
    "    all_probs, all_true, all_mask = [], [], []\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        smiles = batch[\"smiles\"]\n",
    "        desc   = batch[\"desc\"].to(device, non_blocking=True)\n",
    "        y      = batch[\"y\"].to(device, non_blocking=True)\n",
    "        y_mask = batch[\"y_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "        ctx = torch.amp.autocast(\"cuda\", dtype=torch.float16) if (USE_AMP and device.type==\"cuda\") else torch.autocast(device_type=\"cpu\", enabled=False)\n",
    "        with ctx:\n",
    "            logits, _ = v7_shared(smiles, desc, return_intermediates=False)  # (B,12)\n",
    "            loss = criterion(logits, y, y_mask)\n",
    "\n",
    "        if is_train:\n",
    "            loss = loss / GRAD_ACCUM\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % GRAD_ACCUM == 0:\n",
    "                # gradient clipping\n",
    "                if CLIP_NORM is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(v7_shared.parameters(), CLIP_NORM)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "\n",
    "        running_loss += float(loss.item()) * GRAD_ACCUM\n",
    "        steps += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            y_np  = y.detach().cpu().numpy()\n",
    "            m_np  = y_mask.detach().cpu().numpy()\n",
    "            all_probs.append(probs); all_true.append(y_np); all_mask.append(m_np)\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_true  = np.concatenate(all_true, axis=0)\n",
    "    all_mask  = np.concatenate(all_mask, axis=0)\n",
    "    all_true  = np.where(all_mask, np.nan, all_true)  # ignore missing labels\n",
    "\n",
    "    metrics = eval_metrics(all_true, all_probs)\n",
    "    return running_loss / max(1, steps), metrics\n",
    "\n",
    "# ---------------------------\n",
    "# Orchestrate both stages with early stopping\n",
    "# ---------------------------\n",
    "def save_epoch_ckpt(stage_tag: str, epoch: int, ema: EMA, path: Path):\n",
    "    torch.save({\n",
    "        \"model\": v7_shared.state_dict(),\n",
    "        \"ema\": ema.shadow,\n",
    "        \"config\": {\"stage\": stage_tag, \"epoch\": epoch},\n",
    "    }, path)\n",
    "\n",
    "def train_shared_model():\n",
    "    best_metric = -1.0\n",
    "    best_path   = CKPT_DIR / \"best.pt\"\n",
    "    log_path    = LOGS_DIR / \"train_log.jsonl\"\n",
    "    val_log_path= ARTIF_DIR / \"val_metrics.jsonl\"\n",
    "    for p in [log_path, val_log_path]:\n",
    "        if p.exists(): p.unlink()\n",
    "\n",
    "    # ===== Stage A: warm-up (text frozen) =====\n",
    "    print(\"\\n=== Stage A: Warm-up (text backbone frozen) ===\")\n",
    "    opt = build_optimizer(v7_shared, lr=LR, wd=WEIGHT_DECAY)\n",
    "    total_steps = math.ceil(len(train_loader) / GRAD_ACCUM) * MAX_EPOCHS_STAGE_A\n",
    "    scheduler = LambdaLR(opt, build_warmup_cosine(total_steps, warmup_ratio=WARMUP_RATIO))\n",
    "    ema = EMA(v7_shared, decay=EMA_DECAY)\n",
    "    patience = EARLY_STOP_PATIENCE\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS_STAGE_A + 1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_metrics = run_epoch(v7_shared, train_loader, opt, scheduler, ema, stage_name=\"train\")\n",
    "        with torch.no_grad():\n",
    "            ema.copy_to(v7_shared)\n",
    "            va_loss, va_metrics = run_epoch(v7_shared, val_loader, optimizer=None, scheduler=None, ema=None, stage_name=\"val\")\n",
    "\n",
    "        row = {\n",
    "            \"stage\": \"A\", \"epoch\": epoch,\n",
    "            \"train_loss\": tr_loss, **{f\"train_{k}\": v for k,v in tr_metrics.items()},\n",
    "            \"val_loss\": va_loss, **{f\"val_{k}\": v for k,v in va_metrics.items()},\n",
    "            \"epoch_time_s\": round(time.time()-t0, 2),\n",
    "        }\n",
    "        with open(log_path, \"a\") as f: f.write(json.dumps(row) + \"\\n\")\n",
    "        with open(val_log_path, \"a\") as f: f.write(json.dumps({\"stage\":\"A\", \"epoch\": epoch, **va_metrics}) + \"\\n\")\n",
    "        print(row)\n",
    "\n",
    "        # Save per-epoch checkpoint\n",
    "        save_epoch_ckpt(\"A\", epoch, ema, CKPT_DIR / f\"epoch_A{epoch:02d}.pt\")\n",
    "\n",
    "        score = va_metrics[\"macro_pr_auc\"]\n",
    "        if score > best_metric:\n",
    "            best_metric = score\n",
    "            torch.save({\"model\": v7_shared.state_dict(), \"ema\": ema.shadow, \"config\": {\"stage\":\"A\"}}, best_path)\n",
    "            patience = EARLY_STOP_PATIENCE\n",
    "            print(f\"  âœ… New best (Stage A) macro PR-AUC: {best_metric:.4f} â†’ checkpoint saved.\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"  â¹ Early stop in Stage A.\")\n",
    "                break\n",
    "\n",
    "    # ===== Stage B: finetune (unfreeze last N) =====\n",
    "    print(\"\\n=== Stage B: Finetune (unfreeze last 2 ChemBERTa layers) ===\")\n",
    "    v7_shared.freeze_text_backbone(n_unfrozen_layers=UNFREEZE_LAST_N)\n",
    "\n",
    "    opt = build_optimizer(v7_shared, lr=LR * 0.5, wd=WEIGHT_DECAY)\n",
    "    total_steps = math.ceil(len(train_loader) / GRAD_ACCUM) * MAX_EPOCHS_STAGE_B\n",
    "    scheduler = LambdaLR(opt, build_warmup_cosine(total_steps, warmup_ratio=WARMUP_RATIO))\n",
    "    ema = EMA(v7_shared, decay=EMA_DECAY)\n",
    "    patience = EARLY_STOP_PATIENCE\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS_STAGE_B + 1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_metrics = run_epoch(v7_shared, train_loader, opt, scheduler, ema, stage_name=\"train\")\n",
    "        with torch.no_grad():\n",
    "            ema.copy_to(v7_shared)\n",
    "            va_loss, va_metrics = run_epoch(v7_shared, val_loader, optimizer=None, scheduler=None, ema=None, stage_name=\"val\")\n",
    "\n",
    "        row = {\n",
    "            \"stage\": \"B\", \"epoch\": epoch,\n",
    "            \"train_loss\": tr_loss, **{f\"train_{k}\": v for k,v in tr_metrics.items()},\n",
    "            \"val_loss\": va_loss, **{f\"val_{k}\": v for k,v in va_metrics.items()},\n",
    "            \"epoch_time_s\": round(time.time()-t0, 2),\n",
    "        }\n",
    "        with open(log_path, \"a\") as f: f.write(json.dumps(row) + \"\\n\")\n",
    "        with open(val_log_path, \"a\") as f: f.write(json.dumps({\"stage\":\"B\", \"epoch\": epoch, **va_metrics}) + \"\\n\")\n",
    "        print(row)\n",
    "\n",
    "        # Save per-epoch checkpoint\n",
    "        save_epoch_ckpt(\"B\", epoch, ema, CKPT_DIR / f\"epoch_B{epoch:02d}.pt\")\n",
    "\n",
    "        score = va_metrics[\"macro_pr_auc\"]\n",
    "        if score > best_metric:\n",
    "            best_metric = score\n",
    "            torch.save({\"model\": v7_shared.state_dict(), \"ema\": ema.shadow, \"config\": {\"stage\":\"B\"}}, best_path)\n",
    "            patience = EARLY_STOP_PATIENCE\n",
    "            print(f\"  âœ… New best (Stage B) macro PR-AUC: {best_metric:.4f} â†’ checkpoint saved.\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"  â¹ Early stop in Stage B.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\nðŸŽ¯ Training complete. Best macro PR-AUC: {best_metric:.4f} | Best ckpt: {best_path}\")\n",
    "    print(f\"Logs â†’ {LOGS_DIR/'train_log.jsonl'}\")\n",
    "    print(f\"Val metrics â†’ {ARTIF_DIR/'val_metrics.jsonl'}\")\n",
    "    print(f\"Checkpoints â†’ {CKPT_DIR}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Kick it off\n",
    "# ---------------------------\n",
    "train_shared_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196d80cb",
   "metadata": {},
   "source": [
    "#### 1b) fine-tune the shared model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5dc2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage C: Extended finetune (unfreeze last 4 ChemBERTa layers) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:20] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 1, 'train_loss': 0.03059412927295153, 'train_macro_pr_auc': 0.2940897781197137, 'train_macro_roc_auc': 0.7738731249518018, 'val_loss': 0.11911626175045967, 'val_macro_pr_auc': 0.2313494757438764, 'val_macro_roc_auc': 0.7179151724089748, 'epoch_time_s': 7.7}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2313 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:23] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 2, 'train_loss': 0.02976563608703705, 'train_macro_pr_auc': 0.32309141373025596, 'train_macro_roc_auc': 0.7923732547730861, 'val_loss': 0.11889535903930665, 'val_macro_pr_auc': 0.23306384630623586, 'val_macro_roc_auc': 0.7202172791739457, 'epoch_time_s': 7.43}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2331 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:35] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 3, 'train_loss': 0.02977958160619705, 'train_macro_pr_auc': 0.3266218849580093, 'train_macro_roc_auc': 0.7896876136921706, 'val_loss': 0.11872626304626464, 'val_macro_pr_auc': 0.2347075514972754, 'val_macro_roc_auc': 0.7224862467323508, 'epoch_time_s': 7.44}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2347 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 4, 'train_loss': 0.02964736360292404, 'train_macro_pr_auc': 0.33171891965683115, 'train_macro_roc_auc': 0.7927982693500946, 'val_loss': 0.11857076600193978, 'val_macro_pr_auc': 0.2361485101560743, 'val_macro_roc_auc': 0.7242838603441172, 'epoch_time_s': 7.34}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2361 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:51] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 5, 'train_loss': 0.02982027809111736, 'train_macro_pr_auc': 0.31763875566657007, 'train_macro_roc_auc': 0.7874365553179355, 'val_loss': 0.1184364566206932, 'val_macro_pr_auc': 0.23835799454299564, 'val_macro_roc_auc': 0.7257010995233294, 'epoch_time_s': 7.28}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2384 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:53] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 6, 'train_loss': 0.02947875121369576, 'train_macro_pr_auc': 0.3321220359256137, 'train_macro_roc_auc': 0.8006438802397594, 'val_loss': 0.11827784195542336, 'val_macro_pr_auc': 0.23832505697959694, 'val_macro_roc_auc': 0.727440428255865, 'epoch_time_s': 7.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:00] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 7, 'train_loss': 0.02935470045090486, 'train_macro_pr_auc': 0.3434656936237351, 'train_macro_roc_auc': 0.8023428187366073, 'val_loss': 0.11815975934267044, 'val_macro_pr_auc': 0.2393710062395936, 'val_macro_roc_auc': 0.7287112230799639, 'epoch_time_s': 7.25}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2394 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:10] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 8, 'train_loss': 0.029187886216319524, 'train_macro_pr_auc': 0.3496728510187987, 'train_macro_roc_auc': 0.8075722167410296, 'val_loss': 0.11802344962954521, 'val_macro_pr_auc': 0.240041162238516, 'val_macro_roc_auc': 0.7303042554528408, 'epoch_time_s': 7.22}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2400 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:19] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 9, 'train_loss': 0.029373395767731545, 'train_macro_pr_auc': 0.3405598536923287, 'train_macro_roc_auc': 0.805858050747465, 'val_loss': 0.11795216917991638, 'val_macro_pr_auc': 0.24077963991117687, 'val_macro_roc_auc': 0.7314910443527664, 'epoch_time_s': 7.28}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2408 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 10, 'train_loss': 0.02929941861388775, 'train_macro_pr_auc': 0.34471098714367926, 'train_macro_roc_auc': 0.8097664124196781, 'val_loss': 0.11790973782539367, 'val_macro_pr_auc': 0.24119206327093037, 'val_macro_roc_auc': 0.7322239918733962, 'epoch_time_s': 7.28}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2412 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:32] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 11, 'train_loss': 0.02930923376041345, 'train_macro_pr_auc': 0.34093325952352727, 'train_macro_roc_auc': 0.8092587990161633, 'val_loss': 0.1178943282365799, 'val_macro_pr_auc': 0.24154978012502534, 'val_macro_roc_auc': 0.7326831399686627, 'epoch_time_s': 7.2}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2415 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:38] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 12, 'train_loss': 0.029186946392441408, 'train_macro_pr_auc': 0.3597753005981579, 'train_macro_roc_auc': 0.8138671743287641, 'val_loss': 0.1178495578467846, 'val_macro_pr_auc': 0.2416392331610726, 'val_macro_roc_auc': 0.7333118541081923, 'epoch_time_s': 7.23}\n",
      "  âœ… New best (Stage C) macro PR-AUC: 0.2416 â†’ checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 13, 'train_loss': 0.029342801329226065, 'train_macro_pr_auc': 0.3456173786396817, 'train_macro_roc_auc': 0.8124665169572586, 'val_loss': 0.1178448085486889, 'val_macro_pr_auc': 0.24113489174898617, 'val_macro_roc_auc': 0.7337164190361283, 'epoch_time_s': 7.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:56] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 14, 'train_loss': 0.029542968245461966, 'train_macro_pr_auc': 0.34503395319943714, 'train_macro_roc_auc': 0.8082895448255402, 'val_loss': 0.11786730423569679, 'val_macro_pr_auc': 0.24131952642285667, 'val_macro_roc_auc': 0.733945058303203, 'epoch_time_s': 7.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:51:00] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 15, 'train_loss': 0.029530828427045772, 'train_macro_pr_auc': 0.34638721395507566, 'train_macro_roc_auc': 0.8117984862586809, 'val_loss': 0.11786027416586876, 'val_macro_pr_auc': 0.2411037459323543, 'val_macro_roc_auc': 0.7342894549870769, 'epoch_time_s': 7.17}\n",
      "\n",
      "ðŸŽ¯ Stage C done. Best macro PR-AUC now: 0.2416 | Best ckpt: v7\\model\\checkpoints\\shared\\best.pt\n"
     ]
    }
   ],
   "source": [
    "# === Phase 3 â€” Cell 1c: Optional extended finetune (Stage C) ===\n",
    "import json, math, time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "\n",
    "BASE_DIR   = Path(\"v7\")\n",
    "DATA_PREP  = BASE_DIR / \"data\" / \"prepared\"\n",
    "RESULTS_DIR= BASE_DIR / \"results\"\n",
    "LOGS_DIR   = RESULTS_DIR / \"logs\"\n",
    "ARTIF_DIR  = RESULTS_DIR / \"artifacts\"\n",
    "CKPT_DIR   = BASE_DIR / \"model\" / \"checkpoints\" / \"shared\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reuse objects from previous cell:\n",
    "# - v7_shared, train_loader, val_loader, criterion, scaler, GRAD_ACCUM, USE_AMP,\n",
    "#   build_optimizer, EMA, build_warmup_cosine, eval_metrics, run_epoch (already defined)\n",
    "\n",
    "BEST_PATH = CKPT_DIR / \"best.pt\"\n",
    "assert BEST_PATH.exists(), \"Best checkpoint not found from Stage B.\"\n",
    "\n",
    "# ---- Load best weights (EMA weights baked in via copy_to flow) ----\n",
    "ckpt = torch.load(BEST_PATH, map_location=device)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "# ---- Stage C config (slightly smaller LR, unfreeze more) ----\n",
    "MAX_EPOCHS_STAGE_C   = 15\n",
    "UNFREEZE_LAST_N_MORE = 4   # total unfreezing depth for Stage C\n",
    "LR_C                 = 1e-4\n",
    "WEIGHT_DECAY         = 1e-2\n",
    "WARMUP_RATIO         = 0.1\n",
    "EARLY_STOP_PATIENCE  = 5\n",
    "EMA_DECAY            = 0.999\n",
    "CLIP_NORM            = 1.0\n",
    "\n",
    "print(\"\\n=== Stage C: Extended finetune (unfreeze last 4 ChemBERTa layers) ===\")\n",
    "v7_shared.freeze_text_backbone(n_unfrozen_layers=UNFREEZE_LAST_N_MORE)\n",
    "\n",
    "opt = build_optimizer(v7_shared, lr=LR_C, wd=WEIGHT_DECAY)\n",
    "total_steps = (len(train_loader) // max(1, GRAD_ACCUM)) * MAX_EPOCHS_STAGE_C\n",
    "scheduler = LambdaLR(opt, build_warmup_cosine(total_steps, warmup_ratio=WARMUP_RATIO))\n",
    "ema = EMA(v7_shared, decay=EMA_DECAY)\n",
    "\n",
    "# Retrieve current best to compare\n",
    "best_metric = -1.0\n",
    "try:\n",
    "    with open(ARTIF_DIR / \"val_metrics.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            m = json.loads(line)\n",
    "            best_metric = max(best_metric, m.get(\"macro_pr_auc\", -1.0))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "patience = EARLY_STOP_PATIENCE\n",
    "for epoch in range(1, MAX_EPOCHS_STAGE_C + 1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_metrics = run_epoch(v7_shared, train_loader, opt, scheduler, ema, stage_name=\"train\")\n",
    "    with torch.no_grad():\n",
    "        ema.copy_to(v7_shared)\n",
    "        va_loss, va_metrics = run_epoch(v7_shared, val_loader, optimizer=None, scheduler=None, ema=None, stage_name=\"val\")\n",
    "    row = {\n",
    "        \"stage\": \"C\", \"epoch\": epoch,\n",
    "        \"train_loss\": tr_loss, **{f\"train_{k}\": v for k,v in tr_metrics.items()},\n",
    "        \"val_loss\": va_loss, **{f\"val_{k}\": v for k,v in va_metrics.items()},\n",
    "        \"epoch_time_s\": round(time.time()-t0, 2),\n",
    "    }\n",
    "    with open(LOGS_DIR / \"train_log.jsonl\", \"a\") as f: f.write(json.dumps(row) + \"\\n\")\n",
    "    with open(ARTIF_DIR / \"val_metrics.jsonl\", \"a\") as f: f.write(json.dumps({\"stage\":\"C\",\"epoch\":epoch, **va_metrics}) + \"\\n\")\n",
    "    print(row)\n",
    "\n",
    "    score = va_metrics[\"macro_pr_auc\"]\n",
    "    if score > best_metric:\n",
    "        best_metric = score\n",
    "        torch.save({\"model\": v7_shared.state_dict(), \"ema\": ema.shadow, \"config\": {\"stage\":\"C\"}}, BEST_PATH)\n",
    "        patience = EARLY_STOP_PATIENCE\n",
    "        print(f\"  âœ… New best (Stage C) macro PR-AUC: {best_metric:.4f} â†’ checkpoint saved.\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience <= 0:\n",
    "            print(\"  â¹ Early stop in Stage C.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nðŸŽ¯ Stage C done. Best macro PR-AUC now: {best_metric:.4f} | Best ckpt: {BEST_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806e50e0",
   "metadata": {},
   "source": [
    "### 2 (boosted): Label-Specialist Heads (extended + multi-seed)\n",
    "\n",
    "We train one-vs-rest heads for each of the 12 Tox21 labels:\n",
    "\n",
    "- Cached fused features (from the shared model): `v7/data/fused/`\n",
    "- Head MLP: 768 â†’ 512 â†’ 256 â†’ 128 â†’ 1 (GELU + LayerNorm + Dropout 0.30, residual)\n",
    "- Loss: **Binary ASL** (Î³â»=5.0, Î³âº=1.0) with **class-balanced Î±**\n",
    "- Sampler: **class-balanced WeightedRandomSampler** to ensure positives per batch\n",
    "- Optimiser & schedule: AdamW (lr=3e-3) + **cosine warmup** (10% warmup)\n",
    "- Regularisation: **EMA**, AMP, grad-clip (L2-norm=1.0), WD=1e-2\n",
    "- Early stopping: patience=10 on **val AP**\n",
    "- **5 seeds** per label (can change via `SEEDS`)\n",
    "\n",
    "Artifacts:\n",
    "- `v7/model/ensembles/<LABEL>/seedXX/best.pt`\n",
    "- `v7/model/ensembles/<LABEL>/seedXX/val_preds.npz` (for calibration)\n",
    "- `v7/model/ensembles/ensemble_summary.json`\n",
    "\n",
    "> Expect ~30â€“50 mins on a 4070 Ti (depends on your desktop load). You can lower `SEEDS` or `EPOCHS_MAX` if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155f9891",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:55:07] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached train: fused (6265, 768)\n",
      "Cached val: fused (783, 768)\n",
      "Fused shapes â†’ train: (6265, 768) | val: (783, 768)\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-AR (label 0)\n",
      "==============================\n",
      "[NR-AR | seed 13] ep 01  loss 0.0007  val AP 0.0165\n",
      "  âœ… New best AP: 0.0165 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 02  loss 0.0005  val AP 0.0169\n",
      "  âœ… New best AP: 0.0169 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 03  loss 0.0005  val AP 0.0176\n",
      "  âœ… New best AP: 0.0176 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 04  loss 0.0005  val AP 0.0187\n",
      "  âœ… New best AP: 0.0187 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 05  loss 0.0006  val AP 0.0224\n",
      "  âœ… New best AP: 0.0224 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 06  loss 0.0005  val AP 0.0935\n",
      "  âœ… New best AP: 0.0935 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 07  loss 0.0005  val AP 0.1298\n",
      "  âœ… New best AP: 0.1298 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 08  loss 0.0005  val AP 0.1305\n",
      "  âœ… New best AP: 0.1305 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 09  loss 0.0004  val AP 0.1311\n",
      "  âœ… New best AP: 0.1311 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 10  loss 0.0006  val AP 0.1541\n",
      "  âœ… New best AP: 0.1541 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 11  loss 0.0005  val AP 0.1550\n",
      "  âœ… New best AP: 0.1550 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 12  loss 0.0004  val AP 0.1558\n",
      "  âœ… New best AP: 0.1558 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 13  loss 0.0004  val AP 0.1564\n",
      "  âœ… New best AP: 0.1564 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 14  loss 0.0004  val AP 0.1568\n",
      "  âœ… New best AP: 0.1568 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 15  loss 0.0004  val AP 0.1572\n",
      "  âœ… New best AP: 0.1572 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 16  loss 0.0006  val AP 0.1575\n",
      "  âœ… New best AP: 0.1575 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 17  loss 0.0004  val AP 0.1583\n",
      "  âœ… New best AP: 0.1583 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 18  loss 0.0007  val AP 0.1587\n",
      "  âœ… New best AP: 0.1587 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 19  loss 0.0005  val AP 0.1594\n",
      "  âœ… New best AP: 0.1594 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 20  loss 0.0005  val AP 0.1599\n",
      "  âœ… New best AP: 0.1599 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 21  loss 0.0004  val AP 0.1605\n",
      "  âœ… New best AP: 0.1605 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 22  loss 0.0005  val AP 0.1608\n",
      "  âœ… New best AP: 0.1608 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 23  loss 0.0004  val AP 0.1612\n",
      "  âœ… New best AP: 0.1612 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 24  loss 0.0004  val AP 0.1614\n",
      "  âœ… New best AP: 0.1614 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 25  loss 0.0004  val AP 0.1618\n",
      "  âœ… New best AP: 0.1618 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 26  loss 0.0004  val AP 0.1620\n",
      "  âœ… New best AP: 0.1620 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 27  loss 0.0004  val AP 0.1621\n",
      "  âœ… New best AP: 0.1621 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 28  loss 0.0004  val AP 0.1623\n",
      "  âœ… New best AP: 0.1623 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 29  loss 0.0004  val AP 0.1625\n",
      "  âœ… New best AP: 0.1625 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 30  loss 0.0004  val AP 0.1625\n",
      "  âœ… New best AP: 0.1625 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 31  loss 0.0004  val AP 0.1626\n",
      "  âœ… New best AP: 0.1626 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 32  loss 0.0004  val AP 0.1626\n",
      "  âœ… New best AP: 0.1626 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 33  loss 0.0004  val AP 0.1626\n",
      "  âœ… New best AP: 0.1626 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 34  loss 0.0004  val AP 0.1629\n",
      "  âœ… New best AP: 0.1629 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 35  loss 0.0004  val AP 0.1631\n",
      "  âœ… New best AP: 0.1631 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 36  loss 0.0004  val AP 0.1633\n",
      "  âœ… New best AP: 0.1633 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 37  loss 0.0004  val AP 0.1634\n",
      "  âœ… New best AP: 0.1634 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 38  loss 0.0004  val AP 0.1635\n",
      "  âœ… New best AP: 0.1635 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 39  loss 0.0004  val AP 0.1637\n",
      "  âœ… New best AP: 0.1637 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 40  loss 0.0004  val AP 0.1638\n",
      "  âœ… New best AP: 0.1638 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 41  loss 0.0004  val AP 0.1639\n",
      "  âœ… New best AP: 0.1639 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 42  loss 0.0004  val AP 0.1640\n",
      "  âœ… New best AP: 0.1640 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 43  loss 0.0004  val AP 0.1640\n",
      "  âœ… New best AP: 0.1640 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 44  loss 0.0004  val AP 0.1640\n",
      "  âœ… New best AP: 0.1640 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 45  loss 0.0004  val AP 0.1640\n",
      "[NR-AR | seed 13] ep 46  loss 0.0004  val AP 0.1641\n",
      "  âœ… New best AP: 0.1641 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 47  loss 0.0004  val AP 0.1641\n",
      "[NR-AR | seed 13] ep 48  loss 0.0004  val AP 0.1642\n",
      "  âœ… New best AP: 0.1642 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 49  loss 0.0004  val AP 0.1643\n",
      "  âœ… New best AP: 0.1643 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 50  loss 0.0004  val AP 0.1644\n",
      "  âœ… New best AP: 0.1644 â†’ v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 29] ep 01  loss 0.0008  val AP 0.0563\n",
      "  âœ… New best AP: 0.0563 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 02  loss 0.0006  val AP 0.0698\n",
      "  âœ… New best AP: 0.0698 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 03  loss 0.0006  val AP 0.0826\n",
      "  âœ… New best AP: 0.0826 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 04  loss 0.0006  val AP 0.0975\n",
      "  âœ… New best AP: 0.0975 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 05  loss 0.0006  val AP 0.1016\n",
      "  âœ… New best AP: 0.1016 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 06  loss 0.0006  val AP 0.1452\n",
      "  âœ… New best AP: 0.1452 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 07  loss 0.0006  val AP 0.1668\n",
      "  âœ… New best AP: 0.1668 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 08  loss 0.0006  val AP 0.1669\n",
      "  âœ… New best AP: 0.1669 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 09  loss 0.0007  val AP 0.1676\n",
      "  âœ… New best AP: 0.1676 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 10  loss 0.0005  val AP 0.1675\n",
      "[NR-AR | seed 29] ep 11  loss 0.0004  val AP 0.1677\n",
      "  âœ… New best AP: 0.1677 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 12  loss 0.0005  val AP 0.1682\n",
      "  âœ… New best AP: 0.1682 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 13  loss 0.0005  val AP 0.1683\n",
      "  âœ… New best AP: 0.1683 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 14  loss 0.0006  val AP 0.1687\n",
      "  âœ… New best AP: 0.1687 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 15  loss 0.0005  val AP 0.1689\n",
      "  âœ… New best AP: 0.1689 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 16  loss 0.0005  val AP 0.1688\n",
      "[NR-AR | seed 29] ep 17  loss 0.0004  val AP 0.1689\n",
      "  âœ… New best AP: 0.1689 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 18  loss 0.0005  val AP 0.1689\n",
      "  âœ… New best AP: 0.1689 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 19  loss 0.0004  val AP 0.1689\n",
      "[NR-AR | seed 29] ep 20  loss 0.0004  val AP 0.1691\n",
      "  âœ… New best AP: 0.1691 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 21  loss 0.0005  val AP 0.1690\n",
      "[NR-AR | seed 29] ep 22  loss 0.0004  val AP 0.1690\n",
      "[NR-AR | seed 29] ep 23  loss 0.0005  val AP 0.1695\n",
      "  âœ… New best AP: 0.1695 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 24  loss 0.0004  val AP 0.1695\n",
      "  âœ… New best AP: 0.1695 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 25  loss 0.0005  val AP 0.1695\n",
      "[NR-AR | seed 29] ep 26  loss 0.0005  val AP 0.1696\n",
      "  âœ… New best AP: 0.1696 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 27  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 28  loss 0.0004  val AP 0.1696\n",
      "  âœ… New best AP: 0.1696 â†’ v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 29  loss 0.0004  val AP 0.1695\n",
      "[NR-AR | seed 29] ep 30  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 31  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 32  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 33  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 34  loss 0.0004  val AP 0.1695\n",
      "[NR-AR | seed 29] ep 35  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 36  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 37  loss 0.0004  val AP 0.1695\n",
      "[NR-AR | seed 29] ep 38  loss 0.0004  val AP 0.1694\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.1696\n",
      "[NR-AR | seed 47] ep 01  loss 0.0009  val AP 0.0319\n",
      "  âœ… New best AP: 0.0319 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 02  loss 0.0006  val AP 0.0329\n",
      "  âœ… New best AP: 0.0329 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 03  loss 0.0006  val AP 0.0335\n",
      "  âœ… New best AP: 0.0335 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 04  loss 0.0007  val AP 0.0352\n",
      "  âœ… New best AP: 0.0352 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 05  loss 0.0006  val AP 0.0398\n",
      "  âœ… New best AP: 0.0398 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 06  loss 0.0006  val AP 0.0448\n",
      "  âœ… New best AP: 0.0448 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 07  loss 0.0007  val AP 0.0540\n",
      "  âœ… New best AP: 0.0540 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 08  loss 0.0007  val AP 0.0697\n",
      "  âœ… New best AP: 0.0697 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 09  loss 0.0006  val AP 0.0921\n",
      "  âœ… New best AP: 0.0921 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 10  loss 0.0005  val AP 0.1209\n",
      "  âœ… New best AP: 0.1209 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 11  loss 0.0005  val AP 0.1536\n",
      "  âœ… New best AP: 0.1536 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 12  loss 0.0005  val AP 0.1770\n",
      "  âœ… New best AP: 0.1770 â†’ v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 13  loss 0.0006  val AP 0.1768\n",
      "[NR-AR | seed 47] ep 14  loss 0.0005  val AP 0.1759\n",
      "[NR-AR | seed 47] ep 15  loss 0.0005  val AP 0.1753\n",
      "[NR-AR | seed 47] ep 16  loss 0.0004  val AP 0.1752\n",
      "[NR-AR | seed 47] ep 17  loss 0.0004  val AP 0.1750\n",
      "[NR-AR | seed 47] ep 18  loss 0.0005  val AP 0.1750\n",
      "[NR-AR | seed 47] ep 19  loss 0.0004  val AP 0.1745\n",
      "[NR-AR | seed 47] ep 20  loss 0.0004  val AP 0.1745\n",
      "[NR-AR | seed 47] ep 21  loss 0.0005  val AP 0.1745\n",
      "[NR-AR | seed 47] ep 22  loss 0.0004  val AP 0.1747\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.1770\n",
      "[NR-AR | seed 61] ep 01  loss 0.0010  val AP 0.0196\n",
      "  âœ… New best AP: 0.0196 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 02  loss 0.0007  val AP 0.0200\n",
      "  âœ… New best AP: 0.0200 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 03  loss 0.0007  val AP 0.0206\n",
      "  âœ… New best AP: 0.0206 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 04  loss 0.0007  val AP 0.0216\n",
      "  âœ… New best AP: 0.0216 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 05  loss 0.0006  val AP 0.0241\n",
      "  âœ… New best AP: 0.0241 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 06  loss 0.0006  val AP 0.0293\n",
      "  âœ… New best AP: 0.0293 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 07  loss 0.0009  val AP 0.0376\n",
      "  âœ… New best AP: 0.0376 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 08  loss 0.0006  val AP 0.0471\n",
      "  âœ… New best AP: 0.0471 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 09  loss 0.0007  val AP 0.0527\n",
      "  âœ… New best AP: 0.0527 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 10  loss 0.0005  val AP 0.0731\n",
      "  âœ… New best AP: 0.0731 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 11  loss 0.0005  val AP 0.0784\n",
      "  âœ… New best AP: 0.0784 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 12  loss 0.0005  val AP 0.1304\n",
      "  âœ… New best AP: 0.1304 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 13  loss 0.0006  val AP 0.1646\n",
      "  âœ… New best AP: 0.1646 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 14  loss 0.0005  val AP 0.1651\n",
      "  âœ… New best AP: 0.1651 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 15  loss 0.0005  val AP 0.1657\n",
      "  âœ… New best AP: 0.1657 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 16  loss 0.0005  val AP 0.1662\n",
      "  âœ… New best AP: 0.1662 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 17  loss 0.0004  val AP 0.1669\n",
      "  âœ… New best AP: 0.1669 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 18  loss 0.0005  val AP 0.1675\n",
      "  âœ… New best AP: 0.1675 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 19  loss 0.0005  val AP 0.1679\n",
      "  âœ… New best AP: 0.1679 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 20  loss 0.0005  val AP 0.1683\n",
      "  âœ… New best AP: 0.1683 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 21  loss 0.0005  val AP 0.1685\n",
      "  âœ… New best AP: 0.1685 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 22  loss 0.0005  val AP 0.1688\n",
      "  âœ… New best AP: 0.1688 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 23  loss 0.0004  val AP 0.1689\n",
      "  âœ… New best AP: 0.1689 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 24  loss 0.0005  val AP 0.1690\n",
      "  âœ… New best AP: 0.1690 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 25  loss 0.0004  val AP 0.1691\n",
      "  âœ… New best AP: 0.1691 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 26  loss 0.0004  val AP 0.1692\n",
      "  âœ… New best AP: 0.1692 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 27  loss 0.0005  val AP 0.1694\n",
      "  âœ… New best AP: 0.1694 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 28  loss 0.0005  val AP 0.1694\n",
      "[NR-AR | seed 61] ep 29  loss 0.0004  val AP 0.1693\n",
      "[NR-AR | seed 61] ep 30  loss 0.0004  val AP 0.1695\n",
      "  âœ… New best AP: 0.1695 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 31  loss 0.0004  val AP 0.1696\n",
      "  âœ… New best AP: 0.1696 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 32  loss 0.0004  val AP 0.1697\n",
      "  âœ… New best AP: 0.1697 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 33  loss 0.0004  val AP 0.1698\n",
      "  âœ… New best AP: 0.1698 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 34  loss 0.0004  val AP 0.1697\n",
      "[NR-AR | seed 61] ep 35  loss 0.0004  val AP 0.1696\n",
      "[NR-AR | seed 61] ep 36  loss 0.0004  val AP 0.1696\n",
      "[NR-AR | seed 61] ep 37  loss 0.0004  val AP 0.1696\n",
      "[NR-AR | seed 61] ep 38  loss 0.0004  val AP 0.1697\n",
      "[NR-AR | seed 61] ep 39  loss 0.0004  val AP 0.1697\n",
      "[NR-AR | seed 61] ep 40  loss 0.0004  val AP 0.1698\n",
      "  âœ… New best AP: 0.1698 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 41  loss 0.0004  val AP 0.1699\n",
      "  âœ… New best AP: 0.1699 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 42  loss 0.0004  val AP 0.1700\n",
      "  âœ… New best AP: 0.1700 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 43  loss 0.0004  val AP 0.1700\n",
      "  âœ… New best AP: 0.1700 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 44  loss 0.0005  val AP 0.1701\n",
      "  âœ… New best AP: 0.1701 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 45  loss 0.0004  val AP 0.1703\n",
      "  âœ… New best AP: 0.1703 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 46  loss 0.0005  val AP 0.1703\n",
      "  âœ… New best AP: 0.1703 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 47  loss 0.0004  val AP 0.1703\n",
      "  âœ… New best AP: 0.1703 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 48  loss 0.0004  val AP 0.1704\n",
      "  âœ… New best AP: 0.1704 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 49  loss 0.0005  val AP 0.1705\n",
      "  âœ… New best AP: 0.1705 â†’ v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 50  loss 0.0004  val AP 0.1705\n",
      "[NR-AR | seed 83] ep 01  loss 0.0008  val AP 0.0249\n",
      "  âœ… New best AP: 0.0249 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 02  loss 0.0006  val AP 0.0251\n",
      "  âœ… New best AP: 0.0251 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 03  loss 0.0006  val AP 0.0260\n",
      "  âœ… New best AP: 0.0260 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 04  loss 0.0006  val AP 0.0278\n",
      "  âœ… New best AP: 0.0278 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 05  loss 0.0005  val AP 0.0306\n",
      "  âœ… New best AP: 0.0306 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 06  loss 0.0007  val AP 0.0358\n",
      "  âœ… New best AP: 0.0358 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 07  loss 0.0006  val AP 0.0459\n",
      "  âœ… New best AP: 0.0459 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 08  loss 0.0007  val AP 0.0533\n",
      "  âœ… New best AP: 0.0533 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 09  loss 0.0005  val AP 0.0653\n",
      "  âœ… New best AP: 0.0653 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 10  loss 0.0007  val AP 0.0798\n",
      "  âœ… New best AP: 0.0798 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 11  loss 0.0005  val AP 0.0897\n",
      "  âœ… New best AP: 0.0897 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 12  loss 0.0006  val AP 0.1293\n",
      "  âœ… New best AP: 0.1293 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 13  loss 0.0004  val AP 0.1294\n",
      "  âœ… New best AP: 0.1294 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 14  loss 0.0005  val AP 0.1292\n",
      "[NR-AR | seed 83] ep 15  loss 0.0004  val AP 0.1294\n",
      "[NR-AR | seed 83] ep 16  loss 0.0005  val AP 0.1294\n",
      "[NR-AR | seed 83] ep 17  loss 0.0005  val AP 0.1294\n",
      "  âœ… New best AP: 0.1294 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 18  loss 0.0004  val AP 0.1294\n",
      "[NR-AR | seed 83] ep 19  loss 0.0005  val AP 0.1294\n",
      "[NR-AR | seed 83] ep 20  loss 0.0004  val AP 0.1406\n",
      "  âœ… New best AP: 0.1406 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 21  loss 0.0005  val AP 0.1405\n",
      "[NR-AR | seed 83] ep 22  loss 0.0004  val AP 0.1405\n",
      "[NR-AR | seed 83] ep 23  loss 0.0005  val AP 0.1405\n",
      "[NR-AR | seed 83] ep 24  loss 0.0004  val AP 0.1406\n",
      "[NR-AR | seed 83] ep 25  loss 0.0004  val AP 0.1408\n",
      "  âœ… New best AP: 0.1408 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 26  loss 0.0004  val AP 0.1408\n",
      "[NR-AR | seed 83] ep 27  loss 0.0004  val AP 0.1409\n",
      "  âœ… New best AP: 0.1409 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 28  loss 0.0005  val AP 0.1410\n",
      "  âœ… New best AP: 0.1410 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 29  loss 0.0004  val AP 0.1409\n",
      "[NR-AR | seed 83] ep 30  loss 0.0005  val AP 0.1409\n",
      "[NR-AR | seed 83] ep 31  loss 0.0004  val AP 0.1409\n",
      "[NR-AR | seed 83] ep 32  loss 0.0004  val AP 0.1409\n",
      "[NR-AR | seed 83] ep 33  loss 0.0004  val AP 0.1410\n",
      "[NR-AR | seed 83] ep 34  loss 0.0004  val AP 0.1412\n",
      "  âœ… New best AP: 0.1412 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 35  loss 0.0004  val AP 0.1413\n",
      "  âœ… New best AP: 0.1413 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 36  loss 0.0004  val AP 0.1413\n",
      "  âœ… New best AP: 0.1413 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 37  loss 0.0004  val AP 0.1414\n",
      "  âœ… New best AP: 0.1414 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 38  loss 0.0004  val AP 0.1415\n",
      "  âœ… New best AP: 0.1415 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 39  loss 0.0004  val AP 0.1416\n",
      "  âœ… New best AP: 0.1416 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 40  loss 0.0004  val AP 0.1416\n",
      "  âœ… New best AP: 0.1416 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 41  loss 0.0005  val AP 0.1417\n",
      "  âœ… New best AP: 0.1417 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 42  loss 0.0004  val AP 0.1417\n",
      "  âœ… New best AP: 0.1417 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 43  loss 0.0004  val AP 0.1418\n",
      "  âœ… New best AP: 0.1418 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 44  loss 0.0004  val AP 0.1419\n",
      "  âœ… New best AP: 0.1419 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 45  loss 0.0004  val AP 0.1418\n",
      "[NR-AR | seed 83] ep 46  loss 0.0004  val AP 0.1418\n",
      "[NR-AR | seed 83] ep 47  loss 0.0004  val AP 0.1419\n",
      "[NR-AR | seed 83] ep 48  loss 0.0005  val AP 0.1420\n",
      "  âœ… New best AP: 0.1420 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 49  loss 0.0004  val AP 0.1419\n",
      "[NR-AR | seed 83] ep 50  loss 0.0004  val AP 0.1420\n",
      "  âœ… New best AP: 0.1420 â†’ v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-AR-LBD (label 1)\n",
      "==============================\n",
      "[NR-AR-LBD | seed 13] ep 01  loss 0.0009  val AP 0.0153\n",
      "  âœ… New best AP: 0.0153 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 02  loss 0.0007  val AP 0.0164\n",
      "  âœ… New best AP: 0.0164 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 03  loss 0.0006  val AP 0.0184\n",
      "  âœ… New best AP: 0.0184 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 04  loss 0.0006  val AP 0.0223\n",
      "  âœ… New best AP: 0.0223 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 05  loss 0.0006  val AP 0.0315\n",
      "  âœ… New best AP: 0.0315 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 06  loss 0.0006  val AP 0.1510\n",
      "  âœ… New best AP: 0.1510 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 07  loss 0.0005  val AP 0.2017\n",
      "  âœ… New best AP: 0.2017 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 08  loss 0.0006  val AP 0.2167\n",
      "  âœ… New best AP: 0.2167 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 09  loss 0.0005  val AP 0.2384\n",
      "  âœ… New best AP: 0.2384 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 10  loss 0.0006  val AP 0.2389\n",
      "  âœ… New best AP: 0.2389 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 11  loss 0.0005  val AP 0.2435\n",
      "  âœ… New best AP: 0.2435 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 12  loss 0.0005  val AP 0.2439\n",
      "  âœ… New best AP: 0.2439 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 13  loss 0.0005  val AP 0.2448\n",
      "  âœ… New best AP: 0.2448 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 14  loss 0.0005  val AP 0.2455\n",
      "  âœ… New best AP: 0.2455 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 15  loss 0.0005  val AP 0.2462\n",
      "  âœ… New best AP: 0.2462 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 16  loss 0.0005  val AP 0.2466\n",
      "  âœ… New best AP: 0.2466 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 17  loss 0.0005  val AP 0.2473\n",
      "  âœ… New best AP: 0.2473 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 18  loss 0.0005  val AP 0.2482\n",
      "  âœ… New best AP: 0.2482 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 19  loss 0.0006  val AP 0.2485\n",
      "  âœ… New best AP: 0.2485 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 20  loss 0.0005  val AP 0.2487\n",
      "  âœ… New best AP: 0.2487 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 21  loss 0.0005  val AP 0.2862\n",
      "  âœ… New best AP: 0.2862 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 22  loss 0.0005  val AP 0.2868\n",
      "  âœ… New best AP: 0.2868 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 23  loss 0.0005  val AP 0.2876\n",
      "  âœ… New best AP: 0.2876 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 24  loss 0.0005  val AP 0.2878\n",
      "  âœ… New best AP: 0.2878 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 25  loss 0.0006  val AP 0.2880\n",
      "  âœ… New best AP: 0.2880 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 26  loss 0.0005  val AP 0.2884\n",
      "  âœ… New best AP: 0.2884 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 27  loss 0.0005  val AP 0.2889\n",
      "  âœ… New best AP: 0.2889 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 28  loss 0.0005  val AP 0.2891\n",
      "  âœ… New best AP: 0.2891 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 29  loss 0.0005  val AP 0.2895\n",
      "  âœ… New best AP: 0.2895 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 30  loss 0.0005  val AP 0.2896\n",
      "  âœ… New best AP: 0.2896 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 31  loss 0.0005  val AP 0.2899\n",
      "  âœ… New best AP: 0.2899 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 32  loss 0.0004  val AP 0.2900\n",
      "  âœ… New best AP: 0.2900 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 33  loss 0.0004  val AP 0.2901\n",
      "  âœ… New best AP: 0.2901 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 34  loss 0.0005  val AP 0.2905\n",
      "  âœ… New best AP: 0.2905 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 35  loss 0.0005  val AP 0.2905\n",
      "  âœ… New best AP: 0.2905 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 36  loss 0.0005  val AP 0.2906\n",
      "  âœ… New best AP: 0.2906 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 37  loss 0.0005  val AP 0.2907\n",
      "  âœ… New best AP: 0.2907 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 38  loss 0.0005  val AP 0.2910\n",
      "  âœ… New best AP: 0.2910 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 39  loss 0.0004  val AP 0.2910\n",
      "[NR-AR-LBD | seed 13] ep 40  loss 0.0004  val AP 0.2911\n",
      "  âœ… New best AP: 0.2911 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 41  loss 0.0005  val AP 0.2914\n",
      "  âœ… New best AP: 0.2914 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 42  loss 0.0005  val AP 0.2917\n",
      "  âœ… New best AP: 0.2917 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 43  loss 0.0005  val AP 0.2919\n",
      "  âœ… New best AP: 0.2919 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 44  loss 0.0005  val AP 0.2920\n",
      "  âœ… New best AP: 0.2920 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 45  loss 0.0005  val AP 0.2920\n",
      "[NR-AR-LBD | seed 13] ep 46  loss 0.0005  val AP 0.2920\n",
      "[NR-AR-LBD | seed 13] ep 47  loss 0.0005  val AP 0.2920\n",
      "[NR-AR-LBD | seed 13] ep 48  loss 0.0005  val AP 0.2921\n",
      "  âœ… New best AP: 0.2921 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 49  loss 0.0005  val AP 0.2922\n",
      "  âœ… New best AP: 0.2922 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 50  loss 0.0005  val AP 0.2922\n",
      "  âœ… New best AP: 0.2922 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 01  loss 0.0010  val AP 0.0610\n",
      "  âœ… New best AP: 0.0610 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 02  loss 0.0007  val AP 0.0902\n",
      "  âœ… New best AP: 0.0902 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 03  loss 0.0008  val AP 0.1255\n",
      "  âœ… New best AP: 0.1255 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 04  loss 0.0006  val AP 0.1442\n",
      "  âœ… New best AP: 0.1442 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 05  loss 0.0006  val AP 0.2254\n",
      "  âœ… New best AP: 0.2254 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 06  loss 0.0007  val AP 0.2677\n",
      "  âœ… New best AP: 0.2677 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 07  loss 0.0007  val AP 0.2721\n",
      "  âœ… New best AP: 0.2721 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 08  loss 0.0009  val AP 0.2701\n",
      "[NR-AR-LBD | seed 29] ep 09  loss 0.0006  val AP 0.2705\n",
      "[NR-AR-LBD | seed 29] ep 10  loss 0.0005  val AP 0.2727\n",
      "  âœ… New best AP: 0.2727 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 11  loss 0.0005  val AP 0.2747\n",
      "  âœ… New best AP: 0.2747 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 12  loss 0.0005  val AP 0.2740\n",
      "[NR-AR-LBD | seed 29] ep 13  loss 0.0006  val AP 0.2746\n",
      "[NR-AR-LBD | seed 29] ep 14  loss 0.0005  val AP 0.2750\n",
      "  âœ… New best AP: 0.2750 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 15  loss 0.0006  val AP 0.2772\n",
      "  âœ… New best AP: 0.2772 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 16  loss 0.0006  val AP 0.2397\n",
      "[NR-AR-LBD | seed 29] ep 17  loss 0.0005  val AP 0.2400\n",
      "[NR-AR-LBD | seed 29] ep 18  loss 0.0005  val AP 0.2433\n",
      "[NR-AR-LBD | seed 29] ep 19  loss 0.0005  val AP 0.2476\n",
      "[NR-AR-LBD | seed 29] ep 20  loss 0.0005  val AP 0.2479\n",
      "[NR-AR-LBD | seed 29] ep 21  loss 0.0005  val AP 0.2480\n",
      "[NR-AR-LBD | seed 29] ep 22  loss 0.0005  val AP 0.2480\n",
      "[NR-AR-LBD | seed 29] ep 23  loss 0.0005  val AP 0.2484\n",
      "[NR-AR-LBD | seed 29] ep 24  loss 0.0006  val AP 0.2484\n",
      "[NR-AR-LBD | seed 29] ep 25  loss 0.0005  val AP 0.2488\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.2772\n",
      "[NR-AR-LBD | seed 47] ep 01  loss 0.0011  val AP 0.0221\n",
      "  âœ… New best AP: 0.0221 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 02  loss 0.0007  val AP 0.0236\n",
      "  âœ… New best AP: 0.0236 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 03  loss 0.0007  val AP 0.0243\n",
      "  âœ… New best AP: 0.0243 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 04  loss 0.0008  val AP 0.0263\n",
      "  âœ… New best AP: 0.0263 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 05  loss 0.0008  val AP 0.0300\n",
      "  âœ… New best AP: 0.0300 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 06  loss 0.0006  val AP 0.0420\n",
      "  âœ… New best AP: 0.0420 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 07  loss 0.0008  val AP 0.0560\n",
      "  âœ… New best AP: 0.0560 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 08  loss 0.0007  val AP 0.1045\n",
      "  âœ… New best AP: 0.1045 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 09  loss 0.0006  val AP 0.1293\n",
      "  âœ… New best AP: 0.1293 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 10  loss 0.0005  val AP 0.1429\n",
      "  âœ… New best AP: 0.1429 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 11  loss 0.0006  val AP 0.1634\n",
      "  âœ… New best AP: 0.1634 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 12  loss 0.0007  val AP 0.1644\n",
      "  âœ… New best AP: 0.1644 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 13  loss 0.0007  val AP 0.1660\n",
      "  âœ… New best AP: 0.1660 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 14  loss 0.0005  val AP 0.1689\n",
      "  âœ… New best AP: 0.1689 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 15  loss 0.0005  val AP 0.1698\n",
      "  âœ… New best AP: 0.1698 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 16  loss 0.0005  val AP 0.1698\n",
      "[NR-AR-LBD | seed 47] ep 17  loss 0.0005  val AP 0.1707\n",
      "  âœ… New best AP: 0.1707 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 18  loss 0.0005  val AP 0.1728\n",
      "  âœ… New best AP: 0.1728 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 19  loss 0.0005  val AP 0.1755\n",
      "  âœ… New best AP: 0.1755 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 20  loss 0.0005  val AP 0.1768\n",
      "  âœ… New best AP: 0.1768 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 21  loss 0.0005  val AP 0.1772\n",
      "  âœ… New best AP: 0.1772 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 22  loss 0.0005  val AP 0.1774\n",
      "  âœ… New best AP: 0.1774 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 23  loss 0.0006  val AP 0.1772\n",
      "[NR-AR-LBD | seed 47] ep 24  loss 0.0005  val AP 0.1791\n",
      "  âœ… New best AP: 0.1791 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 25  loss 0.0005  val AP 0.1792\n",
      "  âœ… New best AP: 0.1792 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 26  loss 0.0005  val AP 0.1795\n",
      "  âœ… New best AP: 0.1795 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 27  loss 0.0005  val AP 0.1795\n",
      "[NR-AR-LBD | seed 47] ep 28  loss 0.0005  val AP 0.1797\n",
      "  âœ… New best AP: 0.1797 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 29  loss 0.0005  val AP 0.1799\n",
      "  âœ… New best AP: 0.1799 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 30  loss 0.0005  val AP 0.1801\n",
      "  âœ… New best AP: 0.1801 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 31  loss 0.0005  val AP 0.1801\n",
      "  âœ… New best AP: 0.1801 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 32  loss 0.0005  val AP 0.1801\n",
      "  âœ… New best AP: 0.1801 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 33  loss 0.0005  val AP 0.1803\n",
      "  âœ… New best AP: 0.1803 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 34  loss 0.0005  val AP 0.1804\n",
      "  âœ… New best AP: 0.1804 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 35  loss 0.0005  val AP 0.1805\n",
      "  âœ… New best AP: 0.1805 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 36  loss 0.0005  val AP 0.1806\n",
      "  âœ… New best AP: 0.1806 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 37  loss 0.0005  val AP 0.1806\n",
      "  âœ… New best AP: 0.1806 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 38  loss 0.0005  val AP 0.1827\n",
      "  âœ… New best AP: 0.1827 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 39  loss 0.0004  val AP 0.1830\n",
      "  âœ… New best AP: 0.1830 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 40  loss 0.0005  val AP 0.1831\n",
      "  âœ… New best AP: 0.1831 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 41  loss 0.0005  val AP 0.1834\n",
      "  âœ… New best AP: 0.1834 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 42  loss 0.0005  val AP 0.1835\n",
      "  âœ… New best AP: 0.1835 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 43  loss 0.0005  val AP 0.1836\n",
      "  âœ… New best AP: 0.1836 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 44  loss 0.0005  val AP 0.1837\n",
      "  âœ… New best AP: 0.1837 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 45  loss 0.0005  val AP 0.1838\n",
      "  âœ… New best AP: 0.1838 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 46  loss 0.0005  val AP 0.1839\n",
      "  âœ… New best AP: 0.1839 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 47  loss 0.0005  val AP 0.1840\n",
      "  âœ… New best AP: 0.1840 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 48  loss 0.0005  val AP 0.1841\n",
      "  âœ… New best AP: 0.1841 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 49  loss 0.0005  val AP 0.1842\n",
      "  âœ… New best AP: 0.1842 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 50  loss 0.0005  val AP 0.1842\n",
      "[NR-AR-LBD | seed 61] ep 01  loss 0.0013  val AP 0.0202\n",
      "  âœ… New best AP: 0.0202 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 02  loss 0.0008  val AP 0.0220\n",
      "  âœ… New best AP: 0.0220 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 03  loss 0.0008  val AP 0.0245\n",
      "  âœ… New best AP: 0.0245 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 04  loss 0.0009  val AP 0.0287\n",
      "  âœ… New best AP: 0.0287 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 05  loss 0.0007  val AP 0.0400\n",
      "  âœ… New best AP: 0.0400 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 06  loss 0.0007  val AP 0.0539\n",
      "  âœ… New best AP: 0.0539 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 07  loss 0.0008  val AP 0.0670\n",
      "  âœ… New best AP: 0.0670 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 08  loss 0.0008  val AP 0.0791\n",
      "  âœ… New best AP: 0.0791 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 09  loss 0.0007  val AP 0.0904\n",
      "  âœ… New best AP: 0.0904 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 10  loss 0.0006  val AP 0.1446\n",
      "  âœ… New best AP: 0.1446 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 11  loss 0.0005  val AP 0.2057\n",
      "  âœ… New best AP: 0.2057 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 12  loss 0.0007  val AP 0.2361\n",
      "  âœ… New best AP: 0.2361 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 13  loss 0.0006  val AP 0.2370\n",
      "  âœ… New best AP: 0.2370 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 14  loss 0.0007  val AP 0.2383\n",
      "  âœ… New best AP: 0.2383 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 15  loss 0.0005  val AP 0.2398\n",
      "  âœ… New best AP: 0.2398 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 16  loss 0.0005  val AP 0.2411\n",
      "  âœ… New best AP: 0.2411 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 17  loss 0.0005  val AP 0.1860\n",
      "[NR-AR-LBD | seed 61] ep 18  loss 0.0005  val AP 0.1913\n",
      "[NR-AR-LBD | seed 61] ep 19  loss 0.0005  val AP 0.1914\n",
      "[NR-AR-LBD | seed 61] ep 20  loss 0.0005  val AP 0.1946\n",
      "[NR-AR-LBD | seed 61] ep 21  loss 0.0005  val AP 0.1955\n",
      "[NR-AR-LBD | seed 61] ep 22  loss 0.0005  val AP 0.1957\n",
      "[NR-AR-LBD | seed 61] ep 23  loss 0.0005  val AP 0.1961\n",
      "[NR-AR-LBD | seed 61] ep 24  loss 0.0005  val AP 0.1965\n",
      "[NR-AR-LBD | seed 61] ep 25  loss 0.0005  val AP 0.1967\n",
      "[NR-AR-LBD | seed 61] ep 26  loss 0.0005  val AP 0.1971\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.2411\n",
      "[NR-AR-LBD | seed 83] ep 01  loss 0.0010  val AP 0.0176\n",
      "  âœ… New best AP: 0.0176 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 02  loss 0.0007  val AP 0.0189\n",
      "  âœ… New best AP: 0.0189 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 03  loss 0.0008  val AP 0.0215\n",
      "  âœ… New best AP: 0.0215 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 04  loss 0.0006  val AP 0.0264\n",
      "  âœ… New best AP: 0.0264 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 05  loss 0.0006  val AP 0.0352\n",
      "  âœ… New best AP: 0.0352 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 06  loss 0.0006  val AP 0.0524\n",
      "  âœ… New best AP: 0.0524 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 07  loss 0.0007  val AP 0.0859\n",
      "  âœ… New best AP: 0.0859 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 08  loss 0.0006  val AP 0.1295\n",
      "  âœ… New best AP: 0.1295 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 09  loss 0.0007  val AP 0.1399\n",
      "  âœ… New best AP: 0.1399 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 10  loss 0.0006  val AP 0.1574\n",
      "  âœ… New best AP: 0.1574 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 11  loss 0.0007  val AP 0.2139\n",
      "  âœ… New best AP: 0.2139 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 12  loss 0.0005  val AP 0.2250\n",
      "  âœ… New best AP: 0.2250 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 13  loss 0.0006  val AP 0.2252\n",
      "  âœ… New best AP: 0.2252 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 14  loss 0.0005  val AP 0.2364\n",
      "  âœ… New best AP: 0.2364 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 15  loss 0.0007  val AP 0.2368\n",
      "  âœ… New best AP: 0.2368 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 16  loss 0.0005  val AP 0.2369\n",
      "  âœ… New best AP: 0.2369 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 17  loss 0.0006  val AP 0.2370\n",
      "  âœ… New best AP: 0.2370 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 18  loss 0.0005  val AP 0.2559\n",
      "  âœ… New best AP: 0.2559 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 19  loss 0.0005  val AP 0.2562\n",
      "  âœ… New best AP: 0.2562 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 20  loss 0.0005  val AP 0.2563\n",
      "  âœ… New best AP: 0.2563 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 21  loss 0.0005  val AP 0.2566\n",
      "  âœ… New best AP: 0.2566 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 22  loss 0.0005  val AP 0.2569\n",
      "  âœ… New best AP: 0.2569 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 23  loss 0.0005  val AP 0.2571\n",
      "  âœ… New best AP: 0.2571 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 24  loss 0.0006  val AP 0.2576\n",
      "  âœ… New best AP: 0.2576 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 25  loss 0.0005  val AP 0.2580\n",
      "  âœ… New best AP: 0.2580 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 26  loss 0.0004  val AP 0.2586\n",
      "  âœ… New best AP: 0.2586 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 27  loss 0.0005  val AP 0.2593\n",
      "  âœ… New best AP: 0.2593 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 28  loss 0.0005  val AP 0.2597\n",
      "  âœ… New best AP: 0.2597 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 29  loss 0.0004  val AP 0.2599\n",
      "  âœ… New best AP: 0.2599 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 30  loss 0.0005  val AP 0.2600\n",
      "  âœ… New best AP: 0.2600 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 31  loss 0.0005  val AP 0.2601\n",
      "  âœ… New best AP: 0.2601 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 32  loss 0.0005  val AP 0.2605\n",
      "  âœ… New best AP: 0.2605 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 33  loss 0.0005  val AP 0.2606\n",
      "  âœ… New best AP: 0.2606 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 34  loss 0.0005  val AP 0.2611\n",
      "  âœ… New best AP: 0.2611 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 35  loss 0.0005  val AP 0.2615\n",
      "  âœ… New best AP: 0.2615 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 36  loss 0.0005  val AP 0.2620\n",
      "  âœ… New best AP: 0.2620 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 37  loss 0.0005  val AP 0.2625\n",
      "  âœ… New best AP: 0.2625 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 38  loss 0.0005  val AP 0.2626\n",
      "  âœ… New best AP: 0.2626 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 39  loss 0.0005  val AP 0.2626\n",
      "  âœ… New best AP: 0.2626 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 40  loss 0.0005  val AP 0.2627\n",
      "  âœ… New best AP: 0.2627 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 41  loss 0.0005  val AP 0.2628\n",
      "  âœ… New best AP: 0.2628 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 42  loss 0.0005  val AP 0.2629\n",
      "  âœ… New best AP: 0.2629 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 43  loss 0.0005  val AP 0.2628\n",
      "[NR-AR-LBD | seed 83] ep 44  loss 0.0005  val AP 0.2629\n",
      "  âœ… New best AP: 0.2629 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 45  loss 0.0005  val AP 0.2999\n",
      "  âœ… New best AP: 0.2999 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 46  loss 0.0005  val AP 0.3000\n",
      "  âœ… New best AP: 0.3000 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 47  loss 0.0005  val AP 0.3006\n",
      "  âœ… New best AP: 0.3006 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 48  loss 0.0005  val AP 0.3006\n",
      "  âœ… New best AP: 0.3006 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 49  loss 0.0005  val AP 0.3007\n",
      "  âœ… New best AP: 0.3007 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 50  loss 0.0005  val AP 0.3007\n",
      "  âœ… New best AP: 0.3007 â†’ v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-AhR (label 2)\n",
      "==============================\n",
      "[NR-AhR | seed 13] ep 01  loss 0.0004  val AP 0.0435\n",
      "  âœ… New best AP: 0.0435 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 02  loss 0.0003  val AP 0.0448\n",
      "  âœ… New best AP: 0.0448 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 03  loss 0.0003  val AP 0.0472\n",
      "  âœ… New best AP: 0.0472 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 04  loss 0.0003  val AP 0.0517\n",
      "  âœ… New best AP: 0.0517 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 05  loss 0.0003  val AP 0.0601\n",
      "  âœ… New best AP: 0.0601 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 06  loss 0.0004  val AP 0.0773\n",
      "  âœ… New best AP: 0.0773 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 07  loss 0.0003  val AP 0.1098\n",
      "  âœ… New best AP: 0.1098 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 08  loss 0.0004  val AP 0.1619\n",
      "  âœ… New best AP: 0.1619 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 09  loss 0.0003  val AP 0.2332\n",
      "  âœ… New best AP: 0.2332 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 10  loss 0.0003  val AP 0.3089\n",
      "  âœ… New best AP: 0.3089 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 11  loss 0.0003  val AP 0.3432\n",
      "  âœ… New best AP: 0.3432 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 12  loss 0.0003  val AP 0.3746\n",
      "  âœ… New best AP: 0.3746 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 13  loss 0.0002  val AP 0.3983\n",
      "  âœ… New best AP: 0.3983 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 14  loss 0.0003  val AP 0.4104\n",
      "  âœ… New best AP: 0.4104 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 15  loss 0.0002  val AP 0.4211\n",
      "  âœ… New best AP: 0.4211 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 16  loss 0.0003  val AP 0.4387\n",
      "  âœ… New best AP: 0.4387 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 17  loss 0.0002  val AP 0.4445\n",
      "  âœ… New best AP: 0.4445 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 18  loss 0.0003  val AP 0.4519\n",
      "  âœ… New best AP: 0.4519 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 19  loss 0.0002  val AP 0.4542\n",
      "  âœ… New best AP: 0.4542 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 20  loss 0.0003  val AP 0.4576\n",
      "  âœ… New best AP: 0.4576 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 21  loss 0.0002  val AP 0.4630\n",
      "  âœ… New best AP: 0.4630 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 22  loss 0.0003  val AP 0.4649\n",
      "  âœ… New best AP: 0.4649 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 23  loss 0.0002  val AP 0.4676\n",
      "  âœ… New best AP: 0.4676 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 24  loss 0.0003  val AP 0.4713\n",
      "  âœ… New best AP: 0.4713 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 25  loss 0.0003  val AP 0.4715\n",
      "  âœ… New best AP: 0.4715 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 26  loss 0.0003  val AP 0.4741\n",
      "  âœ… New best AP: 0.4741 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 27  loss 0.0002  val AP 0.4781\n",
      "  âœ… New best AP: 0.4781 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 28  loss 0.0002  val AP 0.4785\n",
      "  âœ… New best AP: 0.4785 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 29  loss 0.0002  val AP 0.4798\n",
      "  âœ… New best AP: 0.4798 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 30  loss 0.0003  val AP 0.4800\n",
      "  âœ… New best AP: 0.4800 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 31  loss 0.0002  val AP 0.4797\n",
      "[NR-AhR | seed 13] ep 32  loss 0.0002  val AP 0.4802\n",
      "  âœ… New best AP: 0.4802 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 33  loss 0.0002  val AP 0.4819\n",
      "  âœ… New best AP: 0.4819 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 34  loss 0.0002  val AP 0.4820\n",
      "  âœ… New best AP: 0.4820 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 35  loss 0.0002  val AP 0.4841\n",
      "  âœ… New best AP: 0.4841 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 36  loss 0.0002  val AP 0.4858\n",
      "  âœ… New best AP: 0.4858 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 37  loss 0.0002  val AP 0.4878\n",
      "  âœ… New best AP: 0.4878 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 38  loss 0.0002  val AP 0.4871\n",
      "[NR-AhR | seed 13] ep 39  loss 0.0003  val AP 0.4882\n",
      "  âœ… New best AP: 0.4882 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 40  loss 0.0002  val AP 0.4885\n",
      "  âœ… New best AP: 0.4885 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 41  loss 0.0002  val AP 0.4890\n",
      "  âœ… New best AP: 0.4890 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 42  loss 0.0002  val AP 0.4887\n",
      "[NR-AhR | seed 13] ep 43  loss 0.0002  val AP 0.4886\n",
      "[NR-AhR | seed 13] ep 44  loss 0.0002  val AP 0.4898\n",
      "  âœ… New best AP: 0.4898 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 45  loss 0.0002  val AP 0.4891\n",
      "[NR-AhR | seed 13] ep 46  loss 0.0002  val AP 0.4892\n",
      "[NR-AhR | seed 13] ep 47  loss 0.0002  val AP 0.4909\n",
      "  âœ… New best AP: 0.4909 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 48  loss 0.0002  val AP 0.4965\n",
      "  âœ… New best AP: 0.4965 â†’ v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 49  loss 0.0003  val AP 0.4964\n",
      "[NR-AhR | seed 13] ep 50  loss 0.0002  val AP 0.4962\n",
      "[NR-AhR | seed 29] ep 01  loss 0.0005  val AP 0.1463\n",
      "  âœ… New best AP: 0.1463 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 02  loss 0.0003  val AP 0.1783\n",
      "  âœ… New best AP: 0.1783 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 03  loss 0.0004  val AP 0.2228\n",
      "  âœ… New best AP: 0.2228 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 04  loss 0.0004  val AP 0.2970\n",
      "  âœ… New best AP: 0.2970 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 05  loss 0.0003  val AP 0.3429\n",
      "  âœ… New best AP: 0.3429 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 06  loss 0.0003  val AP 0.3760\n",
      "  âœ… New best AP: 0.3760 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 07  loss 0.0003  val AP 0.3963\n",
      "  âœ… New best AP: 0.3963 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 08  loss 0.0004  val AP 0.4069\n",
      "  âœ… New best AP: 0.4069 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 09  loss 0.0003  val AP 0.4157\n",
      "  âœ… New best AP: 0.4157 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 10  loss 0.0003  val AP 0.4281\n",
      "  âœ… New best AP: 0.4281 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 11  loss 0.0003  val AP 0.4382\n",
      "  âœ… New best AP: 0.4382 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 12  loss 0.0002  val AP 0.4465\n",
      "  âœ… New best AP: 0.4465 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 13  loss 0.0003  val AP 0.4537\n",
      "  âœ… New best AP: 0.4537 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 14  loss 0.0003  val AP 0.4622\n",
      "  âœ… New best AP: 0.4622 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 15  loss 0.0003  val AP 0.4742\n",
      "  âœ… New best AP: 0.4742 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 16  loss 0.0002  val AP 0.4782\n",
      "  âœ… New best AP: 0.4782 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 17  loss 0.0003  val AP 0.4809\n",
      "  âœ… New best AP: 0.4809 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 18  loss 0.0002  val AP 0.4916\n",
      "  âœ… New best AP: 0.4916 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 19  loss 0.0003  val AP 0.4930\n",
      "  âœ… New best AP: 0.4930 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 20  loss 0.0003  val AP 0.4928\n",
      "[NR-AhR | seed 29] ep 21  loss 0.0002  val AP 0.4987\n",
      "  âœ… New best AP: 0.4987 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 22  loss 0.0002  val AP 0.5025\n",
      "  âœ… New best AP: 0.5025 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 23  loss 0.0002  val AP 0.5063\n",
      "  âœ… New best AP: 0.5063 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 24  loss 0.0002  val AP 0.5064\n",
      "  âœ… New best AP: 0.5064 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 25  loss 0.0002  val AP 0.5152\n",
      "  âœ… New best AP: 0.5152 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 26  loss 0.0002  val AP 0.5150\n",
      "[NR-AhR | seed 29] ep 27  loss 0.0002  val AP 0.5174\n",
      "  âœ… New best AP: 0.5174 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 28  loss 0.0003  val AP 0.5227\n",
      "  âœ… New best AP: 0.5227 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 29  loss 0.0002  val AP 0.5232\n",
      "  âœ… New best AP: 0.5232 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 30  loss 0.0003  val AP 0.5251\n",
      "  âœ… New best AP: 0.5251 â†’ v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 31  loss 0.0002  val AP 0.5234\n",
      "[NR-AhR | seed 29] ep 32  loss 0.0003  val AP 0.5232\n",
      "[NR-AhR | seed 29] ep 33  loss 0.0002  val AP 0.5205\n",
      "[NR-AhR | seed 29] ep 34  loss 0.0002  val AP 0.5217\n",
      "[NR-AhR | seed 29] ep 35  loss 0.0002  val AP 0.5216\n",
      "[NR-AhR | seed 29] ep 36  loss 0.0003  val AP 0.5214\n",
      "[NR-AhR | seed 29] ep 37  loss 0.0002  val AP 0.5221\n",
      "[NR-AhR | seed 29] ep 38  loss 0.0002  val AP 0.5236\n",
      "[NR-AhR | seed 29] ep 39  loss 0.0002  val AP 0.5220\n",
      "[NR-AhR | seed 29] ep 40  loss 0.0002  val AP 0.5222\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.5251\n",
      "[NR-AhR | seed 47] ep 01  loss 0.0004  val AP 0.2954\n",
      "  âœ… New best AP: 0.2954 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 02  loss 0.0003  val AP 0.3084\n",
      "  âœ… New best AP: 0.3084 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 03  loss 0.0004  val AP 0.3248\n",
      "  âœ… New best AP: 0.3248 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 04  loss 0.0004  val AP 0.3428\n",
      "  âœ… New best AP: 0.3428 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 05  loss 0.0003  val AP 0.3663\n",
      "  âœ… New best AP: 0.3663 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 06  loss 0.0003  val AP 0.3803\n",
      "  âœ… New best AP: 0.3803 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 07  loss 0.0003  val AP 0.3906\n",
      "  âœ… New best AP: 0.3906 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 08  loss 0.0003  val AP 0.4022\n",
      "  âœ… New best AP: 0.4022 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 09  loss 0.0004  val AP 0.4048\n",
      "  âœ… New best AP: 0.4048 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 10  loss 0.0003  val AP 0.4110\n",
      "  âœ… New best AP: 0.4110 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 11  loss 0.0003  val AP 0.4125\n",
      "  âœ… New best AP: 0.4125 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 12  loss 0.0002  val AP 0.4136\n",
      "  âœ… New best AP: 0.4136 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 13  loss 0.0003  val AP 0.4150\n",
      "  âœ… New best AP: 0.4150 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 14  loss 0.0002  val AP 0.4197\n",
      "  âœ… New best AP: 0.4197 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 15  loss 0.0003  val AP 0.4206\n",
      "  âœ… New best AP: 0.4206 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 16  loss 0.0003  val AP 0.4206\n",
      "  âœ… New best AP: 0.4206 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 17  loss 0.0002  val AP 0.4217\n",
      "  âœ… New best AP: 0.4217 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 18  loss 0.0003  val AP 0.4271\n",
      "  âœ… New best AP: 0.4271 â†’ v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 19  loss 0.0002  val AP 0.4268\n",
      "[NR-AhR | seed 47] ep 20  loss 0.0002  val AP 0.4267\n",
      "[NR-AhR | seed 47] ep 21  loss 0.0002  val AP 0.4253\n",
      "[NR-AhR | seed 47] ep 22  loss 0.0003  val AP 0.4235\n",
      "[NR-AhR | seed 47] ep 23  loss 0.0002  val AP 0.4243\n",
      "[NR-AhR | seed 47] ep 24  loss 0.0003  val AP 0.4236\n",
      "[NR-AhR | seed 47] ep 25  loss 0.0002  val AP 0.4240\n",
      "[NR-AhR | seed 47] ep 26  loss 0.0002  val AP 0.4245\n",
      "[NR-AhR | seed 47] ep 27  loss 0.0002  val AP 0.4240\n",
      "[NR-AhR | seed 47] ep 28  loss 0.0002  val AP 0.4254\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.4271\n",
      "[NR-AhR | seed 61] ep 01  loss 0.0006  val AP 0.1098\n",
      "  âœ… New best AP: 0.1098 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 02  loss 0.0003  val AP 0.1275\n",
      "  âœ… New best AP: 0.1275 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 03  loss 0.0004  val AP 0.1531\n",
      "  âœ… New best AP: 0.1531 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 04  loss 0.0004  val AP 0.1907\n",
      "  âœ… New best AP: 0.1907 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 05  loss 0.0003  val AP 0.2298\n",
      "  âœ… New best AP: 0.2298 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 06  loss 0.0003  val AP 0.2699\n",
      "  âœ… New best AP: 0.2699 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 07  loss 0.0003  val AP 0.3004\n",
      "  âœ… New best AP: 0.3004 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 08  loss 0.0004  val AP 0.3174\n",
      "  âœ… New best AP: 0.3174 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 09  loss 0.0003  val AP 0.3381\n",
      "  âœ… New best AP: 0.3381 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 10  loss 0.0003  val AP 0.3478\n",
      "  âœ… New best AP: 0.3478 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 11  loss 0.0003  val AP 0.3587\n",
      "  âœ… New best AP: 0.3587 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 12  loss 0.0003  val AP 0.3694\n",
      "  âœ… New best AP: 0.3694 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 13  loss 0.0003  val AP 0.3844\n",
      "  âœ… New best AP: 0.3844 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 14  loss 0.0003  val AP 0.3907\n",
      "  âœ… New best AP: 0.3907 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 15  loss 0.0003  val AP 0.4004\n",
      "  âœ… New best AP: 0.4004 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 16  loss 0.0003  val AP 0.4105\n",
      "  âœ… New best AP: 0.4105 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 17  loss 0.0003  val AP 0.4138\n",
      "  âœ… New best AP: 0.4138 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 18  loss 0.0002  val AP 0.4254\n",
      "  âœ… New best AP: 0.4254 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 19  loss 0.0003  val AP 0.4280\n",
      "  âœ… New best AP: 0.4280 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 20  loss 0.0002  val AP 0.4323\n",
      "  âœ… New best AP: 0.4323 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 21  loss 0.0002  val AP 0.4386\n",
      "  âœ… New best AP: 0.4386 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 22  loss 0.0003  val AP 0.4399\n",
      "  âœ… New best AP: 0.4399 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 23  loss 0.0002  val AP 0.4434\n",
      "  âœ… New best AP: 0.4434 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 24  loss 0.0003  val AP 0.4448\n",
      "  âœ… New best AP: 0.4448 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 25  loss 0.0002  val AP 0.4476\n",
      "  âœ… New best AP: 0.4476 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 26  loss 0.0002  val AP 0.4512\n",
      "  âœ… New best AP: 0.4512 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 27  loss 0.0002  val AP 0.4532\n",
      "  âœ… New best AP: 0.4532 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 28  loss 0.0002  val AP 0.4552\n",
      "  âœ… New best AP: 0.4552 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 29  loss 0.0003  val AP 0.4500\n",
      "[NR-AhR | seed 61] ep 30  loss 0.0002  val AP 0.4512\n",
      "[NR-AhR | seed 61] ep 31  loss 0.0003  val AP 0.4541\n",
      "[NR-AhR | seed 61] ep 32  loss 0.0002  val AP 0.4565\n",
      "  âœ… New best AP: 0.4565 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 33  loss 0.0002  val AP 0.4514\n",
      "[NR-AhR | seed 61] ep 34  loss 0.0002  val AP 0.4541\n",
      "[NR-AhR | seed 61] ep 35  loss 0.0002  val AP 0.4578\n",
      "  âœ… New best AP: 0.4578 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 36  loss 0.0002  val AP 0.4577\n",
      "[NR-AhR | seed 61] ep 37  loss 0.0002  val AP 0.4585\n",
      "  âœ… New best AP: 0.4585 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 38  loss 0.0002  val AP 0.4592\n",
      "  âœ… New best AP: 0.4592 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 39  loss 0.0002  val AP 0.4604\n",
      "  âœ… New best AP: 0.4604 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 40  loss 0.0002  val AP 0.4621\n",
      "  âœ… New best AP: 0.4621 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 41  loss 0.0002  val AP 0.4633\n",
      "  âœ… New best AP: 0.4633 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 42  loss 0.0002  val AP 0.4641\n",
      "  âœ… New best AP: 0.4641 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 43  loss 0.0002  val AP 0.4641\n",
      "[NR-AhR | seed 61] ep 44  loss 0.0003  val AP 0.4648\n",
      "  âœ… New best AP: 0.4648 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 45  loss 0.0002  val AP 0.4660\n",
      "  âœ… New best AP: 0.4660 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 46  loss 0.0002  val AP 0.4677\n",
      "  âœ… New best AP: 0.4677 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 47  loss 0.0002  val AP 0.4681\n",
      "  âœ… New best AP: 0.4681 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 48  loss 0.0003  val AP 0.4683\n",
      "  âœ… New best AP: 0.4683 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 49  loss 0.0002  val AP 0.4691\n",
      "  âœ… New best AP: 0.4691 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 50  loss 0.0002  val AP 0.4693\n",
      "  âœ… New best AP: 0.4693 â†’ v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 83] ep 01  loss 0.0004  val AP 0.0667\n",
      "  âœ… New best AP: 0.0667 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 02  loss 0.0003  val AP 0.0718\n",
      "  âœ… New best AP: 0.0718 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 03  loss 0.0004  val AP 0.0813\n",
      "  âœ… New best AP: 0.0813 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 04  loss 0.0004  val AP 0.0978\n",
      "  âœ… New best AP: 0.0978 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 05  loss 0.0003  val AP 0.1263\n",
      "  âœ… New best AP: 0.1263 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 06  loss 0.0003  val AP 0.1662\n",
      "  âœ… New best AP: 0.1662 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 07  loss 0.0004  val AP 0.2027\n",
      "  âœ… New best AP: 0.2027 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 08  loss 0.0003  val AP 0.2321\n",
      "  âœ… New best AP: 0.2321 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 09  loss 0.0003  val AP 0.2562\n",
      "  âœ… New best AP: 0.2562 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 10  loss 0.0003  val AP 0.2742\n",
      "  âœ… New best AP: 0.2742 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 11  loss 0.0003  val AP 0.3033\n",
      "  âœ… New best AP: 0.3033 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 12  loss 0.0003  val AP 0.3276\n",
      "  âœ… New best AP: 0.3276 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 13  loss 0.0002  val AP 0.3417\n",
      "  âœ… New best AP: 0.3417 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 14  loss 0.0002  val AP 0.3503\n",
      "  âœ… New best AP: 0.3503 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 15  loss 0.0003  val AP 0.3610\n",
      "  âœ… New best AP: 0.3610 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 16  loss 0.0003  val AP 0.3679\n",
      "  âœ… New best AP: 0.3679 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 17  loss 0.0003  val AP 0.3736\n",
      "  âœ… New best AP: 0.3736 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 18  loss 0.0003  val AP 0.3780\n",
      "  âœ… New best AP: 0.3780 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 19  loss 0.0002  val AP 0.3829\n",
      "  âœ… New best AP: 0.3829 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 20  loss 0.0003  val AP 0.3999\n",
      "  âœ… New best AP: 0.3999 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 21  loss 0.0002  val AP 0.4033\n",
      "  âœ… New best AP: 0.4033 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 22  loss 0.0003  val AP 0.4074\n",
      "  âœ… New best AP: 0.4074 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 23  loss 0.0002  val AP 0.4140\n",
      "  âœ… New best AP: 0.4140 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 24  loss 0.0003  val AP 0.4192\n",
      "  âœ… New best AP: 0.4192 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 25  loss 0.0002  val AP 0.4215\n",
      "  âœ… New best AP: 0.4215 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 26  loss 0.0003  val AP 0.4297\n",
      "  âœ… New best AP: 0.4297 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 27  loss 0.0002  val AP 0.4314\n",
      "  âœ… New best AP: 0.4314 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 28  loss 0.0003  val AP 0.4342\n",
      "  âœ… New best AP: 0.4342 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 29  loss 0.0002  val AP 0.4348\n",
      "  âœ… New best AP: 0.4348 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 30  loss 0.0002  val AP 0.4344\n",
      "[NR-AhR | seed 83] ep 31  loss 0.0002  val AP 0.4348\n",
      "[NR-AhR | seed 83] ep 32  loss 0.0002  val AP 0.4371\n",
      "  âœ… New best AP: 0.4371 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 33  loss 0.0002  val AP 0.4374\n",
      "  âœ… New best AP: 0.4374 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 34  loss 0.0002  val AP 0.4384\n",
      "  âœ… New best AP: 0.4384 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 35  loss 0.0002  val AP 0.4390\n",
      "  âœ… New best AP: 0.4390 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 36  loss 0.0002  val AP 0.4408\n",
      "  âœ… New best AP: 0.4408 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 37  loss 0.0002  val AP 0.4433\n",
      "  âœ… New best AP: 0.4433 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 38  loss 0.0002  val AP 0.4462\n",
      "  âœ… New best AP: 0.4462 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 39  loss 0.0002  val AP 0.4475\n",
      "  âœ… New best AP: 0.4475 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 40  loss 0.0002  val AP 0.4477\n",
      "  âœ… New best AP: 0.4477 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 41  loss 0.0003  val AP 0.4482\n",
      "  âœ… New best AP: 0.4482 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 42  loss 0.0003  val AP 0.4490\n",
      "  âœ… New best AP: 0.4490 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 43  loss 0.0003  val AP 0.4502\n",
      "  âœ… New best AP: 0.4502 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 44  loss 0.0003  val AP 0.4508\n",
      "  âœ… New best AP: 0.4508 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 45  loss 0.0002  val AP 0.4514\n",
      "  âœ… New best AP: 0.4514 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 46  loss 0.0003  val AP 0.4525\n",
      "  âœ… New best AP: 0.4525 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 47  loss 0.0002  val AP 0.4536\n",
      "  âœ… New best AP: 0.4536 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 48  loss 0.0002  val AP 0.4538\n",
      "  âœ… New best AP: 0.4538 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 49  loss 0.0002  val AP 0.4528\n",
      "[NR-AhR | seed 83] ep 50  loss 0.0002  val AP 0.4540\n",
      "  âœ… New best AP: 0.4540 â†’ v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-Aromatase (label 3)\n",
      "==============================\n",
      "[NR-Aromatase | seed 13] ep 01  loss 0.0009  val AP 0.0446\n",
      "  âœ… New best AP: 0.0446 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 02  loss 0.0007  val AP 0.0452\n",
      "  âœ… New best AP: 0.0452 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 03  loss 0.0007  val AP 0.0467\n",
      "  âœ… New best AP: 0.0467 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 04  loss 0.0009  val AP 0.0495\n",
      "  âœ… New best AP: 0.0495 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 05  loss 0.0007  val AP 0.0517\n",
      "  âœ… New best AP: 0.0517 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 06  loss 0.0006  val AP 0.0553\n",
      "  âœ… New best AP: 0.0553 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 07  loss 0.0006  val AP 0.0599\n",
      "  âœ… New best AP: 0.0599 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 08  loss 0.0007  val AP 0.0668\n",
      "  âœ… New best AP: 0.0668 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 09  loss 0.0007  val AP 0.0740\n",
      "  âœ… New best AP: 0.0740 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 10  loss 0.0008  val AP 0.0822\n",
      "  âœ… New best AP: 0.0822 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 11  loss 0.0007  val AP 0.0899\n",
      "  âœ… New best AP: 0.0899 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 12  loss 0.0006  val AP 0.0982\n",
      "  âœ… New best AP: 0.0982 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 13  loss 0.0006  val AP 0.1073\n",
      "  âœ… New best AP: 0.1073 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 14  loss 0.0006  val AP 0.1133\n",
      "  âœ… New best AP: 0.1133 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 15  loss 0.0006  val AP 0.1202\n",
      "  âœ… New best AP: 0.1202 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 16  loss 0.0008  val AP 0.1273\n",
      "  âœ… New best AP: 0.1273 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 17  loss 0.0006  val AP 0.1329\n",
      "  âœ… New best AP: 0.1329 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 18  loss 0.0006  val AP 0.1386\n",
      "  âœ… New best AP: 0.1386 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 19  loss 0.0006  val AP 0.1428\n",
      "  âœ… New best AP: 0.1428 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 20  loss 0.0006  val AP 0.1479\n",
      "  âœ… New best AP: 0.1479 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 21  loss 0.0006  val AP 0.1567\n",
      "  âœ… New best AP: 0.1567 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 22  loss 0.0006  val AP 0.1596\n",
      "  âœ… New best AP: 0.1596 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 23  loss 0.0005  val AP 0.1677\n",
      "  âœ… New best AP: 0.1677 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 24  loss 0.0006  val AP 0.1708\n",
      "  âœ… New best AP: 0.1708 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 25  loss 0.0006  val AP 0.1736\n",
      "  âœ… New best AP: 0.1736 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 26  loss 0.0006  val AP 0.1757\n",
      "  âœ… New best AP: 0.1757 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 27  loss 0.0006  val AP 0.1923\n",
      "  âœ… New best AP: 0.1923 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 28  loss 0.0005  val AP 0.1957\n",
      "  âœ… New best AP: 0.1957 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 29  loss 0.0006  val AP 0.1981\n",
      "  âœ… New best AP: 0.1981 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 30  loss 0.0005  val AP 0.2031\n",
      "  âœ… New best AP: 0.2031 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 31  loss 0.0005  val AP 0.2062\n",
      "  âœ… New best AP: 0.2062 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 32  loss 0.0006  val AP 0.2084\n",
      "  âœ… New best AP: 0.2084 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 33  loss 0.0005  val AP 0.2093\n",
      "  âœ… New best AP: 0.2093 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 34  loss 0.0006  val AP 0.2107\n",
      "  âœ… New best AP: 0.2107 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 35  loss 0.0005  val AP 0.2117\n",
      "  âœ… New best AP: 0.2117 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 36  loss 0.0006  val AP 0.2126\n",
      "  âœ… New best AP: 0.2126 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 37  loss 0.0006  val AP 0.2144\n",
      "  âœ… New best AP: 0.2144 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 38  loss 0.0006  val AP 0.2152\n",
      "  âœ… New best AP: 0.2152 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 39  loss 0.0006  val AP 0.2157\n",
      "  âœ… New best AP: 0.2157 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 40  loss 0.0006  val AP 0.2209\n",
      "  âœ… New best AP: 0.2209 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 41  loss 0.0005  val AP 0.2211\n",
      "  âœ… New best AP: 0.2211 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 42  loss 0.0006  val AP 0.2239\n",
      "  âœ… New best AP: 0.2239 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 43  loss 0.0006  val AP 0.2242\n",
      "  âœ… New best AP: 0.2242 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 44  loss 0.0006  val AP 0.2254\n",
      "  âœ… New best AP: 0.2254 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 45  loss 0.0006  val AP 0.2256\n",
      "  âœ… New best AP: 0.2256 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 46  loss 0.0006  val AP 0.2268\n",
      "  âœ… New best AP: 0.2268 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 47  loss 0.0006  val AP 0.2306\n",
      "  âœ… New best AP: 0.2306 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 48  loss 0.0006  val AP 0.2317\n",
      "  âœ… New best AP: 0.2317 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 49  loss 0.0006  val AP 0.2337\n",
      "  âœ… New best AP: 0.2337 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 50  loss 0.0006  val AP 0.2439\n",
      "  âœ… New best AP: 0.2439 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 01  loss 0.0012  val AP 0.0793\n",
      "  âœ… New best AP: 0.0793 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 02  loss 0.0008  val AP 0.0833\n",
      "  âœ… New best AP: 0.0833 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 03  loss 0.0010  val AP 0.0887\n",
      "  âœ… New best AP: 0.0887 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 04  loss 0.0010  val AP 0.0992\n",
      "  âœ… New best AP: 0.0992 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 05  loss 0.0007  val AP 0.1043\n",
      "  âœ… New best AP: 0.1043 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 06  loss 0.0007  val AP 0.1083\n",
      "  âœ… New best AP: 0.1083 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 07  loss 0.0008  val AP 0.1123\n",
      "  âœ… New best AP: 0.1123 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 08  loss 0.0007  val AP 0.1165\n",
      "  âœ… New best AP: 0.1165 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 09  loss 0.0007  val AP 0.1222\n",
      "  âœ… New best AP: 0.1222 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 10  loss 0.0007  val AP 0.1243\n",
      "  âœ… New best AP: 0.1243 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 11  loss 0.0006  val AP 0.1276\n",
      "  âœ… New best AP: 0.1276 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 12  loss 0.0006  val AP 0.1310\n",
      "  âœ… New best AP: 0.1310 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 13  loss 0.0007  val AP 0.1356\n",
      "  âœ… New best AP: 0.1356 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 14  loss 0.0006  val AP 0.1368\n",
      "  âœ… New best AP: 0.1368 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 15  loss 0.0006  val AP 0.1387\n",
      "  âœ… New best AP: 0.1387 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 16  loss 0.0006  val AP 0.1413\n",
      "  âœ… New best AP: 0.1413 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 17  loss 0.0006  val AP 0.1452\n",
      "  âœ… New best AP: 0.1452 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 18  loss 0.0006  val AP 0.1484\n",
      "  âœ… New best AP: 0.1484 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 19  loss 0.0006  val AP 0.1503\n",
      "  âœ… New best AP: 0.1503 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 20  loss 0.0006  val AP 0.1525\n",
      "  âœ… New best AP: 0.1525 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 21  loss 0.0006  val AP 0.1547\n",
      "  âœ… New best AP: 0.1547 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 22  loss 0.0005  val AP 0.1558\n",
      "  âœ… New best AP: 0.1558 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 23  loss 0.0006  val AP 0.1576\n",
      "  âœ… New best AP: 0.1576 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 24  loss 0.0006  val AP 0.1595\n",
      "  âœ… New best AP: 0.1595 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 25  loss 0.0006  val AP 0.1620\n",
      "  âœ… New best AP: 0.1620 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 26  loss 0.0006  val AP 0.1638\n",
      "  âœ… New best AP: 0.1638 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 27  loss 0.0006  val AP 0.1679\n",
      "  âœ… New best AP: 0.1679 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 28  loss 0.0006  val AP 0.1704\n",
      "  âœ… New best AP: 0.1704 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 29  loss 0.0006  val AP 0.1754\n",
      "  âœ… New best AP: 0.1754 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 30  loss 0.0006  val AP 0.1782\n",
      "  âœ… New best AP: 0.1782 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 31  loss 0.0006  val AP 0.1786\n",
      "  âœ… New best AP: 0.1786 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 32  loss 0.0006  val AP 0.1821\n",
      "  âœ… New best AP: 0.1821 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 33  loss 0.0006  val AP 0.1859\n",
      "  âœ… New best AP: 0.1859 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 34  loss 0.0006  val AP 0.1873\n",
      "  âœ… New best AP: 0.1873 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 35  loss 0.0006  val AP 0.1881\n",
      "  âœ… New best AP: 0.1881 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 36  loss 0.0006  val AP 0.1889\n",
      "  âœ… New best AP: 0.1889 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 37  loss 0.0006  val AP 0.1903\n",
      "  âœ… New best AP: 0.1903 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 38  loss 0.0006  val AP 0.1906\n",
      "  âœ… New best AP: 0.1906 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 39  loss 0.0006  val AP 0.1908\n",
      "  âœ… New best AP: 0.1908 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 40  loss 0.0006  val AP 0.1906\n",
      "[NR-Aromatase | seed 29] ep 41  loss 0.0006  val AP 0.1917\n",
      "  âœ… New best AP: 0.1917 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 42  loss 0.0006  val AP 0.1929\n",
      "  âœ… New best AP: 0.1929 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 43  loss 0.0006  val AP 0.1940\n",
      "  âœ… New best AP: 0.1940 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 44  loss 0.0006  val AP 0.1960\n",
      "  âœ… New best AP: 0.1960 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 45  loss 0.0006  val AP 0.1955\n",
      "[NR-Aromatase | seed 29] ep 46  loss 0.0006  val AP 0.1961\n",
      "  âœ… New best AP: 0.1961 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 47  loss 0.0006  val AP 0.1971\n",
      "  âœ… New best AP: 0.1971 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 48  loss 0.0006  val AP 0.1983\n",
      "  âœ… New best AP: 0.1983 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 49  loss 0.0006  val AP 0.1988\n",
      "  âœ… New best AP: 0.1988 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 50  loss 0.0006  val AP 0.2004\n",
      "  âœ… New best AP: 0.2004 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 01  loss 0.0011  val AP 0.1492\n",
      "  âœ… New best AP: 0.1492 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 02  loss 0.0008  val AP 0.1587\n",
      "  âœ… New best AP: 0.1587 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 03  loss 0.0009  val AP 0.1618\n",
      "  âœ… New best AP: 0.1618 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 04  loss 0.0011  val AP 0.1690\n",
      "  âœ… New best AP: 0.1690 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 05  loss 0.0007  val AP 0.1778\n",
      "  âœ… New best AP: 0.1778 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 06  loss 0.0007  val AP 0.1831\n",
      "  âœ… New best AP: 0.1831 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 07  loss 0.0007  val AP 0.1870\n",
      "  âœ… New best AP: 0.1870 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 08  loss 0.0007  val AP 0.1889\n",
      "  âœ… New best AP: 0.1889 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 09  loss 0.0006  val AP 0.1921\n",
      "  âœ… New best AP: 0.1921 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 10  loss 0.0007  val AP 0.1959\n",
      "  âœ… New best AP: 0.1959 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 11  loss 0.0006  val AP 0.1975\n",
      "  âœ… New best AP: 0.1975 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 12  loss 0.0008  val AP 0.1987\n",
      "  âœ… New best AP: 0.1987 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 13  loss 0.0006  val AP 0.2003\n",
      "  âœ… New best AP: 0.2003 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 14  loss 0.0006  val AP 0.2022\n",
      "  âœ… New best AP: 0.2022 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 15  loss 0.0006  val AP 0.2035\n",
      "  âœ… New best AP: 0.2035 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 16  loss 0.0006  val AP 0.2050\n",
      "  âœ… New best AP: 0.2050 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 17  loss 0.0006  val AP 0.2066\n",
      "  âœ… New best AP: 0.2066 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 18  loss 0.0006  val AP 0.2090\n",
      "  âœ… New best AP: 0.2090 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 19  loss 0.0006  val AP 0.2098\n",
      "  âœ… New best AP: 0.2098 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 20  loss 0.0006  val AP 0.2103\n",
      "  âœ… New best AP: 0.2103 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 21  loss 0.0006  val AP 0.2128\n",
      "  âœ… New best AP: 0.2128 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 22  loss 0.0005  val AP 0.2151\n",
      "  âœ… New best AP: 0.2151 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 23  loss 0.0006  val AP 0.2163\n",
      "  âœ… New best AP: 0.2163 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 24  loss 0.0006  val AP 0.2178\n",
      "  âœ… New best AP: 0.2178 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 25  loss 0.0005  val AP 0.2183\n",
      "  âœ… New best AP: 0.2183 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 26  loss 0.0006  val AP 0.2208\n",
      "  âœ… New best AP: 0.2208 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 27  loss 0.0006  val AP 0.2251\n",
      "  âœ… New best AP: 0.2251 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 28  loss 0.0005  val AP 0.2259\n",
      "  âœ… New best AP: 0.2259 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 29  loss 0.0006  val AP 0.2259\n",
      "[NR-Aromatase | seed 47] ep 30  loss 0.0006  val AP 0.2263\n",
      "  âœ… New best AP: 0.2263 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 31  loss 0.0006  val AP 0.2284\n",
      "  âœ… New best AP: 0.2284 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 32  loss 0.0006  val AP 0.2322\n",
      "  âœ… New best AP: 0.2322 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 33  loss 0.0006  val AP 0.2354\n",
      "  âœ… New best AP: 0.2354 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 34  loss 0.0006  val AP 0.2359\n",
      "  âœ… New best AP: 0.2359 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 35  loss 0.0006  val AP 0.2365\n",
      "  âœ… New best AP: 0.2365 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 36  loss 0.0006  val AP 0.2365\n",
      "[NR-Aromatase | seed 47] ep 37  loss 0.0006  val AP 0.2370\n",
      "  âœ… New best AP: 0.2370 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 38  loss 0.0006  val AP 0.2364\n",
      "[NR-Aromatase | seed 47] ep 39  loss 0.0006  val AP 0.2384\n",
      "  âœ… New best AP: 0.2384 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 40  loss 0.0006  val AP 0.2389\n",
      "  âœ… New best AP: 0.2389 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 41  loss 0.0006  val AP 0.2388\n",
      "[NR-Aromatase | seed 47] ep 42  loss 0.0006  val AP 0.2389\n",
      "[NR-Aromatase | seed 47] ep 43  loss 0.0006  val AP 0.2390\n",
      "  âœ… New best AP: 0.2390 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 44  loss 0.0006  val AP 0.2377\n",
      "[NR-Aromatase | seed 47] ep 45  loss 0.0006  val AP 0.2378\n",
      "[NR-Aromatase | seed 47] ep 46  loss 0.0006  val AP 0.2380\n",
      "[NR-Aromatase | seed 47] ep 47  loss 0.0006  val AP 0.2379\n",
      "[NR-Aromatase | seed 47] ep 48  loss 0.0006  val AP 0.2378\n",
      "[NR-Aromatase | seed 47] ep 49  loss 0.0006  val AP 0.2388\n",
      "[NR-Aromatase | seed 47] ep 50  loss 0.0006  val AP 0.2383\n",
      "[NR-Aromatase | seed 61] ep 01  loss 0.0014  val AP 0.0926\n",
      "  âœ… New best AP: 0.0926 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 02  loss 0.0009  val AP 0.1002\n",
      "  âœ… New best AP: 0.1002 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 03  loss 0.0009  val AP 0.1122\n",
      "  âœ… New best AP: 0.1122 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 04  loss 0.0010  val AP 0.1212\n",
      "  âœ… New best AP: 0.1212 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 05  loss 0.0008  val AP 0.1403\n",
      "  âœ… New best AP: 0.1403 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 06  loss 0.0008  val AP 0.1597\n",
      "  âœ… New best AP: 0.1597 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 07  loss 0.0008  val AP 0.1740\n",
      "  âœ… New best AP: 0.1740 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 08  loss 0.0007  val AP 0.1971\n",
      "  âœ… New best AP: 0.1971 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 09  loss 0.0007  val AP 0.2087\n",
      "  âœ… New best AP: 0.2087 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 10  loss 0.0007  val AP 0.2221\n",
      "  âœ… New best AP: 0.2221 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 11  loss 0.0007  val AP 0.2123\n",
      "[NR-Aromatase | seed 61] ep 12  loss 0.0006  val AP 0.2184\n",
      "[NR-Aromatase | seed 61] ep 13  loss 0.0007  val AP 0.2258\n",
      "  âœ… New best AP: 0.2258 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 14  loss 0.0007  val AP 0.2325\n",
      "  âœ… New best AP: 0.2325 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 15  loss 0.0006  val AP 0.2326\n",
      "  âœ… New best AP: 0.2326 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 16  loss 0.0006  val AP 0.2372\n",
      "  âœ… New best AP: 0.2372 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 17  loss 0.0006  val AP 0.2414\n",
      "  âœ… New best AP: 0.2414 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 18  loss 0.0006  val AP 0.2539\n",
      "  âœ… New best AP: 0.2539 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 19  loss 0.0006  val AP 0.2572\n",
      "  âœ… New best AP: 0.2572 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 20  loss 0.0006  val AP 0.2594\n",
      "  âœ… New best AP: 0.2594 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 21  loss 0.0005  val AP 0.2606\n",
      "  âœ… New best AP: 0.2606 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 22  loss 0.0006  val AP 0.2622\n",
      "  âœ… New best AP: 0.2622 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 23  loss 0.0006  val AP 0.2639\n",
      "  âœ… New best AP: 0.2639 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 24  loss 0.0006  val AP 0.2643\n",
      "  âœ… New best AP: 0.2643 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 25  loss 0.0006  val AP 0.2626\n",
      "[NR-Aromatase | seed 61] ep 26  loss 0.0006  val AP 0.2632\n",
      "[NR-Aromatase | seed 61] ep 27  loss 0.0006  val AP 0.2638\n",
      "[NR-Aromatase | seed 61] ep 28  loss 0.0005  val AP 0.2645\n",
      "  âœ… New best AP: 0.2645 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 29  loss 0.0006  val AP 0.2650\n",
      "  âœ… New best AP: 0.2650 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 30  loss 0.0006  val AP 0.2650\n",
      "  âœ… New best AP: 0.2650 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 31  loss 0.0006  val AP 0.2646\n",
      "[NR-Aromatase | seed 61] ep 32  loss 0.0006  val AP 0.2693\n",
      "  âœ… New best AP: 0.2693 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 33  loss 0.0006  val AP 0.2693\n",
      "[NR-Aromatase | seed 61] ep 34  loss 0.0006  val AP 0.2681\n",
      "[NR-Aromatase | seed 61] ep 35  loss 0.0006  val AP 0.2683\n",
      "[NR-Aromatase | seed 61] ep 36  loss 0.0006  val AP 0.2689\n",
      "[NR-Aromatase | seed 61] ep 37  loss 0.0006  val AP 0.2697\n",
      "  âœ… New best AP: 0.2697 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 38  loss 0.0006  val AP 0.2699\n",
      "  âœ… New best AP: 0.2699 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 39  loss 0.0006  val AP 0.2701\n",
      "  âœ… New best AP: 0.2701 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 40  loss 0.0006  val AP 0.2705\n",
      "  âœ… New best AP: 0.2705 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 41  loss 0.0006  val AP 0.2706\n",
      "  âœ… New best AP: 0.2706 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 42  loss 0.0006  val AP 0.2704\n",
      "[NR-Aromatase | seed 61] ep 43  loss 0.0006  val AP 0.2708\n",
      "  âœ… New best AP: 0.2708 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 44  loss 0.0006  val AP 0.2708\n",
      "  âœ… New best AP: 0.2708 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 45  loss 0.0006  val AP 0.2708\n",
      "[NR-Aromatase | seed 61] ep 46  loss 0.0006  val AP 0.2709\n",
      "  âœ… New best AP: 0.2709 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 47  loss 0.0006  val AP 0.2708\n",
      "[NR-Aromatase | seed 61] ep 48  loss 0.0006  val AP 0.2707\n",
      "[NR-Aromatase | seed 61] ep 49  loss 0.0006  val AP 0.2753\n",
      "  âœ… New best AP: 0.2753 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 50  loss 0.0006  val AP 0.2754\n",
      "  âœ… New best AP: 0.2754 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 01  loss 0.0010  val AP 0.0987\n",
      "  âœ… New best AP: 0.0987 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 02  loss 0.0007  val AP 0.1008\n",
      "  âœ… New best AP: 0.1008 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 03  loss 0.0009  val AP 0.1092\n",
      "  âœ… New best AP: 0.1092 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 04  loss 0.0011  val AP 0.1199\n",
      "  âœ… New best AP: 0.1199 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 05  loss 0.0007  val AP 0.1344\n",
      "  âœ… New best AP: 0.1344 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 06  loss 0.0007  val AP 0.1469\n",
      "  âœ… New best AP: 0.1469 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 07  loss 0.0008  val AP 0.1571\n",
      "  âœ… New best AP: 0.1571 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 08  loss 0.0007  val AP 0.1656\n",
      "  âœ… New best AP: 0.1656 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 09  loss 0.0006  val AP 0.1702\n",
      "  âœ… New best AP: 0.1702 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 10  loss 0.0006  val AP 0.1776\n",
      "  âœ… New best AP: 0.1776 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 11  loss 0.0006  val AP 0.1831\n",
      "  âœ… New best AP: 0.1831 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 12  loss 0.0007  val AP 0.2032\n",
      "  âœ… New best AP: 0.2032 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 13  loss 0.0007  val AP 0.2105\n",
      "  âœ… New best AP: 0.2105 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 14  loss 0.0006  val AP 0.2166\n",
      "  âœ… New best AP: 0.2166 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 15  loss 0.0006  val AP 0.2151\n",
      "[NR-Aromatase | seed 83] ep 16  loss 0.0006  val AP 0.2189\n",
      "  âœ… New best AP: 0.2189 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 17  loss 0.0006  val AP 0.2430\n",
      "  âœ… New best AP: 0.2430 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 18  loss 0.0006  val AP 0.2504\n",
      "  âœ… New best AP: 0.2504 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 19  loss 0.0005  val AP 0.2510\n",
      "  âœ… New best AP: 0.2510 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 20  loss 0.0007  val AP 0.2533\n",
      "  âœ… New best AP: 0.2533 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 21  loss 0.0006  val AP 0.2532\n",
      "[NR-Aromatase | seed 83] ep 22  loss 0.0006  val AP 0.2548\n",
      "  âœ… New best AP: 0.2548 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 23  loss 0.0005  val AP 0.2553\n",
      "  âœ… New best AP: 0.2553 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 24  loss 0.0005  val AP 0.2554\n",
      "  âœ… New best AP: 0.2554 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 25  loss 0.0006  val AP 0.2563\n",
      "  âœ… New best AP: 0.2563 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 26  loss 0.0005  val AP 0.2566\n",
      "  âœ… New best AP: 0.2566 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 27  loss 0.0005  val AP 0.2574\n",
      "  âœ… New best AP: 0.2574 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 28  loss 0.0005  val AP 0.2580\n",
      "  âœ… New best AP: 0.2580 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 29  loss 0.0006  val AP 0.2578\n",
      "[NR-Aromatase | seed 83] ep 30  loss 0.0006  val AP 0.2606\n",
      "  âœ… New best AP: 0.2606 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 31  loss 0.0005  val AP 0.2617\n",
      "  âœ… New best AP: 0.2617 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 32  loss 0.0006  val AP 0.2612\n",
      "[NR-Aromatase | seed 83] ep 33  loss 0.0006  val AP 0.2619\n",
      "  âœ… New best AP: 0.2619 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 34  loss 0.0006  val AP 0.2628\n",
      "  âœ… New best AP: 0.2628 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 35  loss 0.0006  val AP 0.2637\n",
      "  âœ… New best AP: 0.2637 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 36  loss 0.0006  val AP 0.2634\n",
      "[NR-Aromatase | seed 83] ep 37  loss 0.0006  val AP 0.2646\n",
      "  âœ… New best AP: 0.2646 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 38  loss 0.0006  val AP 0.2663\n",
      "  âœ… New best AP: 0.2663 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 39  loss 0.0006  val AP 0.2672\n",
      "  âœ… New best AP: 0.2672 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 40  loss 0.0006  val AP 0.2669\n",
      "[NR-Aromatase | seed 83] ep 41  loss 0.0006  val AP 0.2671\n",
      "[NR-Aromatase | seed 83] ep 42  loss 0.0006  val AP 0.2672\n",
      "  âœ… New best AP: 0.2672 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 43  loss 0.0006  val AP 0.2672\n",
      "[NR-Aromatase | seed 83] ep 44  loss 0.0006  val AP 0.2682\n",
      "  âœ… New best AP: 0.2682 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 45  loss 0.0006  val AP 0.2684\n",
      "  âœ… New best AP: 0.2684 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 46  loss 0.0006  val AP 0.2697\n",
      "  âœ… New best AP: 0.2697 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 47  loss 0.0006  val AP 0.2711\n",
      "  âœ… New best AP: 0.2711 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 48  loss 0.0006  val AP 0.2716\n",
      "  âœ… New best AP: 0.2716 â†’ v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 49  loss 0.0006  val AP 0.2713\n",
      "[NR-Aromatase | seed 83] ep 50  loss 0.0006  val AP 0.2714\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-ER (label 4)\n",
      "==============================\n",
      "[NR-ER | seed 13] ep 01  loss 0.0004  val AP 0.0694\n",
      "  âœ… New best AP: 0.0694 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 02  loss 0.0004  val AP 0.0697\n",
      "  âœ… New best AP: 0.0697 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 03  loss 0.0003  val AP 0.0702\n",
      "  âœ… New best AP: 0.0702 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 04  loss 0.0003  val AP 0.0710\n",
      "  âœ… New best AP: 0.0710 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 05  loss 0.0003  val AP 0.0725\n",
      "  âœ… New best AP: 0.0725 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 06  loss 0.0003  val AP 0.0748\n",
      "  âœ… New best AP: 0.0748 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 07  loss 0.0005  val AP 0.0772\n",
      "  âœ… New best AP: 0.0772 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 08  loss 0.0003  val AP 0.0804\n",
      "  âœ… New best AP: 0.0804 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 09  loss 0.0004  val AP 0.0850\n",
      "  âœ… New best AP: 0.0850 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 10  loss 0.0003  val AP 0.0909\n",
      "  âœ… New best AP: 0.0909 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 11  loss 0.0003  val AP 0.0968\n",
      "  âœ… New best AP: 0.0968 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 12  loss 0.0003  val AP 0.1045\n",
      "  âœ… New best AP: 0.1045 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 13  loss 0.0003  val AP 0.1154\n",
      "  âœ… New best AP: 0.1154 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 14  loss 0.0004  val AP 0.1272\n",
      "  âœ… New best AP: 0.1272 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 15  loss 0.0003  val AP 0.1412\n",
      "  âœ… New best AP: 0.1412 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 16  loss 0.0003  val AP 0.1549\n",
      "  âœ… New best AP: 0.1549 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 17  loss 0.0003  val AP 0.1620\n",
      "  âœ… New best AP: 0.1620 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 18  loss 0.0003  val AP 0.1687\n",
      "  âœ… New best AP: 0.1687 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 19  loss 0.0003  val AP 0.1929\n",
      "  âœ… New best AP: 0.1929 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 20  loss 0.0003  val AP 0.2026\n",
      "  âœ… New best AP: 0.2026 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 21  loss 0.0003  val AP 0.2052\n",
      "  âœ… New best AP: 0.2052 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 22  loss 0.0004  val AP 0.2062\n",
      "  âœ… New best AP: 0.2062 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 23  loss 0.0003  val AP 0.2057\n",
      "[NR-ER | seed 13] ep 24  loss 0.0003  val AP 0.2121\n",
      "  âœ… New best AP: 0.2121 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 25  loss 0.0003  val AP 0.2131\n",
      "  âœ… New best AP: 0.2131 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 26  loss 0.0003  val AP 0.2116\n",
      "[NR-ER | seed 13] ep 27  loss 0.0003  val AP 0.2109\n",
      "[NR-ER | seed 13] ep 28  loss 0.0003  val AP 0.2186\n",
      "  âœ… New best AP: 0.2186 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 29  loss 0.0003  val AP 0.2187\n",
      "  âœ… New best AP: 0.2187 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 30  loss 0.0003  val AP 0.2207\n",
      "  âœ… New best AP: 0.2207 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 31  loss 0.0003  val AP 0.2195\n",
      "[NR-ER | seed 13] ep 32  loss 0.0003  val AP 0.2195\n",
      "[NR-ER | seed 13] ep 33  loss 0.0003  val AP 0.2191\n",
      "[NR-ER | seed 13] ep 34  loss 0.0003  val AP 0.2193\n",
      "[NR-ER | seed 13] ep 35  loss 0.0003  val AP 0.2222\n",
      "  âœ… New best AP: 0.2222 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 36  loss 0.0003  val AP 0.2223\n",
      "  âœ… New best AP: 0.2223 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 37  loss 0.0003  val AP 0.2245\n",
      "  âœ… New best AP: 0.2245 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 38  loss 0.0003  val AP 0.2252\n",
      "  âœ… New best AP: 0.2252 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 39  loss 0.0003  val AP 0.2251\n",
      "[NR-ER | seed 13] ep 40  loss 0.0003  val AP 0.2240\n",
      "[NR-ER | seed 13] ep 41  loss 0.0003  val AP 0.2261\n",
      "  âœ… New best AP: 0.2261 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 42  loss 0.0003  val AP 0.2261\n",
      "  âœ… New best AP: 0.2261 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 43  loss 0.0003  val AP 0.2261\n",
      "  âœ… New best AP: 0.2261 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 44  loss 0.0003  val AP 0.2264\n",
      "  âœ… New best AP: 0.2264 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 45  loss 0.0003  val AP 0.2304\n",
      "  âœ… New best AP: 0.2304 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 46  loss 0.0003  val AP 0.2305\n",
      "  âœ… New best AP: 0.2305 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 47  loss 0.0003  val AP 0.2306\n",
      "  âœ… New best AP: 0.2306 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 48  loss 0.0003  val AP 0.2308\n",
      "  âœ… New best AP: 0.2308 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 49  loss 0.0003  val AP 0.2309\n",
      "  âœ… New best AP: 0.2309 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 50  loss 0.0003  val AP 0.2309\n",
      "  âœ… New best AP: 0.2309 â†’ v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 29] ep 01  loss 0.0005  val AP 0.1210\n",
      "  âœ… New best AP: 0.1210 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 02  loss 0.0004  val AP 0.1232\n",
      "  âœ… New best AP: 0.1232 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 03  loss 0.0004  val AP 0.1245\n",
      "  âœ… New best AP: 0.1245 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 04  loss 0.0005  val AP 0.1275\n",
      "  âœ… New best AP: 0.1275 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 05  loss 0.0004  val AP 0.1290\n",
      "  âœ… New best AP: 0.1290 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 06  loss 0.0004  val AP 0.1331\n",
      "  âœ… New best AP: 0.1331 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 07  loss 0.0004  val AP 0.1347\n",
      "  âœ… New best AP: 0.1347 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 08  loss 0.0004  val AP 0.1378\n",
      "  âœ… New best AP: 0.1378 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 09  loss 0.0004  val AP 0.1392\n",
      "  âœ… New best AP: 0.1392 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 10  loss 0.0003  val AP 0.1409\n",
      "  âœ… New best AP: 0.1409 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 11  loss 0.0003  val AP 0.1496\n",
      "  âœ… New best AP: 0.1496 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 12  loss 0.0004  val AP 0.1501\n",
      "  âœ… New best AP: 0.1501 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 13  loss 0.0004  val AP 0.1536\n",
      "  âœ… New best AP: 0.1536 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 14  loss 0.0003  val AP 0.1554\n",
      "  âœ… New best AP: 0.1554 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 15  loss 0.0003  val AP 0.1566\n",
      "  âœ… New best AP: 0.1566 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 16  loss 0.0003  val AP 0.1569\n",
      "  âœ… New best AP: 0.1569 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 17  loss 0.0003  val AP 0.1581\n",
      "  âœ… New best AP: 0.1581 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 18  loss 0.0003  val AP 0.1588\n",
      "  âœ… New best AP: 0.1588 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 19  loss 0.0003  val AP 0.1609\n",
      "  âœ… New best AP: 0.1609 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 20  loss 0.0003  val AP 0.1622\n",
      "  âœ… New best AP: 0.1622 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 21  loss 0.0003  val AP 0.1627\n",
      "  âœ… New best AP: 0.1627 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 22  loss 0.0003  val AP 0.1640\n",
      "  âœ… New best AP: 0.1640 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 23  loss 0.0003  val AP 0.1662\n",
      "  âœ… New best AP: 0.1662 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 24  loss 0.0003  val AP 0.1675\n",
      "  âœ… New best AP: 0.1675 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 25  loss 0.0003  val AP 0.1694\n",
      "  âœ… New best AP: 0.1694 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 26  loss 0.0003  val AP 0.1706\n",
      "  âœ… New best AP: 0.1706 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 27  loss 0.0003  val AP 0.1715\n",
      "  âœ… New best AP: 0.1715 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 28  loss 0.0003  val AP 0.1715\n",
      "  âœ… New best AP: 0.1715 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 29  loss 0.0003  val AP 0.1722\n",
      "  âœ… New best AP: 0.1722 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 30  loss 0.0003  val AP 0.1734\n",
      "  âœ… New best AP: 0.1734 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 31  loss 0.0003  val AP 0.1737\n",
      "  âœ… New best AP: 0.1737 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 32  loss 0.0003  val AP 0.1740\n",
      "  âœ… New best AP: 0.1740 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 33  loss 0.0003  val AP 0.1750\n",
      "  âœ… New best AP: 0.1750 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 34  loss 0.0003  val AP 0.1761\n",
      "  âœ… New best AP: 0.1761 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 35  loss 0.0003  val AP 0.1768\n",
      "  âœ… New best AP: 0.1768 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 36  loss 0.0003  val AP 0.1775\n",
      "  âœ… New best AP: 0.1775 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 37  loss 0.0003  val AP 0.1785\n",
      "  âœ… New best AP: 0.1785 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 38  loss 0.0003  val AP 0.1795\n",
      "  âœ… New best AP: 0.1795 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 39  loss 0.0003  val AP 0.1808\n",
      "  âœ… New best AP: 0.1808 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 40  loss 0.0003  val AP 0.1812\n",
      "  âœ… New best AP: 0.1812 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 41  loss 0.0003  val AP 0.1819\n",
      "  âœ… New best AP: 0.1819 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 42  loss 0.0003  val AP 0.1830\n",
      "  âœ… New best AP: 0.1830 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 43  loss 0.0003  val AP 0.1835\n",
      "  âœ… New best AP: 0.1835 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 44  loss 0.0003  val AP 0.1835\n",
      "  âœ… New best AP: 0.1835 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 45  loss 0.0003  val AP 0.1837\n",
      "  âœ… New best AP: 0.1837 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 46  loss 0.0003  val AP 0.1846\n",
      "  âœ… New best AP: 0.1846 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 47  loss 0.0003  val AP 0.1853\n",
      "  âœ… New best AP: 0.1853 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 48  loss 0.0003  val AP 0.1856\n",
      "  âœ… New best AP: 0.1856 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 49  loss 0.0003  val AP 0.1855\n",
      "[NR-ER | seed 29] ep 50  loss 0.0003  val AP 0.1859\n",
      "  âœ… New best AP: 0.1859 â†’ v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 47] ep 01  loss 0.0005  val AP 0.1717\n",
      "  âœ… New best AP: 0.1717 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 02  loss 0.0004  val AP 0.1729\n",
      "  âœ… New best AP: 0.1729 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 03  loss 0.0004  val AP 0.1747\n",
      "  âœ… New best AP: 0.1747 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 04  loss 0.0005  val AP 0.1739\n",
      "[NR-ER | seed 47] ep 05  loss 0.0004  val AP 0.1760\n",
      "  âœ… New best AP: 0.1760 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 06  loss 0.0004  val AP 0.1793\n",
      "  âœ… New best AP: 0.1793 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 07  loss 0.0003  val AP 0.1831\n",
      "  âœ… New best AP: 0.1831 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 08  loss 0.0004  val AP 0.1855\n",
      "  âœ… New best AP: 0.1855 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 09  loss 0.0003  val AP 0.1885\n",
      "  âœ… New best AP: 0.1885 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 10  loss 0.0003  val AP 0.1893\n",
      "  âœ… New best AP: 0.1893 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 11  loss 0.0003  val AP 0.1902\n",
      "  âœ… New best AP: 0.1902 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 12  loss 0.0003  val AP 0.1916\n",
      "  âœ… New best AP: 0.1916 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 13  loss 0.0003  val AP 0.1926\n",
      "  âœ… New best AP: 0.1926 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 14  loss 0.0003  val AP 0.1931\n",
      "  âœ… New best AP: 0.1931 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 15  loss 0.0003  val AP 0.1933\n",
      "  âœ… New best AP: 0.1933 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 16  loss 0.0003  val AP 0.1947\n",
      "  âœ… New best AP: 0.1947 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 17  loss 0.0003  val AP 0.1984\n",
      "  âœ… New best AP: 0.1984 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 18  loss 0.0004  val AP 0.1997\n",
      "  âœ… New best AP: 0.1997 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 19  loss 0.0003  val AP 0.2015\n",
      "  âœ… New best AP: 0.2015 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 20  loss 0.0003  val AP 0.2029\n",
      "  âœ… New best AP: 0.2029 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 21  loss 0.0003  val AP 0.2040\n",
      "  âœ… New best AP: 0.2040 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 22  loss 0.0003  val AP 0.2049\n",
      "  âœ… New best AP: 0.2049 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 23  loss 0.0003  val AP 0.2064\n",
      "  âœ… New best AP: 0.2064 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 24  loss 0.0003  val AP 0.2078\n",
      "  âœ… New best AP: 0.2078 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 25  loss 0.0003  val AP 0.2094\n",
      "  âœ… New best AP: 0.2094 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 26  loss 0.0003  val AP 0.2106\n",
      "  âœ… New best AP: 0.2106 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 27  loss 0.0003  val AP 0.2115\n",
      "  âœ… New best AP: 0.2115 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 28  loss 0.0003  val AP 0.2119\n",
      "  âœ… New best AP: 0.2119 â†’ v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 29  loss 0.0003  val AP 0.2117\n",
      "[NR-ER | seed 47] ep 30  loss 0.0003  val AP 0.2117\n",
      "[NR-ER | seed 47] ep 31  loss 0.0003  val AP 0.2119\n",
      "[NR-ER | seed 47] ep 32  loss 0.0003  val AP 0.2117\n",
      "[NR-ER | seed 47] ep 33  loss 0.0003  val AP 0.2112\n",
      "[NR-ER | seed 47] ep 34  loss 0.0003  val AP 0.2114\n",
      "[NR-ER | seed 47] ep 35  loss 0.0003  val AP 0.2116\n",
      "[NR-ER | seed 47] ep 36  loss 0.0003  val AP 0.2115\n",
      "[NR-ER | seed 47] ep 37  loss 0.0003  val AP 0.2118\n",
      "[NR-ER | seed 47] ep 38  loss 0.0003  val AP 0.2119\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.2119\n",
      "[NR-ER | seed 61] ep 01  loss 0.0006  val AP 0.1161\n",
      "  âœ… New best AP: 0.1161 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 02  loss 0.0004  val AP 0.1168\n",
      "  âœ… New best AP: 0.1168 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 03  loss 0.0004  val AP 0.1203\n",
      "  âœ… New best AP: 0.1203 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 04  loss 0.0005  val AP 0.1248\n",
      "  âœ… New best AP: 0.1248 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 05  loss 0.0004  val AP 0.1305\n",
      "  âœ… New best AP: 0.1305 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 06  loss 0.0004  val AP 0.1412\n",
      "  âœ… New best AP: 0.1412 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 07  loss 0.0004  val AP 0.1486\n",
      "  âœ… New best AP: 0.1486 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 08  loss 0.0004  val AP 0.1512\n",
      "  âœ… New best AP: 0.1512 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 09  loss 0.0003  val AP 0.1561\n",
      "  âœ… New best AP: 0.1561 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 10  loss 0.0003  val AP 0.1591\n",
      "  âœ… New best AP: 0.1591 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 11  loss 0.0003  val AP 0.1649\n",
      "  âœ… New best AP: 0.1649 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 12  loss 0.0004  val AP 0.1733\n",
      "  âœ… New best AP: 0.1733 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 13  loss 0.0003  val AP 0.1911\n",
      "  âœ… New best AP: 0.1911 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 14  loss 0.0003  val AP 0.1981\n",
      "  âœ… New best AP: 0.1981 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 15  loss 0.0003  val AP 0.1991\n",
      "  âœ… New best AP: 0.1991 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 16  loss 0.0003  val AP 0.1996\n",
      "  âœ… New best AP: 0.1996 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 17  loss 0.0003  val AP 0.2000\n",
      "  âœ… New best AP: 0.2000 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 18  loss 0.0003  val AP 0.2057\n",
      "  âœ… New best AP: 0.2057 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 19  loss 0.0003  val AP 0.2059\n",
      "  âœ… New best AP: 0.2059 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 20  loss 0.0003  val AP 0.2050\n",
      "[NR-ER | seed 61] ep 21  loss 0.0003  val AP 0.2059\n",
      "[NR-ER | seed 61] ep 22  loss 0.0003  val AP 0.2062\n",
      "  âœ… New best AP: 0.2062 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 23  loss 0.0003  val AP 0.2066\n",
      "  âœ… New best AP: 0.2066 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 24  loss 0.0003  val AP 0.2075\n",
      "  âœ… New best AP: 0.2075 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 25  loss 0.0003  val AP 0.2077\n",
      "  âœ… New best AP: 0.2077 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 26  loss 0.0003  val AP 0.2079\n",
      "  âœ… New best AP: 0.2079 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 27  loss 0.0003  val AP 0.2098\n",
      "  âœ… New best AP: 0.2098 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 28  loss 0.0003  val AP 0.2110\n",
      "  âœ… New best AP: 0.2110 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 29  loss 0.0003  val AP 0.2117\n",
      "  âœ… New best AP: 0.2117 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 30  loss 0.0003  val AP 0.2120\n",
      "  âœ… New best AP: 0.2120 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 31  loss 0.0003  val AP 0.2125\n",
      "  âœ… New best AP: 0.2125 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 32  loss 0.0003  val AP 0.2133\n",
      "  âœ… New best AP: 0.2133 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 33  loss 0.0003  val AP 0.2128\n",
      "[NR-ER | seed 61] ep 34  loss 0.0003  val AP 0.2131\n",
      "[NR-ER | seed 61] ep 35  loss 0.0003  val AP 0.2144\n",
      "  âœ… New best AP: 0.2144 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 36  loss 0.0003  val AP 0.2148\n",
      "  âœ… New best AP: 0.2148 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 37  loss 0.0003  val AP 0.2154\n",
      "  âœ… New best AP: 0.2154 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 38  loss 0.0003  val AP 0.2177\n",
      "  âœ… New best AP: 0.2177 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 39  loss 0.0003  val AP 0.2182\n",
      "  âœ… New best AP: 0.2182 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 40  loss 0.0003  val AP 0.2183\n",
      "  âœ… New best AP: 0.2183 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 41  loss 0.0003  val AP 0.2185\n",
      "  âœ… New best AP: 0.2185 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 42  loss 0.0003  val AP 0.2184\n",
      "[NR-ER | seed 61] ep 43  loss 0.0003  val AP 0.2191\n",
      "  âœ… New best AP: 0.2191 â†’ v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 44  loss 0.0003  val AP 0.2190\n",
      "[NR-ER | seed 61] ep 45  loss 0.0003  val AP 0.2190\n",
      "[NR-ER | seed 61] ep 46  loss 0.0003  val AP 0.2188\n",
      "[NR-ER | seed 61] ep 47  loss 0.0003  val AP 0.2189\n",
      "[NR-ER | seed 61] ep 48  loss 0.0003  val AP 0.2184\n",
      "[NR-ER | seed 61] ep 49  loss 0.0003  val AP 0.2187\n",
      "[NR-ER | seed 61] ep 50  loss 0.0003  val AP 0.2189\n",
      "[NR-ER | seed 83] ep 01  loss 0.0005  val AP 0.0816\n",
      "  âœ… New best AP: 0.0816 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 02  loss 0.0004  val AP 0.0825\n",
      "  âœ… New best AP: 0.0825 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 03  loss 0.0004  val AP 0.0849\n",
      "  âœ… New best AP: 0.0849 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 04  loss 0.0005  val AP 0.0879\n",
      "  âœ… New best AP: 0.0879 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 05  loss 0.0004  val AP 0.0924\n",
      "  âœ… New best AP: 0.0924 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 06  loss 0.0003  val AP 0.0979\n",
      "  âœ… New best AP: 0.0979 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 07  loss 0.0004  val AP 0.1044\n",
      "  âœ… New best AP: 0.1044 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 08  loss 0.0004  val AP 0.1106\n",
      "  âœ… New best AP: 0.1106 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 09  loss 0.0004  val AP 0.1201\n",
      "  âœ… New best AP: 0.1201 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 10  loss 0.0003  val AP 0.1280\n",
      "  âœ… New best AP: 0.1280 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 11  loss 0.0003  val AP 0.1379\n",
      "  âœ… New best AP: 0.1379 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 12  loss 0.0003  val AP 0.1397\n",
      "  âœ… New best AP: 0.1397 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 13  loss 0.0004  val AP 0.1446\n",
      "  âœ… New best AP: 0.1446 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 14  loss 0.0003  val AP 0.1485\n",
      "  âœ… New best AP: 0.1485 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 15  loss 0.0003  val AP 0.1541\n",
      "  âœ… New best AP: 0.1541 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 16  loss 0.0003  val AP 0.1578\n",
      "  âœ… New best AP: 0.1578 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 17  loss 0.0003  val AP 0.1606\n",
      "  âœ… New best AP: 0.1606 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 18  loss 0.0003  val AP 0.1656\n",
      "  âœ… New best AP: 0.1656 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 19  loss 0.0003  val AP 0.1696\n",
      "  âœ… New best AP: 0.1696 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 20  loss 0.0003  val AP 0.1772\n",
      "  âœ… New best AP: 0.1772 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 21  loss 0.0003  val AP 0.1827\n",
      "  âœ… New best AP: 0.1827 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 22  loss 0.0003  val AP 0.1864\n",
      "  âœ… New best AP: 0.1864 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 23  loss 0.0003  val AP 0.1873\n",
      "  âœ… New best AP: 0.1873 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 24  loss 0.0003  val AP 0.1978\n",
      "  âœ… New best AP: 0.1978 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 25  loss 0.0003  val AP 0.1989\n",
      "  âœ… New best AP: 0.1989 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 26  loss 0.0003  val AP 0.1995\n",
      "  âœ… New best AP: 0.1995 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 27  loss 0.0003  val AP 0.2022\n",
      "  âœ… New best AP: 0.2022 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 28  loss 0.0003  val AP 0.2023\n",
      "  âœ… New best AP: 0.2023 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 29  loss 0.0003  val AP 0.2032\n",
      "  âœ… New best AP: 0.2032 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 30  loss 0.0003  val AP 0.2048\n",
      "  âœ… New best AP: 0.2048 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 31  loss 0.0003  val AP 0.2152\n",
      "  âœ… New best AP: 0.2152 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 32  loss 0.0003  val AP 0.2142\n",
      "[NR-ER | seed 83] ep 33  loss 0.0003  val AP 0.2153\n",
      "  âœ… New best AP: 0.2153 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 34  loss 0.0003  val AP 0.2159\n",
      "  âœ… New best AP: 0.2159 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 35  loss 0.0003  val AP 0.2173\n",
      "  âœ… New best AP: 0.2173 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 36  loss 0.0003  val AP 0.2173\n",
      "[NR-ER | seed 83] ep 37  loss 0.0003  val AP 0.2220\n",
      "  âœ… New best AP: 0.2220 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 38  loss 0.0003  val AP 0.2229\n",
      "  âœ… New best AP: 0.2229 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 39  loss 0.0003  val AP 0.2229\n",
      "  âœ… New best AP: 0.2229 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 40  loss 0.0003  val AP 0.2230\n",
      "  âœ… New best AP: 0.2230 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 41  loss 0.0003  val AP 0.2232\n",
      "  âœ… New best AP: 0.2232 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 42  loss 0.0003  val AP 0.2252\n",
      "  âœ… New best AP: 0.2252 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 43  loss 0.0003  val AP 0.2255\n",
      "  âœ… New best AP: 0.2255 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 44  loss 0.0003  val AP 0.2266\n",
      "  âœ… New best AP: 0.2266 â†’ v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 45  loss 0.0003  val AP 0.2261\n",
      "[NR-ER | seed 83] ep 46  loss 0.0003  val AP 0.2262\n",
      "[NR-ER | seed 83] ep 47  loss 0.0003  val AP 0.2262\n",
      "[NR-ER | seed 83] ep 48  loss 0.0003  val AP 0.2257\n",
      "[NR-ER | seed 83] ep 49  loss 0.0003  val AP 0.2257\n",
      "[NR-ER | seed 83] ep 50  loss 0.0003  val AP 0.2257\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-ER-LBD (label 5)\n",
      "==============================\n",
      "[NR-ER-LBD | seed 13] ep 01  loss 0.0008  val AP 0.0263\n",
      "  âœ… New best AP: 0.0263 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 02  loss 0.0006  val AP 0.0267\n",
      "  âœ… New best AP: 0.0267 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 03  loss 0.0005  val AP 0.0273\n",
      "  âœ… New best AP: 0.0273 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 04  loss 0.0005  val AP 0.0287\n",
      "  âœ… New best AP: 0.0287 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 05  loss 0.0007  val AP 0.0306\n",
      "  âœ… New best AP: 0.0306 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 06  loss 0.0005  val AP 0.0332\n",
      "  âœ… New best AP: 0.0332 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 07  loss 0.0007  val AP 0.0367\n",
      "  âœ… New best AP: 0.0367 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 08  loss 0.0005  val AP 0.0411\n",
      "  âœ… New best AP: 0.0411 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 09  loss 0.0005  val AP 0.0461\n",
      "  âœ… New best AP: 0.0461 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 10  loss 0.0005  val AP 0.0559\n",
      "  âœ… New best AP: 0.0559 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 11  loss 0.0007  val AP 0.0639\n",
      "  âœ… New best AP: 0.0639 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 12  loss 0.0005  val AP 0.0741\n",
      "  âœ… New best AP: 0.0741 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 13  loss 0.0005  val AP 0.0870\n",
      "  âœ… New best AP: 0.0870 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 14  loss 0.0005  val AP 0.0921\n",
      "  âœ… New best AP: 0.0921 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 15  loss 0.0005  val AP 0.0925\n",
      "  âœ… New best AP: 0.0925 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 16  loss 0.0005  val AP 0.0942\n",
      "  âœ… New best AP: 0.0942 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 17  loss 0.0005  val AP 0.0955\n",
      "  âœ… New best AP: 0.0955 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 18  loss 0.0005  val AP 0.1248\n",
      "  âœ… New best AP: 0.1248 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 19  loss 0.0005  val AP 0.1294\n",
      "  âœ… New best AP: 0.1294 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 20  loss 0.0005  val AP 0.1291\n",
      "[NR-ER-LBD | seed 13] ep 21  loss 0.0005  val AP 0.1294\n",
      "[NR-ER-LBD | seed 13] ep 22  loss 0.0005  val AP 0.1261\n",
      "[NR-ER-LBD | seed 13] ep 23  loss 0.0004  val AP 0.1269\n",
      "[NR-ER-LBD | seed 13] ep 24  loss 0.0005  val AP 0.1272\n",
      "[NR-ER-LBD | seed 13] ep 25  loss 0.0005  val AP 0.1284\n",
      "[NR-ER-LBD | seed 13] ep 26  loss 0.0004  val AP 0.1283\n",
      "[NR-ER-LBD | seed 13] ep 27  loss 0.0004  val AP 0.1335\n",
      "  âœ… New best AP: 0.1335 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 28  loss 0.0004  val AP 0.1335\n",
      "  âœ… New best AP: 0.1335 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 29  loss 0.0004  val AP 0.1336\n",
      "  âœ… New best AP: 0.1336 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 30  loss 0.0004  val AP 0.1334\n",
      "[NR-ER-LBD | seed 13] ep 31  loss 0.0005  val AP 0.1384\n",
      "  âœ… New best AP: 0.1384 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 32  loss 0.0005  val AP 0.1333\n",
      "[NR-ER-LBD | seed 13] ep 33  loss 0.0005  val AP 0.1329\n",
      "[NR-ER-LBD | seed 13] ep 34  loss 0.0005  val AP 0.1384\n",
      "[NR-ER-LBD | seed 13] ep 35  loss 0.0005  val AP 0.1380\n",
      "[NR-ER-LBD | seed 13] ep 36  loss 0.0004  val AP 0.1380\n",
      "[NR-ER-LBD | seed 13] ep 37  loss 0.0005  val AP 0.1472\n",
      "  âœ… New best AP: 0.1472 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 38  loss 0.0004  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 39  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 40  loss 0.0005  val AP 0.1457\n",
      "[NR-ER-LBD | seed 13] ep 41  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 42  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 43  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 44  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 45  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 46  loss 0.0005  val AP 0.1457\n",
      "[NR-ER-LBD | seed 13] ep 47  loss 0.0005  val AP 0.1457\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.1472\n",
      "[NR-ER-LBD | seed 29] ep 01  loss 0.0009  val AP 0.0403\n",
      "  âœ… New best AP: 0.0403 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 02  loss 0.0006  val AP 0.0481\n",
      "  âœ… New best AP: 0.0481 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 03  loss 0.0007  val AP 0.0491\n",
      "  âœ… New best AP: 0.0491 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 04  loss 0.0008  val AP 0.0502\n",
      "  âœ… New best AP: 0.0502 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 05  loss 0.0007  val AP 0.0617\n",
      "  âœ… New best AP: 0.0617 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 06  loss 0.0006  val AP 0.0635\n",
      "  âœ… New best AP: 0.0635 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 07  loss 0.0006  val AP 0.0645\n",
      "  âœ… New best AP: 0.0645 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 08  loss 0.0007  val AP 0.0940\n",
      "  âœ… New best AP: 0.0940 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 09  loss 0.0006  val AP 0.0956\n",
      "  âœ… New best AP: 0.0956 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 10  loss 0.0006  val AP 0.0988\n",
      "  âœ… New best AP: 0.0988 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 11  loss 0.0005  val AP 0.1014\n",
      "  âœ… New best AP: 0.1014 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 12  loss 0.0005  val AP 0.1031\n",
      "  âœ… New best AP: 0.1031 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 13  loss 0.0005  val AP 0.1047\n",
      "  âœ… New best AP: 0.1047 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 14  loss 0.0006  val AP 0.1059\n",
      "  âœ… New best AP: 0.1059 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 15  loss 0.0005  val AP 0.1078\n",
      "  âœ… New best AP: 0.1078 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 16  loss 0.0005  val AP 0.1090\n",
      "  âœ… New best AP: 0.1090 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 17  loss 0.0005  val AP 0.1098\n",
      "  âœ… New best AP: 0.1098 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 18  loss 0.0006  val AP 0.1094\n",
      "[NR-ER-LBD | seed 29] ep 19  loss 0.0005  val AP 0.1093\n",
      "[NR-ER-LBD | seed 29] ep 20  loss 0.0006  val AP 0.1105\n",
      "  âœ… New best AP: 0.1105 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 21  loss 0.0005  val AP 0.1129\n",
      "  âœ… New best AP: 0.1129 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 22  loss 0.0006  val AP 0.1155\n",
      "  âœ… New best AP: 0.1155 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 23  loss 0.0005  val AP 0.1154\n",
      "[NR-ER-LBD | seed 29] ep 24  loss 0.0005  val AP 0.1158\n",
      "  âœ… New best AP: 0.1158 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 25  loss 0.0005  val AP 0.1165\n",
      "  âœ… New best AP: 0.1165 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 26  loss 0.0005  val AP 0.1170\n",
      "  âœ… New best AP: 0.1170 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 27  loss 0.0005  val AP 0.1171\n",
      "  âœ… New best AP: 0.1171 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 28  loss 0.0005  val AP 0.1173\n",
      "  âœ… New best AP: 0.1173 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 29  loss 0.0005  val AP 0.1187\n",
      "  âœ… New best AP: 0.1187 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 30  loss 0.0005  val AP 0.1197\n",
      "  âœ… New best AP: 0.1197 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 31  loss 0.0004  val AP 0.1197\n",
      "  âœ… New best AP: 0.1197 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 32  loss 0.0005  val AP 0.1219\n",
      "  âœ… New best AP: 0.1219 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 33  loss 0.0005  val AP 0.1220\n",
      "  âœ… New best AP: 0.1220 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 34  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 35  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 36  loss 0.0005  val AP 0.1218\n",
      "[NR-ER-LBD | seed 29] ep 37  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 38  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 39  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 40  loss 0.0005  val AP 0.1217\n",
      "[NR-ER-LBD | seed 29] ep 41  loss 0.0005  val AP 0.1216\n",
      "[NR-ER-LBD | seed 29] ep 42  loss 0.0005  val AP 0.1217\n",
      "[NR-ER-LBD | seed 29] ep 43  loss 0.0005  val AP 0.1232\n",
      "  âœ… New best AP: 0.1232 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 44  loss 0.0005  val AP 0.1233\n",
      "  âœ… New best AP: 0.1233 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 45  loss 0.0005  val AP 0.1233\n",
      "  âœ… New best AP: 0.1233 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 46  loss 0.0005  val AP 0.1233\n",
      "[NR-ER-LBD | seed 29] ep 47  loss 0.0005  val AP 0.1233\n",
      "[NR-ER-LBD | seed 29] ep 48  loss 0.0005  val AP 0.1233\n",
      "[NR-ER-LBD | seed 29] ep 49  loss 0.0005  val AP 0.1252\n",
      "  âœ… New best AP: 0.1252 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 50  loss 0.0005  val AP 0.1254\n",
      "  âœ… New best AP: 0.1254 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 01  loss 0.0009  val AP 0.0319\n",
      "  âœ… New best AP: 0.0319 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 02  loss 0.0006  val AP 0.0322\n",
      "  âœ… New best AP: 0.0322 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 03  loss 0.0007  val AP 0.0331\n",
      "  âœ… New best AP: 0.0331 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 04  loss 0.0007  val AP 0.0337\n",
      "  âœ… New best AP: 0.0337 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 05  loss 0.0007  val AP 0.0345\n",
      "  âœ… New best AP: 0.0345 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 06  loss 0.0006  val AP 0.0356\n",
      "  âœ… New best AP: 0.0356 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 07  loss 0.0006  val AP 0.0371\n",
      "  âœ… New best AP: 0.0371 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 08  loss 0.0006  val AP 0.0383\n",
      "  âœ… New best AP: 0.0383 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 09  loss 0.0007  val AP 0.0401\n",
      "  âœ… New best AP: 0.0401 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 10  loss 0.0006  val AP 0.0412\n",
      "  âœ… New best AP: 0.0412 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 11  loss 0.0006  val AP 0.0437\n",
      "  âœ… New best AP: 0.0437 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 12  loss 0.0005  val AP 0.0441\n",
      "  âœ… New best AP: 0.0441 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 13  loss 0.0006  val AP 0.0448\n",
      "  âœ… New best AP: 0.0448 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 14  loss 0.0005  val AP 0.0456\n",
      "  âœ… New best AP: 0.0456 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 15  loss 0.0005  val AP 0.0465\n",
      "  âœ… New best AP: 0.0465 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 16  loss 0.0005  val AP 0.0485\n",
      "  âœ… New best AP: 0.0485 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 17  loss 0.0004  val AP 0.0488\n",
      "  âœ… New best AP: 0.0488 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 18  loss 0.0005  val AP 0.0501\n",
      "  âœ… New best AP: 0.0501 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 19  loss 0.0004  val AP 0.0536\n",
      "  âœ… New best AP: 0.0536 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 20  loss 0.0005  val AP 0.0536\n",
      "  âœ… New best AP: 0.0536 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 21  loss 0.0004  val AP 0.0546\n",
      "  âœ… New best AP: 0.0546 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 22  loss 0.0005  val AP 0.0548\n",
      "  âœ… New best AP: 0.0548 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 23  loss 0.0005  val AP 0.0576\n",
      "  âœ… New best AP: 0.0576 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 24  loss 0.0005  val AP 0.0576\n",
      "[NR-ER-LBD | seed 47] ep 25  loss 0.0004  val AP 0.0623\n",
      "  âœ… New best AP: 0.0623 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 26  loss 0.0004  val AP 0.0636\n",
      "  âœ… New best AP: 0.0636 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 27  loss 0.0005  val AP 0.0636\n",
      "  âœ… New best AP: 0.0636 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 28  loss 0.0005  val AP 0.0637\n",
      "  âœ… New best AP: 0.0637 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 29  loss 0.0004  val AP 0.0652\n",
      "  âœ… New best AP: 0.0652 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 30  loss 0.0004  val AP 0.0672\n",
      "  âœ… New best AP: 0.0672 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 31  loss 0.0005  val AP 0.0672\n",
      "[NR-ER-LBD | seed 47] ep 32  loss 0.0005  val AP 0.0671\n",
      "[NR-ER-LBD | seed 47] ep 33  loss 0.0004  val AP 0.0672\n",
      "[NR-ER-LBD | seed 47] ep 34  loss 0.0004  val AP 0.0699\n",
      "  âœ… New best AP: 0.0699 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 35  loss 0.0004  val AP 0.0737\n",
      "  âœ… New best AP: 0.0737 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 36  loss 0.0005  val AP 0.0737\n",
      "  âœ… New best AP: 0.0737 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 37  loss 0.0005  val AP 0.0737\n",
      "  âœ… New best AP: 0.0737 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 38  loss 0.0004  val AP 0.0738\n",
      "  âœ… New best AP: 0.0738 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 39  loss 0.0005  val AP 0.0737\n",
      "[NR-ER-LBD | seed 47] ep 40  loss 0.0005  val AP 0.0737\n",
      "[NR-ER-LBD | seed 47] ep 41  loss 0.0005  val AP 0.0793\n",
      "  âœ… New best AP: 0.0793 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 42  loss 0.0005  val AP 0.0793\n",
      "[NR-ER-LBD | seed 47] ep 43  loss 0.0005  val AP 0.0793\n",
      "[NR-ER-LBD | seed 47] ep 44  loss 0.0005  val AP 0.0793\n",
      "[NR-ER-LBD | seed 47] ep 45  loss 0.0005  val AP 0.0793\n",
      "  âœ… New best AP: 0.0793 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 46  loss 0.0005  val AP 0.0794\n",
      "  âœ… New best AP: 0.0794 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 47  loss 0.0005  val AP 0.0795\n",
      "  âœ… New best AP: 0.0795 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 48  loss 0.0005  val AP 0.0796\n",
      "  âœ… New best AP: 0.0796 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 49  loss 0.0005  val AP 0.0798\n",
      "  âœ… New best AP: 0.0798 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 50  loss 0.0005  val AP 0.0798\n",
      "[NR-ER-LBD | seed 61] ep 01  loss 0.0011  val AP 0.0256\n",
      "  âœ… New best AP: 0.0256 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 02  loss 0.0007  val AP 0.0263\n",
      "  âœ… New best AP: 0.0263 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 03  loss 0.0007  val AP 0.0294\n",
      "  âœ… New best AP: 0.0294 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 04  loss 0.0008  val AP 0.0314\n",
      "  âœ… New best AP: 0.0314 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 05  loss 0.0007  val AP 0.0344\n",
      "  âœ… New best AP: 0.0344 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 06  loss 0.0006  val AP 0.0392\n",
      "  âœ… New best AP: 0.0392 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 07  loss 0.0006  val AP 0.0409\n",
      "  âœ… New best AP: 0.0409 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 08  loss 0.0008  val AP 0.0469\n",
      "  âœ… New best AP: 0.0469 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 09  loss 0.0006  val AP 0.0475\n",
      "  âœ… New best AP: 0.0475 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 10  loss 0.0006  val AP 0.0484\n",
      "  âœ… New best AP: 0.0484 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 11  loss 0.0006  val AP 0.0871\n",
      "  âœ… New best AP: 0.0871 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 12  loss 0.0005  val AP 0.0879\n",
      "  âœ… New best AP: 0.0879 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 13  loss 0.0005  val AP 0.0613\n",
      "[NR-ER-LBD | seed 61] ep 14  loss 0.0005  val AP 0.0633\n",
      "[NR-ER-LBD | seed 61] ep 15  loss 0.0005  val AP 0.0661\n",
      "[NR-ER-LBD | seed 61] ep 16  loss 0.0005  val AP 0.0687\n",
      "[NR-ER-LBD | seed 61] ep 17  loss 0.0005  val AP 0.0711\n",
      "[NR-ER-LBD | seed 61] ep 18  loss 0.0005  val AP 0.0765\n",
      "[NR-ER-LBD | seed 61] ep 19  loss 0.0005  val AP 0.0799\n",
      "[NR-ER-LBD | seed 61] ep 20  loss 0.0005  val AP 0.0845\n",
      "[NR-ER-LBD | seed 61] ep 21  loss 0.0005  val AP 0.0905\n",
      "  âœ… New best AP: 0.0905 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 22  loss 0.0005  val AP 0.0916\n",
      "  âœ… New best AP: 0.0916 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 23  loss 0.0005  val AP 0.1287\n",
      "  âœ… New best AP: 0.1287 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 24  loss 0.0005  val AP 0.1289\n",
      "  âœ… New best AP: 0.1289 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 25  loss 0.0005  val AP 0.1299\n",
      "  âœ… New best AP: 0.1299 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 26  loss 0.0005  val AP 0.1300\n",
      "  âœ… New best AP: 0.1300 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 27  loss 0.0005  val AP 0.1301\n",
      "  âœ… New best AP: 0.1301 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 28  loss 0.0005  val AP 0.1302\n",
      "  âœ… New best AP: 0.1302 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 29  loss 0.0005  val AP 0.1309\n",
      "  âœ… New best AP: 0.1309 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 30  loss 0.0005  val AP 0.1316\n",
      "  âœ… New best AP: 0.1316 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 31  loss 0.0005  val AP 0.1324\n",
      "  âœ… New best AP: 0.1324 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 32  loss 0.0005  val AP 0.1333\n",
      "  âœ… New best AP: 0.1333 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 33  loss 0.0005  val AP 0.1333\n",
      "  âœ… New best AP: 0.1333 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 34  loss 0.0005  val AP 0.1518\n",
      "  âœ… New best AP: 0.1518 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 35  loss 0.0005  val AP 0.1519\n",
      "  âœ… New best AP: 0.1519 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 36  loss 0.0005  val AP 0.1520\n",
      "  âœ… New best AP: 0.1520 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 37  loss 0.0005  val AP 0.1519\n",
      "[NR-ER-LBD | seed 61] ep 38  loss 0.0005  val AP 0.1519\n",
      "[NR-ER-LBD | seed 61] ep 39  loss 0.0005  val AP 0.1519\n",
      "[NR-ER-LBD | seed 61] ep 40  loss 0.0005  val AP 0.1522\n",
      "  âœ… New best AP: 0.1522 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 41  loss 0.0005  val AP 0.1523\n",
      "  âœ… New best AP: 0.1523 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 42  loss 0.0005  val AP 0.1526\n",
      "  âœ… New best AP: 0.1526 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 43  loss 0.0005  val AP 0.1527\n",
      "  âœ… New best AP: 0.1527 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 44  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 45  loss 0.0005  val AP 0.1527\n",
      "  âœ… New best AP: 0.1527 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 46  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 47  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 48  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 49  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 50  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 83] ep 01  loss 0.0008  val AP 0.0249\n",
      "  âœ… New best AP: 0.0249 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 02  loss 0.0006  val AP 0.0253\n",
      "  âœ… New best AP: 0.0253 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 03  loss 0.0007  val AP 0.0262\n",
      "  âœ… New best AP: 0.0262 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 04  loss 0.0006  val AP 0.0270\n",
      "  âœ… New best AP: 0.0270 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 05  loss 0.0007  val AP 0.0285\n",
      "  âœ… New best AP: 0.0285 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 06  loss 0.0006  val AP 0.0302\n",
      "  âœ… New best AP: 0.0302 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 07  loss 0.0007  val AP 0.0328\n",
      "  âœ… New best AP: 0.0328 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 08  loss 0.0005  val AP 0.0346\n",
      "  âœ… New best AP: 0.0346 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 09  loss 0.0007  val AP 0.0365\n",
      "  âœ… New best AP: 0.0365 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 10  loss 0.0005  val AP 0.0395\n",
      "  âœ… New best AP: 0.0395 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 11  loss 0.0006  val AP 0.0431\n",
      "  âœ… New best AP: 0.0431 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 12  loss 0.0005  val AP 0.0489\n",
      "  âœ… New best AP: 0.0489 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 13  loss 0.0006  val AP 0.0510\n",
      "  âœ… New best AP: 0.0510 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 14  loss 0.0004  val AP 0.0588\n",
      "  âœ… New best AP: 0.0588 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 15  loss 0.0005  val AP 0.0654\n",
      "  âœ… New best AP: 0.0654 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 16  loss 0.0005  val AP 0.0760\n",
      "  âœ… New best AP: 0.0760 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 17  loss 0.0005  val AP 0.0759\n",
      "[NR-ER-LBD | seed 83] ep 18  loss 0.0005  val AP 0.1050\n",
      "  âœ… New best AP: 0.1050 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 19  loss 0.0005  val AP 0.1045\n",
      "[NR-ER-LBD | seed 83] ep 20  loss 0.0005  val AP 0.1081\n",
      "  âœ… New best AP: 0.1081 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 21  loss 0.0004  val AP 0.1084\n",
      "  âœ… New best AP: 0.1084 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 22  loss 0.0005  val AP 0.1110\n",
      "  âœ… New best AP: 0.1110 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 23  loss 0.0004  val AP 0.1111\n",
      "  âœ… New best AP: 0.1111 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 24  loss 0.0005  val AP 0.1118\n",
      "  âœ… New best AP: 0.1118 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 25  loss 0.0005  val AP 0.1139\n",
      "  âœ… New best AP: 0.1139 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 26  loss 0.0004  val AP 0.1139\n",
      "  âœ… New best AP: 0.1139 â†’ v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 27  loss 0.0004  val AP 0.1134\n",
      "[NR-ER-LBD | seed 83] ep 28  loss 0.0005  val AP 0.1131\n",
      "[NR-ER-LBD | seed 83] ep 29  loss 0.0004  val AP 0.1131\n",
      "[NR-ER-LBD | seed 83] ep 30  loss 0.0005  val AP 0.1134\n",
      "[NR-ER-LBD | seed 83] ep 31  loss 0.0005  val AP 0.1134\n",
      "[NR-ER-LBD | seed 83] ep 32  loss 0.0005  val AP 0.1134\n",
      "[NR-ER-LBD | seed 83] ep 33  loss 0.0004  val AP 0.1132\n",
      "[NR-ER-LBD | seed 83] ep 34  loss 0.0004  val AP 0.1132\n",
      "[NR-ER-LBD | seed 83] ep 35  loss 0.0004  val AP 0.1136\n",
      "[NR-ER-LBD | seed 83] ep 36  loss 0.0005  val AP 0.1136\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.1139\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-PPAR-gamma (label 6)\n",
      "==============================\n",
      "[NR-PPAR-gamma | seed 13] ep 01  loss 0.0016  val AP 0.0408\n",
      "  âœ… New best AP: 0.0408 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 02  loss 0.0012  val AP 0.0413\n",
      "  âœ… New best AP: 0.0413 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 03  loss 0.0010  val AP 0.0418\n",
      "  âœ… New best AP: 0.0418 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 04  loss 0.0014  val AP 0.0425\n",
      "  âœ… New best AP: 0.0425 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 05  loss 0.0010  val AP 0.0436\n",
      "  âœ… New best AP: 0.0436 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 06  loss 0.0015  val AP 0.0447\n",
      "  âœ… New best AP: 0.0447 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 07  loss 0.0010  val AP 0.0459\n",
      "  âœ… New best AP: 0.0459 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 08  loss 0.0013  val AP 0.0471\n",
      "  âœ… New best AP: 0.0471 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 09  loss 0.0010  val AP 0.0482\n",
      "  âœ… New best AP: 0.0482 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 10  loss 0.0012  val AP 0.0494\n",
      "  âœ… New best AP: 0.0494 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 11  loss 0.0009  val AP 0.0503\n",
      "  âœ… New best AP: 0.0503 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 12  loss 0.0009  val AP 0.0513\n",
      "  âœ… New best AP: 0.0513 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 13  loss 0.0011  val AP 0.0524\n",
      "  âœ… New best AP: 0.0524 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 14  loss 0.0009  val AP 0.0535\n",
      "  âœ… New best AP: 0.0535 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 15  loss 0.0017  val AP 0.0542\n",
      "  âœ… New best AP: 0.0542 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 16  loss 0.0010  val AP 0.0552\n",
      "  âœ… New best AP: 0.0552 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 17  loss 0.0009  val AP 0.0559\n",
      "  âœ… New best AP: 0.0559 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 18  loss 0.0009  val AP 0.0565\n",
      "  âœ… New best AP: 0.0565 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 19  loss 0.0009  val AP 0.0571\n",
      "  âœ… New best AP: 0.0571 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 20  loss 0.0013  val AP 0.0574\n",
      "  âœ… New best AP: 0.0574 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 21  loss 0.0009  val AP 0.0580\n",
      "  âœ… New best AP: 0.0580 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 22  loss 0.0011  val AP 0.0589\n",
      "  âœ… New best AP: 0.0589 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 23  loss 0.0008  val AP 0.0595\n",
      "  âœ… New best AP: 0.0595 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 24  loss 0.0009  val AP 0.0602\n",
      "  âœ… New best AP: 0.0602 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 25  loss 0.0008  val AP 0.0611\n",
      "  âœ… New best AP: 0.0611 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 26  loss 0.0010  val AP 0.0616\n",
      "  âœ… New best AP: 0.0616 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 27  loss 0.0009  val AP 0.0624\n",
      "  âœ… New best AP: 0.0624 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 28  loss 0.0009  val AP 0.0630\n",
      "  âœ… New best AP: 0.0630 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 29  loss 0.0009  val AP 0.0639\n",
      "  âœ… New best AP: 0.0639 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 30  loss 0.0009  val AP 0.0643\n",
      "  âœ… New best AP: 0.0643 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 31  loss 0.0009  val AP 0.0650\n",
      "  âœ… New best AP: 0.0650 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 32  loss 0.0009  val AP 0.0655\n",
      "  âœ… New best AP: 0.0655 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 33  loss 0.0010  val AP 0.0661\n",
      "  âœ… New best AP: 0.0661 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 34  loss 0.0009  val AP 0.0669\n",
      "  âœ… New best AP: 0.0669 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 35  loss 0.0009  val AP 0.0676\n",
      "  âœ… New best AP: 0.0676 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 36  loss 0.0009  val AP 0.0682\n",
      "  âœ… New best AP: 0.0682 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 37  loss 0.0009  val AP 0.0685\n",
      "  âœ… New best AP: 0.0685 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 38  loss 0.0009  val AP 0.0687\n",
      "  âœ… New best AP: 0.0687 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 39  loss 0.0009  val AP 0.0692\n",
      "  âœ… New best AP: 0.0692 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 40  loss 0.0009  val AP 0.0695\n",
      "  âœ… New best AP: 0.0695 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 41  loss 0.0009  val AP 0.0700\n",
      "  âœ… New best AP: 0.0700 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 42  loss 0.0009  val AP 0.0703\n",
      "  âœ… New best AP: 0.0703 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 43  loss 0.0009  val AP 0.0706\n",
      "  âœ… New best AP: 0.0706 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 44  loss 0.0009  val AP 0.0708\n",
      "  âœ… New best AP: 0.0708 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 45  loss 0.0009  val AP 0.0712\n",
      "  âœ… New best AP: 0.0712 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 46  loss 0.0009  val AP 0.0714\n",
      "  âœ… New best AP: 0.0714 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 47  loss 0.0009  val AP 0.0715\n",
      "  âœ… New best AP: 0.0715 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 48  loss 0.0009  val AP 0.0717\n",
      "  âœ… New best AP: 0.0717 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 49  loss 0.0009  val AP 0.0718\n",
      "  âœ… New best AP: 0.0718 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 50  loss 0.0009  val AP 0.0721\n",
      "  âœ… New best AP: 0.0721 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 01  loss 0.0019  val AP 0.0565\n",
      "  âœ… New best AP: 0.0565 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 02  loss 0.0013  val AP 0.0572\n",
      "  âœ… New best AP: 0.0572 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 03  loss 0.0014  val AP 0.0591\n",
      "  âœ… New best AP: 0.0591 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 04  loss 0.0016  val AP 0.0613\n",
      "  âœ… New best AP: 0.0613 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 05  loss 0.0013  val AP 0.0702\n",
      "  âœ… New best AP: 0.0702 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 06  loss 0.0011  val AP 0.0873\n",
      "  âœ… New best AP: 0.0873 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 07  loss 0.0012  val AP 0.0876\n",
      "  âœ… New best AP: 0.0876 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 08  loss 0.0013  val AP 0.0881\n",
      "  âœ… New best AP: 0.0881 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 09  loss 0.0010  val AP 0.0884\n",
      "  âœ… New best AP: 0.0884 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 10  loss 0.0015  val AP 0.0889\n",
      "  âœ… New best AP: 0.0889 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 11  loss 0.0009  val AP 0.0892\n",
      "  âœ… New best AP: 0.0892 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 12  loss 0.0012  val AP 0.0898\n",
      "  âœ… New best AP: 0.0898 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 13  loss 0.0010  val AP 0.0907\n",
      "  âœ… New best AP: 0.0907 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 14  loss 0.0009  val AP 0.0918\n",
      "  âœ… New best AP: 0.0918 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 15  loss 0.0011  val AP 0.0690\n",
      "[NR-PPAR-gamma | seed 29] ep 16  loss 0.0009  val AP 0.0668\n",
      "[NR-PPAR-gamma | seed 29] ep 17  loss 0.0010  val AP 0.0658\n",
      "[NR-PPAR-gamma | seed 29] ep 18  loss 0.0011  val AP 0.0656\n",
      "[NR-PPAR-gamma | seed 29] ep 19  loss 0.0010  val AP 0.0645\n",
      "[NR-PPAR-gamma | seed 29] ep 20  loss 0.0009  val AP 0.0645\n",
      "[NR-PPAR-gamma | seed 29] ep 21  loss 0.0009  val AP 0.0645\n",
      "[NR-PPAR-gamma | seed 29] ep 22  loss 0.0011  val AP 0.0655\n",
      "[NR-PPAR-gamma | seed 29] ep 23  loss 0.0010  val AP 0.0666\n",
      "[NR-PPAR-gamma | seed 29] ep 24  loss 0.0009  val AP 0.0672\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.0918\n",
      "[NR-PPAR-gamma | seed 47] ep 01  loss 0.0018  val AP 0.0351\n",
      "  âœ… New best AP: 0.0351 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 02  loss 0.0012  val AP 0.0354\n",
      "  âœ… New best AP: 0.0354 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 03  loss 0.0013  val AP 0.0360\n",
      "  âœ… New best AP: 0.0360 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 04  loss 0.0015  val AP 0.0369\n",
      "  âœ… New best AP: 0.0369 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 05  loss 0.0012  val AP 0.0382\n",
      "  âœ… New best AP: 0.0382 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 06  loss 0.0012  val AP 0.0399\n",
      "  âœ… New best AP: 0.0399 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 07  loss 0.0011  val AP 0.0414\n",
      "  âœ… New best AP: 0.0414 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 08  loss 0.0014  val AP 0.0428\n",
      "  âœ… New best AP: 0.0428 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 09  loss 0.0011  val AP 0.0441\n",
      "  âœ… New best AP: 0.0441 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 10  loss 0.0013  val AP 0.0453\n",
      "  âœ… New best AP: 0.0453 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 11  loss 0.0010  val AP 0.0462\n",
      "  âœ… New best AP: 0.0462 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 12  loss 0.0009  val AP 0.0472\n",
      "  âœ… New best AP: 0.0472 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 13  loss 0.0010  val AP 0.0480\n",
      "  âœ… New best AP: 0.0480 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 14  loss 0.0011  val AP 0.0488\n",
      "  âœ… New best AP: 0.0488 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 15  loss 0.0010  val AP 0.0496\n",
      "  âœ… New best AP: 0.0496 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 16  loss 0.0010  val AP 0.0505\n",
      "  âœ… New best AP: 0.0505 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 17  loss 0.0009  val AP 0.0511\n",
      "  âœ… New best AP: 0.0511 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 18  loss 0.0009  val AP 0.0519\n",
      "  âœ… New best AP: 0.0519 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 19  loss 0.0008  val AP 0.0524\n",
      "  âœ… New best AP: 0.0524 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 20  loss 0.0011  val AP 0.0530\n",
      "  âœ… New best AP: 0.0530 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 21  loss 0.0010  val AP 0.0535\n",
      "  âœ… New best AP: 0.0535 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 22  loss 0.0010  val AP 0.0541\n",
      "  âœ… New best AP: 0.0541 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 23  loss 0.0009  val AP 0.0546\n",
      "  âœ… New best AP: 0.0546 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 24  loss 0.0009  val AP 0.0550\n",
      "  âœ… New best AP: 0.0550 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 25  loss 0.0009  val AP 0.0556\n",
      "  âœ… New best AP: 0.0556 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 26  loss 0.0009  val AP 0.0560\n",
      "  âœ… New best AP: 0.0560 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 27  loss 0.0009  val AP 0.0564\n",
      "  âœ… New best AP: 0.0564 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 28  loss 0.0009  val AP 0.0570\n",
      "  âœ… New best AP: 0.0570 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 29  loss 0.0008  val AP 0.0577\n",
      "  âœ… New best AP: 0.0577 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 30  loss 0.0010  val AP 0.0582\n",
      "  âœ… New best AP: 0.0582 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 31  loss 0.0008  val AP 0.0587\n",
      "  âœ… New best AP: 0.0587 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 32  loss 0.0009  val AP 0.0590\n",
      "  âœ… New best AP: 0.0590 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 33  loss 0.0009  val AP 0.0597\n",
      "  âœ… New best AP: 0.0597 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 34  loss 0.0009  val AP 0.0601\n",
      "  âœ… New best AP: 0.0601 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 35  loss 0.0009  val AP 0.0604\n",
      "  âœ… New best AP: 0.0604 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 36  loss 0.0008  val AP 0.0609\n",
      "  âœ… New best AP: 0.0609 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 37  loss 0.0009  val AP 0.0612\n",
      "  âœ… New best AP: 0.0612 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 38  loss 0.0009  val AP 0.0616\n",
      "  âœ… New best AP: 0.0616 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 39  loss 0.0009  val AP 0.0620\n",
      "  âœ… New best AP: 0.0620 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 40  loss 0.0009  val AP 0.0622\n",
      "  âœ… New best AP: 0.0622 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 41  loss 0.0010  val AP 0.0626\n",
      "  âœ… New best AP: 0.0626 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 42  loss 0.0009  val AP 0.0630\n",
      "  âœ… New best AP: 0.0630 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 43  loss 0.0009  val AP 0.0632\n",
      "  âœ… New best AP: 0.0632 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 44  loss 0.0010  val AP 0.0634\n",
      "  âœ… New best AP: 0.0634 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 45  loss 0.0009  val AP 0.0637\n",
      "  âœ… New best AP: 0.0637 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 46  loss 0.0010  val AP 0.0640\n",
      "  âœ… New best AP: 0.0640 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 47  loss 0.0010  val AP 0.0643\n",
      "  âœ… New best AP: 0.0643 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 48  loss 0.0009  val AP 0.0643\n",
      "  âœ… New best AP: 0.0643 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 49  loss 0.0010  val AP 0.0645\n",
      "  âœ… New best AP: 0.0645 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 50  loss 0.0009  val AP 0.0648\n",
      "  âœ… New best AP: 0.0648 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 01  loss 0.0022  val AP 0.0447\n",
      "  âœ… New best AP: 0.0447 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 02  loss 0.0013  val AP 0.0457\n",
      "  âœ… New best AP: 0.0457 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 03  loss 0.0014  val AP 0.0449\n",
      "[NR-PPAR-gamma | seed 61] ep 04  loss 0.0016  val AP 0.0443\n",
      "[NR-PPAR-gamma | seed 61] ep 05  loss 0.0013  val AP 0.0443\n",
      "[NR-PPAR-gamma | seed 61] ep 06  loss 0.0013  val AP 0.0450\n",
      "[NR-PPAR-gamma | seed 61] ep 07  loss 0.0013  val AP 0.0467\n",
      "  âœ… New best AP: 0.0467 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 08  loss 0.0014  val AP 0.0477\n",
      "  âœ… New best AP: 0.0477 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 09  loss 0.0015  val AP 0.0490\n",
      "  âœ… New best AP: 0.0490 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 10  loss 0.0011  val AP 0.0505\n",
      "  âœ… New best AP: 0.0505 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 11  loss 0.0011  val AP 0.0515\n",
      "  âœ… New best AP: 0.0515 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 12  loss 0.0010  val AP 0.0522\n",
      "  âœ… New best AP: 0.0522 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 13  loss 0.0011  val AP 0.0527\n",
      "  âœ… New best AP: 0.0527 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 14  loss 0.0012  val AP 0.0535\n",
      "  âœ… New best AP: 0.0535 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 15  loss 0.0009  val AP 0.0546\n",
      "  âœ… New best AP: 0.0546 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 16  loss 0.0014  val AP 0.0549\n",
      "  âœ… New best AP: 0.0549 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 17  loss 0.0009  val AP 0.0555\n",
      "  âœ… New best AP: 0.0555 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 18  loss 0.0010  val AP 0.0564\n",
      "  âœ… New best AP: 0.0564 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 19  loss 0.0009  val AP 0.0568\n",
      "  âœ… New best AP: 0.0568 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 20  loss 0.0009  val AP 0.0573\n",
      "  âœ… New best AP: 0.0573 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 21  loss 0.0010  val AP 0.0579\n",
      "  âœ… New best AP: 0.0579 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 22  loss 0.0009  val AP 0.0587\n",
      "  âœ… New best AP: 0.0587 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 23  loss 0.0010  val AP 0.0593\n",
      "  âœ… New best AP: 0.0593 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 24  loss 0.0009  val AP 0.0597\n",
      "  âœ… New best AP: 0.0597 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 25  loss 0.0010  val AP 0.0601\n",
      "  âœ… New best AP: 0.0601 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 26  loss 0.0009  val AP 0.0610\n",
      "  âœ… New best AP: 0.0610 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 27  loss 0.0010  val AP 0.0616\n",
      "  âœ… New best AP: 0.0616 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 28  loss 0.0009  val AP 0.0622\n",
      "  âœ… New best AP: 0.0622 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 29  loss 0.0010  val AP 0.0629\n",
      "  âœ… New best AP: 0.0629 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 30  loss 0.0009  val AP 0.0634\n",
      "  âœ… New best AP: 0.0634 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 31  loss 0.0009  val AP 0.0639\n",
      "  âœ… New best AP: 0.0639 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 32  loss 0.0009  val AP 0.0643\n",
      "  âœ… New best AP: 0.0643 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 33  loss 0.0009  val AP 0.0647\n",
      "  âœ… New best AP: 0.0647 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 34  loss 0.0009  val AP 0.0650\n",
      "  âœ… New best AP: 0.0650 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 35  loss 0.0009  val AP 0.0652\n",
      "  âœ… New best AP: 0.0652 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 36  loss 0.0009  val AP 0.0658\n",
      "  âœ… New best AP: 0.0658 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 37  loss 0.0009  val AP 0.0661\n",
      "  âœ… New best AP: 0.0661 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 38  loss 0.0009  val AP 0.0665\n",
      "  âœ… New best AP: 0.0665 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 39  loss 0.0009  val AP 0.0669\n",
      "  âœ… New best AP: 0.0669 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 40  loss 0.0009  val AP 0.0671\n",
      "  âœ… New best AP: 0.0671 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 41  loss 0.0009  val AP 0.0675\n",
      "  âœ… New best AP: 0.0675 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 42  loss 0.0009  val AP 0.0677\n",
      "  âœ… New best AP: 0.0677 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 43  loss 0.0009  val AP 0.0679\n",
      "  âœ… New best AP: 0.0679 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 44  loss 0.0009  val AP 0.0682\n",
      "  âœ… New best AP: 0.0682 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 45  loss 0.0009  val AP 0.0683\n",
      "  âœ… New best AP: 0.0683 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 46  loss 0.0009  val AP 0.0686\n",
      "  âœ… New best AP: 0.0686 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 47  loss 0.0010  val AP 0.0688\n",
      "  âœ… New best AP: 0.0688 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 48  loss 0.0009  val AP 0.0688\n",
      "  âœ… New best AP: 0.0688 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 49  loss 0.0009  val AP 0.0689\n",
      "  âœ… New best AP: 0.0689 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 50  loss 0.0009  val AP 0.0691\n",
      "  âœ… New best AP: 0.0691 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 01  loss 0.0018  val AP 0.0411\n",
      "  âœ… New best AP: 0.0411 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 02  loss 0.0012  val AP 0.0416\n",
      "  âœ… New best AP: 0.0416 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 03  loss 0.0013  val AP 0.0421\n",
      "  âœ… New best AP: 0.0421 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 04  loss 0.0014  val AP 0.0426\n",
      "  âœ… New best AP: 0.0426 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 05  loss 0.0014  val AP 0.0433\n",
      "  âœ… New best AP: 0.0433 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 06  loss 0.0011  val AP 0.0444\n",
      "  âœ… New best AP: 0.0444 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 07  loss 0.0014  val AP 0.0451\n",
      "  âœ… New best AP: 0.0451 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 08  loss 0.0010  val AP 0.0458\n",
      "  âœ… New best AP: 0.0458 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 09  loss 0.0016  val AP 0.0463\n",
      "  âœ… New best AP: 0.0463 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 10  loss 0.0010  val AP 0.0467\n",
      "  âœ… New best AP: 0.0467 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 11  loss 0.0012  val AP 0.0469\n",
      "  âœ… New best AP: 0.0469 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 12  loss 0.0009  val AP 0.0472\n",
      "  âœ… New best AP: 0.0472 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 13  loss 0.0010  val AP 0.0476\n",
      "  âœ… New best AP: 0.0476 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 14  loss 0.0009  val AP 0.0481\n",
      "  âœ… New best AP: 0.0481 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 15  loss 0.0010  val AP 0.0485\n",
      "  âœ… New best AP: 0.0485 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 16  loss 0.0009  val AP 0.0491\n",
      "  âœ… New best AP: 0.0491 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 17  loss 0.0012  val AP 0.0497\n",
      "  âœ… New best AP: 0.0497 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 18  loss 0.0009  val AP 0.0504\n",
      "  âœ… New best AP: 0.0504 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 19  loss 0.0012  val AP 0.0509\n",
      "  âœ… New best AP: 0.0509 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 20  loss 0.0009  val AP 0.0515\n",
      "  âœ… New best AP: 0.0515 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 21  loss 0.0009  val AP 0.0522\n",
      "  âœ… New best AP: 0.0522 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 22  loss 0.0010  val AP 0.0528\n",
      "  âœ… New best AP: 0.0528 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 23  loss 0.0009  val AP 0.0535\n",
      "  âœ… New best AP: 0.0535 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 24  loss 0.0009  val AP 0.0542\n",
      "  âœ… New best AP: 0.0542 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 25  loss 0.0009  val AP 0.0552\n",
      "  âœ… New best AP: 0.0552 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 26  loss 0.0009  val AP 0.0561\n",
      "  âœ… New best AP: 0.0561 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 27  loss 0.0011  val AP 0.0565\n",
      "  âœ… New best AP: 0.0565 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 28  loss 0.0009  val AP 0.0573\n",
      "  âœ… New best AP: 0.0573 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 29  loss 0.0009  val AP 0.0581\n",
      "  âœ… New best AP: 0.0581 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 30  loss 0.0009  val AP 0.0584\n",
      "  âœ… New best AP: 0.0584 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 31  loss 0.0009  val AP 0.0590\n",
      "  âœ… New best AP: 0.0590 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 32  loss 0.0009  val AP 0.0594\n",
      "  âœ… New best AP: 0.0594 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 33  loss 0.0009  val AP 0.0600\n",
      "  âœ… New best AP: 0.0600 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 34  loss 0.0010  val AP 0.0606\n",
      "  âœ… New best AP: 0.0606 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 35  loss 0.0009  val AP 0.0609\n",
      "  âœ… New best AP: 0.0609 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 36  loss 0.0009  val AP 0.0612\n",
      "  âœ… New best AP: 0.0612 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 37  loss 0.0009  val AP 0.0618\n",
      "  âœ… New best AP: 0.0618 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 38  loss 0.0009  val AP 0.0623\n",
      "  âœ… New best AP: 0.0623 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 39  loss 0.0009  val AP 0.0627\n",
      "  âœ… New best AP: 0.0627 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 40  loss 0.0009  val AP 0.0630\n",
      "  âœ… New best AP: 0.0630 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 41  loss 0.0009  val AP 0.0634\n",
      "  âœ… New best AP: 0.0634 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 42  loss 0.0009  val AP 0.0637\n",
      "  âœ… New best AP: 0.0637 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 43  loss 0.0009  val AP 0.0641\n",
      "  âœ… New best AP: 0.0641 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 44  loss 0.0009  val AP 0.0645\n",
      "  âœ… New best AP: 0.0645 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 45  loss 0.0009  val AP 0.0650\n",
      "  âœ… New best AP: 0.0650 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 46  loss 0.0009  val AP 0.0656\n",
      "  âœ… New best AP: 0.0656 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 47  loss 0.0010  val AP 0.0661\n",
      "  âœ… New best AP: 0.0661 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 48  loss 0.0009  val AP 0.0664\n",
      "  âœ… New best AP: 0.0664 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 49  loss 0.0009  val AP 0.0667\n",
      "  âœ… New best AP: 0.0667 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 50  loss 0.0009  val AP 0.0671\n",
      "  âœ… New best AP: 0.0671 â†’ v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-ARE (label 7)\n",
      "==============================\n",
      "[SR-ARE | seed 13] ep 01  loss 0.0004  val AP 0.1361\n",
      "  âœ… New best AP: 0.1361 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 02  loss 0.0003  val AP 0.1379\n",
      "  âœ… New best AP: 0.1379 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 03  loss 0.0003  val AP 0.1412\n",
      "  âœ… New best AP: 0.1412 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 04  loss 0.0004  val AP 0.1457\n",
      "  âœ… New best AP: 0.1457 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 05  loss 0.0003  val AP 0.1518\n",
      "  âœ… New best AP: 0.1518 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 06  loss 0.0003  val AP 0.1622\n",
      "  âœ… New best AP: 0.1622 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 07  loss 0.0003  val AP 0.1751\n",
      "  âœ… New best AP: 0.1751 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 08  loss 0.0003  val AP 0.1888\n",
      "  âœ… New best AP: 0.1888 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 09  loss 0.0003  val AP 0.2044\n",
      "  âœ… New best AP: 0.2044 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 10  loss 0.0003  val AP 0.2173\n",
      "  âœ… New best AP: 0.2173 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 11  loss 0.0003  val AP 0.2305\n",
      "  âœ… New best AP: 0.2305 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 12  loss 0.0003  val AP 0.2426\n",
      "  âœ… New best AP: 0.2426 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 13  loss 0.0003  val AP 0.2596\n",
      "  âœ… New best AP: 0.2596 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 14  loss 0.0003  val AP 0.2707\n",
      "  âœ… New best AP: 0.2707 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 15  loss 0.0003  val AP 0.2792\n",
      "  âœ… New best AP: 0.2792 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 16  loss 0.0003  val AP 0.2868\n",
      "  âœ… New best AP: 0.2868 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 17  loss 0.0003  val AP 0.2921\n",
      "  âœ… New best AP: 0.2921 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 18  loss 0.0003  val AP 0.2985\n",
      "  âœ… New best AP: 0.2985 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 19  loss 0.0002  val AP 0.3026\n",
      "  âœ… New best AP: 0.3026 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 20  loss 0.0002  val AP 0.3044\n",
      "  âœ… New best AP: 0.3044 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 21  loss 0.0003  val AP 0.3095\n",
      "  âœ… New best AP: 0.3095 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 22  loss 0.0003  val AP 0.3127\n",
      "  âœ… New best AP: 0.3127 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 23  loss 0.0002  val AP 0.3144\n",
      "  âœ… New best AP: 0.3144 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 24  loss 0.0003  val AP 0.3162\n",
      "  âœ… New best AP: 0.3162 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 25  loss 0.0002  val AP 0.3172\n",
      "  âœ… New best AP: 0.3172 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 26  loss 0.0003  val AP 0.3176\n",
      "  âœ… New best AP: 0.3176 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 27  loss 0.0002  val AP 0.3187\n",
      "  âœ… New best AP: 0.3187 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 28  loss 0.0003  val AP 0.3217\n",
      "  âœ… New best AP: 0.3217 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 29  loss 0.0002  val AP 0.3228\n",
      "  âœ… New best AP: 0.3228 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 30  loss 0.0003  val AP 0.3237\n",
      "  âœ… New best AP: 0.3237 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 31  loss 0.0003  val AP 0.3238\n",
      "  âœ… New best AP: 0.3238 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 32  loss 0.0003  val AP 0.3245\n",
      "  âœ… New best AP: 0.3245 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 33  loss 0.0002  val AP 0.3269\n",
      "  âœ… New best AP: 0.3269 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 34  loss 0.0002  val AP 0.3270\n",
      "  âœ… New best AP: 0.3270 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 35  loss 0.0002  val AP 0.3279\n",
      "  âœ… New best AP: 0.3279 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 36  loss 0.0003  val AP 0.3287\n",
      "  âœ… New best AP: 0.3287 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 37  loss 0.0003  val AP 0.3308\n",
      "  âœ… New best AP: 0.3308 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 38  loss 0.0003  val AP 0.3316\n",
      "  âœ… New best AP: 0.3316 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 39  loss 0.0002  val AP 0.3327\n",
      "  âœ… New best AP: 0.3327 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 40  loss 0.0003  val AP 0.3326\n",
      "[SR-ARE | seed 13] ep 41  loss 0.0003  val AP 0.3330\n",
      "  âœ… New best AP: 0.3330 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 42  loss 0.0003  val AP 0.3333\n",
      "  âœ… New best AP: 0.3333 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 43  loss 0.0002  val AP 0.3336\n",
      "  âœ… New best AP: 0.3336 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 44  loss 0.0003  val AP 0.3338\n",
      "  âœ… New best AP: 0.3338 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 45  loss 0.0003  val AP 0.3346\n",
      "  âœ… New best AP: 0.3346 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 46  loss 0.0003  val AP 0.3345\n",
      "[SR-ARE | seed 13] ep 47  loss 0.0003  val AP 0.3346\n",
      "  âœ… New best AP: 0.3346 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 48  loss 0.0003  val AP 0.3347\n",
      "  âœ… New best AP: 0.3347 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 49  loss 0.0003  val AP 0.3350\n",
      "  âœ… New best AP: 0.3350 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 50  loss 0.0003  val AP 0.3360\n",
      "  âœ… New best AP: 0.3360 â†’ v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 29] ep 01  loss 0.0005  val AP 0.1791\n",
      "  âœ… New best AP: 0.1791 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 02  loss 0.0003  val AP 0.1833\n",
      "  âœ… New best AP: 0.1833 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 03  loss 0.0004  val AP 0.1912\n",
      "  âœ… New best AP: 0.1912 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 04  loss 0.0004  val AP 0.2007\n",
      "  âœ… New best AP: 0.2007 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 05  loss 0.0003  val AP 0.2102\n",
      "  âœ… New best AP: 0.2102 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 06  loss 0.0003  val AP 0.2156\n",
      "  âœ… New best AP: 0.2156 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 07  loss 0.0004  val AP 0.2281\n",
      "  âœ… New best AP: 0.2281 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 08  loss 0.0003  val AP 0.2338\n",
      "  âœ… New best AP: 0.2338 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 09  loss 0.0003  val AP 0.2398\n",
      "  âœ… New best AP: 0.2398 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 10  loss 0.0003  val AP 0.2422\n",
      "  âœ… New best AP: 0.2422 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 11  loss 0.0003  val AP 0.2450\n",
      "  âœ… New best AP: 0.2450 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 12  loss 0.0003  val AP 0.2471\n",
      "  âœ… New best AP: 0.2471 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 13  loss 0.0003  val AP 0.2514\n",
      "  âœ… New best AP: 0.2514 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 14  loss 0.0003  val AP 0.2528\n",
      "  âœ… New best AP: 0.2528 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 15  loss 0.0003  val AP 0.2540\n",
      "  âœ… New best AP: 0.2540 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 16  loss 0.0003  val AP 0.2561\n",
      "  âœ… New best AP: 0.2561 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 17  loss 0.0003  val AP 0.2593\n",
      "  âœ… New best AP: 0.2593 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 18  loss 0.0003  val AP 0.2617\n",
      "  âœ… New best AP: 0.2617 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 19  loss 0.0003  val AP 0.2629\n",
      "  âœ… New best AP: 0.2629 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 20  loss 0.0003  val AP 0.2640\n",
      "  âœ… New best AP: 0.2640 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 21  loss 0.0003  val AP 0.2652\n",
      "  âœ… New best AP: 0.2652 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 22  loss 0.0003  val AP 0.2671\n",
      "  âœ… New best AP: 0.2671 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 23  loss 0.0003  val AP 0.2685\n",
      "  âœ… New best AP: 0.2685 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 24  loss 0.0003  val AP 0.2698\n",
      "  âœ… New best AP: 0.2698 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 25  loss 0.0002  val AP 0.2713\n",
      "  âœ… New best AP: 0.2713 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 26  loss 0.0003  val AP 0.2730\n",
      "  âœ… New best AP: 0.2730 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 27  loss 0.0003  val AP 0.2749\n",
      "  âœ… New best AP: 0.2749 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 28  loss 0.0003  val AP 0.2763\n",
      "  âœ… New best AP: 0.2763 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 29  loss 0.0003  val AP 0.2788\n",
      "  âœ… New best AP: 0.2788 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 30  loss 0.0003  val AP 0.2802\n",
      "  âœ… New best AP: 0.2802 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 31  loss 0.0003  val AP 0.2815\n",
      "  âœ… New best AP: 0.2815 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 32  loss 0.0003  val AP 0.2835\n",
      "  âœ… New best AP: 0.2835 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 33  loss 0.0003  val AP 0.2867\n",
      "  âœ… New best AP: 0.2867 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 34  loss 0.0003  val AP 0.2880\n",
      "  âœ… New best AP: 0.2880 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 35  loss 0.0003  val AP 0.2894\n",
      "  âœ… New best AP: 0.2894 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 36  loss 0.0003  val AP 0.2916\n",
      "  âœ… New best AP: 0.2916 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 37  loss 0.0003  val AP 0.2929\n",
      "  âœ… New best AP: 0.2929 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 38  loss 0.0003  val AP 0.2948\n",
      "  âœ… New best AP: 0.2948 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 39  loss 0.0003  val AP 0.2958\n",
      "  âœ… New best AP: 0.2958 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 40  loss 0.0003  val AP 0.2966\n",
      "  âœ… New best AP: 0.2966 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 41  loss 0.0003  val AP 0.2986\n",
      "  âœ… New best AP: 0.2986 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 42  loss 0.0003  val AP 0.2990\n",
      "  âœ… New best AP: 0.2990 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 43  loss 0.0003  val AP 0.3004\n",
      "  âœ… New best AP: 0.3004 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 44  loss 0.0003  val AP 0.3005\n",
      "  âœ… New best AP: 0.3005 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 45  loss 0.0003  val AP 0.3007\n",
      "  âœ… New best AP: 0.3007 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 46  loss 0.0003  val AP 0.3011\n",
      "  âœ… New best AP: 0.3011 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 47  loss 0.0003  val AP 0.3019\n",
      "  âœ… New best AP: 0.3019 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 48  loss 0.0003  val AP 0.3021\n",
      "  âœ… New best AP: 0.3021 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 49  loss 0.0003  val AP 0.3036\n",
      "  âœ… New best AP: 0.3036 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 50  loss 0.0003  val AP 0.3038\n",
      "  âœ… New best AP: 0.3038 â†’ v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 47] ep 01  loss 0.0004  val AP 0.1807\n",
      "  âœ… New best AP: 0.1807 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 02  loss 0.0003  val AP 0.1847\n",
      "  âœ… New best AP: 0.1847 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 03  loss 0.0004  val AP 0.1903\n",
      "  âœ… New best AP: 0.1903 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 04  loss 0.0005  val AP 0.1985\n",
      "  âœ… New best AP: 0.1985 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 05  loss 0.0003  val AP 0.2040\n",
      "  âœ… New best AP: 0.2040 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 06  loss 0.0003  val AP 0.2097\n",
      "  âœ… New best AP: 0.2097 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 07  loss 0.0003  val AP 0.2173\n",
      "  âœ… New best AP: 0.2173 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 08  loss 0.0003  val AP 0.2214\n",
      "  âœ… New best AP: 0.2214 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 09  loss 0.0003  val AP 0.2259\n",
      "  âœ… New best AP: 0.2259 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 10  loss 0.0003  val AP 0.2283\n",
      "  âœ… New best AP: 0.2283 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 11  loss 0.0003  val AP 0.2320\n",
      "  âœ… New best AP: 0.2320 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 12  loss 0.0003  val AP 0.2345\n",
      "  âœ… New best AP: 0.2345 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 13  loss 0.0003  val AP 0.2359\n",
      "  âœ… New best AP: 0.2359 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 14  loss 0.0003  val AP 0.2376\n",
      "  âœ… New best AP: 0.2376 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 15  loss 0.0003  val AP 0.2394\n",
      "  âœ… New best AP: 0.2394 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 16  loss 0.0003  val AP 0.2413\n",
      "  âœ… New best AP: 0.2413 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 17  loss 0.0003  val AP 0.2433\n",
      "  âœ… New best AP: 0.2433 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 18  loss 0.0003  val AP 0.2466\n",
      "  âœ… New best AP: 0.2466 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 19  loss 0.0003  val AP 0.2490\n",
      "  âœ… New best AP: 0.2490 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 20  loss 0.0003  val AP 0.2506\n",
      "  âœ… New best AP: 0.2506 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 21  loss 0.0003  val AP 0.2525\n",
      "  âœ… New best AP: 0.2525 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 22  loss 0.0003  val AP 0.2535\n",
      "  âœ… New best AP: 0.2535 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 23  loss 0.0002  val AP 0.2549\n",
      "  âœ… New best AP: 0.2549 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 24  loss 0.0003  val AP 0.2563\n",
      "  âœ… New best AP: 0.2563 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 25  loss 0.0002  val AP 0.2589\n",
      "  âœ… New best AP: 0.2589 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 26  loss 0.0002  val AP 0.2599\n",
      "  âœ… New best AP: 0.2599 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 27  loss 0.0003  val AP 0.2604\n",
      "  âœ… New best AP: 0.2604 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 28  loss 0.0002  val AP 0.2615\n",
      "  âœ… New best AP: 0.2615 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 29  loss 0.0002  val AP 0.2615\n",
      "  âœ… New best AP: 0.2615 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 30  loss 0.0003  val AP 0.2625\n",
      "  âœ… New best AP: 0.2625 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 31  loss 0.0003  val AP 0.2623\n",
      "[SR-ARE | seed 47] ep 32  loss 0.0002  val AP 0.2632\n",
      "  âœ… New best AP: 0.2632 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 33  loss 0.0002  val AP 0.2637\n",
      "  âœ… New best AP: 0.2637 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 34  loss 0.0002  val AP 0.2640\n",
      "  âœ… New best AP: 0.2640 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 35  loss 0.0003  val AP 0.2655\n",
      "  âœ… New best AP: 0.2655 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 36  loss 0.0003  val AP 0.2647\n",
      "[SR-ARE | seed 47] ep 37  loss 0.0003  val AP 0.2655\n",
      "  âœ… New best AP: 0.2655 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 38  loss 0.0002  val AP 0.2658\n",
      "  âœ… New best AP: 0.2658 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 39  loss 0.0003  val AP 0.2663\n",
      "  âœ… New best AP: 0.2663 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 40  loss 0.0003  val AP 0.2670\n",
      "  âœ… New best AP: 0.2670 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 41  loss 0.0003  val AP 0.2668\n",
      "[SR-ARE | seed 47] ep 42  loss 0.0003  val AP 0.2673\n",
      "  âœ… New best AP: 0.2673 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 43  loss 0.0002  val AP 0.2684\n",
      "  âœ… New best AP: 0.2684 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 44  loss 0.0003  val AP 0.2689\n",
      "  âœ… New best AP: 0.2689 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 45  loss 0.0003  val AP 0.2689\n",
      "  âœ… New best AP: 0.2689 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 46  loss 0.0003  val AP 0.2696\n",
      "  âœ… New best AP: 0.2696 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 47  loss 0.0003  val AP 0.2696\n",
      "  âœ… New best AP: 0.2696 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 48  loss 0.0003  val AP 0.2698\n",
      "  âœ… New best AP: 0.2698 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 49  loss 0.0003  val AP 0.2698\n",
      "  âœ… New best AP: 0.2698 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 50  loss 0.0003  val AP 0.2708\n",
      "  âœ… New best AP: 0.2708 â†’ v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 61] ep 01  loss 0.0006  val AP 0.1439\n",
      "  âœ… New best AP: 0.1439 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 02  loss 0.0004  val AP 0.1481\n",
      "  âœ… New best AP: 0.1481 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 03  loss 0.0004  val AP 0.1524\n",
      "  âœ… New best AP: 0.1524 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 04  loss 0.0004  val AP 0.1583\n",
      "  âœ… New best AP: 0.1583 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 05  loss 0.0003  val AP 0.1679\n",
      "  âœ… New best AP: 0.1679 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 06  loss 0.0004  val AP 0.1796\n",
      "  âœ… New best AP: 0.1796 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 07  loss 0.0004  val AP 0.1908\n",
      "  âœ… New best AP: 0.1908 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 08  loss 0.0003  val AP 0.2031\n",
      "  âœ… New best AP: 0.2031 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 09  loss 0.0003  val AP 0.2111\n",
      "  âœ… New best AP: 0.2111 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 10  loss 0.0003  val AP 0.2206\n",
      "  âœ… New best AP: 0.2206 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 11  loss 0.0003  val AP 0.2342\n",
      "  âœ… New best AP: 0.2342 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 12  loss 0.0003  val AP 0.2409\n",
      "  âœ… New best AP: 0.2409 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 13  loss 0.0003  val AP 0.2466\n",
      "  âœ… New best AP: 0.2466 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 14  loss 0.0003  val AP 0.2523\n",
      "  âœ… New best AP: 0.2523 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 15  loss 0.0003  val AP 0.2583\n",
      "  âœ… New best AP: 0.2583 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 16  loss 0.0003  val AP 0.2632\n",
      "  âœ… New best AP: 0.2632 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 17  loss 0.0003  val AP 0.2667\n",
      "  âœ… New best AP: 0.2667 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 18  loss 0.0003  val AP 0.2705\n",
      "  âœ… New best AP: 0.2705 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 19  loss 0.0003  val AP 0.2746\n",
      "  âœ… New best AP: 0.2746 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 20  loss 0.0003  val AP 0.2770\n",
      "  âœ… New best AP: 0.2770 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 21  loss 0.0003  val AP 0.2801\n",
      "  âœ… New best AP: 0.2801 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 22  loss 0.0003  val AP 0.2859\n",
      "  âœ… New best AP: 0.2859 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 23  loss 0.0003  val AP 0.2897\n",
      "  âœ… New best AP: 0.2897 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 24  loss 0.0003  val AP 0.2919\n",
      "  âœ… New best AP: 0.2919 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 25  loss 0.0003  val AP 0.2961\n",
      "  âœ… New best AP: 0.2961 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 26  loss 0.0003  val AP 0.2981\n",
      "  âœ… New best AP: 0.2981 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 27  loss 0.0003  val AP 0.3001\n",
      "  âœ… New best AP: 0.3001 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 28  loss 0.0003  val AP 0.3031\n",
      "  âœ… New best AP: 0.3031 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 29  loss 0.0003  val AP 0.3056\n",
      "  âœ… New best AP: 0.3056 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 30  loss 0.0003  val AP 0.3067\n",
      "  âœ… New best AP: 0.3067 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 31  loss 0.0003  val AP 0.3095\n",
      "  âœ… New best AP: 0.3095 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 32  loss 0.0003  val AP 0.3096\n",
      "  âœ… New best AP: 0.3096 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 33  loss 0.0003  val AP 0.3108\n",
      "  âœ… New best AP: 0.3108 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 34  loss 0.0003  val AP 0.3117\n",
      "  âœ… New best AP: 0.3117 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 35  loss 0.0003  val AP 0.3134\n",
      "  âœ… New best AP: 0.3134 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 36  loss 0.0003  val AP 0.3140\n",
      "  âœ… New best AP: 0.3140 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 37  loss 0.0002  val AP 0.3153\n",
      "  âœ… New best AP: 0.3153 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 38  loss 0.0003  val AP 0.3165\n",
      "  âœ… New best AP: 0.3165 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 39  loss 0.0003  val AP 0.3171\n",
      "  âœ… New best AP: 0.3171 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 40  loss 0.0003  val AP 0.3176\n",
      "  âœ… New best AP: 0.3176 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 41  loss 0.0003  val AP 0.3179\n",
      "  âœ… New best AP: 0.3179 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 42  loss 0.0003  val AP 0.3201\n",
      "  âœ… New best AP: 0.3201 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 43  loss 0.0003  val AP 0.3211\n",
      "  âœ… New best AP: 0.3211 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 44  loss 0.0003  val AP 0.3215\n",
      "  âœ… New best AP: 0.3215 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 45  loss 0.0003  val AP 0.3217\n",
      "  âœ… New best AP: 0.3217 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 46  loss 0.0003  val AP 0.3219\n",
      "  âœ… New best AP: 0.3219 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 47  loss 0.0003  val AP 0.3224\n",
      "  âœ… New best AP: 0.3224 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 48  loss 0.0003  val AP 0.3231\n",
      "  âœ… New best AP: 0.3231 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 49  loss 0.0003  val AP 0.3233\n",
      "  âœ… New best AP: 0.3233 â†’ v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 50  loss 0.0003  val AP 0.3232\n",
      "[SR-ARE | seed 83] ep 01  loss 0.0004  val AP 0.2069\n",
      "  âœ… New best AP: 0.2069 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 02  loss 0.0003  val AP 0.2111\n",
      "  âœ… New best AP: 0.2111 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 03  loss 0.0004  val AP 0.2195\n",
      "  âœ… New best AP: 0.2195 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 04  loss 0.0004  val AP 0.2254\n",
      "  âœ… New best AP: 0.2254 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 05  loss 0.0003  val AP 0.2365\n",
      "  âœ… New best AP: 0.2365 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 06  loss 0.0003  val AP 0.2425\n",
      "  âœ… New best AP: 0.2425 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 07  loss 0.0003  val AP 0.2514\n",
      "  âœ… New best AP: 0.2514 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 08  loss 0.0004  val AP 0.2573\n",
      "  âœ… New best AP: 0.2573 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 09  loss 0.0004  val AP 0.2673\n",
      "  âœ… New best AP: 0.2673 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 10  loss 0.0003  val AP 0.2760\n",
      "  âœ… New best AP: 0.2760 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 11  loss 0.0003  val AP 0.2776\n",
      "  âœ… New best AP: 0.2776 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 12  loss 0.0003  val AP 0.2797\n",
      "  âœ… New best AP: 0.2797 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 13  loss 0.0003  val AP 0.2804\n",
      "  âœ… New best AP: 0.2804 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 14  loss 0.0003  val AP 0.2823\n",
      "  âœ… New best AP: 0.2823 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 15  loss 0.0003  val AP 0.2862\n",
      "  âœ… New best AP: 0.2862 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 16  loss 0.0003  val AP 0.2873\n",
      "  âœ… New best AP: 0.2873 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 17  loss 0.0003  val AP 0.2879\n",
      "  âœ… New best AP: 0.2879 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 18  loss 0.0003  val AP 0.2892\n",
      "  âœ… New best AP: 0.2892 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 19  loss 0.0003  val AP 0.2916\n",
      "  âœ… New best AP: 0.2916 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 20  loss 0.0003  val AP 0.2932\n",
      "  âœ… New best AP: 0.2932 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 21  loss 0.0003  val AP 0.3034\n",
      "  âœ… New best AP: 0.3034 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 22  loss 0.0003  val AP 0.3055\n",
      "  âœ… New best AP: 0.3055 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 23  loss 0.0003  val AP 0.3068\n",
      "  âœ… New best AP: 0.3068 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 24  loss 0.0003  val AP 0.3083\n",
      "  âœ… New best AP: 0.3083 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 25  loss 0.0003  val AP 0.3079\n",
      "[SR-ARE | seed 83] ep 26  loss 0.0003  val AP 0.3091\n",
      "  âœ… New best AP: 0.3091 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 27  loss 0.0002  val AP 0.3100\n",
      "  âœ… New best AP: 0.3100 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 28  loss 0.0003  val AP 0.3095\n",
      "[SR-ARE | seed 83] ep 29  loss 0.0003  val AP 0.3101\n",
      "  âœ… New best AP: 0.3101 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 30  loss 0.0003  val AP 0.3105\n",
      "  âœ… New best AP: 0.3105 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 31  loss 0.0002  val AP 0.3108\n",
      "  âœ… New best AP: 0.3108 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 32  loss 0.0003  val AP 0.3117\n",
      "  âœ… New best AP: 0.3117 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 33  loss 0.0003  val AP 0.3124\n",
      "  âœ… New best AP: 0.3124 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 34  loss 0.0003  val AP 0.3120\n",
      "[SR-ARE | seed 83] ep 35  loss 0.0003  val AP 0.3128\n",
      "  âœ… New best AP: 0.3128 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 36  loss 0.0003  val AP 0.3132\n",
      "  âœ… New best AP: 0.3132 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 37  loss 0.0003  val AP 0.3137\n",
      "  âœ… New best AP: 0.3137 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 38  loss 0.0003  val AP 0.3138\n",
      "  âœ… New best AP: 0.3138 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 39  loss 0.0003  val AP 0.3139\n",
      "  âœ… New best AP: 0.3139 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 40  loss 0.0003  val AP 0.3146\n",
      "  âœ… New best AP: 0.3146 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 41  loss 0.0003  val AP 0.3145\n",
      "[SR-ARE | seed 83] ep 42  loss 0.0003  val AP 0.3140\n",
      "[SR-ARE | seed 83] ep 43  loss 0.0003  val AP 0.3145\n",
      "[SR-ARE | seed 83] ep 44  loss 0.0003  val AP 0.3146\n",
      "[SR-ARE | seed 83] ep 45  loss 0.0003  val AP 0.3167\n",
      "  âœ… New best AP: 0.3167 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 46  loss 0.0003  val AP 0.3174\n",
      "  âœ… New best AP: 0.3174 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 47  loss 0.0003  val AP 0.3194\n",
      "  âœ… New best AP: 0.3194 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 48  loss 0.0003  val AP 0.3205\n",
      "  âœ… New best AP: 0.3205 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 49  loss 0.0003  val AP 0.3204\n",
      "[SR-ARE | seed 83] ep 50  loss 0.0003  val AP 0.3232\n",
      "  âœ… New best AP: 0.3232 â†’ v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-ATAD5 (label 8)\n",
      "==============================\n",
      "[SR-ATAD5 | seed 13] ep 01  loss 0.0011  val AP 0.0402\n",
      "  âœ… New best AP: 0.0402 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 02  loss 0.0008  val AP 0.0409\n",
      "  âœ… New best AP: 0.0409 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 03  loss 0.0008  val AP 0.0421\n",
      "  âœ… New best AP: 0.0421 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 04  loss 0.0009  val AP 0.0439\n",
      "  âœ… New best AP: 0.0439 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 05  loss 0.0007  val AP 0.0464\n",
      "  âœ… New best AP: 0.0464 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 06  loss 0.0010  val AP 0.0504\n",
      "  âœ… New best AP: 0.0504 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 07  loss 0.0007  val AP 0.0554\n",
      "  âœ… New best AP: 0.0554 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 08  loss 0.0012  val AP 0.0608\n",
      "  âœ… New best AP: 0.0608 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 09  loss 0.0007  val AP 0.0687\n",
      "  âœ… New best AP: 0.0687 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 10  loss 0.0007  val AP 0.0777\n",
      "  âœ… New best AP: 0.0777 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 11  loss 0.0006  val AP 0.0851\n",
      "  âœ… New best AP: 0.0851 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 12  loss 0.0007  val AP 0.0915\n",
      "  âœ… New best AP: 0.0915 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 13  loss 0.0007  val AP 0.0944\n",
      "  âœ… New best AP: 0.0944 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 14  loss 0.0008  val AP 0.1017\n",
      "  âœ… New best AP: 0.1017 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 15  loss 0.0006  val AP 0.1048\n",
      "  âœ… New best AP: 0.1048 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 16  loss 0.0010  val AP 0.1077\n",
      "  âœ… New best AP: 0.1077 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 17  loss 0.0006  val AP 0.1116\n",
      "  âœ… New best AP: 0.1116 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 18  loss 0.0007  val AP 0.1206\n",
      "  âœ… New best AP: 0.1206 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 19  loss 0.0006  val AP 0.1244\n",
      "  âœ… New best AP: 0.1244 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 20  loss 0.0008  val AP 0.1282\n",
      "  âœ… New best AP: 0.1282 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 21  loss 0.0007  val AP 0.1295\n",
      "  âœ… New best AP: 0.1295 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 22  loss 0.0008  val AP 0.1384\n",
      "  âœ… New best AP: 0.1384 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 23  loss 0.0006  val AP 0.1408\n",
      "  âœ… New best AP: 0.1408 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 24  loss 0.0006  val AP 0.1369\n",
      "[SR-ATAD5 | seed 13] ep 25  loss 0.0006  val AP 0.1369\n",
      "[SR-ATAD5 | seed 13] ep 26  loss 0.0006  val AP 0.1374\n",
      "[SR-ATAD5 | seed 13] ep 27  loss 0.0007  val AP 0.1380\n",
      "[SR-ATAD5 | seed 13] ep 28  loss 0.0006  val AP 0.1385\n",
      "[SR-ATAD5 | seed 13] ep 29  loss 0.0006  val AP 0.1390\n",
      "[SR-ATAD5 | seed 13] ep 30  loss 0.0006  val AP 0.1390\n",
      "[SR-ATAD5 | seed 13] ep 31  loss 0.0006  val AP 0.1393\n",
      "[SR-ATAD5 | seed 13] ep 32  loss 0.0006  val AP 0.1395\n",
      "[SR-ATAD5 | seed 13] ep 33  loss 0.0006  val AP 0.1576\n",
      "  âœ… New best AP: 0.1576 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 34  loss 0.0007  val AP 0.1578\n",
      "  âœ… New best AP: 0.1578 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 35  loss 0.0006  val AP 0.1580\n",
      "  âœ… New best AP: 0.1580 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 36  loss 0.0006  val AP 0.1579\n",
      "[SR-ATAD5 | seed 13] ep 37  loss 0.0006  val AP 0.1579\n",
      "[SR-ATAD5 | seed 13] ep 38  loss 0.0006  val AP 0.1576\n",
      "[SR-ATAD5 | seed 13] ep 39  loss 0.0006  val AP 0.1578\n",
      "[SR-ATAD5 | seed 13] ep 40  loss 0.0006  val AP 0.1580\n",
      "  âœ… New best AP: 0.1580 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 41  loss 0.0006  val AP 0.1568\n",
      "[SR-ATAD5 | seed 13] ep 42  loss 0.0006  val AP 0.1569\n",
      "[SR-ATAD5 | seed 13] ep 43  loss 0.0006  val AP 0.1570\n",
      "[SR-ATAD5 | seed 13] ep 44  loss 0.0006  val AP 0.1572\n",
      "[SR-ATAD5 | seed 13] ep 45  loss 0.0006  val AP 0.1574\n",
      "[SR-ATAD5 | seed 13] ep 46  loss 0.0006  val AP 0.1574\n",
      "[SR-ATAD5 | seed 13] ep 47  loss 0.0007  val AP 0.1577\n",
      "[SR-ATAD5 | seed 13] ep 48  loss 0.0006  val AP 0.1577\n",
      "[SR-ATAD5 | seed 13] ep 49  loss 0.0006  val AP 0.1577\n",
      "[SR-ATAD5 | seed 13] ep 50  loss 0.0006  val AP 0.1578\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.1580\n",
      "[SR-ATAD5 | seed 29] ep 01  loss 0.0013  val AP 0.0665\n",
      "  âœ… New best AP: 0.0665 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 02  loss 0.0008  val AP 0.0766\n",
      "  âœ… New best AP: 0.0766 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 03  loss 0.0010  val AP 0.0906\n",
      "  âœ… New best AP: 0.0906 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 04  loss 0.0012  val AP 0.1090\n",
      "  âœ… New best AP: 0.1090 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 05  loss 0.0009  val AP 0.1694\n",
      "  âœ… New best AP: 0.1694 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 06  loss 0.0008  val AP 0.1777\n",
      "  âœ… New best AP: 0.1777 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 07  loss 0.0008  val AP 0.1951\n",
      "  âœ… New best AP: 0.1951 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 08  loss 0.0010  val AP 0.1996\n",
      "  âœ… New best AP: 0.1996 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 09  loss 0.0009  val AP 0.2009\n",
      "  âœ… New best AP: 0.2009 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 10  loss 0.0009  val AP 0.2019\n",
      "  âœ… New best AP: 0.2019 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 11  loss 0.0007  val AP 0.2013\n",
      "[SR-ATAD5 | seed 29] ep 12  loss 0.0007  val AP 0.2020\n",
      "  âœ… New best AP: 0.2020 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 13  loss 0.0007  val AP 0.2077\n",
      "  âœ… New best AP: 0.2077 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 14  loss 0.0009  val AP 0.2091\n",
      "  âœ… New best AP: 0.2091 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 15  loss 0.0007  val AP 0.2119\n",
      "  âœ… New best AP: 0.2119 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 16  loss 0.0007  val AP 0.2112\n",
      "[SR-ATAD5 | seed 29] ep 17  loss 0.0007  val AP 0.2114\n",
      "[SR-ATAD5 | seed 29] ep 18  loss 0.0007  val AP 0.2110\n",
      "[SR-ATAD5 | seed 29] ep 19  loss 0.0006  val AP 0.2115\n",
      "[SR-ATAD5 | seed 29] ep 20  loss 0.0007  val AP 0.2145\n",
      "  âœ… New best AP: 0.2145 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 21  loss 0.0006  val AP 0.2143\n",
      "[SR-ATAD5 | seed 29] ep 22  loss 0.0008  val AP 0.2166\n",
      "  âœ… New best AP: 0.2166 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 23  loss 0.0006  val AP 0.2233\n",
      "  âœ… New best AP: 0.2233 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 24  loss 0.0007  val AP 0.2149\n",
      "[SR-ATAD5 | seed 29] ep 25  loss 0.0006  val AP 0.2236\n",
      "  âœ… New best AP: 0.2236 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 26  loss 0.0007  val AP 0.2235\n",
      "[SR-ATAD5 | seed 29] ep 27  loss 0.0006  val AP 0.2263\n",
      "  âœ… New best AP: 0.2263 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 28  loss 0.0007  val AP 0.2265\n",
      "  âœ… New best AP: 0.2265 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 29  loss 0.0006  val AP 0.2270\n",
      "  âœ… New best AP: 0.2270 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 30  loss 0.0007  val AP 0.2272\n",
      "  âœ… New best AP: 0.2272 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 31  loss 0.0006  val AP 0.2277\n",
      "  âœ… New best AP: 0.2277 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 32  loss 0.0006  val AP 0.2236\n",
      "[SR-ATAD5 | seed 29] ep 33  loss 0.0006  val AP 0.2238\n",
      "[SR-ATAD5 | seed 29] ep 34  loss 0.0006  val AP 0.2242\n",
      "[SR-ATAD5 | seed 29] ep 35  loss 0.0006  val AP 0.2190\n",
      "[SR-ATAD5 | seed 29] ep 36  loss 0.0006  val AP 0.2165\n",
      "[SR-ATAD5 | seed 29] ep 37  loss 0.0006  val AP 0.2166\n",
      "[SR-ATAD5 | seed 29] ep 38  loss 0.0006  val AP 0.2126\n",
      "[SR-ATAD5 | seed 29] ep 39  loss 0.0006  val AP 0.2104\n",
      "[SR-ATAD5 | seed 29] ep 40  loss 0.0006  val AP 0.2091\n",
      "[SR-ATAD5 | seed 29] ep 41  loss 0.0007  val AP 0.2096\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.2277\n",
      "[SR-ATAD5 | seed 47] ep 01  loss 0.0011  val AP 0.1028\n",
      "  âœ… New best AP: 0.1028 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 02  loss 0.0008  val AP 0.1051\n",
      "  âœ… New best AP: 0.1051 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 03  loss 0.0010  val AP 0.1103\n",
      "  âœ… New best AP: 0.1103 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 04  loss 0.0011  val AP 0.1143\n",
      "  âœ… New best AP: 0.1143 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 05  loss 0.0009  val AP 0.1204\n",
      "  âœ… New best AP: 0.1204 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 06  loss 0.0007  val AP 0.1306\n",
      "  âœ… New best AP: 0.1306 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 07  loss 0.0009  val AP 0.1327\n",
      "  âœ… New best AP: 0.1327 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 08  loss 0.0010  val AP 0.1342\n",
      "  âœ… New best AP: 0.1342 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 09  loss 0.0008  val AP 0.1365\n",
      "  âœ… New best AP: 0.1365 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 10  loss 0.0007  val AP 0.1397\n",
      "  âœ… New best AP: 0.1397 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 11  loss 0.0007  val AP 0.1381\n",
      "[SR-ATAD5 | seed 47] ep 12  loss 0.0007  val AP 0.1410\n",
      "  âœ… New best AP: 0.1410 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 13  loss 0.0007  val AP 0.1409\n",
      "[SR-ATAD5 | seed 47] ep 14  loss 0.0007  val AP 0.1416\n",
      "  âœ… New best AP: 0.1416 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 15  loss 0.0007  val AP 0.1447\n",
      "  âœ… New best AP: 0.1447 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 16  loss 0.0007  val AP 0.1428\n",
      "[SR-ATAD5 | seed 47] ep 17  loss 0.0006  val AP 0.1429\n",
      "[SR-ATAD5 | seed 47] ep 18  loss 0.0008  val AP 0.1495\n",
      "  âœ… New best AP: 0.1495 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 19  loss 0.0006  val AP 0.1524\n",
      "  âœ… New best AP: 0.1524 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 20  loss 0.0006  val AP 0.1474\n",
      "[SR-ATAD5 | seed 47] ep 21  loss 0.0006  val AP 0.1422\n",
      "[SR-ATAD5 | seed 47] ep 22  loss 0.0006  val AP 0.1424\n",
      "[SR-ATAD5 | seed 47] ep 23  loss 0.0006  val AP 0.1426\n",
      "[SR-ATAD5 | seed 47] ep 24  loss 0.0006  val AP 0.1436\n",
      "[SR-ATAD5 | seed 47] ep 25  loss 0.0006  val AP 0.1435\n",
      "[SR-ATAD5 | seed 47] ep 26  loss 0.0006  val AP 0.1437\n",
      "[SR-ATAD5 | seed 47] ep 27  loss 0.0006  val AP 0.1440\n",
      "[SR-ATAD5 | seed 47] ep 28  loss 0.0006  val AP 0.1473\n",
      "[SR-ATAD5 | seed 47] ep 29  loss 0.0006  val AP 0.1486\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.1524\n",
      "[SR-ATAD5 | seed 61] ep 01  loss 0.0015  val AP 0.0491\n",
      "  âœ… New best AP: 0.0491 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 02  loss 0.0009  val AP 0.0525\n",
      "  âœ… New best AP: 0.0525 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 03  loss 0.0010  val AP 0.0575\n",
      "  âœ… New best AP: 0.0575 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 04  loss 0.0011  val AP 0.0638\n",
      "  âœ… New best AP: 0.0638 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 05  loss 0.0009  val AP 0.0728\n",
      "  âœ… New best AP: 0.0728 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 06  loss 0.0008  val AP 0.0801\n",
      "  âœ… New best AP: 0.0801 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 07  loss 0.0010  val AP 0.0825\n",
      "  âœ… New best AP: 0.0825 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 08  loss 0.0009  val AP 0.0856\n",
      "  âœ… New best AP: 0.0856 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 09  loss 0.0010  val AP 0.0861\n",
      "  âœ… New best AP: 0.0861 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 10  loss 0.0009  val AP 0.0913\n",
      "  âœ… New best AP: 0.0913 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 11  loss 0.0008  val AP 0.0955\n",
      "  âœ… New best AP: 0.0955 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 12  loss 0.0007  val AP 0.0977\n",
      "  âœ… New best AP: 0.0977 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 13  loss 0.0008  val AP 0.0994\n",
      "  âœ… New best AP: 0.0994 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 14  loss 0.0007  val AP 0.1020\n",
      "  âœ… New best AP: 0.1020 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 15  loss 0.0008  val AP 0.1094\n",
      "  âœ… New best AP: 0.1094 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 16  loss 0.0007  val AP 0.1085\n",
      "[SR-ATAD5 | seed 61] ep 17  loss 0.0007  val AP 0.1110\n",
      "  âœ… New best AP: 0.1110 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 18  loss 0.0006  val AP 0.1121\n",
      "  âœ… New best AP: 0.1121 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 19  loss 0.0007  val AP 0.1129\n",
      "  âœ… New best AP: 0.1129 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 20  loss 0.0006  val AP 0.1131\n",
      "  âœ… New best AP: 0.1131 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 21  loss 0.0007  val AP 0.1208\n",
      "  âœ… New best AP: 0.1208 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 22  loss 0.0006  val AP 0.1216\n",
      "  âœ… New best AP: 0.1216 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 23  loss 0.0006  val AP 0.1226\n",
      "  âœ… New best AP: 0.1226 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 24  loss 0.0006  val AP 0.1241\n",
      "  âœ… New best AP: 0.1241 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 25  loss 0.0006  val AP 0.1251\n",
      "  âœ… New best AP: 0.1251 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 26  loss 0.0006  val AP 0.1244\n",
      "[SR-ATAD5 | seed 61] ep 27  loss 0.0006  val AP 0.1264\n",
      "  âœ… New best AP: 0.1264 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 28  loss 0.0006  val AP 0.1273\n",
      "  âœ… New best AP: 0.1273 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 29  loss 0.0007  val AP 0.1288\n",
      "  âœ… New best AP: 0.1288 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 30  loss 0.0006  val AP 0.1292\n",
      "  âœ… New best AP: 0.1292 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 31  loss 0.0007  val AP 0.1313\n",
      "  âœ… New best AP: 0.1313 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 32  loss 0.0006  val AP 0.1315\n",
      "  âœ… New best AP: 0.1315 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 33  loss 0.0006  val AP 0.1318\n",
      "  âœ… New best AP: 0.1318 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 34  loss 0.0007  val AP 0.1339\n",
      "  âœ… New best AP: 0.1339 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 35  loss 0.0006  val AP 0.1333\n",
      "[SR-ATAD5 | seed 61] ep 36  loss 0.0006  val AP 0.1339\n",
      "  âœ… New best AP: 0.1339 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 37  loss 0.0007  val AP 0.1341\n",
      "  âœ… New best AP: 0.1341 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 38  loss 0.0006  val AP 0.1340\n",
      "[SR-ATAD5 | seed 61] ep 39  loss 0.0007  val AP 0.1338\n",
      "[SR-ATAD5 | seed 61] ep 40  loss 0.0007  val AP 0.1338\n",
      "[SR-ATAD5 | seed 61] ep 41  loss 0.0006  val AP 0.1339\n",
      "[SR-ATAD5 | seed 61] ep 42  loss 0.0006  val AP 0.1341\n",
      "[SR-ATAD5 | seed 61] ep 43  loss 0.0006  val AP 0.1343\n",
      "  âœ… New best AP: 0.1343 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 44  loss 0.0006  val AP 0.1361\n",
      "  âœ… New best AP: 0.1361 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 45  loss 0.0007  val AP 0.1362\n",
      "  âœ… New best AP: 0.1362 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 46  loss 0.0006  val AP 0.1362\n",
      "  âœ… New best AP: 0.1362 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 47  loss 0.0007  val AP 0.1364\n",
      "  âœ… New best AP: 0.1364 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 48  loss 0.0007  val AP 0.1383\n",
      "  âœ… New best AP: 0.1383 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 49  loss 0.0006  val AP 0.1384\n",
      "  âœ… New best AP: 0.1384 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 50  loss 0.0007  val AP 0.1385\n",
      "  âœ… New best AP: 0.1385 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 01  loss 0.0011  val AP 0.0418\n",
      "  âœ… New best AP: 0.0418 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 02  loss 0.0008  val AP 0.0428\n",
      "  âœ… New best AP: 0.0428 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 03  loss 0.0009  val AP 0.0442\n",
      "  âœ… New best AP: 0.0442 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 04  loss 0.0009  val AP 0.0465\n",
      "  âœ… New best AP: 0.0465 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 05  loss 0.0007  val AP 0.0499\n",
      "  âœ… New best AP: 0.0499 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 06  loss 0.0008  val AP 0.0542\n",
      "  âœ… New best AP: 0.0542 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 07  loss 0.0008  val AP 0.0582\n",
      "  âœ… New best AP: 0.0582 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 08  loss 0.0009  val AP 0.0637\n",
      "  âœ… New best AP: 0.0637 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 09  loss 0.0007  val AP 0.0690\n",
      "  âœ… New best AP: 0.0690 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 10  loss 0.0008  val AP 0.0764\n",
      "  âœ… New best AP: 0.0764 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 11  loss 0.0008  val AP 0.0830\n",
      "  âœ… New best AP: 0.0830 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 12  loss 0.0007  val AP 0.0877\n",
      "  âœ… New best AP: 0.0877 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 13  loss 0.0007  val AP 0.0956\n",
      "  âœ… New best AP: 0.0956 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 14  loss 0.0006  val AP 0.0983\n",
      "  âœ… New best AP: 0.0983 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 15  loss 0.0007  val AP 0.1032\n",
      "  âœ… New best AP: 0.1032 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 16  loss 0.0007  val AP 0.1273\n",
      "  âœ… New best AP: 0.1273 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 17  loss 0.0007  val AP 0.1366\n",
      "  âœ… New best AP: 0.1366 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 18  loss 0.0006  val AP 0.1581\n",
      "  âœ… New best AP: 0.1581 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 19  loss 0.0008  val AP 0.1594\n",
      "  âœ… New best AP: 0.1594 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 20  loss 0.0006  val AP 0.1672\n",
      "  âœ… New best AP: 0.1672 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 21  loss 0.0007  val AP 0.1693\n",
      "  âœ… New best AP: 0.1693 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 22  loss 0.0006  val AP 0.1699\n",
      "  âœ… New best AP: 0.1699 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 23  loss 0.0008  val AP 0.1708\n",
      "  âœ… New best AP: 0.1708 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 24  loss 0.0007  val AP 0.1716\n",
      "  âœ… New best AP: 0.1716 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 25  loss 0.0007  val AP 0.1728\n",
      "  âœ… New best AP: 0.1728 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 26  loss 0.0006  val AP 0.1736\n",
      "  âœ… New best AP: 0.1736 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 27  loss 0.0006  val AP 0.1737\n",
      "  âœ… New best AP: 0.1737 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 28  loss 0.0006  val AP 0.1752\n",
      "  âœ… New best AP: 0.1752 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 29  loss 0.0006  val AP 0.1752\n",
      "  âœ… New best AP: 0.1752 â†’ v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 30  loss 0.0006  val AP 0.1644\n",
      "[SR-ATAD5 | seed 83] ep 31  loss 0.0006  val AP 0.1650\n",
      "[SR-ATAD5 | seed 83] ep 32  loss 0.0006  val AP 0.1651\n",
      "[SR-ATAD5 | seed 83] ep 33  loss 0.0006  val AP 0.1653\n",
      "[SR-ATAD5 | seed 83] ep 34  loss 0.0006  val AP 0.1653\n",
      "[SR-ATAD5 | seed 83] ep 35  loss 0.0006  val AP 0.1658\n",
      "[SR-ATAD5 | seed 83] ep 36  loss 0.0006  val AP 0.1660\n",
      "[SR-ATAD5 | seed 83] ep 37  loss 0.0006  val AP 0.1616\n",
      "[SR-ATAD5 | seed 83] ep 38  loss 0.0006  val AP 0.1617\n",
      "[SR-ATAD5 | seed 83] ep 39  loss 0.0006  val AP 0.1629\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.1752\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-HSE (label 9)\n",
      "==============================\n",
      "[SR-HSE | seed 13] ep 01  loss 0.0008  val AP 0.0507\n",
      "  âœ… New best AP: 0.0507 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 02  loss 0.0006  val AP 0.0518\n",
      "  âœ… New best AP: 0.0518 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 03  loss 0.0006  val AP 0.0529\n",
      "  âœ… New best AP: 0.0529 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 04  loss 0.0006  val AP 0.0549\n",
      "  âœ… New best AP: 0.0549 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 05  loss 0.0006  val AP 0.0579\n",
      "  âœ… New best AP: 0.0579 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 06  loss 0.0010  val AP 0.0619\n",
      "  âœ… New best AP: 0.0619 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 07  loss 0.0006  val AP 0.0650\n",
      "  âœ… New best AP: 0.0650 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 08  loss 0.0008  val AP 0.0690\n",
      "  âœ… New best AP: 0.0690 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 09  loss 0.0006  val AP 0.0741\n",
      "  âœ… New best AP: 0.0741 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 10  loss 0.0006  val AP 0.0791\n",
      "  âœ… New best AP: 0.0791 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 11  loss 0.0006  val AP 0.0852\n",
      "  âœ… New best AP: 0.0852 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 12  loss 0.0006  val AP 0.0914\n",
      "  âœ… New best AP: 0.0914 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 13  loss 0.0006  val AP 0.0974\n",
      "  âœ… New best AP: 0.0974 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 14  loss 0.0007  val AP 0.1035\n",
      "  âœ… New best AP: 0.1035 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 15  loss 0.0005  val AP 0.1087\n",
      "  âœ… New best AP: 0.1087 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 16  loss 0.0006  val AP 0.1158\n",
      "  âœ… New best AP: 0.1158 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 17  loss 0.0006  val AP 0.1215\n",
      "  âœ… New best AP: 0.1215 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 18  loss 0.0008  val AP 0.1250\n",
      "  âœ… New best AP: 0.1250 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 19  loss 0.0006  val AP 0.1331\n",
      "  âœ… New best AP: 0.1331 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 20  loss 0.0006  val AP 0.1375\n",
      "  âœ… New best AP: 0.1375 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 21  loss 0.0005  val AP 0.1427\n",
      "  âœ… New best AP: 0.1427 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 22  loss 0.0006  val AP 0.1446\n",
      "  âœ… New best AP: 0.1446 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 23  loss 0.0005  val AP 0.1486\n",
      "  âœ… New best AP: 0.1486 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 24  loss 0.0006  val AP 0.1504\n",
      "  âœ… New best AP: 0.1504 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 25  loss 0.0005  val AP 0.1525\n",
      "  âœ… New best AP: 0.1525 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 26  loss 0.0005  val AP 0.1533\n",
      "  âœ… New best AP: 0.1533 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 27  loss 0.0005  val AP 0.1565\n",
      "  âœ… New best AP: 0.1565 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 28  loss 0.0005  val AP 0.1607\n",
      "  âœ… New best AP: 0.1607 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 29  loss 0.0005  val AP 0.1632\n",
      "  âœ… New best AP: 0.1632 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 30  loss 0.0006  val AP 0.1640\n",
      "  âœ… New best AP: 0.1640 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 31  loss 0.0005  val AP 0.1657\n",
      "  âœ… New best AP: 0.1657 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 32  loss 0.0005  val AP 0.1677\n",
      "  âœ… New best AP: 0.1677 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 33  loss 0.0005  val AP 0.1682\n",
      "  âœ… New best AP: 0.1682 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 34  loss 0.0005  val AP 0.1700\n",
      "  âœ… New best AP: 0.1700 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 35  loss 0.0005  val AP 0.1712\n",
      "  âœ… New best AP: 0.1712 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 36  loss 0.0005  val AP 0.1731\n",
      "  âœ… New best AP: 0.1731 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 37  loss 0.0005  val AP 0.1738\n",
      "  âœ… New best AP: 0.1738 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 38  loss 0.0005  val AP 0.1752\n",
      "  âœ… New best AP: 0.1752 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 39  loss 0.0005  val AP 0.1760\n",
      "  âœ… New best AP: 0.1760 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 40  loss 0.0005  val AP 0.1770\n",
      "  âœ… New best AP: 0.1770 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 41  loss 0.0005  val AP 0.1784\n",
      "  âœ… New best AP: 0.1784 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 42  loss 0.0005  val AP 0.1783\n",
      "[SR-HSE | seed 13] ep 43  loss 0.0005  val AP 0.1787\n",
      "  âœ… New best AP: 0.1787 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 44  loss 0.0005  val AP 0.1792\n",
      "  âœ… New best AP: 0.1792 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 45  loss 0.0005  val AP 0.1797\n",
      "  âœ… New best AP: 0.1797 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 46  loss 0.0006  val AP 0.1804\n",
      "  âœ… New best AP: 0.1804 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 47  loss 0.0005  val AP 0.1807\n",
      "  âœ… New best AP: 0.1807 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 48  loss 0.0005  val AP 0.1814\n",
      "  âœ… New best AP: 0.1814 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 49  loss 0.0006  val AP 0.1815\n",
      "  âœ… New best AP: 0.1815 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 50  loss 0.0005  val AP 0.1816\n",
      "  âœ… New best AP: 0.1816 â†’ v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 29] ep 01  loss 0.0010  val AP 0.0864\n",
      "  âœ… New best AP: 0.0864 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 02  loss 0.0007  val AP 0.0904\n",
      "  âœ… New best AP: 0.0904 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 03  loss 0.0009  val AP 0.0977\n",
      "  âœ… New best AP: 0.0977 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 04  loss 0.0009  val AP 0.1021\n",
      "  âœ… New best AP: 0.1021 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 05  loss 0.0007  val AP 0.1087\n",
      "  âœ… New best AP: 0.1087 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 06  loss 0.0007  val AP 0.1197\n",
      "  âœ… New best AP: 0.1197 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 07  loss 0.0009  val AP 0.1202\n",
      "  âœ… New best AP: 0.1202 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 08  loss 0.0008  val AP 0.1366\n",
      "  âœ… New best AP: 0.1366 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 09  loss 0.0007  val AP 0.1490\n",
      "  âœ… New best AP: 0.1490 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 10  loss 0.0006  val AP 0.1488\n",
      "[SR-HSE | seed 29] ep 11  loss 0.0006  val AP 0.1486\n",
      "[SR-HSE | seed 29] ep 12  loss 0.0006  val AP 0.1516\n",
      "  âœ… New best AP: 0.1516 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 13  loss 0.0006  val AP 0.1548\n",
      "  âœ… New best AP: 0.1548 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 14  loss 0.0006  val AP 0.1568\n",
      "  âœ… New best AP: 0.1568 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 15  loss 0.0007  val AP 0.1608\n",
      "  âœ… New best AP: 0.1608 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 16  loss 0.0006  val AP 0.1547\n",
      "[SR-HSE | seed 29] ep 17  loss 0.0006  val AP 0.1444\n",
      "[SR-HSE | seed 29] ep 18  loss 0.0006  val AP 0.1472\n",
      "[SR-HSE | seed 29] ep 19  loss 0.0005  val AP 0.1443\n",
      "[SR-HSE | seed 29] ep 20  loss 0.0006  val AP 0.1432\n",
      "[SR-HSE | seed 29] ep 21  loss 0.0005  val AP 0.1473\n",
      "[SR-HSE | seed 29] ep 22  loss 0.0006  val AP 0.1520\n",
      "[SR-HSE | seed 29] ep 23  loss 0.0006  val AP 0.1551\n",
      "[SR-HSE | seed 29] ep 24  loss 0.0006  val AP 0.1586\n",
      "[SR-HSE | seed 29] ep 25  loss 0.0005  val AP 0.1652\n",
      "  âœ… New best AP: 0.1652 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 26  loss 0.0006  val AP 0.1659\n",
      "  âœ… New best AP: 0.1659 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 27  loss 0.0005  val AP 0.1728\n",
      "  âœ… New best AP: 0.1728 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 28  loss 0.0005  val AP 0.1804\n",
      "  âœ… New best AP: 0.1804 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 29  loss 0.0006  val AP 0.1860\n",
      "  âœ… New best AP: 0.1860 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 30  loss 0.0006  val AP 0.1886\n",
      "  âœ… New best AP: 0.1886 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 31  loss 0.0005  val AP 0.1943\n",
      "  âœ… New best AP: 0.1943 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 32  loss 0.0006  val AP 0.1960\n",
      "  âœ… New best AP: 0.1960 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 33  loss 0.0005  val AP 0.1985\n",
      "  âœ… New best AP: 0.1985 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 34  loss 0.0005  val AP 0.2018\n",
      "  âœ… New best AP: 0.2018 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 35  loss 0.0006  val AP 0.2028\n",
      "  âœ… New best AP: 0.2028 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 36  loss 0.0005  val AP 0.2049\n",
      "  âœ… New best AP: 0.2049 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 37  loss 0.0006  val AP 0.2067\n",
      "  âœ… New best AP: 0.2067 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 38  loss 0.0006  val AP 0.2116\n",
      "  âœ… New best AP: 0.2116 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 39  loss 0.0006  val AP 0.2122\n",
      "  âœ… New best AP: 0.2122 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 40  loss 0.0006  val AP 0.2157\n",
      "  âœ… New best AP: 0.2157 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 41  loss 0.0006  val AP 0.2166\n",
      "  âœ… New best AP: 0.2166 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 42  loss 0.0006  val AP 0.2174\n",
      "  âœ… New best AP: 0.2174 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 43  loss 0.0006  val AP 0.2197\n",
      "  âœ… New best AP: 0.2197 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 44  loss 0.0005  val AP 0.2203\n",
      "  âœ… New best AP: 0.2203 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 45  loss 0.0006  val AP 0.2233\n",
      "  âœ… New best AP: 0.2233 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 46  loss 0.0006  val AP 0.2283\n",
      "  âœ… New best AP: 0.2283 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 47  loss 0.0006  val AP 0.2280\n",
      "[SR-HSE | seed 29] ep 48  loss 0.0006  val AP 0.2308\n",
      "  âœ… New best AP: 0.2308 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 49  loss 0.0006  val AP 0.2310\n",
      "  âœ… New best AP: 0.2310 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 50  loss 0.0006  val AP 0.2323\n",
      "  âœ… New best AP: 0.2323 â†’ v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 47] ep 01  loss 0.0010  val AP 0.0650\n",
      "  âœ… New best AP: 0.0650 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 02  loss 0.0007  val AP 0.0654\n",
      "  âœ… New best AP: 0.0654 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 03  loss 0.0008  val AP 0.0663\n",
      "  âœ… New best AP: 0.0663 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 04  loss 0.0009  val AP 0.0677\n",
      "  âœ… New best AP: 0.0677 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 05  loss 0.0007  val AP 0.0697\n",
      "  âœ… New best AP: 0.0697 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 06  loss 0.0006  val AP 0.0715\n",
      "  âœ… New best AP: 0.0715 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 07  loss 0.0007  val AP 0.0729\n",
      "  âœ… New best AP: 0.0729 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 08  loss 0.0007  val AP 0.0734\n",
      "  âœ… New best AP: 0.0734 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 09  loss 0.0009  val AP 0.0745\n",
      "  âœ… New best AP: 0.0745 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 10  loss 0.0006  val AP 0.0765\n",
      "  âœ… New best AP: 0.0765 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 11  loss 0.0006  val AP 0.0784\n",
      "  âœ… New best AP: 0.0784 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 12  loss 0.0005  val AP 0.0809\n",
      "  âœ… New best AP: 0.0809 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 13  loss 0.0007  val AP 0.0830\n",
      "  âœ… New best AP: 0.0830 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 14  loss 0.0006  val AP 0.0845\n",
      "  âœ… New best AP: 0.0845 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 15  loss 0.0006  val AP 0.0870\n",
      "  âœ… New best AP: 0.0870 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 16  loss 0.0006  val AP 0.0889\n",
      "  âœ… New best AP: 0.0889 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 17  loss 0.0006  val AP 0.0902\n",
      "  âœ… New best AP: 0.0902 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 18  loss 0.0005  val AP 0.0920\n",
      "  âœ… New best AP: 0.0920 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 19  loss 0.0007  val AP 0.0944\n",
      "  âœ… New best AP: 0.0944 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 20  loss 0.0005  val AP 0.0967\n",
      "  âœ… New best AP: 0.0967 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 21  loss 0.0005  val AP 0.0974\n",
      "  âœ… New best AP: 0.0974 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 22  loss 0.0006  val AP 0.0984\n",
      "  âœ… New best AP: 0.0984 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 23  loss 0.0006  val AP 0.1020\n",
      "  âœ… New best AP: 0.1020 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 24  loss 0.0006  val AP 0.1030\n",
      "  âœ… New best AP: 0.1030 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 25  loss 0.0005  val AP 0.1082\n",
      "  âœ… New best AP: 0.1082 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 26  loss 0.0006  val AP 0.1092\n",
      "  âœ… New best AP: 0.1092 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 27  loss 0.0005  val AP 0.1115\n",
      "  âœ… New best AP: 0.1115 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 28  loss 0.0005  val AP 0.1125\n",
      "  âœ… New best AP: 0.1125 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 29  loss 0.0005  val AP 0.1134\n",
      "  âœ… New best AP: 0.1134 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 30  loss 0.0005  val AP 0.1144\n",
      "  âœ… New best AP: 0.1144 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 31  loss 0.0005  val AP 0.1150\n",
      "  âœ… New best AP: 0.1150 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 32  loss 0.0005  val AP 0.1171\n",
      "  âœ… New best AP: 0.1171 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 33  loss 0.0005  val AP 0.1183\n",
      "  âœ… New best AP: 0.1183 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 34  loss 0.0005  val AP 0.1201\n",
      "  âœ… New best AP: 0.1201 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 35  loss 0.0005  val AP 0.1209\n",
      "  âœ… New best AP: 0.1209 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 36  loss 0.0005  val AP 0.1215\n",
      "  âœ… New best AP: 0.1215 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 37  loss 0.0005  val AP 0.1223\n",
      "  âœ… New best AP: 0.1223 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 38  loss 0.0005  val AP 0.1233\n",
      "  âœ… New best AP: 0.1233 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 39  loss 0.0005  val AP 0.1245\n",
      "  âœ… New best AP: 0.1245 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 40  loss 0.0005  val AP 0.1252\n",
      "  âœ… New best AP: 0.1252 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 41  loss 0.0005  val AP 0.1274\n",
      "  âœ… New best AP: 0.1274 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 42  loss 0.0005  val AP 0.1283\n",
      "  âœ… New best AP: 0.1283 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 43  loss 0.0005  val AP 0.1290\n",
      "  âœ… New best AP: 0.1290 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 44  loss 0.0005  val AP 0.1302\n",
      "  âœ… New best AP: 0.1302 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 45  loss 0.0005  val AP 0.1302\n",
      "[SR-HSE | seed 47] ep 46  loss 0.0005  val AP 0.1312\n",
      "  âœ… New best AP: 0.1312 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 47  loss 0.0005  val AP 0.1316\n",
      "  âœ… New best AP: 0.1316 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 48  loss 0.0005  val AP 0.1319\n",
      "  âœ… New best AP: 0.1319 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 49  loss 0.0006  val AP 0.1324\n",
      "  âœ… New best AP: 0.1324 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 50  loss 0.0005  val AP 0.1327\n",
      "  âœ… New best AP: 0.1327 â†’ v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 61] ep 01  loss 0.0011  val AP 0.0753\n",
      "  âœ… New best AP: 0.0753 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 02  loss 0.0008  val AP 0.0790\n",
      "  âœ… New best AP: 0.0790 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 03  loss 0.0008  val AP 0.0841\n",
      "  âœ… New best AP: 0.0841 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 04  loss 0.0009  val AP 0.0876\n",
      "  âœ… New best AP: 0.0876 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 05  loss 0.0008  val AP 0.0934\n",
      "  âœ… New best AP: 0.0934 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 06  loss 0.0007  val AP 0.0996\n",
      "  âœ… New best AP: 0.0996 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 07  loss 0.0007  val AP 0.1034\n",
      "  âœ… New best AP: 0.1034 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 08  loss 0.0008  val AP 0.1064\n",
      "  âœ… New best AP: 0.1064 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 09  loss 0.0007  val AP 0.1132\n",
      "  âœ… New best AP: 0.1132 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 10  loss 0.0007  val AP 0.1279\n",
      "  âœ… New best AP: 0.1279 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 11  loss 0.0007  val AP 0.1328\n",
      "  âœ… New best AP: 0.1328 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 12  loss 0.0006  val AP 0.1351\n",
      "  âœ… New best AP: 0.1351 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 13  loss 0.0006  val AP 0.1417\n",
      "  âœ… New best AP: 0.1417 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 14  loss 0.0007  val AP 0.1431\n",
      "  âœ… New best AP: 0.1431 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 15  loss 0.0006  val AP 0.1443\n",
      "  âœ… New best AP: 0.1443 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 16  loss 0.0007  val AP 0.1476\n",
      "  âœ… New best AP: 0.1476 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 17  loss 0.0006  val AP 0.1493\n",
      "  âœ… New best AP: 0.1493 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 18  loss 0.0006  val AP 0.1383\n",
      "[SR-HSE | seed 61] ep 19  loss 0.0006  val AP 0.1395\n",
      "[SR-HSE | seed 61] ep 20  loss 0.0006  val AP 0.1406\n",
      "[SR-HSE | seed 61] ep 21  loss 0.0006  val AP 0.1418\n",
      "[SR-HSE | seed 61] ep 22  loss 0.0006  val AP 0.1429\n",
      "[SR-HSE | seed 61] ep 23  loss 0.0006  val AP 0.1445\n",
      "[SR-HSE | seed 61] ep 24  loss 0.0006  val AP 0.1470\n",
      "[SR-HSE | seed 61] ep 25  loss 0.0005  val AP 0.1482\n",
      "[SR-HSE | seed 61] ep 26  loss 0.0005  val AP 0.1497\n",
      "  âœ… New best AP: 0.1497 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 27  loss 0.0005  val AP 0.1516\n",
      "  âœ… New best AP: 0.1516 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 28  loss 0.0006  val AP 0.1523\n",
      "  âœ… New best AP: 0.1523 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 29  loss 0.0005  val AP 0.1537\n",
      "  âœ… New best AP: 0.1537 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 30  loss 0.0005  val AP 0.1547\n",
      "  âœ… New best AP: 0.1547 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 31  loss 0.0005  val AP 0.1632\n",
      "  âœ… New best AP: 0.1632 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 32  loss 0.0005  val AP 0.1646\n",
      "  âœ… New best AP: 0.1646 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 33  loss 0.0005  val AP 0.1658\n",
      "  âœ… New best AP: 0.1658 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 34  loss 0.0005  val AP 0.1666\n",
      "  âœ… New best AP: 0.1666 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 35  loss 0.0005  val AP 0.1673\n",
      "  âœ… New best AP: 0.1673 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 36  loss 0.0005  val AP 0.1676\n",
      "  âœ… New best AP: 0.1676 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 37  loss 0.0006  val AP 0.1683\n",
      "  âœ… New best AP: 0.1683 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 38  loss 0.0006  val AP 0.1683\n",
      "  âœ… New best AP: 0.1683 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 39  loss 0.0005  val AP 0.1684\n",
      "  âœ… New best AP: 0.1684 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 40  loss 0.0006  val AP 0.1691\n",
      "  âœ… New best AP: 0.1691 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 41  loss 0.0006  val AP 0.1693\n",
      "  âœ… New best AP: 0.1693 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 42  loss 0.0005  val AP 0.1694\n",
      "  âœ… New best AP: 0.1694 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 43  loss 0.0006  val AP 0.1704\n",
      "  âœ… New best AP: 0.1704 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 44  loss 0.0006  val AP 0.1709\n",
      "  âœ… New best AP: 0.1709 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 45  loss 0.0006  val AP 0.1713\n",
      "  âœ… New best AP: 0.1713 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 46  loss 0.0005  val AP 0.1720\n",
      "  âœ… New best AP: 0.1720 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 47  loss 0.0005  val AP 0.1728\n",
      "  âœ… New best AP: 0.1728 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 48  loss 0.0006  val AP 0.1731\n",
      "  âœ… New best AP: 0.1731 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 49  loss 0.0005  val AP 0.1735\n",
      "  âœ… New best AP: 0.1735 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 50  loss 0.0005  val AP 0.1736\n",
      "  âœ… New best AP: 0.1736 â†’ v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 83] ep 01  loss 0.0009  val AP 0.0853\n",
      "  âœ… New best AP: 0.0853 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 02  loss 0.0007  val AP 0.0881\n",
      "  âœ… New best AP: 0.0881 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 03  loss 0.0008  val AP 0.0902\n",
      "  âœ… New best AP: 0.0902 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 04  loss 0.0008  val AP 0.0906\n",
      "  âœ… New best AP: 0.0906 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 05  loss 0.0006  val AP 0.0887\n",
      "[SR-HSE | seed 83] ep 06  loss 0.0007  val AP 0.0919\n",
      "  âœ… New best AP: 0.0919 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 07  loss 0.0007  val AP 0.0958\n",
      "  âœ… New best AP: 0.0958 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 08  loss 0.0007  val AP 0.0974\n",
      "  âœ… New best AP: 0.0974 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 09  loss 0.0006  val AP 0.1003\n",
      "  âœ… New best AP: 0.1003 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 10  loss 0.0007  val AP 0.1014\n",
      "  âœ… New best AP: 0.1014 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 11  loss 0.0006  val AP 0.1021\n",
      "  âœ… New best AP: 0.1021 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 12  loss 0.0006  val AP 0.1045\n",
      "  âœ… New best AP: 0.1045 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 13  loss 0.0006  val AP 0.1080\n",
      "  âœ… New best AP: 0.1080 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 14  loss 0.0006  val AP 0.1111\n",
      "  âœ… New best AP: 0.1111 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 15  loss 0.0007  val AP 0.1127\n",
      "  âœ… New best AP: 0.1127 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 16  loss 0.0005  val AP 0.1146\n",
      "  âœ… New best AP: 0.1146 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 17  loss 0.0006  val AP 0.1166\n",
      "  âœ… New best AP: 0.1166 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 18  loss 0.0005  val AP 0.1188\n",
      "  âœ… New best AP: 0.1188 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 19  loss 0.0007  val AP 0.1214\n",
      "  âœ… New best AP: 0.1214 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 20  loss 0.0005  val AP 0.1224\n",
      "  âœ… New best AP: 0.1224 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 21  loss 0.0005  val AP 0.1249\n",
      "  âœ… New best AP: 0.1249 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 22  loss 0.0006  val AP 0.1261\n",
      "  âœ… New best AP: 0.1261 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 23  loss 0.0005  val AP 0.1263\n",
      "  âœ… New best AP: 0.1263 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 24  loss 0.0005  val AP 0.1277\n",
      "  âœ… New best AP: 0.1277 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 25  loss 0.0006  val AP 0.1300\n",
      "  âœ… New best AP: 0.1300 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 26  loss 0.0005  val AP 0.1324\n",
      "  âœ… New best AP: 0.1324 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 27  loss 0.0006  val AP 0.1343\n",
      "  âœ… New best AP: 0.1343 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 28  loss 0.0005  val AP 0.1360\n",
      "  âœ… New best AP: 0.1360 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 29  loss 0.0005  val AP 0.1373\n",
      "  âœ… New best AP: 0.1373 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 30  loss 0.0005  val AP 0.1383\n",
      "  âœ… New best AP: 0.1383 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 31  loss 0.0006  val AP 0.1398\n",
      "  âœ… New best AP: 0.1398 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 32  loss 0.0005  val AP 0.1409\n",
      "  âœ… New best AP: 0.1409 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 33  loss 0.0005  val AP 0.1423\n",
      "  âœ… New best AP: 0.1423 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 34  loss 0.0005  val AP 0.1441\n",
      "  âœ… New best AP: 0.1441 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 35  loss 0.0005  val AP 0.1453\n",
      "  âœ… New best AP: 0.1453 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 36  loss 0.0005  val AP 0.1463\n",
      "  âœ… New best AP: 0.1463 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 37  loss 0.0006  val AP 0.1494\n",
      "  âœ… New best AP: 0.1494 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 38  loss 0.0005  val AP 0.1506\n",
      "  âœ… New best AP: 0.1506 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 39  loss 0.0005  val AP 0.1536\n",
      "  âœ… New best AP: 0.1536 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 40  loss 0.0005  val AP 0.1542\n",
      "  âœ… New best AP: 0.1542 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 41  loss 0.0005  val AP 0.1558\n",
      "  âœ… New best AP: 0.1558 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 42  loss 0.0005  val AP 0.1561\n",
      "  âœ… New best AP: 0.1561 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 43  loss 0.0005  val AP 0.1574\n",
      "  âœ… New best AP: 0.1574 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 44  loss 0.0005  val AP 0.1585\n",
      "  âœ… New best AP: 0.1585 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 45  loss 0.0006  val AP 0.1594\n",
      "  âœ… New best AP: 0.1594 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 46  loss 0.0006  val AP 0.1597\n",
      "  âœ… New best AP: 0.1597 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 47  loss 0.0005  val AP 0.1600\n",
      "  âœ… New best AP: 0.1600 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 48  loss 0.0005  val AP 0.1605\n",
      "  âœ… New best AP: 0.1605 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 49  loss 0.0005  val AP 0.1620\n",
      "  âœ… New best AP: 0.1620 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 50  loss 0.0006  val AP 0.1629\n",
      "  âœ… New best AP: 0.1629 â†’ v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-MMP (label 10)\n",
      "==============================\n",
      "[SR-MMP | seed 13] ep 01  loss 0.0004  val AP 0.0771\n",
      "  âœ… New best AP: 0.0771 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 02  loss 0.0003  val AP 0.0781\n",
      "  âœ… New best AP: 0.0781 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 03  loss 0.0003  val AP 0.0798\n",
      "  âœ… New best AP: 0.0798 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 04  loss 0.0003  val AP 0.0827\n",
      "  âœ… New best AP: 0.0827 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 05  loss 0.0003  val AP 0.0873\n",
      "  âœ… New best AP: 0.0873 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 06  loss 0.0003  val AP 0.0956\n",
      "  âœ… New best AP: 0.0956 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 07  loss 0.0003  val AP 0.1068\n",
      "  âœ… New best AP: 0.1068 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 08  loss 0.0003  val AP 0.1330\n",
      "  âœ… New best AP: 0.1330 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 09  loss 0.0004  val AP 0.1506\n",
      "  âœ… New best AP: 0.1506 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 10  loss 0.0003  val AP 0.1721\n",
      "  âœ… New best AP: 0.1721 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 11  loss 0.0002  val AP 0.1967\n",
      "  âœ… New best AP: 0.1967 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 12  loss 0.0002  val AP 0.2263\n",
      "  âœ… New best AP: 0.2263 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 13  loss 0.0002  val AP 0.2425\n",
      "  âœ… New best AP: 0.2425 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 14  loss 0.0003  val AP 0.2567\n",
      "  âœ… New best AP: 0.2567 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 15  loss 0.0002  val AP 0.2710\n",
      "  âœ… New best AP: 0.2710 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 16  loss 0.0003  val AP 0.2826\n",
      "  âœ… New best AP: 0.2826 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 17  loss 0.0002  val AP 0.2932\n",
      "  âœ… New best AP: 0.2932 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 18  loss 0.0002  val AP 0.3044\n",
      "  âœ… New best AP: 0.3044 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 19  loss 0.0003  val AP 0.3156\n",
      "  âœ… New best AP: 0.3156 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 20  loss 0.0003  val AP 0.3208\n",
      "  âœ… New best AP: 0.3208 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 21  loss 0.0002  val AP 0.3208\n",
      "  âœ… New best AP: 0.3208 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 22  loss 0.0002  val AP 0.3268\n",
      "  âœ… New best AP: 0.3268 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 23  loss 0.0003  val AP 0.3301\n",
      "  âœ… New best AP: 0.3301 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 24  loss 0.0002  val AP 0.3330\n",
      "  âœ… New best AP: 0.3330 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 25  loss 0.0002  val AP 0.3383\n",
      "  âœ… New best AP: 0.3383 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 26  loss 0.0002  val AP 0.3414\n",
      "  âœ… New best AP: 0.3414 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 27  loss 0.0002  val AP 0.3461\n",
      "  âœ… New best AP: 0.3461 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 28  loss 0.0002  val AP 0.3517\n",
      "  âœ… New best AP: 0.3517 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 29  loss 0.0002  val AP 0.3536\n",
      "  âœ… New best AP: 0.3536 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 30  loss 0.0002  val AP 0.3562\n",
      "  âœ… New best AP: 0.3562 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 31  loss 0.0002  val AP 0.3568\n",
      "  âœ… New best AP: 0.3568 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 32  loss 0.0002  val AP 0.3575\n",
      "  âœ… New best AP: 0.3575 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 33  loss 0.0002  val AP 0.3590\n",
      "  âœ… New best AP: 0.3590 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 34  loss 0.0002  val AP 0.3661\n",
      "  âœ… New best AP: 0.3661 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 35  loss 0.0002  val AP 0.3676\n",
      "  âœ… New best AP: 0.3676 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 36  loss 0.0002  val AP 0.3703\n",
      "  âœ… New best AP: 0.3703 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 37  loss 0.0002  val AP 0.3712\n",
      "  âœ… New best AP: 0.3712 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 38  loss 0.0002  val AP 0.3719\n",
      "  âœ… New best AP: 0.3719 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 39  loss 0.0002  val AP 0.3733\n",
      "  âœ… New best AP: 0.3733 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 40  loss 0.0002  val AP 0.3736\n",
      "  âœ… New best AP: 0.3736 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 41  loss 0.0002  val AP 0.3747\n",
      "  âœ… New best AP: 0.3747 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 42  loss 0.0002  val AP 0.3748\n",
      "  âœ… New best AP: 0.3748 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 43  loss 0.0002  val AP 0.3761\n",
      "  âœ… New best AP: 0.3761 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 44  loss 0.0002  val AP 0.3772\n",
      "  âœ… New best AP: 0.3772 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 45  loss 0.0002  val AP 0.3777\n",
      "  âœ… New best AP: 0.3777 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 46  loss 0.0002  val AP 0.3789\n",
      "  âœ… New best AP: 0.3789 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 47  loss 0.0002  val AP 0.3796\n",
      "  âœ… New best AP: 0.3796 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 48  loss 0.0002  val AP 0.3803\n",
      "  âœ… New best AP: 0.3803 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 49  loss 0.0002  val AP 0.3808\n",
      "  âœ… New best AP: 0.3808 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 50  loss 0.0002  val AP 0.3811\n",
      "  âœ… New best AP: 0.3811 â†’ v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 29] ep 01  loss 0.0005  val AP 0.1965\n",
      "  âœ… New best AP: 0.1965 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 02  loss 0.0003  val AP 0.2079\n",
      "  âœ… New best AP: 0.2079 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 03  loss 0.0004  val AP 0.2252\n",
      "  âœ… New best AP: 0.2252 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 04  loss 0.0004  val AP 0.2431\n",
      "  âœ… New best AP: 0.2431 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 05  loss 0.0003  val AP 0.2608\n",
      "  âœ… New best AP: 0.2608 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 06  loss 0.0003  val AP 0.2841\n",
      "  âœ… New best AP: 0.2841 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 07  loss 0.0003  val AP 0.2860\n",
      "  âœ… New best AP: 0.2860 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 08  loss 0.0003  val AP 0.2921\n",
      "  âœ… New best AP: 0.2921 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 09  loss 0.0003  val AP 0.2989\n",
      "  âœ… New best AP: 0.2989 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 10  loss 0.0003  val AP 0.3087\n",
      "  âœ… New best AP: 0.3087 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 11  loss 0.0002  val AP 0.3118\n",
      "  âœ… New best AP: 0.3118 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 12  loss 0.0003  val AP 0.3212\n",
      "  âœ… New best AP: 0.3212 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 13  loss 0.0002  val AP 0.3277\n",
      "  âœ… New best AP: 0.3277 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 14  loss 0.0002  val AP 0.3300\n",
      "  âœ… New best AP: 0.3300 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 15  loss 0.0002  val AP 0.3406\n",
      "  âœ… New best AP: 0.3406 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 16  loss 0.0003  val AP 0.3466\n",
      "  âœ… New best AP: 0.3466 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 17  loss 0.0003  val AP 0.3545\n",
      "  âœ… New best AP: 0.3545 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 18  loss 0.0002  val AP 0.3606\n",
      "  âœ… New best AP: 0.3606 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 19  loss 0.0002  val AP 0.3724\n",
      "  âœ… New best AP: 0.3724 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 20  loss 0.0002  val AP 0.3785\n",
      "  âœ… New best AP: 0.3785 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 21  loss 0.0002  val AP 0.3851\n",
      "  âœ… New best AP: 0.3851 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 22  loss 0.0002  val AP 0.3929\n",
      "  âœ… New best AP: 0.3929 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 23  loss 0.0002  val AP 0.3964\n",
      "  âœ… New best AP: 0.3964 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 24  loss 0.0002  val AP 0.4028\n",
      "  âœ… New best AP: 0.4028 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 25  loss 0.0002  val AP 0.4069\n",
      "  âœ… New best AP: 0.4069 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 26  loss 0.0002  val AP 0.4092\n",
      "  âœ… New best AP: 0.4092 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 27  loss 0.0002  val AP 0.4117\n",
      "  âœ… New best AP: 0.4117 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 28  loss 0.0002  val AP 0.4174\n",
      "  âœ… New best AP: 0.4174 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 29  loss 0.0002  val AP 0.4187\n",
      "  âœ… New best AP: 0.4187 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 30  loss 0.0002  val AP 0.4263\n",
      "  âœ… New best AP: 0.4263 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 31  loss 0.0002  val AP 0.4275\n",
      "  âœ… New best AP: 0.4275 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 32  loss 0.0002  val AP 0.4286\n",
      "  âœ… New best AP: 0.4286 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 33  loss 0.0002  val AP 0.4317\n",
      "  âœ… New best AP: 0.4317 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 34  loss 0.0002  val AP 0.4338\n",
      "  âœ… New best AP: 0.4338 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 35  loss 0.0002  val AP 0.4360\n",
      "  âœ… New best AP: 0.4360 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 36  loss 0.0002  val AP 0.4371\n",
      "  âœ… New best AP: 0.4371 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 37  loss 0.0002  val AP 0.4377\n",
      "  âœ… New best AP: 0.4377 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 38  loss 0.0002  val AP 0.4387\n",
      "  âœ… New best AP: 0.4387 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 39  loss 0.0002  val AP 0.4391\n",
      "  âœ… New best AP: 0.4391 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 40  loss 0.0002  val AP 0.4401\n",
      "  âœ… New best AP: 0.4401 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 41  loss 0.0002  val AP 0.4410\n",
      "  âœ… New best AP: 0.4410 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 42  loss 0.0002  val AP 0.4418\n",
      "  âœ… New best AP: 0.4418 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 43  loss 0.0002  val AP 0.4423\n",
      "  âœ… New best AP: 0.4423 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 44  loss 0.0002  val AP 0.4430\n",
      "  âœ… New best AP: 0.4430 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 45  loss 0.0002  val AP 0.4431\n",
      "  âœ… New best AP: 0.4431 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 46  loss 0.0002  val AP 0.4439\n",
      "  âœ… New best AP: 0.4439 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 47  loss 0.0002  val AP 0.4445\n",
      "  âœ… New best AP: 0.4445 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 48  loss 0.0002  val AP 0.4452\n",
      "  âœ… New best AP: 0.4452 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 49  loss 0.0002  val AP 0.4457\n",
      "  âœ… New best AP: 0.4457 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 50  loss 0.0002  val AP 0.4462\n",
      "  âœ… New best AP: 0.4462 â†’ v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 47] ep 01  loss 0.0004  val AP 0.2174\n",
      "  âœ… New best AP: 0.2174 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 02  loss 0.0003  val AP 0.2276\n",
      "  âœ… New best AP: 0.2276 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 03  loss 0.0004  val AP 0.2362\n",
      "  âœ… New best AP: 0.2362 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 04  loss 0.0004  val AP 0.2469\n",
      "  âœ… New best AP: 0.2469 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 05  loss 0.0003  val AP 0.2631\n",
      "  âœ… New best AP: 0.2631 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 06  loss 0.0003  val AP 0.2799\n",
      "  âœ… New best AP: 0.2799 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 07  loss 0.0003  val AP 0.2909\n",
      "  âœ… New best AP: 0.2909 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 08  loss 0.0003  val AP 0.2941\n",
      "  âœ… New best AP: 0.2941 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 09  loss 0.0003  val AP 0.3013\n",
      "  âœ… New best AP: 0.3013 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 10  loss 0.0003  val AP 0.3061\n",
      "  âœ… New best AP: 0.3061 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 11  loss 0.0002  val AP 0.3118\n",
      "  âœ… New best AP: 0.3118 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 12  loss 0.0003  val AP 0.3154\n",
      "  âœ… New best AP: 0.3154 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 13  loss 0.0003  val AP 0.3190\n",
      "  âœ… New best AP: 0.3190 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 14  loss 0.0003  val AP 0.3249\n",
      "  âœ… New best AP: 0.3249 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 15  loss 0.0002  val AP 0.3289\n",
      "  âœ… New best AP: 0.3289 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 16  loss 0.0002  val AP 0.3323\n",
      "  âœ… New best AP: 0.3323 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 17  loss 0.0003  val AP 0.3346\n",
      "  âœ… New best AP: 0.3346 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 18  loss 0.0002  val AP 0.3377\n",
      "  âœ… New best AP: 0.3377 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 19  loss 0.0002  val AP 0.3396\n",
      "  âœ… New best AP: 0.3396 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 20  loss 0.0002  val AP 0.3437\n",
      "  âœ… New best AP: 0.3437 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 21  loss 0.0002  val AP 0.3443\n",
      "  âœ… New best AP: 0.3443 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 22  loss 0.0002  val AP 0.3494\n",
      "  âœ… New best AP: 0.3494 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 23  loss 0.0002  val AP 0.3506\n",
      "  âœ… New best AP: 0.3506 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 24  loss 0.0002  val AP 0.3518\n",
      "  âœ… New best AP: 0.3518 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 25  loss 0.0002  val AP 0.3531\n",
      "  âœ… New best AP: 0.3531 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 26  loss 0.0002  val AP 0.3560\n",
      "  âœ… New best AP: 0.3560 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 27  loss 0.0002  val AP 0.3567\n",
      "  âœ… New best AP: 0.3567 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 28  loss 0.0002  val AP 0.3582\n",
      "  âœ… New best AP: 0.3582 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 29  loss 0.0002  val AP 0.3596\n",
      "  âœ… New best AP: 0.3596 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 30  loss 0.0002  val AP 0.3617\n",
      "  âœ… New best AP: 0.3617 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 31  loss 0.0002  val AP 0.3629\n",
      "  âœ… New best AP: 0.3629 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 32  loss 0.0002  val AP 0.3632\n",
      "  âœ… New best AP: 0.3632 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 33  loss 0.0002  val AP 0.3642\n",
      "  âœ… New best AP: 0.3642 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 34  loss 0.0002  val AP 0.3645\n",
      "  âœ… New best AP: 0.3645 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 35  loss 0.0002  val AP 0.3653\n",
      "  âœ… New best AP: 0.3653 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 36  loss 0.0002  val AP 0.3661\n",
      "  âœ… New best AP: 0.3661 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 37  loss 0.0002  val AP 0.3664\n",
      "  âœ… New best AP: 0.3664 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 38  loss 0.0002  val AP 0.3672\n",
      "  âœ… New best AP: 0.3672 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 39  loss 0.0002  val AP 0.3685\n",
      "  âœ… New best AP: 0.3685 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 40  loss 0.0002  val AP 0.3692\n",
      "  âœ… New best AP: 0.3692 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 41  loss 0.0002  val AP 0.3698\n",
      "  âœ… New best AP: 0.3698 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 42  loss 0.0002  val AP 0.3704\n",
      "  âœ… New best AP: 0.3704 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 43  loss 0.0002  val AP 0.3706\n",
      "  âœ… New best AP: 0.3706 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 44  loss 0.0002  val AP 0.3713\n",
      "  âœ… New best AP: 0.3713 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 45  loss 0.0002  val AP 0.3721\n",
      "  âœ… New best AP: 0.3721 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 46  loss 0.0002  val AP 0.3723\n",
      "  âœ… New best AP: 0.3723 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 47  loss 0.0002  val AP 0.3726\n",
      "  âœ… New best AP: 0.3726 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 48  loss 0.0002  val AP 0.3732\n",
      "  âœ… New best AP: 0.3732 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 49  loss 0.0002  val AP 0.3737\n",
      "  âœ… New best AP: 0.3737 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 50  loss 0.0002  val AP 0.3740\n",
      "  âœ… New best AP: 0.3740 â†’ v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 61] ep 01  loss 0.0006  val AP 0.1793\n",
      "  âœ… New best AP: 0.1793 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 02  loss 0.0003  val AP 0.1921\n",
      "  âœ… New best AP: 0.1921 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 03  loss 0.0004  val AP 0.2100\n",
      "  âœ… New best AP: 0.2100 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 04  loss 0.0004  val AP 0.2293\n",
      "  âœ… New best AP: 0.2293 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 05  loss 0.0003  val AP 0.2488\n",
      "  âœ… New best AP: 0.2488 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 06  loss 0.0003  val AP 0.2724\n",
      "  âœ… New best AP: 0.2724 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 07  loss 0.0003  val AP 0.2906\n",
      "  âœ… New best AP: 0.2906 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 08  loss 0.0003  val AP 0.3073\n",
      "  âœ… New best AP: 0.3073 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 09  loss 0.0003  val AP 0.3330\n",
      "  âœ… New best AP: 0.3330 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 10  loss 0.0003  val AP 0.3384\n",
      "  âœ… New best AP: 0.3384 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 11  loss 0.0003  val AP 0.3480\n",
      "  âœ… New best AP: 0.3480 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 12  loss 0.0002  val AP 0.3567\n",
      "  âœ… New best AP: 0.3567 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 13  loss 0.0003  val AP 0.3629\n",
      "  âœ… New best AP: 0.3629 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 14  loss 0.0002  val AP 0.3674\n",
      "  âœ… New best AP: 0.3674 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 15  loss 0.0002  val AP 0.3726\n",
      "  âœ… New best AP: 0.3726 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 16  loss 0.0002  val AP 0.3784\n",
      "  âœ… New best AP: 0.3784 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 17  loss 0.0002  val AP 0.3804\n",
      "  âœ… New best AP: 0.3804 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 18  loss 0.0003  val AP 0.3825\n",
      "  âœ… New best AP: 0.3825 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 19  loss 0.0002  val AP 0.3854\n",
      "  âœ… New best AP: 0.3854 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 20  loss 0.0002  val AP 0.3869\n",
      "  âœ… New best AP: 0.3869 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 21  loss 0.0002  val AP 0.3902\n",
      "  âœ… New best AP: 0.3902 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 22  loss 0.0002  val AP 0.3925\n",
      "  âœ… New best AP: 0.3925 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 23  loss 0.0002  val AP 0.3960\n",
      "  âœ… New best AP: 0.3960 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 24  loss 0.0002  val AP 0.3985\n",
      "  âœ… New best AP: 0.3985 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 25  loss 0.0002  val AP 0.4009\n",
      "  âœ… New best AP: 0.4009 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 26  loss 0.0002  val AP 0.4026\n",
      "  âœ… New best AP: 0.4026 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 27  loss 0.0002  val AP 0.4049\n",
      "  âœ… New best AP: 0.4049 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 28  loss 0.0002  val AP 0.4061\n",
      "  âœ… New best AP: 0.4061 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 29  loss 0.0002  val AP 0.4073\n",
      "  âœ… New best AP: 0.4073 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 30  loss 0.0002  val AP 0.4104\n",
      "  âœ… New best AP: 0.4104 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 31  loss 0.0002  val AP 0.4109\n",
      "  âœ… New best AP: 0.4109 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 32  loss 0.0002  val AP 0.4124\n",
      "  âœ… New best AP: 0.4124 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 33  loss 0.0002  val AP 0.4144\n",
      "  âœ… New best AP: 0.4144 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 34  loss 0.0002  val AP 0.4157\n",
      "  âœ… New best AP: 0.4157 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 35  loss 0.0002  val AP 0.4164\n",
      "  âœ… New best AP: 0.4164 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 36  loss 0.0002  val AP 0.4161\n",
      "[SR-MMP | seed 61] ep 37  loss 0.0002  val AP 0.4169\n",
      "  âœ… New best AP: 0.4169 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 38  loss 0.0002  val AP 0.4193\n",
      "  âœ… New best AP: 0.4193 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 39  loss 0.0002  val AP 0.4178\n",
      "[SR-MMP | seed 61] ep 40  loss 0.0002  val AP 0.4184\n",
      "[SR-MMP | seed 61] ep 41  loss 0.0002  val AP 0.4195\n",
      "  âœ… New best AP: 0.4195 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 42  loss 0.0002  val AP 0.4174\n",
      "[SR-MMP | seed 61] ep 43  loss 0.0002  val AP 0.4182\n",
      "[SR-MMP | seed 61] ep 44  loss 0.0002  val AP 0.4190\n",
      "[SR-MMP | seed 61] ep 45  loss 0.0002  val AP 0.4198\n",
      "  âœ… New best AP: 0.4198 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 46  loss 0.0002  val AP 0.4206\n",
      "  âœ… New best AP: 0.4206 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 47  loss 0.0002  val AP 0.4217\n",
      "  âœ… New best AP: 0.4217 â†’ v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 48  loss 0.0002  val AP 0.4211\n",
      "[SR-MMP | seed 61] ep 49  loss 0.0002  val AP 0.4210\n",
      "[SR-MMP | seed 61] ep 50  loss 0.0002  val AP 0.4212\n",
      "[SR-MMP | seed 83] ep 01  loss 0.0004  val AP 0.1483\n",
      "  âœ… New best AP: 0.1483 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 02  loss 0.0003  val AP 0.1554\n",
      "  âœ… New best AP: 0.1554 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 03  loss 0.0003  val AP 0.1683\n",
      "  âœ… New best AP: 0.1683 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 04  loss 0.0004  val AP 0.1846\n",
      "  âœ… New best AP: 0.1846 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 05  loss 0.0003  val AP 0.2074\n",
      "  âœ… New best AP: 0.2074 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 06  loss 0.0003  val AP 0.2324\n",
      "  âœ… New best AP: 0.2324 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 07  loss 0.0003  val AP 0.2515\n",
      "  âœ… New best AP: 0.2515 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 08  loss 0.0003  val AP 0.2768\n",
      "  âœ… New best AP: 0.2768 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 09  loss 0.0003  val AP 0.2942\n",
      "  âœ… New best AP: 0.2942 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 10  loss 0.0002  val AP 0.3047\n",
      "  âœ… New best AP: 0.3047 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 11  loss 0.0002  val AP 0.3149\n",
      "  âœ… New best AP: 0.3149 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 12  loss 0.0003  val AP 0.3270\n",
      "  âœ… New best AP: 0.3270 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 13  loss 0.0003  val AP 0.3326\n",
      "  âœ… New best AP: 0.3326 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 14  loss 0.0003  val AP 0.3398\n",
      "  âœ… New best AP: 0.3398 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 15  loss 0.0002  val AP 0.3475\n",
      "  âœ… New best AP: 0.3475 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 16  loss 0.0002  val AP 0.3527\n",
      "  âœ… New best AP: 0.3527 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 17  loss 0.0003  val AP 0.3570\n",
      "  âœ… New best AP: 0.3570 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 18  loss 0.0002  val AP 0.3660\n",
      "  âœ… New best AP: 0.3660 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 19  loss 0.0002  val AP 0.3747\n",
      "  âœ… New best AP: 0.3747 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 20  loss 0.0002  val AP 0.3731\n",
      "[SR-MMP | seed 83] ep 21  loss 0.0002  val AP 0.3733\n",
      "[SR-MMP | seed 83] ep 22  loss 0.0003  val AP 0.3787\n",
      "  âœ… New best AP: 0.3787 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 23  loss 0.0002  val AP 0.3803\n",
      "  âœ… New best AP: 0.3803 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 24  loss 0.0002  val AP 0.3838\n",
      "  âœ… New best AP: 0.3838 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 25  loss 0.0002  val AP 0.3883\n",
      "  âœ… New best AP: 0.3883 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 26  loss 0.0002  val AP 0.3890\n",
      "  âœ… New best AP: 0.3890 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 27  loss 0.0002  val AP 0.3920\n",
      "  âœ… New best AP: 0.3920 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 28  loss 0.0002  val AP 0.3916\n",
      "[SR-MMP | seed 83] ep 29  loss 0.0002  val AP 0.3916\n",
      "[SR-MMP | seed 83] ep 30  loss 0.0002  val AP 0.3928\n",
      "  âœ… New best AP: 0.3928 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 31  loss 0.0002  val AP 0.3935\n",
      "  âœ… New best AP: 0.3935 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 32  loss 0.0002  val AP 0.3936\n",
      "  âœ… New best AP: 0.3936 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 33  loss 0.0002  val AP 0.3936\n",
      "  âœ… New best AP: 0.3936 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 34  loss 0.0002  val AP 0.3956\n",
      "  âœ… New best AP: 0.3956 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 35  loss 0.0002  val AP 0.3983\n",
      "  âœ… New best AP: 0.3983 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 36  loss 0.0002  val AP 0.3986\n",
      "  âœ… New best AP: 0.3986 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 37  loss 0.0002  val AP 0.3992\n",
      "  âœ… New best AP: 0.3992 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 38  loss 0.0002  val AP 0.4016\n",
      "  âœ… New best AP: 0.4016 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 39  loss 0.0002  val AP 0.4022\n",
      "  âœ… New best AP: 0.4022 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 40  loss 0.0002  val AP 0.4032\n",
      "  âœ… New best AP: 0.4032 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 41  loss 0.0002  val AP 0.4049\n",
      "  âœ… New best AP: 0.4049 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 42  loss 0.0002  val AP 0.4075\n",
      "  âœ… New best AP: 0.4075 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 43  loss 0.0002  val AP 0.4077\n",
      "  âœ… New best AP: 0.4077 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 44  loss 0.0002  val AP 0.4079\n",
      "  âœ… New best AP: 0.4079 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 45  loss 0.0002  val AP 0.4081\n",
      "  âœ… New best AP: 0.4081 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 46  loss 0.0002  val AP 0.4082\n",
      "  âœ… New best AP: 0.4082 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 47  loss 0.0002  val AP 0.4083\n",
      "  âœ… New best AP: 0.4083 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 48  loss 0.0002  val AP 0.4093\n",
      "  âœ… New best AP: 0.4093 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 49  loss 0.0002  val AP 0.4096\n",
      "  âœ… New best AP: 0.4096 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 50  loss 0.0002  val AP 0.4103\n",
      "  âœ… New best AP: 0.4103 â†’ v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-p53 (label 11)\n",
      "==============================\n",
      "[SR-p53 | seed 13] ep 01  loss 0.0007  val AP 0.0549\n",
      "  âœ… New best AP: 0.0549 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 02  loss 0.0005  val AP 0.0562\n",
      "  âœ… New best AP: 0.0562 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 03  loss 0.0005  val AP 0.0586\n",
      "  âœ… New best AP: 0.0586 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 04  loss 0.0005  val AP 0.0628\n",
      "  âœ… New best AP: 0.0628 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 05  loss 0.0006  val AP 0.0703\n",
      "  âœ… New best AP: 0.0703 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 06  loss 0.0005  val AP 0.0833\n",
      "  âœ… New best AP: 0.0833 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 07  loss 0.0006  val AP 0.1088\n",
      "  âœ… New best AP: 0.1088 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 08  loss 0.0005  val AP 0.1189\n",
      "  âœ… New best AP: 0.1189 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 09  loss 0.0007  val AP 0.1316\n",
      "  âœ… New best AP: 0.1316 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 10  loss 0.0005  val AP 0.1348\n",
      "  âœ… New best AP: 0.1348 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 11  loss 0.0005  val AP 0.1495\n",
      "  âœ… New best AP: 0.1495 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 12  loss 0.0004  val AP 0.1540\n",
      "  âœ… New best AP: 0.1540 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 13  loss 0.0005  val AP 0.1629\n",
      "  âœ… New best AP: 0.1629 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 14  loss 0.0004  val AP 0.1873\n",
      "  âœ… New best AP: 0.1873 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 15  loss 0.0008  val AP 0.1936\n",
      "  âœ… New best AP: 0.1936 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 16  loss 0.0005  val AP 0.1974\n",
      "  âœ… New best AP: 0.1974 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 17  loss 0.0005  val AP 0.2032\n",
      "  âœ… New best AP: 0.2032 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 18  loss 0.0004  val AP 0.2091\n",
      "  âœ… New best AP: 0.2091 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 19  loss 0.0004  val AP 0.2098\n",
      "  âœ… New best AP: 0.2098 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 20  loss 0.0004  val AP 0.2093\n",
      "[SR-p53 | seed 13] ep 21  loss 0.0005  val AP 0.2107\n",
      "  âœ… New best AP: 0.2107 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 22  loss 0.0004  val AP 0.2117\n",
      "  âœ… New best AP: 0.2117 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 23  loss 0.0004  val AP 0.2102\n",
      "[SR-p53 | seed 13] ep 24  loss 0.0005  val AP 0.2108\n",
      "[SR-p53 | seed 13] ep 25  loss 0.0004  val AP 0.2111\n",
      "[SR-p53 | seed 13] ep 26  loss 0.0004  val AP 0.2122\n",
      "  âœ… New best AP: 0.2122 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 27  loss 0.0004  val AP 0.2129\n",
      "  âœ… New best AP: 0.2129 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 28  loss 0.0005  val AP 0.2083\n",
      "[SR-p53 | seed 13] ep 29  loss 0.0004  val AP 0.2108\n",
      "[SR-p53 | seed 13] ep 30  loss 0.0004  val AP 0.2110\n",
      "[SR-p53 | seed 13] ep 31  loss 0.0004  val AP 0.2110\n",
      "[SR-p53 | seed 13] ep 32  loss 0.0004  val AP 0.2114\n",
      "[SR-p53 | seed 13] ep 33  loss 0.0004  val AP 0.2122\n",
      "[SR-p53 | seed 13] ep 34  loss 0.0004  val AP 0.2122\n",
      "[SR-p53 | seed 13] ep 35  loss 0.0004  val AP 0.2131\n",
      "  âœ… New best AP: 0.2131 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 36  loss 0.0004  val AP 0.2147\n",
      "  âœ… New best AP: 0.2147 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 37  loss 0.0004  val AP 0.2151\n",
      "  âœ… New best AP: 0.2151 â†’ v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 38  loss 0.0004  val AP 0.2138\n",
      "[SR-p53 | seed 13] ep 39  loss 0.0004  val AP 0.2138\n",
      "[SR-p53 | seed 13] ep 40  loss 0.0004  val AP 0.2132\n",
      "[SR-p53 | seed 13] ep 41  loss 0.0004  val AP 0.2127\n",
      "[SR-p53 | seed 13] ep 42  loss 0.0004  val AP 0.2128\n",
      "[SR-p53 | seed 13] ep 43  loss 0.0004  val AP 0.2130\n",
      "[SR-p53 | seed 13] ep 44  loss 0.0004  val AP 0.2129\n",
      "[SR-p53 | seed 13] ep 45  loss 0.0004  val AP 0.2127\n",
      "[SR-p53 | seed 13] ep 46  loss 0.0004  val AP 0.2138\n",
      "[SR-p53 | seed 13] ep 47  loss 0.0004  val AP 0.2134\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.2151\n",
      "[SR-p53 | seed 29] ep 01  loss 0.0008  val AP 0.1221\n",
      "  âœ… New best AP: 0.1221 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 02  loss 0.0006  val AP 0.1295\n",
      "  âœ… New best AP: 0.1295 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 03  loss 0.0007  val AP 0.1409\n",
      "  âœ… New best AP: 0.1409 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 04  loss 0.0007  val AP 0.1517\n",
      "  âœ… New best AP: 0.1517 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 05  loss 0.0006  val AP 0.1620\n",
      "  âœ… New best AP: 0.1620 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 06  loss 0.0005  val AP 0.1751\n",
      "  âœ… New best AP: 0.1751 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 07  loss 0.0005  val AP 0.1775\n",
      "  âœ… New best AP: 0.1775 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 08  loss 0.0006  val AP 0.1770\n",
      "[SR-p53 | seed 29] ep 09  loss 0.0005  val AP 0.1796\n",
      "  âœ… New best AP: 0.1796 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 10  loss 0.0006  val AP 0.1823\n",
      "  âœ… New best AP: 0.1823 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 11  loss 0.0005  val AP 0.1877\n",
      "  âœ… New best AP: 0.1877 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 12  loss 0.0005  val AP 0.1970\n",
      "  âœ… New best AP: 0.1970 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 13  loss 0.0005  val AP 0.1984\n",
      "  âœ… New best AP: 0.1984 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 14  loss 0.0005  val AP 0.2020\n",
      "  âœ… New best AP: 0.2020 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 15  loss 0.0005  val AP 0.2079\n",
      "  âœ… New best AP: 0.2079 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 16  loss 0.0004  val AP 0.2094\n",
      "  âœ… New best AP: 0.2094 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 17  loss 0.0005  val AP 0.2193\n",
      "  âœ… New best AP: 0.2193 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 18  loss 0.0005  val AP 0.2230\n",
      "  âœ… New best AP: 0.2230 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 19  loss 0.0005  val AP 0.2284\n",
      "  âœ… New best AP: 0.2284 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 20  loss 0.0004  val AP 0.2315\n",
      "  âœ… New best AP: 0.2315 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 21  loss 0.0005  val AP 0.2234\n",
      "[SR-p53 | seed 29] ep 22  loss 0.0005  val AP 0.2241\n",
      "[SR-p53 | seed 29] ep 23  loss 0.0004  val AP 0.2266\n",
      "[SR-p53 | seed 29] ep 24  loss 0.0004  val AP 0.2311\n",
      "[SR-p53 | seed 29] ep 25  loss 0.0005  val AP 0.2334\n",
      "  âœ… New best AP: 0.2334 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 26  loss 0.0004  val AP 0.2346\n",
      "  âœ… New best AP: 0.2346 â†’ v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 27  loss 0.0005  val AP 0.2308\n",
      "[SR-p53 | seed 29] ep 28  loss 0.0004  val AP 0.2303\n",
      "[SR-p53 | seed 29] ep 29  loss 0.0004  val AP 0.2291\n",
      "[SR-p53 | seed 29] ep 30  loss 0.0004  val AP 0.2302\n",
      "[SR-p53 | seed 29] ep 31  loss 0.0004  val AP 0.2261\n",
      "[SR-p53 | seed 29] ep 32  loss 0.0004  val AP 0.2268\n",
      "[SR-p53 | seed 29] ep 33  loss 0.0004  val AP 0.2270\n",
      "[SR-p53 | seed 29] ep 34  loss 0.0004  val AP 0.2267\n",
      "[SR-p53 | seed 29] ep 35  loss 0.0005  val AP 0.2269\n",
      "[SR-p53 | seed 29] ep 36  loss 0.0004  val AP 0.2231\n",
      "  â¹ Early stop (no improve for 10 epochs). Best AP: 0.2346\n",
      "[SR-p53 | seed 47] ep 01  loss 0.0008  val AP 0.0996\n",
      "  âœ… New best AP: 0.0996 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 02  loss 0.0005  val AP 0.1046\n",
      "  âœ… New best AP: 0.1046 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 03  loss 0.0007  val AP 0.1093\n",
      "  âœ… New best AP: 0.1093 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 04  loss 0.0007  val AP 0.1155\n",
      "  âœ… New best AP: 0.1155 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 05  loss 0.0006  val AP 0.1225\n",
      "  âœ… New best AP: 0.1225 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 06  loss 0.0005  val AP 0.1251\n",
      "  âœ… New best AP: 0.1251 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 07  loss 0.0006  val AP 0.1280\n",
      "  âœ… New best AP: 0.1280 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 08  loss 0.0006  val AP 0.1304\n",
      "  âœ… New best AP: 0.1304 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 09  loss 0.0006  val AP 0.1315\n",
      "  âœ… New best AP: 0.1315 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 10  loss 0.0005  val AP 0.1351\n",
      "  âœ… New best AP: 0.1351 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 11  loss 0.0005  val AP 0.1348\n",
      "[SR-p53 | seed 47] ep 12  loss 0.0004  val AP 0.1356\n",
      "  âœ… New best AP: 0.1356 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 13  loss 0.0006  val AP 0.1377\n",
      "  âœ… New best AP: 0.1377 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 14  loss 0.0005  val AP 0.1378\n",
      "  âœ… New best AP: 0.1378 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 15  loss 0.0005  val AP 0.1409\n",
      "  âœ… New best AP: 0.1409 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 16  loss 0.0004  val AP 0.1411\n",
      "  âœ… New best AP: 0.1411 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 17  loss 0.0004  val AP 0.1408\n",
      "[SR-p53 | seed 47] ep 18  loss 0.0004  val AP 0.1395\n",
      "[SR-p53 | seed 47] ep 19  loss 0.0006  val AP 0.1436\n",
      "  âœ… New best AP: 0.1436 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 20  loss 0.0004  val AP 0.1445\n",
      "  âœ… New best AP: 0.1445 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 21  loss 0.0004  val AP 0.1452\n",
      "  âœ… New best AP: 0.1452 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 22  loss 0.0004  val AP 0.1462\n",
      "  âœ… New best AP: 0.1462 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 23  loss 0.0004  val AP 0.1465\n",
      "  âœ… New best AP: 0.1465 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 24  loss 0.0005  val AP 0.1473\n",
      "  âœ… New best AP: 0.1473 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 25  loss 0.0004  val AP 0.1465\n",
      "[SR-p53 | seed 47] ep 26  loss 0.0004  val AP 0.1461\n",
      "[SR-p53 | seed 47] ep 27  loss 0.0004  val AP 0.1468\n",
      "[SR-p53 | seed 47] ep 28  loss 0.0004  val AP 0.1471\n",
      "[SR-p53 | seed 47] ep 29  loss 0.0004  val AP 0.1475\n",
      "  âœ… New best AP: 0.1475 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 30  loss 0.0004  val AP 0.1480\n",
      "  âœ… New best AP: 0.1480 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 31  loss 0.0004  val AP 0.1488\n",
      "  âœ… New best AP: 0.1488 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 32  loss 0.0004  val AP 0.1487\n",
      "[SR-p53 | seed 47] ep 33  loss 0.0004  val AP 0.1489\n",
      "  âœ… New best AP: 0.1489 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 34  loss 0.0004  val AP 0.1484\n",
      "[SR-p53 | seed 47] ep 35  loss 0.0004  val AP 0.1487\n",
      "[SR-p53 | seed 47] ep 36  loss 0.0004  val AP 0.1491\n",
      "  âœ… New best AP: 0.1491 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 37  loss 0.0004  val AP 0.1495\n",
      "  âœ… New best AP: 0.1495 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 38  loss 0.0004  val AP 0.1498\n",
      "  âœ… New best AP: 0.1498 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 39  loss 0.0004  val AP 0.1499\n",
      "  âœ… New best AP: 0.1499 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 40  loss 0.0004  val AP 0.1500\n",
      "  âœ… New best AP: 0.1500 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 41  loss 0.0004  val AP 0.1502\n",
      "  âœ… New best AP: 0.1502 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 42  loss 0.0004  val AP 0.1503\n",
      "  âœ… New best AP: 0.1503 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 43  loss 0.0004  val AP 0.1505\n",
      "  âœ… New best AP: 0.1505 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 44  loss 0.0004  val AP 0.1511\n",
      "  âœ… New best AP: 0.1511 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 45  loss 0.0004  val AP 0.1509\n",
      "[SR-p53 | seed 47] ep 46  loss 0.0004  val AP 0.1510\n",
      "[SR-p53 | seed 47] ep 47  loss 0.0005  val AP 0.1511\n",
      "  âœ… New best AP: 0.1511 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 48  loss 0.0004  val AP 0.1515\n",
      "  âœ… New best AP: 0.1515 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 49  loss 0.0005  val AP 0.1516\n",
      "  âœ… New best AP: 0.1516 â†’ v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 50  loss 0.0004  val AP 0.1515\n",
      "[SR-p53 | seed 61] ep 01  loss 0.0010  val AP 0.0629\n",
      "  âœ… New best AP: 0.0629 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 02  loss 0.0006  val AP 0.0678\n",
      "  âœ… New best AP: 0.0678 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 03  loss 0.0007  val AP 0.0759\n",
      "  âœ… New best AP: 0.0759 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 04  loss 0.0007  val AP 0.0856\n",
      "  âœ… New best AP: 0.0856 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 05  loss 0.0006  val AP 0.0974\n",
      "  âœ… New best AP: 0.0974 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 06  loss 0.0006  val AP 0.1156\n",
      "  âœ… New best AP: 0.1156 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 07  loss 0.0006  val AP 0.1216\n",
      "  âœ… New best AP: 0.1216 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 08  loss 0.0007  val AP 0.1294\n",
      "  âœ… New best AP: 0.1294 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 09  loss 0.0006  val AP 0.1427\n",
      "  âœ… New best AP: 0.1427 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 10  loss 0.0006  val AP 0.1491\n",
      "  âœ… New best AP: 0.1491 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 11  loss 0.0005  val AP 0.1488\n",
      "[SR-p53 | seed 61] ep 12  loss 0.0005  val AP 0.1520\n",
      "  âœ… New best AP: 0.1520 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 13  loss 0.0005  val AP 0.1548\n",
      "  âœ… New best AP: 0.1548 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 14  loss 0.0005  val AP 0.1580\n",
      "  âœ… New best AP: 0.1580 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 15  loss 0.0005  val AP 0.1600\n",
      "  âœ… New best AP: 0.1600 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 16  loss 0.0005  val AP 0.1613\n",
      "  âœ… New best AP: 0.1613 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 17  loss 0.0005  val AP 0.1644\n",
      "  âœ… New best AP: 0.1644 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 18  loss 0.0005  val AP 0.1671\n",
      "  âœ… New best AP: 0.1671 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 19  loss 0.0005  val AP 0.1670\n",
      "[SR-p53 | seed 61] ep 20  loss 0.0005  val AP 0.1673\n",
      "  âœ… New best AP: 0.1673 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 21  loss 0.0005  val AP 0.1689\n",
      "  âœ… New best AP: 0.1689 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 22  loss 0.0004  val AP 0.1709\n",
      "  âœ… New best AP: 0.1709 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 23  loss 0.0004  val AP 0.1713\n",
      "  âœ… New best AP: 0.1713 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 24  loss 0.0004  val AP 0.1720\n",
      "  âœ… New best AP: 0.1720 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 25  loss 0.0004  val AP 0.1737\n",
      "  âœ… New best AP: 0.1737 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 26  loss 0.0004  val AP 0.1752\n",
      "  âœ… New best AP: 0.1752 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 27  loss 0.0004  val AP 0.1762\n",
      "  âœ… New best AP: 0.1762 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 28  loss 0.0004  val AP 0.1764\n",
      "  âœ… New best AP: 0.1764 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 29  loss 0.0005  val AP 0.1758\n",
      "[SR-p53 | seed 61] ep 30  loss 0.0004  val AP 0.1764\n",
      "[SR-p53 | seed 61] ep 31  loss 0.0004  val AP 0.1778\n",
      "  âœ… New best AP: 0.1778 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 32  loss 0.0004  val AP 0.1798\n",
      "  âœ… New best AP: 0.1798 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 33  loss 0.0004  val AP 0.1796\n",
      "[SR-p53 | seed 61] ep 34  loss 0.0004  val AP 0.1792\n",
      "[SR-p53 | seed 61] ep 35  loss 0.0004  val AP 0.1796\n",
      "[SR-p53 | seed 61] ep 36  loss 0.0004  val AP 0.1794\n",
      "[SR-p53 | seed 61] ep 37  loss 0.0004  val AP 0.1795\n",
      "[SR-p53 | seed 61] ep 38  loss 0.0005  val AP 0.1792\n",
      "[SR-p53 | seed 61] ep 39  loss 0.0004  val AP 0.1795\n",
      "[SR-p53 | seed 61] ep 40  loss 0.0004  val AP 0.1800\n",
      "  âœ… New best AP: 0.1800 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 41  loss 0.0004  val AP 0.1809\n",
      "  âœ… New best AP: 0.1809 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 42  loss 0.0004  val AP 0.1810\n",
      "  âœ… New best AP: 0.1810 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 43  loss 0.0004  val AP 0.1807\n",
      "[SR-p53 | seed 61] ep 44  loss 0.0004  val AP 0.1826\n",
      "  âœ… New best AP: 0.1826 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 45  loss 0.0004  val AP 0.1831\n",
      "  âœ… New best AP: 0.1831 â†’ v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 46  loss 0.0004  val AP 0.1829\n",
      "[SR-p53 | seed 61] ep 47  loss 0.0004  val AP 0.1831\n",
      "[SR-p53 | seed 61] ep 48  loss 0.0005  val AP 0.1829\n",
      "[SR-p53 | seed 61] ep 49  loss 0.0004  val AP 0.1831\n",
      "[SR-p53 | seed 61] ep 50  loss 0.0004  val AP 0.1831\n",
      "[SR-p53 | seed 83] ep 01  loss 0.0008  val AP 0.0693\n",
      "  âœ… New best AP: 0.0693 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 02  loss 0.0005  val AP 0.0716\n",
      "  âœ… New best AP: 0.0716 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 03  loss 0.0006  val AP 0.0749\n",
      "  âœ… New best AP: 0.0749 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 04  loss 0.0006  val AP 0.0811\n",
      "  âœ… New best AP: 0.0811 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 05  loss 0.0006  val AP 0.0904\n",
      "  âœ… New best AP: 0.0904 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 06  loss 0.0005  val AP 0.1019\n",
      "  âœ… New best AP: 0.1019 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 07  loss 0.0006  val AP 0.1121\n",
      "  âœ… New best AP: 0.1121 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 08  loss 0.0005  val AP 0.1216\n",
      "  âœ… New best AP: 0.1216 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 09  loss 0.0006  val AP 0.1272\n",
      "  âœ… New best AP: 0.1272 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 10  loss 0.0005  val AP 0.1350\n",
      "  âœ… New best AP: 0.1350 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 11  loss 0.0005  val AP 0.1441\n",
      "  âœ… New best AP: 0.1441 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 12  loss 0.0004  val AP 0.1509\n",
      "  âœ… New best AP: 0.1509 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 13  loss 0.0005  val AP 0.1540\n",
      "  âœ… New best AP: 0.1540 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 14  loss 0.0004  val AP 0.1597\n",
      "  âœ… New best AP: 0.1597 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 15  loss 0.0005  val AP 0.1733\n",
      "  âœ… New best AP: 0.1733 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 16  loss 0.0004  val AP 0.1799\n",
      "  âœ… New best AP: 0.1799 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 17  loss 0.0005  val AP 0.1828\n",
      "  âœ… New best AP: 0.1828 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 18  loss 0.0004  val AP 0.1820\n",
      "[SR-p53 | seed 83] ep 19  loss 0.0005  val AP 0.1834\n",
      "  âœ… New best AP: 0.1834 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 20  loss 0.0005  val AP 0.1842\n",
      "  âœ… New best AP: 0.1842 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 21  loss 0.0005  val AP 0.1842\n",
      "  âœ… New best AP: 0.1842 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 22  loss 0.0004  val AP 0.1867\n",
      "  âœ… New best AP: 0.1867 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 23  loss 0.0004  val AP 0.1875\n",
      "  âœ… New best AP: 0.1875 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 24  loss 0.0004  val AP 0.1886\n",
      "  âœ… New best AP: 0.1886 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 25  loss 0.0005  val AP 0.1892\n",
      "  âœ… New best AP: 0.1892 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 26  loss 0.0004  val AP 0.1869\n",
      "[SR-p53 | seed 83] ep 27  loss 0.0004  val AP 0.1877\n",
      "[SR-p53 | seed 83] ep 28  loss 0.0004  val AP 0.1882\n",
      "[SR-p53 | seed 83] ep 29  loss 0.0004  val AP 0.1894\n",
      "  âœ… New best AP: 0.1894 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 30  loss 0.0004  val AP 0.1874\n",
      "[SR-p53 | seed 83] ep 31  loss 0.0004  val AP 0.1873\n",
      "[SR-p53 | seed 83] ep 32  loss 0.0004  val AP 0.1873\n",
      "[SR-p53 | seed 83] ep 33  loss 0.0004  val AP 0.1881\n",
      "[SR-p53 | seed 83] ep 34  loss 0.0004  val AP 0.1890\n",
      "[SR-p53 | seed 83] ep 35  loss 0.0004  val AP 0.1895\n",
      "  âœ… New best AP: 0.1895 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 36  loss 0.0004  val AP 0.1903\n",
      "  âœ… New best AP: 0.1903 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 37  loss 0.0004  val AP 0.1904\n",
      "  âœ… New best AP: 0.1904 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 38  loss 0.0004  val AP 0.1905\n",
      "  âœ… New best AP: 0.1905 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 39  loss 0.0004  val AP 0.1901\n",
      "[SR-p53 | seed 83] ep 40  loss 0.0004  val AP 0.1906\n",
      "  âœ… New best AP: 0.1906 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 41  loss 0.0004  val AP 0.1911\n",
      "  âœ… New best AP: 0.1911 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 42  loss 0.0004  val AP 0.1913\n",
      "  âœ… New best AP: 0.1913 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 43  loss 0.0004  val AP 0.1916\n",
      "  âœ… New best AP: 0.1916 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 44  loss 0.0004  val AP 0.1916\n",
      "  âœ… New best AP: 0.1916 â†’ v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 45  loss 0.0004  val AP 0.1883\n",
      "[SR-p53 | seed 83] ep 46  loss 0.0004  val AP 0.1884\n",
      "[SR-p53 | seed 83] ep 47  loss 0.0004  val AP 0.1885\n",
      "[SR-p53 | seed 83] ep 48  loss 0.0004  val AP 0.1889\n",
      "[SR-p53 | seed 83] ep 49  loss 0.0004  val AP 0.1887\n",
      "[SR-p53 | seed 83] ep 50  loss 0.0005  val AP 0.1889\n",
      "\n",
      "âœ… Ensemble training complete.\n",
      "{\n",
      "  \"best_per_label\": {\n",
      "    \"NR-AR\": {\n",
      "      \"best_ap\": 0.17695605929185818\n",
      "    },\n",
      "    \"NR-AR-LBD\": {\n",
      "      \"best_ap\": 0.30067178313078136\n",
      "    },\n",
      "    \"NR-AhR\": {\n",
      "      \"best_ap\": 0.5251213928312277\n",
      "    },\n",
      "    \"NR-Aromatase\": {\n",
      "      \"best_ap\": 0.27536501073172526\n",
      "    },\n",
      "    \"NR-ER\": {\n",
      "      \"best_ap\": 0.23092139236215176\n",
      "    },\n",
      "    \"NR-ER-LBD\": {\n",
      "      \"best_ap\": 0.15266075493878553\n",
      "    },\n",
      "    \"NR-PPAR-gamma\": {\n",
      "      \"best_ap\": 0.09182322445982688\n",
      "    },\n",
      "    \"SR-ARE\": {\n",
      "      \"best_ap\": 0.33600945789867853\n",
      "    },\n",
      "    \"SR-ATAD5\": {\n",
      "      \"best_ap\": 0.22774289601379685\n",
      "    },\n",
      "    \"SR-HSE\": {\n",
      "      \"best_ap\": 0.23226244074941663\n",
      "    },\n",
      "    \"SR-MMP\": {\n",
      "      \"best_ap\": 0.4462153762698644\n",
      "    },\n",
      "    \"SR-p53\": {\n",
      "      \"best_ap\": 0.23462777931161904\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# === Phase 3 â€” Cell 2 (boosted) ===\n",
    "import os, json, math, time, random, platform\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# ------------------------------\n",
    "# Paths & globals\n",
    "# ------------------------------\n",
    "BASE_DIR     = Path(\"v7\")\n",
    "DATA_PREP    = BASE_DIR / \"data\" / \"prepared\"\n",
    "FUSED_DIR    = BASE_DIR / \"data\" / \"fused\"\n",
    "FUSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ENSEMBLE_DIR = BASE_DIR / \"model\" / \"ensembles\"\n",
    "CKPT_SHARED  = BASE_DIR / \"model\" / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Expect shared model & helpers from earlier cells:\n",
    "assert CKPT_SHARED.exists(), \"Shared best checkpoint not found. Run Phase 3 Cell 1 (and 1c optional) first.\"\n",
    "\n",
    "# ------------------------------\n",
    "# Utilities / reproducibility\n",
    "# ------------------------------\n",
    "def seed_everything(seed: int):\n",
    "    import numpy as _np, random as _r, torch as _t\n",
    "    _r.seed(seed); _np.random.seed(seed)\n",
    "    _t.manual_seed(seed); _t.cuda.manual_seed_all(seed)\n",
    "\n",
    "try:\n",
    "    LABEL_NAMES\n",
    "    v7_shared\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Missing v7_shared or LABEL_NAMES in memory. Please re-run Phase 2 Cells 1â€“4 and Phase 3 Cell 1.\")\n",
    "\n",
    "# masked_mean fallback (if not in scope)\n",
    "try:\n",
    "    masked_mean\n",
    "except NameError:\n",
    "    def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "        mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "        denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "        return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "# ------------------------------\n",
    "# Restore best shared weights & set eval\n",
    "# ------------------------------\n",
    "ckpt = torch.load(CKPT_SHARED, map_location=DEVICE)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "v7_shared.eval()\n",
    "\n",
    "# ------------------------------\n",
    "# Fused cache helpers\n",
    "# ------------------------------\n",
    "@torch.no_grad()\n",
    "def compute_fused_batch(smiles_list: List[str], desc_feats: torch.Tensor) -> torch.Tensor:\n",
    "    tt, tm = v7_shared.text_encoder(smiles_list, max_length=256)\n",
    "    gn, gm = v7_shared.graph_encoder(smiles_list, max_nodes=128)\n",
    "    tt, tm = tt.to(DEVICE), tm.to(DEVICE)\n",
    "    gn, gm = gn.to(DEVICE), gm.to(DEVICE)\n",
    "    desc_feats = desc_feats.to(DEVICE)\n",
    "    tta = v7_shared.cross(tt, tm, gn, gm)\n",
    "    de  = v7_shared.desc_mlp(desc_feats)\n",
    "    text_pool  = masked_mean(tta, tm, dim=1)\n",
    "    graph_pool = masked_mean(gn,  gm, dim=1)\n",
    "    fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B, 768)\n",
    "    return fused\n",
    "\n",
    "@torch.no_grad()\n",
    "def cache_fused(npz_path: Path, out_prefix: str, batch_size: int = 256):\n",
    "    blob = np.load(npz_path, allow_pickle=True)\n",
    "    smiles = [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "    Xd    = torch.tensor(blob[\"X\"], dtype=torch.float32, device=DEVICE)\n",
    "    Y     = blob[\"Y\"].astype(np.float32)\n",
    "    M     = blob[\"y_missing_mask\"].astype(bool)\n",
    "    molid = blob[\"mol_id\"].tolist()\n",
    "\n",
    "    fused_list = []\n",
    "    for i in range(0, len(smiles), batch_size):\n",
    "        fu = compute_fused_batch(smiles[i:i+batch_size], Xd[i:i+batch_size])\n",
    "        fused_list.append(fu.cpu().numpy())\n",
    "    F = np.concatenate(fused_list, axis=0).astype(np.float32)\n",
    "\n",
    "    np.save(FUSED_DIR / f\"{out_prefix}_fused.npy\", F)\n",
    "    np.save(FUSED_DIR / f\"{out_prefix}_Y.npy\",    Y)\n",
    "    np.save(FUSED_DIR / f\"{out_prefix}_mask.npy\", M)\n",
    "    np.save(FUSED_DIR / f\"{out_prefix}_mol_id.npy\", np.array(molid, dtype=object))\n",
    "    print(f\"Cached {out_prefix}: fused {F.shape}\")\n",
    "\n",
    "# Build/refresh caches if missing\n",
    "if not (FUSED_DIR / \"train_fused.npy\").exists():\n",
    "    cache_fused(DATA_PREP / \"train.npz\", \"train\", batch_size=256)\n",
    "if not (FUSED_DIR / \"val_fused.npy\").exists():\n",
    "    cache_fused(DATA_PREP / \"val.npz\", \"val\", batch_size=256)\n",
    "\n",
    "# ------------------------------\n",
    "# Load caches into memory\n",
    "# ------------------------------\n",
    "Xtr = np.load(FUSED_DIR / \"train_fused.npy\")\n",
    "Ytr = np.load(FUSED_DIR / \"train_Y.npy\")\n",
    "Mtr = np.load(FUSED_DIR / \"train_mask.npy\")\n",
    "Xva = np.load(FUSED_DIR / \"val_fused.npy\")\n",
    "Yva = np.load(FUSED_DIR / \"val_Y.npy\")\n",
    "Mva = np.load(FUSED_DIR / \"val_mask.npy\")\n",
    "print(\"Fused shapes â†’ train:\", Xtr.shape, \"| val:\", Xva.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset & balanced sampler\n",
    "# ------------------------------\n",
    "class FusedLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, M: np.ndarray, j: int):\n",
    "        valid = ~M[:, j]\n",
    "        self.X = X[valid].astype(np.float32)\n",
    "        self.y = Y[valid, j].astype(np.float32)\n",
    "        assert self.X.shape[0] == self.y.shape[0]\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.X[i]), torch.tensor(self.y[i])\n",
    "\n",
    "def make_balanced_sampler(y_np: np.ndarray):\n",
    "    # Pos/neg weights so expected sampling is ~balanced\n",
    "    pos = (y_np == 1).astype(np.float32)\n",
    "    neg = (y_np == 0).astype(np.float32)\n",
    "    n_pos = pos.sum(); n_neg = neg.sum()\n",
    "    # Avoid zero-division; if n_pos==0, fall back to uniform\n",
    "    if n_pos < 1:\n",
    "        w = np.ones_like(y_np, dtype=np.float32)\n",
    "    else:\n",
    "        w_pos = 0.5 / max(n_pos, 1.0)\n",
    "        w_neg = 0.5 / max(n_neg, 1.0)\n",
    "        w = pos * w_pos + neg * w_neg\n",
    "    return torch.DoubleTensor(w)\n",
    "\n",
    "def make_loaders_for_label(j: int, bs: int = 1024):\n",
    "    dtr = FusedLabelDataset(Xtr, Ytr, Mtr, j)\n",
    "    dva = FusedLabelDataset(Xva, Yva, Mva, j)\n",
    "    # Balanced sampler for training\n",
    "    w = make_balanced_sampler(dtr.y)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(weights=w, num_samples=len(dtr), replacement=True)\n",
    "    train_loader = torch.utils.data.DataLoader(dtr, batch_size=bs, sampler=sampler,\n",
    "                                               num_workers=0, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "    val_loader   = torch.utils.data.DataLoader(dva, batch_size=bs, shuffle=False,\n",
    "                                               num_workers=0, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ------------------------------\n",
    "# Loss (Binary ASL with per-label Î± via effective number)\n",
    "# ------------------------------\n",
    "def effective_alpha(pos_count: int, beta: float = 0.999) -> float:\n",
    "    # Class-Balanced factor Î± for positives; scale to mean ~1 across labels later if desired.\n",
    "    eff = (1 - (beta ** max(pos_count, 1))) / (1 - beta)\n",
    "    return float(1.0 / max(eff, 1e-8))\n",
    "\n",
    "class BinaryASL(nn.Module):\n",
    "    def __init__(self, gamma_neg=5.0, gamma_pos=1.0, clip=0.05, alpha: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.alpha = float(alpha)\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits/targets: (B,)\n",
    "        p = torch.sigmoid(logits)\n",
    "        if self.clip:\n",
    "            p = torch.clamp(p, self.clip, 1 - self.clip)\n",
    "        pos = targets\n",
    "        neg = 1 - targets\n",
    "        pt = p * pos + (1 - p) * neg\n",
    "        gamma = self.gamma_pos * pos + self.gamma_neg * neg\n",
    "        focal = torch.pow(1 - pt, gamma)\n",
    "        loss = - (pos * torch.log(p + 1e-8) + neg * torch.log(1 - p + 1e-8))\n",
    "        loss = loss * focal * self.alpha\n",
    "        return loss.mean()\n",
    "\n",
    "# ------------------------------\n",
    "# Head model (stronger MLP + residual)\n",
    "# ------------------------------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)  # residual path to help optimisation\n",
    "    def forward(self, x):  # x: (B,768)\n",
    "        z1 = self.block1(x)\n",
    "        z2 = self.block2(z1)\n",
    "        z3 = self.block3(z2)\n",
    "        # residual from input\n",
    "        z = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "# ------------------------------\n",
    "# Scheduler: warmup + cosine\n",
    "# ------------------------------\n",
    "def build_warmup_cosine(total_steps, warmup_ratio=0.1, min_lr_scale=0.1):\n",
    "    def lr_lambda(step):\n",
    "        warm = int(total_steps * warmup_ratio)\n",
    "        if step < warm:\n",
    "            return float(step) / max(1, warm)\n",
    "        progress = (step - warm) / max(1, total_steps - warm)\n",
    "        cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return min_lr_scale + (1 - min_lr_scale) * cosine\n",
    "    return lr_lambda\n",
    "\n",
    "# ------------------------------\n",
    "# Train one seed for one label\n",
    "# ------------------------------\n",
    "def train_label_seed(label_name: str, j: int, seed: int,\n",
    "                     epochs_max: int = 50, patience: int = 10,\n",
    "                     lr: float = 3e-3, wd: float = 1e-2) -> Dict[str, float]:\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # Loaders & stats\n",
    "    train_loader, val_loader = make_loaders_for_label(j, bs=1024)\n",
    "    pos_count = int((Ytr[~Mtr[:, j], j] == 1).sum())\n",
    "    alpha = effective_alpha(pos_count)  # per-label CB factor\n",
    "\n",
    "    model = LabelHead(in_dim=768, h1=512, h2=256, h3=128, p=0.30).to(DEVICE)\n",
    "    opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    sched = LambdaLR(opt, build_warmup_cosine(total_steps=steps_per_epoch*epochs_max, warmup_ratio=0.1))\n",
    "\n",
    "    criterion = BinaryASL(gamma_neg=5.0, gamma_pos=1.0, clip=0.05, alpha=alpha)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type==\"cuda\"))\n",
    "    ema_decay = 0.999\n",
    "    ema = {n: p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "    def ema_update():\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                ema[n].mul_(ema_decay).add_(p.detach(), alpha=1-ema_decay)\n",
    "\n",
    "    def ema_copy_to():\n",
    "        with torch.no_grad():\n",
    "            for n,p in model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.data.copy_(ema[n])\n",
    "\n",
    "    # paths\n",
    "    label_dir = ENSEMBLE_DIR / label_name / f\"seed{seed:02d}\"\n",
    "    label_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_path  = label_dir / \"train_log.jsonl\"\n",
    "    if log_path.exists(): log_path.unlink()\n",
    "\n",
    "    best_ap, best_path = -1.0, label_dir / \"best.pt\"\n",
    "    wait = patience\n",
    "    t_start = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs_max + 1):\n",
    "        # ---- train\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE, non_blocking=True); yb = yb.to(DEVICE, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type==\"cuda\")):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            # grad clip\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt); scaler.update()\n",
    "            sched.step()\n",
    "            epoch_loss += float(loss.item())\n",
    "            # EMA\n",
    "            ema_update()\n",
    "\n",
    "        # ---- validate (with EMA weights)\n",
    "        ema_copy_to()\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE, non_blocking=True)\n",
    "                logits = model(xb)\n",
    "                p = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        try:\n",
    "            ap = float(average_precision_score(gts, preds))\n",
    "        except Exception:\n",
    "            ap = float(\"nan\")\n",
    "\n",
    "        row = {\"epoch\": epoch, \"train_loss\": epoch_loss/max(1,len(train_loader)), \"val_ap\": ap, \"time_min\": round((time.time()-t_start)/60,2)}\n",
    "        with open(log_path, \"a\") as f: f.write(json.dumps(row) + \"\\n\")\n",
    "        print(f\"[{label_name} | seed {seed:02d}] ep {epoch:02d}  loss {row['train_loss']:.4f}  val AP {ap:.4f}\")\n",
    "\n",
    "        # save val preds for calibration (overwrite each epoch; best saved below)\n",
    "        np.savez_compressed(label_dir / \"val_preds.npz\", preds=preds, y=gts)\n",
    "\n",
    "        if np.isnan(ap):\n",
    "            continue\n",
    "        if ap > best_ap:\n",
    "            best_ap = ap; wait = patience\n",
    "            torch.save({\"model\": model.state_dict(),\n",
    "                        \"config\": {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30},\n",
    "                        \"label\": label_name, \"seed\": seed}, best_path)\n",
    "            # also save best preds snapshot\n",
    "            np.savez_compressed(label_dir / \"val_preds_best.npz\", preds=preds, y=gts)\n",
    "            print(f\"  âœ… New best AP: {best_ap:.4f} â†’ {best_path}\")\n",
    "        else:\n",
    "            wait -= 1\n",
    "            if wait <= 0:\n",
    "                print(f\"  â¹ Early stop (no improve for {patience} epochs). Best AP: {best_ap:.4f}\")\n",
    "                break\n",
    "\n",
    "    # write seed summary\n",
    "    with open(ENSEMBLE_DIR / label_name / f\"seed{seed:02d}\" / \"metrics.json\", \"w\") as f:\n",
    "        f.write(json.dumps({\"best_ap\": best_ap, \"epochs\": epoch}, indent=2))\n",
    "    return {\"label\": label_name, \"seed\": seed, \"best_ap\": best_ap}\n",
    "\n",
    "# ------------------------------\n",
    "# Driver: all labels, multi-seed\n",
    "# ------------------------------\n",
    "SEEDS = [13, 29, 47, 61, 83]    # 5 seeds (tweak as desired)\n",
    "EPOCHS_MAX = 50\n",
    "PATIENCE   = 10\n",
    "\n",
    "summary = []\n",
    "for j, name in enumerate(LABEL_NAMES):\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Training specialist heads for: {name} (label {j})\")\n",
    "    print(\"==============================\")\n",
    "    for seed in SEEDS:\n",
    "        res = train_label_seed(name, j, seed, epochs_max=EPOCHS_MAX, patience=PATIENCE, lr=3e-3, wd=1e-2)\n",
    "        summary.append(res)\n",
    "\n",
    "# Aggregate best AP per label across seeds\n",
    "agg = {}\n",
    "for name in LABEL_NAMES:\n",
    "    best = max((r for r in summary if r[\"label\"] == name), key=lambda r: (r[\"best_ap\"] if not math.isnan(r[\"best_ap\"]) else -1.0))\n",
    "    agg[name] = {\"best_ap\": best[\"best_ap\"]}\n",
    "\n",
    "(ENSEMBLE_DIR / \"ensemble_summary.json\").write_text(json.dumps({\"per_seed\": summary, \"best_per_label\": agg}, indent=2))\n",
    "print(\"\\nâœ… Ensemble training complete.\")\n",
    "print(json.dumps({\"best_per_label\": agg}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c415ff",
   "metadata": {},
   "source": [
    "## phase 4 (Calibrate/Threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e77f11",
   "metadata": {},
   "source": [
    "### 1: Calibration (Temperature Scaling) + Thresholds (F1 / FÎ²)\n",
    "\n",
    "- Pick best specialist head per label (from `ensemble_summary.json`)\n",
    "- Calibrate with per-label **temperature scaling** (on validation logits)\n",
    "- Select **F1-max** and **FÎ²=1.5-max** thresholds per label\n",
    "- Save to `v7/model/calibration/` and curves to `v7/results/calibration/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95312d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ensemble summary for 12 labels.\n",
      "\n",
      "Calibrating: NR-AR\n",
      "  T=4.549  AP_val=0.1770  th_f1=0.573  th_fÎ²1.5=0.573\n",
      "\n",
      "Calibrating: NR-AR-LBD\n",
      "  T=3.660  AP_val=0.3007  th_f1=0.610  th_fÎ²1.5=0.610\n",
      "\n",
      "Calibrating: NR-AhR\n",
      "  T=1.500  AP_val=0.5251  th_f1=0.681  th_fÎ²1.5=0.631\n",
      "\n",
      "Calibrating: NR-Aromatase\n",
      "  T=2.483  AP_val=0.2754  th_f1=0.604  th_fÎ²1.5=0.552\n",
      "\n",
      "Calibrating: NR-ER\n",
      "  T=4.657  AP_val=0.2309  th_f1=0.542  th_fÎ²1.5=0.523\n",
      "\n",
      "Calibrating: NR-ER-LBD\n",
      "  T=1.857  AP_val=0.1527  th_f1=0.650  th_fÎ²1.5=0.616\n",
      "\n",
      "Calibrating: NR-PPAR-gamma\n",
      "  T=4.558  AP_val=0.0918  th_f1=0.521  th_fÎ²1.5=0.521\n",
      "\n",
      "Calibrating: SR-ARE\n",
      "  T=4.500  AP_val=0.3360  th_f1=0.532  th_fÎ²1.5=0.532\n",
      "\n",
      "Calibrating: SR-ATAD5\n",
      "  T=4.475  AP_val=0.2277  th_f1=0.567  th_fÎ²1.5=0.556\n",
      "\n",
      "Calibrating: SR-HSE\n",
      "  T=4.639  AP_val=0.2323  th_f1=0.551  th_fÎ²1.5=0.545\n",
      "\n",
      "Calibrating: SR-MMP\n",
      "  T=1.896  AP_val=0.4462  th_f1=0.600  th_fÎ²1.5=0.600\n",
      "\n",
      "Calibrating: SR-p53\n",
      "  T=4.245  AP_val=0.2346  th_f1=0.546  th_fÎ²1.5=0.546\n",
      "\n",
      "âœ… Saved:\n",
      "  â€¢ temperatures â†’ v7\\model\\calibration\\temps.json\n",
      "  â€¢ thresholds   â†’ v7\\model\\calibration\\thresholds.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# ------------- Paths -------------\n",
    "BASE_DIR    = Path(\"v7\")\n",
    "FUSED_DIR   = BASE_DIR / \"data\" / \"fused\"\n",
    "ENS_DIR     = BASE_DIR / \"model\" / \"ensembles\"\n",
    "CAL_DIR     = BASE_DIR / \"model\" / \"calibration\"\n",
    "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CAL_RES_DIR = BASE_DIR / \"results\" / \"calibration\"\n",
    "CAL_RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load label names from earlier phase\n",
    "with open(BASE_DIR / \"data\" / \"prepared\" / \"dataset_manifest.json\") as f:\n",
    "    ds_manifest = json.load(f)\n",
    "LABEL_NAMES = ds_manifest[\"labels\"]\n",
    "\n",
    "# Load ensemble summary (decide best seed per label)\n",
    "ens_summary = json.loads((ENS_DIR / \"ensemble_summary.json\").read_text())\n",
    "best_per_label = ens_summary[\"best_per_label\"]  # label -> {best_ap: ...}\n",
    "print(\"Loaded ensemble summary for\", len(best_per_label), \"labels.\")\n",
    "\n",
    "# Fused validation cache\n",
    "Xva = np.load(FUSED_DIR / \"val_fused.npy\")    # (N_val, 768)\n",
    "Yva = np.load(FUSED_DIR / \"val_Y.npy\")        # (N_val, 12)\n",
    "Mva = np.load(FUSED_DIR / \"val_mask.npy\")     # (N_val, 12) True where missing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------- Head definition (must match saved config) ---------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def load_best_seed_dir(label: str) -> Path:\n",
    "    # Find the seed folder with highest best_ap for this label\n",
    "    candidates = []\n",
    "    for seed_dir in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mpath = seed_dir / \"metrics.json\"\n",
    "        if mpath.exists():\n",
    "            try:\n",
    "                m = json.loads(mpath.read_text())\n",
    "                candidates.append((float(m.get(\"best_ap\", float(\"nan\"))), seed_dir))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No seed folders with metrics for label {label}\")\n",
    "    candidates.sort(key=lambda x: (x[0] if not math.isnan(x[0]) else -1.0), reverse=True)\n",
    "    return candidates[0][1]  # best seed dir\n",
    "\n",
    "@torch.no_grad()\n",
    "def head_logits_on_val(label: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return logits and ground truth for valid val rows of this label.\"\"\"\n",
    "    # Filter valid rows (not missing)\n",
    "    j = LABEL_NAMES.index(label)\n",
    "    valid = ~Mva[:, j]\n",
    "    X = torch.tensor(Xva[valid], dtype=torch.float32, device=device)\n",
    "    y = Yva[valid, j].astype(np.float32)\n",
    "\n",
    "    # Load head\n",
    "    best_dir = load_best_seed_dir(label)\n",
    "    ckpt = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg  = ckpt.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    head.eval()\n",
    "\n",
    "    logits = []\n",
    "    BS = 4096  # very fast on 4070 Ti\n",
    "    for i in range(0, X.shape[0], BS):\n",
    "        l = head(X[i:i+BS])\n",
    "        logits.append(l.detach().cpu().numpy())\n",
    "    logits = np.concatenate(logits, axis=0)  # (Nv,)\n",
    "    return logits, y\n",
    "\n",
    "def fit_temperature(logits: np.ndarray, y: np.ndarray, max_iter: int = 200, lr: float = 0.05) -> float:\n",
    "    \"\"\"\n",
    "    Fit scalar temperature T>0, minimizing NLL on validation.\n",
    "    \"\"\"\n",
    "    t = torch.tensor([1.0], dtype=torch.float32, requires_grad=True, device=device)\n",
    "    x = torch.tensor(logits, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(y,      dtype=torch.float32, device=device)\n",
    "    opt = torch.optim.Adam([t], lr=lr)\n",
    "    for _ in range(max_iter):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z = x / (t.clamp(min=1e-3))\n",
    "        p = torch.sigmoid(z).clamp(1e-6, 1-1e-6)\n",
    "        loss = - (y*torch.log(p) + (1-y)*torch.log(1-p)).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return float(t.detach().cpu().item())\n",
    "\n",
    "def best_thresholds(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute thresholds that maximize F1 and F-beta (beta=1.5) on validation.\n",
    "    \"\"\"\n",
    "    if probs.ndim != 1: probs = probs.ravel()\n",
    "    precision, recall, th = precision_recall_curve(y_true, probs)\n",
    "    # PR curve returns len(th)+1 points; align F1/Fb on thresholds\n",
    "    eps = 1e-8\n",
    "    f1 = (2*precision*recall) / np.maximum(precision+recall, eps)\n",
    "    beta = 1.5\n",
    "    fb = ((1+beta**2)*precision*recall) / np.maximum((beta**2)*precision + recall, eps)\n",
    "\n",
    "    # The first PR point has no threshold; weâ€™ll map scores to thresholds array length\n",
    "    f1_th  = th[np.nanargmax(f1[1:])]  if th.size>0 else 0.5\n",
    "    fb_th  = th[np.nanargmax(fb[1:])]  if th.size>0 else 0.5\n",
    "    # also report AP for reference\n",
    "    try:\n",
    "        ap = float(average_precision_score(y_true, probs))\n",
    "    except Exception:\n",
    "        ap = float(\"nan\")\n",
    "    return {\"th_f1\": float(f1_th), \"th_fbeta15\": float(fb_th), \"ap_val\": ap}\n",
    "\n",
    "temps = {}\n",
    "thresholds = {}\n",
    "\n",
    "for label in LABEL_NAMES:\n",
    "    print(f\"\\nCalibrating: {label}\")\n",
    "    logits, y = head_logits_on_val(label)\n",
    "    if logits.size == 0 or np.all(y == y[0]):\n",
    "        print(\"  âš ï¸ Skipping (no variance or no valid rows). Using defaults.\")\n",
    "        temps[label] = 1.0\n",
    "        thresholds[label] = {\"th_f1\": 0.5, \"th_fbeta15\": 0.5, \"ap_val\": float(\"nan\")}\n",
    "        continue\n",
    "\n",
    "    T = fit_temperature(logits, y, max_iter=200, lr=0.05)\n",
    "    probs_cal = 1.0 / (1.0 + np.exp(-logits / max(T, 1e-3)))\n",
    "\n",
    "    th_dict = best_thresholds(y, probs_cal)\n",
    "    temps[label] = T\n",
    "    thresholds[label] = th_dict\n",
    "\n",
    "    # optional: save PR curve arrays for later plotting/debug\n",
    "    np.savez_compressed(CAL_RES_DIR / f\"{label}_val_calib.npz\", logits=logits, y=y, T=T, **th_dict)\n",
    "    print(f\"  T={T:.3f}  AP_val={th_dict['ap_val']:.4f}  th_f1={th_dict['th_f1']:.3f}  th_fÎ²1.5={th_dict['th_fbeta15']:.3f}\")\n",
    "\n",
    "# Save calibration artifacts\n",
    "(Path(CAL_DIR / \"temps.json\")).write_text(json.dumps(temps, indent=2))\n",
    "(Path(CAL_DIR / \"thresholds.json\")).write_text(json.dumps(thresholds, indent=2))\n",
    "print(\"\\nâœ… Saved:\")\n",
    "print(\"  â€¢ temperatures â†’\", CAL_DIR / \"temps.json\")\n",
    "print(\"  â€¢ thresholds   â†’\", CAL_DIR / \"thresholds.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2635d6c",
   "metadata": {},
   "source": [
    "## phase 5 (Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abe97b5",
   "metadata": {},
   "source": [
    "### 1:  Inference (calibrated specialist ensemble) + test export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39712280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded shared fusion model.\n",
      "âœ… Loaded specialist heads for all labels.\n",
      "âœ… Inference is ready: call predict_smiles(['CCO'], threshold_mode='fbeta15' or 'f1').\n"
     ]
    }
   ],
   "source": [
    "# === Cold-start Inference (checkpoint name-compatible) ===\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------- Paths & basics ----------------\n",
    "BASE       = Path(\"v7\")\n",
    "PREP_DIR   = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR   = BASE / \"data\" / \"descriptors\"\n",
    "MODEL_DIR  = BASE / \"model\"\n",
    "CKPT_BEST  = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "ENS_DIR    = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR    = MODEL_DIR / \"calibration\"\n",
    "\n",
    "assert CKPT_BEST.exists(), f\"Missing shared checkpoint: {CKPT_BEST}\"\n",
    "assert (PREP_DIR / \"dataset_manifest.json\").exists(), \"Missing dataset manifest.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------- Labels, temps, thresholds ----------------\n",
    "ds_manifest = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABEL_NAMES = ds_manifest[\"labels\"]\n",
    "DESC_IN_DIM = ds_manifest[\"n_features\"]  # 208\n",
    "\n",
    "temps      = json.loads((CAL_DIR / \"temps.json\").read_text())\n",
    "thresholds = json.loads((CAL_DIR / \"thresholds.json\").read_text())\n",
    "\n",
    "# ---------------- Text encoder (ChemBERTa) ----------------\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class ChemBERTaEncoder(nn.Module):\n",
    "    def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "        self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "        self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "        self.ln = nn.LayerNorm(fusion_dim)\n",
    "    def forward(self, smiles_list: List[str], max_length=256, add_special_tokens=True):\n",
    "        enc = self.tokenizer(list(smiles_list), padding=True, truncation=True,\n",
    "                             max_length=max_length, add_special_tokens=add_special_tokens,\n",
    "                             return_tensors=\"pt\")\n",
    "        input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,L,H)\n",
    "        toks = self.ln(self.proj(out))  # (B,L,256)\n",
    "        return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "# ---------------- Graph encoder (match checkpoint names) ----------------\n",
    "from rdkit import Chem as _Chem\n",
    "\n",
    "ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "\n",
    "def _one_hot(v, choices):\n",
    "    z = [0]*len(choices)\n",
    "    if v in choices:\n",
    "        z[choices.index(v)] = 1\n",
    "    return z\n",
    "\n",
    "def _bucket_oh(v, lo, hi):\n",
    "    buckets = list(range(lo, hi+1))\n",
    "    o = [0]*(len(buckets)+1)\n",
    "    idx = v - lo\n",
    "    o[idx if 0 <= idx < len(buckets) else -1] = 1\n",
    "    return o\n",
    "\n",
    "def _atom_feat(atom):\n",
    "    hybs = [\n",
    "        _Chem.rdchem.HybridizationType.S, _Chem.rdchem.HybridizationType.SP,\n",
    "        _Chem.rdchem.HybridizationType.SP2, _Chem.rdchem.HybridizationType.SP3,\n",
    "        _Chem.rdchem.HybridizationType.SP3D, _Chem.rdchem.HybridizationType.SP3D2\n",
    "    ]\n",
    "    chir = [\n",
    "        _Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        _Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        _Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "        _Chem.rdchem.ChiralType.CHI_OTHER\n",
    "    ]\n",
    "    sym = atom.GetSymbol()\n",
    "    feat = _one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST+[\"other\"])\n",
    "    feat += _bucket_oh(atom.GetDegree(), 0, 5)\n",
    "    feat += _bucket_oh(atom.GetFormalCharge(), -2, 2)\n",
    "    feat += (_one_hot(atom.GetHybridization(), hybs)+[0])  # +other\n",
    "    feat += [int(atom.GetIsAromatic())]\n",
    "    feat += [int(atom.IsInRing())]\n",
    "    feat += _one_hot(atom.GetChiralTag(), chir)\n",
    "    feat += _bucket_oh(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)\n",
    "    feat += _bucket_oh(atom.GetTotalValence(), 0, 5)\n",
    "    feat += [atom.GetMass()/200.0]\n",
    "    return feat  # ~51 dims\n",
    "\n",
    "def _smiles_to_graph(smi, max_nodes=128):\n",
    "    mol = _Chem.MolFromSmiles(smi)\n",
    "    if mol is None or mol.GetNumAtoms() == 0:\n",
    "        return np.zeros((0,0), dtype=np.float32), np.zeros((0,0), dtype=np.float32)\n",
    "    feats = [_atom_feat(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "    x = np.asarray(feats, dtype=np.float32)\n",
    "    N = mol.GetNumAtoms()\n",
    "    adj = np.zeros((N, N), dtype=np.float32)\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        adj[i, j] = 1.0; adj[j, i] = 1.0\n",
    "    if N > max_nodes:\n",
    "        x = x[:max_nodes]; adj = adj[:max_nodes, :max_nodes]\n",
    "    return x, adj\n",
    "\n",
    "def _collate_graphs(smiles_batch, max_nodes=128):\n",
    "    graphs = [_smiles_to_graph(s) for s in smiles_batch]\n",
    "    Nmax = max([g[0].shape[0] for g in graphs] + [1])\n",
    "    Fnode = graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "    B = len(graphs)\n",
    "    X = np.zeros((B, Nmax, Fnode), dtype=np.float32)\n",
    "    A = np.zeros((B, Nmax, Nmax), dtype=np.float32)\n",
    "    M = np.zeros((B, Nmax), dtype=np.int64)\n",
    "    for i, (x, a) in enumerate(graphs):\n",
    "        n = x.shape[0]\n",
    "        if n == 0: continue\n",
    "        X[i, :n, :] = x\n",
    "        A[i, :n, :n] = a\n",
    "        M[i, :n] = 1\n",
    "    return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, h=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "        self.mlp = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "    def forward(self, x, adj, mask):\n",
    "        out = (1.0 + self.eps) * x + torch.matmul(adj, x)\n",
    "        out = self.mlp(out)\n",
    "        return out * mask.unsqueeze(-1).to(out.dtype)\n",
    "\n",
    "class GraphGINEncoder(nn.Module):\n",
    "    def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(nn.Linear(node_in_dim, hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden_dim, p) for _ in range(n_layers)])\n",
    "        # IMPORTANT: name must be 'out_ln' to match checkpoint\n",
    "        self.out_ln = nn.LayerNorm(hidden_dim)\n",
    "    def forward(self, smiles_list: List[str], max_nodes=128):\n",
    "        X, A, M = _collate_graphs(smiles_list, max_nodes=max_nodes)\n",
    "        h = self.inp(X)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, A, M)\n",
    "        return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "# ---------------- Fusion blocks ----------------\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "    denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "    return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "        self.ln  = nn.LayerNorm(dim)\n",
    "        self.do  = nn.Dropout(p)\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "        Q = text_tokens.transpose(0,1)   # (L,B,D)\n",
    "        K = graph_nodes.transpose(0,1)   # (N,B,D)\n",
    "        V = graph_nodes.transpose(0,1)\n",
    "        kpm = (graph_mask == 0)          # (B,N) True where pad\n",
    "        attn, _ = self.mha(Q, K, V, key_padding_mask=kpm)\n",
    "        attn = attn.transpose(0,1)       # (B,L,D)\n",
    "        return self.ln(text_tokens + self.do(attn))\n",
    "\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim), nn.GELU(), nn.Dropout(p)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# IMPORTANT: name must be 'mlp' to match checkpoint ('shared_head.mlp.*')\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=12, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, fused_vec):\n",
    "        return self.mlp(fused_vec)\n",
    "\n",
    "class V7FusionModel(nn.Module):\n",
    "    def __init__(self, text_encoder, graph_encoder, desc_in_dim=208, dim=256, n_labels=12, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.text_encoder=text_encoder\n",
    "        self.graph_encoder=graph_encoder\n",
    "        self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "        self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "        self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "    def forward(self, smiles_list, desc_feats, return_intermediates=False):\n",
    "        tt, tm = self.text_encoder(smiles_list, max_length=256)\n",
    "        gn, gm = self.graph_encoder(smiles_list, max_nodes=128)\n",
    "        tt, tm, gn, gm, desc_feats = tt.to(device), tm.to(device), gn.to(device), gm.to(device), desc_feats.to(device)\n",
    "        tta = self.cross(tt, tm, gn, gm)\n",
    "        de  = self.desc_mlp(desc_feats)\n",
    "        text_pool  = masked_mean(tta, tm, 1)\n",
    "        graph_pool = masked_mean(gn,  gm, 1)\n",
    "        fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "        logits = self.shared_head(fused)\n",
    "        if return_intermediates:\n",
    "            return logits, fused\n",
    "        return logits\n",
    "\n",
    "# ---------------- Build & load ----------------\n",
    "text_encoder = ChemBERTaEncoder().to(device)\n",
    "graph_encoder= GraphGINEncoder().to(device)\n",
    "v7_shared    = V7FusionModel(text_encoder, graph_encoder, desc_in_dim=DESC_IN_DIM, n_labels=len(LABEL_NAMES)).to(device)\n",
    "ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "v7_shared.eval()\n",
    "print(\"âœ… Loaded shared fusion model.\")\n",
    "\n",
    "# ---------------- Specialist heads (match boosted Cell 2) ----------------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _load_best_head(label: str) -> nn.Module:\n",
    "    # pick seed dir with highest best_ap\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if mfile.exists():\n",
    "            try:\n",
    "                ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "                cands.append((ap, sd))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(ck[\"model\"], strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS: Dict[str, nn.Module] = {lbl: _load_best_head(lbl) for lbl in LABEL_NAMES}\n",
    "print(\"âœ… Loaded specialist heads for all labels.\")\n",
    "\n",
    "# ---------------- Descriptors for ad-hoc SMILES ----------------\n",
    "# For quick testing without the exact 208-d extractor, use standardized zero vector for descriptors.\n",
    "def prepare_desc_matrix(smiles_list: List[str]) -> torch.Tensor:\n",
    "    n = len(smiles_list)\n",
    "    Z = np.zeros((n, DESC_IN_DIM), dtype=np.float32)  # standardized zeros (mean feature)\n",
    "    return torch.tensor(Z, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---------------- Fused feature builder ----------------\n",
    "@torch.no_grad()\n",
    "def fused_from_smiles(smiles_list: List[str], desc_tensor: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    if desc_tensor is None:\n",
    "        desc_tensor = prepare_desc_matrix(smiles_list)\n",
    "    tt, tm = v7_shared.text_encoder(smiles_list, max_length=256)\n",
    "    gn, gm = v7_shared.graph_encoder(smiles_list, max_nodes=128)\n",
    "    tt, tm = tt.to(device), tm.to(device)\n",
    "    gn, gm = gn.to(device), gm.to(device)\n",
    "    de = v7_shared.desc_mlp(desc_tensor.to(device))\n",
    "    # cross-attend & pool\n",
    "    tta = v7_shared.cross(tt, tm, gn, gm)\n",
    "    text_pool  = masked_mean(tta, tm, 1)\n",
    "    graph_pool = masked_mean(gn,  gm, 1)\n",
    "    return torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "\n",
    "# ---------------- Public API ----------------\n",
    "def predict_smiles(smiles_list: List[str], threshold_mode: str = \"fbeta15\"):\n",
    "    \"\"\"\n",
    "    Returns list[dict]: one per SMILES\n",
    "      label -> {logit, prob_raw, prob_cal, decision}\n",
    "    \"\"\"\n",
    "    assert threshold_mode in (\"f1\", \"fbeta15\")\n",
    "    fused = fused_from_smiles(smiles_list)  # (B,768)\n",
    "    out = []\n",
    "    for i in range(fused.size(0)):\n",
    "        row = {}\n",
    "        x = fused[i:i+1]\n",
    "        for label in LABEL_NAMES:\n",
    "            head = HEADS[label]\n",
    "            with torch.no_grad():\n",
    "                logit = head(x).item()\n",
    "            T   = max(float(temps.get(label, 1.0)), 1e-3)\n",
    "            p_r = 1.0 / (1.0 + math.e**(-logit))\n",
    "            p_c = 1.0 / (1.0 + math.e**(-logit / T))\n",
    "            th  = thresholds[label][\"th_fbeta15\"] if threshold_mode==\"fbeta15\" else thresholds[label][\"th_f1\"]\n",
    "            row[label] = {\"logit\": float(logit), \"prob_raw\": float(p_r), \"prob_cal\": float(p_c), \"decision\": bool(p_c >= float(th))}\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "print(\"âœ… Inference is ready: call predict_smiles(['CCO'], threshold_mode='fbeta15' or 'f1').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7488f168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12/12 label columns in the Excel.\n",
      "\n",
      "SMILES: CCOc1ccc2nc(S(N)(=O)=O)sc2c1\n",
      "  NR-AhR        prob=0.594  th=0.681  â†’ pred=0\n",
      "  SR-ARE        prob=0.533  th=0.532  â†’ pred=1\n",
      "  NR-ER         prob=0.529  th=0.542  â†’ pred=0\n",
      "  SR-ATAD5      prob=0.529  th=0.567  â†’ pred=0\n",
      "  NR-PPAR-gamma  prob=0.521  th=0.521  â†’ pred=1\n",
      "  True positives: NR-AhR, SR-ARE\n",
      "  Pred positives (f1): NR-PPAR-gamma, SR-ARE\n",
      "\n",
      "SMILES: CCN1C(=O)NC(c2ccccc2)C1=O\n",
      "  NR-AhR        prob=0.616  th=0.681  â†’ pred=0\n",
      "  SR-MMP        prob=0.554  th=0.600  â†’ pred=0\n",
      "  SR-ATAD5      prob=0.537  th=0.567  â†’ pred=0\n",
      "  NR-PPAR-gamma  prob=0.536  th=0.521  â†’ pred=1\n",
      "  SR-p53        prob=0.534  th=0.546  â†’ pred=0\n",
      "  True positives: â€”\n",
      "  Pred positives (f1): NR-PPAR-gamma\n",
      "\n",
      "SMILES: O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\n",
      "  NR-AhR        prob=0.670  th=0.681  â†’ pred=0\n",
      "  SR-MMP        prob=0.641  th=0.600  â†’ pred=1\n",
      "  NR-ER-LBD     prob=0.580  th=0.650  â†’ pred=0\n",
      "  NR-Aromatase  prob=0.569  th=0.604  â†’ pred=0\n",
      "  SR-p53        prob=0.560  th=0.546  â†’ pred=1\n",
      "  True positives: NR-ER, SR-ARE, SR-HSE, SR-p53\n",
      "  Pred positives (f1): NR-PPAR-gamma, SR-ARE, SR-MMP, SR-p53\n",
      "\n",
      "SMILES: CC(O)CNCC(C)O\n",
      "  NR-AR         prob=0.516  th=0.573  â†’ pred=0\n",
      "  NR-ER         prob=0.510  th=0.542  â†’ pred=0\n",
      "  SR-HSE        prob=0.503  th=0.551  â†’ pred=0\n",
      "  SR-ARE        prob=0.500  th=0.532  â†’ pred=0\n",
      "  NR-PPAR-gamma  prob=0.494  th=0.521  â†’ pred=0\n",
      "  True positives: â€”\n",
      "  Pred positives (f1): â€”\n",
      "\n",
      "SMILES: O=c1cc(-c2ccccc2)oc2cc(O)cc(O)c12\n",
      "  NR-AhR        prob=0.708  th=0.681  â†’ pred=1\n",
      "  SR-MMP        prob=0.668  th=0.600  â†’ pred=1\n",
      "  NR-ER-LBD     prob=0.617  th=0.650  â†’ pred=0\n",
      "  NR-Aromatase  prob=0.576  th=0.604  â†’ pred=0\n",
      "  SR-ARE        prob=0.567  th=0.532  â†’ pred=1\n",
      "  True positives: NR-AhR, NR-ER, NR-ER-LBD, NR-PPAR-gamma, SR-ARE, SR-MMP\n",
      "  Pred positives (f1): NR-AhR, NR-ER, NR-PPAR-gamma, SR-ARE, SR-HSE, SR-MMP, SR-p53\n",
      "\n",
      "=== Summary (micro over labels with truth present) ===\n",
      "TP=8 FP=3 FN=4\n",
      "Precision=0.727 Recall=0.667 F1=0.696\n",
      "\n",
      "Saved detailed results â†’ v7\\results\\inference\\f1.csv\n"
     ]
    }
   ],
   "source": [
    "# my_smiles = [\"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\"]\n",
    "# mode = \"f1\"  # or \"f1\" fbeta15\n",
    "\n",
    "# results = predict_smiles(my_smiles, threshold_mode=mode)\n",
    "\n",
    "# from operator import itemgetter\n",
    "# for smi, rec in zip(my_smiles, results):\n",
    "#     print(\"\\nSMILES:\", smi)\n",
    "#     top = sorted([(lbl, d[\"prob_cal\"], d[\"decision\"]) for lbl, d in rec.items()],\n",
    "#                  key=itemgetter(1), reverse=True)[:5]\n",
    "#     for lbl, p, dec in top:\n",
    "#         th = thresholds[lbl][\"th_fbeta15\"] if mode==\"fbeta15\" else thresholds[lbl][\"th_f1\"]\n",
    "#         print(f\"  {lbl:12s}  prob={p:.3f}  th={th:.3f}  â†’ pred={int(dec)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ad-hoc evaluation on Excel truth labels (simple)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "import math, json, os\n",
    "\n",
    "# ----------- CONFIG -----------\n",
    "EXCEL_PATH = Path(\"tox21_dualenc_v1/data/raw/Truth Lables.xlsx\")\n",
    "MODE = \"f1\"            # \"f1\" or \"fbeta15\"\n",
    "N_DISPLAY = 5          # how many rows to pretty-print (set to None to print all)\n",
    "OUT_CSV = Path(\"v7/results/inference/f1.csv\")\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------- Checks -----------\n",
    "assert 'predict_smiles' in globals(), \"predict_smiles() not found. Run the cold-start inference cell first.\"\n",
    "assert 'LABEL_NAMES' in globals(), \"LABEL_NAMES not found. Run the cold-start inference cell first.\"\n",
    "assert 'thresholds' in globals(), \"thresholds not found. Run Phase 4 calibration cell first.\"\n",
    "assert EXCEL_PATH.exists(), f\"Cannot find: {EXCEL_PATH}\"\n",
    "\n",
    "# ----------- Load Excel -----------\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "cols_lower = {c.lower(): c for c in df.columns}\n",
    "# find smiles col (case-insensitive)\n",
    "smiles_col = None\n",
    "for key in [\"smiles\", \"smile\", \"SMILES\", \"Smiles\"]:\n",
    "    if key.lower() in cols_lower:\n",
    "        smiles_col = cols_lower[key.lower()]\n",
    "        break\n",
    "if smiles_col is None:\n",
    "    # fallback: first column named like 'smile*'\n",
    "    cand = [c for c in df.columns if c.lower().startswith(\"smiles\")]\n",
    "    smiles_col = cand[0] if cand else None\n",
    "assert smiles_col is not None, \"Could not locate a SMILES column in the Excel file.\"\n",
    "\n",
    "# ----------- Match label columns (case/spacing/hyphen-insensitive) -----------\n",
    "def _norm(s: str) -> str:\n",
    "    return \"\".join(ch for ch in str(s).lower() if ch.isalnum())\n",
    "\n",
    "label_norm = { _norm(lbl): lbl for lbl in LABEL_NAMES }\n",
    "col_for_label = {}  # label -> column name in df (if present)\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == smiles_col: \n",
    "        continue\n",
    "    n = _norm(col)\n",
    "    if n in label_norm:\n",
    "        col_for_label[label_norm[n]] = col\n",
    "\n",
    "available_labels = [lbl for lbl in LABEL_NAMES if lbl in col_for_label]\n",
    "missing_labels = [lbl for lbl in LABEL_NAMES if lbl not in col_for_label]\n",
    "print(f\"Found {len(available_labels)}/{len(LABEL_NAMES)} label columns in the Excel.\")\n",
    "if missing_labels:\n",
    "    print(\"Missing label columns (will be skipped in scoring):\", \", \".join(missing_labels))\n",
    "\n",
    "# ----------- Parse truth values -----------\n",
    "def parse_truth(v):\n",
    "    if pd.isna(v): \n",
    "        return None\n",
    "    if isinstance(v, (int, np.integer)): \n",
    "        return int(v) == 1\n",
    "    if isinstance(v, float): \n",
    "        if math.isnan(v): return None\n",
    "        return int(v) == 1\n",
    "    s = str(v).strip().lower()\n",
    "    if s in (\"1\",\"y\",\"yes\",\"true\",\"t\",\"pos\",\"positive\"):\n",
    "        return True\n",
    "    if s in (\"0\",\"n\",\"no\",\"false\",\"f\",\"neg\",\"negative\"):\n",
    "        return False\n",
    "    # anything else â†’ None (unknown)\n",
    "    return None\n",
    "\n",
    "# ----------- Run predictions -----------\n",
    "smiles_list = df[smiles_col].astype(str).tolist()\n",
    "preds = predict_smiles(smiles_list, threshold_mode=MODE)  # list[dict[label -> details]]\n",
    "\n",
    "# ----------- Build a simple evaluation table -----------\n",
    "rows = []\n",
    "micro_tp = micro_fp = micro_fn = 0\n",
    "\n",
    "for i, (smi, rec) in enumerate(zip(smiles_list, preds)):\n",
    "    # truth set (only for labels available in Excel)\n",
    "    true_pos = set()\n",
    "    true_neg = set()\n",
    "    for lbl in available_labels:\n",
    "        val = parse_truth(df.loc[i, col_for_label[lbl]])\n",
    "        if val is True:\n",
    "            true_pos.add(lbl)\n",
    "        elif val is False:\n",
    "            true_neg.add(lbl)\n",
    "        # None â†’ skip\n",
    "\n",
    "    # predicted positives at chosen threshold\n",
    "    pred_pos = {lbl for lbl, d in rec.items() if d[\"decision\"]}\n",
    "    # accumulate micro counts only on labels where truth is known\n",
    "    for lbl in available_labels:\n",
    "        val = parse_truth(df.loc[i, col_for_label[lbl]])\n",
    "        if val is None: \n",
    "            continue\n",
    "        if lbl in pred_pos and val is True:\n",
    "            micro_tp += 1\n",
    "        elif lbl in pred_pos and val is False:\n",
    "            micro_fp += 1\n",
    "        elif lbl not in pred_pos and val is True:\n",
    "            micro_fn += 1\n",
    "\n",
    "    # top-5 by calibrated probability (for pretty print)\n",
    "    top5 = sorted([(lbl, d[\"prob_cal\"], d[\"decision\"]) for lbl, d in rec.items()],\n",
    "                  key=itemgetter(1), reverse=True)[:5]\n",
    "\n",
    "    # save a row for CSV: include probs & preds, and truths if present\n",
    "    row = {\"smiles\": smi}\n",
    "    for lbl, det in rec.items():\n",
    "        row[f\"{lbl}_prob\"] = det[\"prob_cal\"]\n",
    "        row[f\"{lbl}_pred\"] = int(det[\"decision\"])\n",
    "        if lbl in available_labels:\n",
    "            tv = parse_truth(df.loc[i, col_for_label[lbl]])\n",
    "            row[f\"{lbl}_true\"] = (None if tv is None else int(tv))\n",
    "    rows.append(row)\n",
    "\n",
    "    # pretty print a few rows\n",
    "    if N_DISPLAY is None or i < N_DISPLAY:\n",
    "        print(\"\\nSMILES:\", smi)\n",
    "        for lbl, p, dec in top5:\n",
    "            th = thresholds[lbl][\"th_fbeta15\"] if MODE==\"fbeta15\" else thresholds[lbl][\"th_f1\"]\n",
    "            print(f\"  {lbl:12s}  prob={p:.3f}  th={float(th):.3f}  â†’ pred={int(dec)}\")\n",
    "        if available_labels:\n",
    "            print(\"  True positives:\", \", \".join(sorted(true_pos)) if true_pos else \"â€”\")\n",
    "            chosen = \", \".join(sorted(pred_pos)) if pred_pos else \"â€”\"\n",
    "            print(f\"  Pred positives ({MODE}): {chosen}\")\n",
    "\n",
    "# ----------- Micro summary -----------\n",
    "prec = micro_tp / (micro_tp + micro_fp) if (micro_tp + micro_fp) > 0 else 0.0\n",
    "rec  = micro_tp / (micro_tp + micro_fn) if (micro_tp + micro_fn) > 0 else 0.0\n",
    "f1   = (2*prec*rec)/(prec+rec) if (prec+rec) > 0 else 0.0\n",
    "\n",
    "print(\"\\n=== Summary (micro over labels with truth present) ===\")\n",
    "print(f\"TP={micro_tp} FP={micro_fp} FN={micro_fn}\")\n",
    "print(f\"Precision={prec:.3f} Recall={rec:.3f} F1={f1:.3f}\")\n",
    "\n",
    "# ----------- Save CSV -----------\n",
    "pd.DataFrame(rows).to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\nSaved detailed results â†’ {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c38b0",
   "metadata": {},
   "source": [
    "### 2: Calibrate shared head, create blended ensemble, refit thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885333fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating shared head temperatures on val...\n",
      "  NR-AR: T_shared=0.134\n",
      "  NR-AR-LBD: T_shared=0.132\n",
      "  NR-AhR: T_shared=0.167\n",
      "  NR-Aromatase: T_shared=0.126\n",
      "  NR-ER: T_shared=0.134\n",
      "  NR-ER-LBD: T_shared=0.110\n",
      "  NR-PPAR-gamma: T_shared=0.167\n",
      "  SR-ARE: T_shared=0.260\n",
      "  SR-ATAD5: T_shared=0.146\n",
      "  SR-HSE: T_shared=0.100\n",
      "  SR-MMP: T_shared=0.250\n",
      "  SR-p53: T_shared=0.119\n",
      "Saved â†’ v7\\model\\calibration\\temps_shared.json\n",
      "\n",
      "Blending probs on val with alpha=0.80 (specialist weight)\n",
      "  NR-AR: AP_val=0.171 th_f1=0.653 th_fb15=0.653\n",
      "  NR-AR-LBD: AP_val=0.253 th_f1=0.621 th_fb15=0.621\n",
      "  NR-AhR: AP_val=0.524 th_f1=0.709 th_fb15=0.642\n",
      "  NR-Aromatase: AP_val=0.295 th_f1=0.564 th_fb15=0.474\n",
      "  NR-ER: AP_val=0.253 th_f1=0.547 th_fb15=0.480\n",
      "  NR-ER-LBD: AP_val=0.139 th_f1=0.589 th_fb15=0.589\n",
      "  NR-PPAR-gamma: AP_val=0.063 th_f1=0.441 th_fb15=0.427\n",
      "  SR-ARE: AP_val=0.344 th_f1=0.528 th_fb15=0.528\n",
      "  SR-ATAD5: AP_val=0.171 th_f1=0.483 th_fb15=0.483\n",
      "  SR-HSE: AP_val=0.196 th_f1=0.472 th_fb15=0.459\n",
      "  SR-MMP: AP_val=0.444 th_f1=0.589 th_fb15=0.589\n",
      "  SR-p53: AP_val=0.210 th_f1=0.513 th_fb15=0.478\n",
      "\n",
      "Saved â†’ v7\\model\\calibration\\thresholds_blend.json\n",
      "\n",
      "âœ… Blend ready: use predict_smiles_blend([...], mode='fbeta15' or 'f1').\n"
     ]
    }
   ],
   "source": [
    "# Phase 5 â€” Cell 2 (optional): shared+specialist blend with calibration and new thresholds\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "BASE      = Path(\"v7\")\n",
    "FUSED_DIR = BASE / \"data\" / \"fused\"\n",
    "CAL_DIR   = BASE / \"model\" / \"calibration\"\n",
    "ENS_DIR   = BASE / \"model\" / \"ensembles\"\n",
    "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Expect these in memory from earlier cold-start cell:\n",
    "# v7_shared (with .shared_head), HEADS (specialists), LABEL_NAMES, temps (specialist temps)\n",
    "assert 'v7_shared' in globals() and 'HEADS' in globals() and 'LABEL_NAMES' in globals() and 'temps' in globals()\n",
    "\n",
    "# ---- load val fused + labels/mask ----\n",
    "Xva = np.load(FUSED_DIR / \"val_fused.npy\")     # (N,768)\n",
    "Yva = np.load(FUSED_DIR / \"val_Y.npy\")         # (N,12)\n",
    "Mva = np.load(FUSED_DIR / \"val_mask.npy\")      # (N,12) True where missing\n",
    "\n",
    "Xva_t = torch.tensor(Xva, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---- helper: fit per-label temperature (on logits) ----\n",
    "def fit_temperature(logits: np.ndarray, y: np.ndarray, max_iter=200, lr=0.05) -> float:\n",
    "    t = torch.tensor([1.0], dtype=torch.float32, requires_grad=True, device=device)\n",
    "    x = torch.tensor(logits, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(y,      dtype=torch.float32, device=device)\n",
    "    opt = torch.optim.Adam([t], lr=lr)\n",
    "    for _ in range(max_iter):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z = x / (t.clamp(min=1e-3))\n",
    "        p = torch.sigmoid(z).clamp(1e-6, 1-1e-6)\n",
    "        loss = - (y*torch.log(p) + (1-y)*torch.log(1-p)).mean()\n",
    "        loss.backward(); opt.step()\n",
    "    return float(t.detach().cpu().item())\n",
    "\n",
    "def best_thresholds(y_true: np.ndarray, probs: np.ndarray):\n",
    "    prec, rec, th = precision_recall_curve(y_true, probs)\n",
    "    eps = 1e-8\n",
    "    f1 = (2*prec*rec) / np.maximum(prec+rec, eps)\n",
    "    beta = 1.5\n",
    "    fb = ((1+beta**2)*prec*rec) / np.maximum((beta**2)*prec + rec, eps)\n",
    "    th_f1 = th[np.nanargmax(f1[1:])] if th.size>0 else 0.5\n",
    "    th_fb = th[np.nanargmax(fb[1:])] if th.size>0 else 0.5\n",
    "    try:\n",
    "        ap = float(average_precision_score(y_true, probs))\n",
    "    except Exception:\n",
    "        ap = float(\"nan\")\n",
    "    return {\"th_f1\": float(th_f1), \"th_fbeta15\": float(th_fb), \"ap_val\": ap}\n",
    "\n",
    "# ---- 1) Calibrate SHARED head per label on val ----\n",
    "print(\"Calibrating shared head temperatures on val...\")\n",
    "logits_shared = v7_shared.shared_head(Xva_t).detach().cpu().numpy()  # (N,12)\n",
    "temps_shared = {}\n",
    "for j, lbl in enumerate(LABEL_NAMES):\n",
    "    valid = ~Mva[:, j]\n",
    "    if valid.sum() == 0 or np.all(Yva[valid, j] == Yva[valid, j][0]):\n",
    "        temps_shared[lbl] = 1.0\n",
    "        continue\n",
    "    T = fit_temperature(logits_shared[valid, j], Yva[valid, j])\n",
    "    temps_shared[lbl] = T\n",
    "    print(f\"  {lbl}: T_shared={T:.3f}\")\n",
    "(Path(CAL_DIR / \"temps_shared.json\")).write_text(json.dumps(temps_shared, indent=2))\n",
    "print(\"Saved â†’\", CAL_DIR / \"temps_shared.json\")\n",
    "\n",
    "# ---- 2) Build BLENDED probs on val (alpha specialist, (1-alpha) shared) ----\n",
    "ALPHA = 0.8  # weight on specialist; tweak if desired\n",
    "print(f\"\\nBlending probs on val with alpha={ALPHA:.2f} (specialist weight)\")\n",
    "\n",
    "# specialist logits on val\n",
    "spec_logits = np.zeros_like(logits_shared)\n",
    "with torch.no_grad():\n",
    "    for j, lbl in enumerate(LABEL_NAMES):\n",
    "        head = HEADS[lbl]\n",
    "        spec_logits[:, j] = head(Xva_t).detach().cpu().numpy()\n",
    "\n",
    "# calibrate both streams\n",
    "p_spec_val   = np.zeros_like(spec_logits)\n",
    "p_shared_val = np.zeros_like(logits_shared)\n",
    "for j, lbl in enumerate(LABEL_NAMES):\n",
    "    T_spec   = max(float(temps.get(lbl, 1.0)), 1e-3)\n",
    "    T_shared = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "    p_spec_val[:, j]   = 1. / (1. + np.exp(-spec_logits[:, j]   / T_spec))\n",
    "    p_shared_val[:, j] = 1. / (1. + np.exp(-logits_shared[:, j] / T_shared))\n",
    "\n",
    "p_blend_val = ALPHA * p_spec_val + (1-ALPHA) * p_shared_val\n",
    "p_blend_val = np.clip(p_blend_val, 0.0, 1.0)\n",
    "\n",
    "# ---- 3) Refit thresholds for BLEND on val ----\n",
    "thresholds_blend = {}\n",
    "for j, lbl in enumerate(LABEL_NAMES):\n",
    "    valid = ~Mva[:, j]\n",
    "    if valid.sum() == 0 or np.all(Yva[valid, j] == Yva[valid, j][0]):\n",
    "        thresholds_blend[lbl] = {\"th_f1\": 0.5, \"th_fbeta15\": 0.5, \"ap_val\": float(\"nan\")}\n",
    "        continue\n",
    "    thresholds_blend[lbl] = best_thresholds(Yva[valid, j], p_blend_val[valid, j])\n",
    "    print(f\"  {lbl}: AP_val={thresholds_blend[lbl]['ap_val']:.3f} th_f1={thresholds_blend[lbl]['th_f1']:.3f} th_fb15={thresholds_blend[lbl]['th_fbeta15']:.3f}\")\n",
    "\n",
    "(Path(CAL_DIR / \"thresholds_blend.json\")).write_text(json.dumps({\n",
    "    \"alpha\": ALPHA,\n",
    "    \"thresholds\": thresholds_blend\n",
    "}, indent=2))\n",
    "print(\"\\nSaved â†’\", CAL_DIR / \"thresholds_blend.json\")\n",
    "\n",
    "# ---- 4) Provide a convenience predictor using the BLEND (keep specialist predictor unchanged) ----\n",
    "def predict_smiles_blend(smiles_list, mode: str = \"fbeta15\", alpha: float = ALPHA):\n",
    "    \"\"\"\n",
    "    Returns list[dict]: per SMILES -> label -> {prob_spec, prob_shared, prob_blend, decision}\n",
    "    \"\"\"\n",
    "    assert mode in (\"f1\",\"fbeta15\")\n",
    "    # fused features from shared encoders (desc branch is already wired)\n",
    "    fused = fused_from_smiles(smiles_list)  # (B,768)\n",
    "    out = []\n",
    "    X = fused  # torch Tensor\n",
    "    with torch.no_grad():\n",
    "        logits_shared = v7_shared.shared_head(X).detach().cpu().numpy()\n",
    "    for i in range(X.size(0)):\n",
    "        row = {}\n",
    "        xi = X[i:i+1]\n",
    "        for j, lbl in enumerate(LABEL_NAMES):\n",
    "            # specialist\n",
    "            with torch.no_grad():\n",
    "                logit_spec = HEADS[lbl](xi).item()\n",
    "            T_spec   = max(float(temps.get(lbl, 1.0)), 1e-3)\n",
    "            p_spec   = 1. / (1. + math.e**(-logit_spec / T_spec))\n",
    "            # shared\n",
    "            T_shared = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "            logit_sh = logits_shared[i, j]\n",
    "            p_shared = 1. / (1. + math.e**(-logit_sh   / T_shared))\n",
    "            # blend\n",
    "            p_blend = alpha * p_spec + (1-alpha) * p_shared\n",
    "            # threshold (use blended thresholds we just computed)\n",
    "            th = thresholds_blend[lbl][\"th_fbeta15\"] if mode==\"fbeta15\" else thresholds_blend[lbl][\"th_f1\"]\n",
    "            row[lbl] = {\n",
    "                \"prob_spec\": float(p_spec),\n",
    "                \"prob_shared\": float(p_shared),\n",
    "                \"prob_blend\": float(p_blend),\n",
    "                \"decision\": bool(p_blend >= float(th)),\n",
    "            }\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "print(\"\\nâœ… Blend ready: use predict_smiles_blend([...], mode='fbeta15' or 'f1').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13b4fc1",
   "metadata": {},
   "source": [
    "### 3: Evaluate on test set & export CSV (choose specialist or blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5492de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved: v7\\results\\inference\\predictions_test_blend_fbeta15.csv\n",
      "\n",
      "Summary (test):\n",
      "{\n",
      "  \"mode\": \"blend\",\n",
      "  \"threshold_mode\": \"fbeta15\",\n",
      "  \"macro_pr_auc\": 0.3208,\n",
      "  \"micro_precision\": 0.2079,\n",
      "  \"micro_recall\": 0.5734,\n",
      "  \"micro_f1\": 0.3052\n",
      "}\n",
      "Per-label AP saved in report JSON.\n"
     ]
    }
   ],
   "source": [
    "# Phase 5 â€” Cell 3: Test export + quick metrics\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "BASE       = Path(\"v7\")\n",
    "PREP_DIR   = BASE / \"data\" / \"prepared\"\n",
    "FUSED_DIR  = BASE / \"data\" / \"fused\"\n",
    "RESULTS_DIR= BASE / \"results\" / \"inference\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CAL_DIR    = BASE / \"model\" / \"calibration\"\n",
    "\n",
    "# Choose which predictor to use:\n",
    "USE_BLEND = True     # True â†’ use predict_smiles_blend; False â†’ use specialist-only predict_smiles\n",
    "MODE      = \"fbeta15\"  # \"fbeta15\" or \"f1\"\n",
    "\n",
    "# Load test blobs\n",
    "blob = np.load(PREP_DIR / \"test.npz\", allow_pickle=True)\n",
    "smiles = [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "Yte    = blob[\"Y\"].astype(np.float32)\n",
    "Mte    = blob[\"y_missing_mask\"].astype(bool)\n",
    "\n",
    "# Also load fused for test to speed shared head for blend\n",
    "Xte_fused = np.load(FUSED_DIR / \"test_fused.npy\") if (FUSED_DIR / \"test_fused.npy\").exists() else None\n",
    "\n",
    "# Ensure thresholds for selected path\n",
    "if USE_BLEND:\n",
    "    data = json.loads((CAL_DIR / \"thresholds_blend.json\").read_text())\n",
    "    thresholds_blend = data[\"thresholds\"]\n",
    "else:\n",
    "    thresholds_spec = json.loads((CAL_DIR / \"thresholds.json\").read_text())\n",
    "\n",
    "rows = []\n",
    "probs_mat = np.zeros((len(smiles), len(LABEL_NAMES)), dtype=np.float32)\n",
    "\n",
    "if USE_BLEND:\n",
    "    # Compute via blend predictor\n",
    "    preds = predict_smiles_blend(smiles, mode=MODE)\n",
    "    for i, (smi, rec) in enumerate(zip(smiles, preds)):\n",
    "        row = {\"smiles\": smi}\n",
    "        for j, lbl in enumerate(LABEL_NAMES):\n",
    "            p = rec[lbl][\"prob_blend\"]\n",
    "            d = int(rec[lbl][\"decision\"])\n",
    "            row[f\"{lbl}_prob\"] = p\n",
    "            row[f\"{lbl}_pred\"] = d\n",
    "            probs_mat[i, j] = p\n",
    "        rows.append(row)\n",
    "    out_csv = RESULTS_DIR / f\"predictions_test_blend_{MODE}.csv\"\n",
    "else:\n",
    "    # Specialist-only\n",
    "    preds = predict_smiles(smiles, threshold_mode=MODE)\n",
    "    for i, (smi, rec) in enumerate(zip(smiles, preds)):\n",
    "        row = {\"smiles\": smi}\n",
    "        for j, lbl in enumerate(LABEL_NAMES):\n",
    "            p = rec[lbl][\"prob_cal\"]\n",
    "            d = int(rec[lbl][\"decision\"])\n",
    "            row[f\"{lbl}_prob\"] = p\n",
    "            row[f\"{lbl}_pred\"] = d\n",
    "            probs_mat[i, j] = p\n",
    "        rows.append(row)\n",
    "    out_csv = RESULTS_DIR / f\"predictions_test_specialist_{MODE}.csv\"\n",
    "\n",
    "pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "print(\"âœ… Saved:\", out_csv)\n",
    "\n",
    "# ---- Tiny metrics (test) ----\n",
    "per_label_ap = {}\n",
    "for j, lbl in enumerate(LABEL_NAMES):\n",
    "    valid = ~Mte[:, j]\n",
    "    if valid.sum() == 0 or np.all(Yte[valid, j] == Yte[valid, j][0]):\n",
    "        per_label_ap[lbl] = float(\"nan\"); continue\n",
    "    try:\n",
    "        per_label_ap[lbl] = float(average_precision_score(Yte[valid, j], probs_mat[valid, j]))\n",
    "    except Exception:\n",
    "        per_label_ap[lbl] = float(\"nan\")\n",
    "\n",
    "macro_pr = float(np.nanmean([v for v in per_label_ap.values()]))\n",
    "\n",
    "# micro P/R/F1 using chosen thresholds\n",
    "tp = fp = fn = 0\n",
    "for i in range(len(smiles)):\n",
    "    for j, lbl in enumerate(LABEL_NAMES):\n",
    "        if Mte[i, j]: \n",
    "            continue\n",
    "        truth = int(Yte[i, j])\n",
    "        pred  = rows[i][f\"{lbl}_pred\"]\n",
    "        tp += int(pred == 1 and truth == 1)\n",
    "        fp += int(pred == 1 and truth == 0)\n",
    "        fn += int(pred == 0 and truth == 1)\n",
    "\n",
    "prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "\n",
    "report = {\n",
    "    \"mode\": (\"blend\" if USE_BLEND else \"specialist\"),\n",
    "    \"threshold_mode\": MODE,\n",
    "    \"macro_pr_auc\": macro_pr,\n",
    "    \"micro_precision\": prec,\n",
    "    \"micro_recall\": rec,\n",
    "    \"micro_f1\": f1,\n",
    "    \"per_label_ap\": per_label_ap\n",
    "}\n",
    "report_path = RESULTS_DIR / f\"test_report_{'blend' if USE_BLEND else 'specialist'}_{MODE}.json\"\n",
    "report_path.write_text(json.dumps(report, indent=2))\n",
    "print(\"\\nSummary (test):\")\n",
    "print(json.dumps({k: (round(v,4) if isinstance(v, float) else v) for k,v in report.items() if k!='per_label_ap'}, indent=2))\n",
    "print(\"Per-label AP saved in report JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58a0540",
   "metadata": {},
   "source": [
    "### 4: test reg after cell 2& 3 (gave very strong results!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7d85fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Blend test rig ready. Example:\n"
     ]
    }
   ],
   "source": [
    "# === V7: Single-SMILES/SMARTS Test Rig (BLENDED: specialist + shared) ===\n",
    "# Uses:\n",
    "#   v7/model/checkpoints/shared/best.pt\n",
    "#   v7/model/ensembles/<label>/seed*/best.pt\n",
    "#   v7/model/calibration/temps.json           (specialist temps)\n",
    "#   v7/model/calibration/temps_shared.json    (shared temps)\n",
    "#   v7/model/calibration/thresholds_blend.json (alpha + per-label thresholds)\n",
    "\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "BASE       = Path(\"v7\")\n",
    "PREP_DIR   = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR   = BASE / \"data\" / \"descriptors\"\n",
    "MODEL_DIR  = BASE / \"model\"\n",
    "CKPT_BEST  = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "ENS_DIR    = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR    = MODEL_DIR / \"calibration\"\n",
    "\n",
    "assert CKPT_BEST.exists(), f\"Missing shared checkpoint: {CKPT_BEST}\"\n",
    "assert (PREP_DIR / \"dataset_manifest.json\").exists(), \"Missing dataset manifest.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Labels & calibration artifacts ---\n",
    "ds_manifest = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABEL_NAMES: List[str] = ds_manifest[\"labels\"]\n",
    "DESC_IN_DIM = ds_manifest[\"n_features\"]  # 208\n",
    "\n",
    "temps_spec    = json.loads((CAL_DIR / \"temps.json\").read_text())           # specialist\n",
    "temps_shared  = json.loads((CAL_DIR / \"temps_shared.json\").read_text())    # shared\n",
    "blend_payload = json.loads((CAL_DIR / \"thresholds_blend.json\").read_text())\n",
    "ALPHA         = float(blend_payload.get(\"alpha\", 0.8))\n",
    "thr_blend     = blend_payload[\"thresholds\"]  # label -> {th_f1, th_fbeta15, ap_val}\n",
    "\n",
    "# --- Text encoder (ChemBERTa) ---\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "class ChemBERTaEncoder(nn.Module):\n",
    "    def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "        self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "        self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "        self.ln = nn.LayerNorm(fusion_dim)\n",
    "    def forward(self, smiles_list: List[str], max_length=256, add_special_tokens=True):\n",
    "        enc = self.tokenizer(list(smiles_list), padding=True, truncation=True,\n",
    "                             max_length=max_length, add_special_tokens=add_special_tokens,\n",
    "                             return_tensors=\"pt\")\n",
    "        input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,L,H)\n",
    "        toks = self.ln(self.proj(out))  # (B,L,256)\n",
    "        return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "# --- Graph encoder (names matched to checkpoint) ---\n",
    "from rdkit import Chem\n",
    "ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "\n",
    "def _one_hot(v, choices):\n",
    "    z = [0]*len(choices)\n",
    "    if v in choices: z[choices.index(v)] = 1\n",
    "    return z\n",
    "\n",
    "def _bucket_oh(v, lo, hi):\n",
    "    buckets = list(range(lo, hi+1))\n",
    "    o = [0]*(len(buckets)+1)\n",
    "    idx = v - lo\n",
    "    o[idx if 0 <= idx < len(buckets) else -1] = 1\n",
    "    return o\n",
    "\n",
    "def _atom_feat(atom):\n",
    "    hybs = [Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "    chir = [Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "    sym = atom.GetSymbol()\n",
    "    feat = _one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST+[\"other\"])\n",
    "    feat += _bucket_oh(atom.GetDegree(), 0, 5)\n",
    "    feat += _bucket_oh(atom.GetFormalCharge(), -2, 2)\n",
    "    feat += (_one_hot(atom.GetHybridization(), hybs)+[0])\n",
    "    feat += [int(atom.GetIsAromatic())]\n",
    "    feat += [int(atom.IsInRing())]\n",
    "    feat += _one_hot(atom.GetChiralTag(), chir)\n",
    "    feat += _bucket_oh(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)\n",
    "    feat += _bucket_oh(atom.GetTotalValence(), 0, 5)\n",
    "    feat += [atom.GetMass()/200.0]\n",
    "    return feat  # ~51 dims\n",
    "\n",
    "def _smiles_to_graph(smi, max_nodes=128):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None or mol.GetNumAtoms() == 0:\n",
    "        return np.zeros((0,0), dtype=np.float32), np.zeros((0,0), dtype=np.float32)\n",
    "    feats = [_atom_feat(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "    x = np.asarray(feats, dtype=np.float32)\n",
    "    N = mol.GetNumAtoms()\n",
    "    adj = np.zeros((N, N), dtype=np.float32)\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        adj[i, j] = 1.0; adj[j, i] = 1.0\n",
    "    if N > max_nodes:\n",
    "        x = x[:max_nodes]; adj = adj[:max_nodes, :max_nodes]\n",
    "    return x, adj\n",
    "\n",
    "def _collate_graphs(smiles_batch, max_nodes=128):\n",
    "    graphs = [_smiles_to_graph(s) for s in smiles_batch]\n",
    "    Nmax = max([g[0].shape[0] for g in graphs] + [1])\n",
    "    Fnode = graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "    B = len(graphs)\n",
    "    X = np.zeros((B, Nmax, Fnode), dtype=np.float32)\n",
    "    A = np.zeros((B, Nmax, Nmax), dtype=np.float32)\n",
    "    M = np.zeros((B, Nmax), dtype=np.int64)\n",
    "    for i, (x, a) in enumerate(graphs):\n",
    "        n = x.shape[0]\n",
    "        if n == 0: continue\n",
    "        X[i, :n, :] = x\n",
    "        A[i, :n, :n] = a\n",
    "        M[i, :n] = 1\n",
    "    return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, h=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "        self.mlp = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "    def forward(self, x, adj, mask):\n",
    "        out = (1.0 + self.eps) * x + torch.matmul(adj, x)\n",
    "        out = self.mlp(out)\n",
    "        return out * mask.unsqueeze(-1).to(out.dtype)\n",
    "\n",
    "class GraphGINEncoder(nn.Module):\n",
    "    def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(nn.Linear(node_in_dim, hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden_dim, p) for _ in range(n_layers)])\n",
    "        self.out_ln = nn.LayerNorm(hidden_dim)  # name matches checkpoint\n",
    "    def forward(self, smiles_list: List[str], max_nodes=128):\n",
    "        X, A, M = _collate_graphs(smiles_list, max_nodes=max_nodes)\n",
    "        h = self.inp(X)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, A, M)\n",
    "        return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "# --- Fusion & heads ---\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "    denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "    return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "        self.ln  = nn.LayerNorm(dim)\n",
    "        self.do  = nn.Dropout(p)\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "        Q = text_tokens.transpose(0,1)   # (L,B,D)\n",
    "        K = graph_nodes.transpose(0,1)   # (N,B,D)\n",
    "        V = graph_nodes.transpose(0,1)\n",
    "        kpm = (graph_mask == 0)          # (B,N)\n",
    "        attn, _ = self.mha(Q, K, V, key_padding_mask=kpm)\n",
    "        attn = attn.transpose(0,1)       # (B,L,D)\n",
    "        return self.ln(text_tokens + self.do(attn))\n",
    "\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim), nn.GELU(), nn.Dropout(p)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    # name 'mlp' matches checkpoint ('shared_head.mlp.*')\n",
    "    def __init__(self, dim=256, n_labels=12, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, fused_vec): return self.mlp(fused_vec)\n",
    "\n",
    "class V7FusionModel(nn.Module):\n",
    "    def __init__(self, text_encoder, graph_encoder, desc_in_dim=208, dim=256, n_labels=12, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.text_encoder=text_encoder\n",
    "        self.graph_encoder=graph_encoder\n",
    "        self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "        self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "        self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "    def forward(self, smiles_list, desc_feats):\n",
    "        tt, tm = self.text_encoder(smiles_list, max_length=256)\n",
    "        gn, gm = self.graph_encoder(smiles_list, max_nodes=128)\n",
    "        tta = self.cross(tt.to(device), tm.to(device), gn.to(device), gm.to(device))\n",
    "        de  = self.desc_mlp(desc_feats.to(device))\n",
    "        text_pool  = masked_mean(tta, tm.to(device), 1)\n",
    "        graph_pool = masked_mean(gn.to(device),  gm.to(device), 1)\n",
    "        fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "        logits = self.shared_head(fused)\n",
    "        return logits, fused\n",
    "\n",
    "# Build model & load checkpoint\n",
    "text_encoder = ChemBERTaEncoder().to(device)\n",
    "graph_encoder= GraphGINEncoder().to(device)\n",
    "v7_shared    = V7FusionModel(text_encoder, graph_encoder, desc_in_dim=DESC_IN_DIM, n_labels=len(LABEL_NAMES)).to(device)\n",
    "ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "v7_shared.eval()\n",
    "\n",
    "# Specialist heads (same as trained)\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _load_best_head(label: str) -> nn.Module:\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if mfile.exists():\n",
    "            try:\n",
    "                ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "                cands.append((ap, sd))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not cands: raise FileNotFoundError(f\"No trained heads for label {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(ck[\"model\"], strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS: Dict[str, nn.Module] = {lbl: _load_best_head(lbl) for lbl in LABEL_NAMES}\n",
    "\n",
    "# Descriptors for ad-hoc inputs: standardized zeros (keeps it simple & robust)\n",
    "def prepare_desc_matrix(smiles_list: List[str]) -> torch.Tensor:\n",
    "    Z = np.zeros((len(smiles_list), DESC_IN_DIM), dtype=np.float32)\n",
    "    return torch.tensor(Z, dtype=torch.float32, device=device)\n",
    "\n",
    "# Normalize SMARTSâ†’SMILES if needed\n",
    "def normalize_smiles_or_smarts(s: str) -> str:\n",
    "    if not isinstance(s, str): s = str(s)\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol: return Chem.MolToSmiles(mol)\n",
    "    q = Chem.MolFromSmarts(s)\n",
    "    if q:\n",
    "        try:\n",
    "            smi = Chem.MolToSmiles(q)\n",
    "            return smi if smi else s\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "@torch.no_grad()\n",
    "def fused_from_smiles(smiles_list: List[str]) -> torch.Tensor:\n",
    "    smiles_list = [normalize_smiles_or_smarts(s) for s in smiles_list]\n",
    "    desc = prepare_desc_matrix(smiles_list)\n",
    "    logits_sh, fused = v7_shared(smiles_list, desc)  # logits not used here directly\n",
    "    return fused  # (B,768)\n",
    "\n",
    "def predict_one_blend(smi: str, mode: str = \"fbeta15\", topk: int = 5):\n",
    "    \"\"\"\n",
    "    Blended prediction for one SMILES/SMARTS using:\n",
    "      prob_blend = alpha*P_spec + (1-alpha)*P_shared\n",
    "    Thresholds taken from thresholds_blend.json for chosen mode (\"f1\" or \"fbeta15\").\n",
    "    Prints a clean summary and returns a dict[label]->details.\n",
    "    \"\"\"\n",
    "    assert mode in (\"f1\",\"fbeta15\")\n",
    "    fused = fused_from_smiles([smi])\n",
    "    x = fused[0:1]\n",
    "\n",
    "    # Shared logits and calibrated probs\n",
    "    with torch.no_grad():\n",
    "        logits_shared = v7_shared.shared_head(x).detach().cpu().numpy()[0]  # (12,)\n",
    "\n",
    "    rec = {}\n",
    "    for j, lbl in enumerate(LABEL_NAMES):\n",
    "        # Specialist prob (with its temperature)\n",
    "        with torch.no_grad():\n",
    "            logit_spec = HEADS[lbl](x).item()\n",
    "        T_spec   = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "        p_spec   = 1. / (1. + math.e**(-logit_spec / T_spec))\n",
    "\n",
    "        # Shared prob (with shared temperature)\n",
    "        T_shared = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "        p_shared = 1. / (1. + math.e**(-float(logits_shared[j]) / T_shared))\n",
    "\n",
    "        # Blend\n",
    "        p_blend = ALPHA * p_spec + (1.0 - ALPHA) * p_shared\n",
    "\n",
    "        # Threshold\n",
    "        th = thr_blend[lbl][\"th_fbeta15\"] if mode==\"fbeta15\" else thr_blend[lbl][\"th_f1\"]\n",
    "        rec[lbl] = {\n",
    "            \"prob_spec\": float(p_spec),\n",
    "            \"prob_shared\": float(p_shared),\n",
    "            \"prob_blend\": float(p_blend),\n",
    "            \"threshold\": float(th),\n",
    "            \"decision\": bool(p_blend >= float(th)),\n",
    "        }\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"\\nSMILES/SMARTS:\", smi, f\"(alpha={ALPHA:.2f}, mode={mode})\")\n",
    "    top = sorted([(lbl, d[\"prob_blend\"], d[\"decision\"]) for lbl, d in rec.items()],\n",
    "                 key=lambda z: z[1], reverse=True)[:topk]\n",
    "    for lbl, p, dec in top:\n",
    "        th = rec[lbl][\"threshold\"]\n",
    "        print(f\"  {lbl:12s}  prob_blend={p:.3f}  th={th:.3f}  â†’ pred={int(dec)}\")\n",
    "    pos = [lbl for lbl, d in rec.items() if d[\"decision\"]]\n",
    "    print(\"  Positives:\", (\", \".join(sorted(pos)) if pos else \"none\"))\n",
    "    return rec\n",
    "\n",
    "print(\"âœ… Blend test rig ready. Example:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e349be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMILES/SMARTS: O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1 (alpha=0.80, mode=fbeta15)\n",
      "  NR-AhR        prob_blend=0.700  th=0.642  â†’ pred=1\n",
      "  SR-MMP        prob_blend=0.680  th=0.589  â†’ pred=1\n",
      "  SR-ARE        prob_blend=0.588  th=0.528  â†’ pred=1\n",
      "  NR-ER         prob_blend=0.556  th=0.480  â†’ pred=1\n",
      "  SR-p53        prob_blend=0.551  th=0.478  â†’ pred=1\n",
      "  NR-ER-LBD     prob_blend=0.499  th=0.589  â†’ pred=0\n",
      "  NR-Aromatase  prob_blend=0.498  th=0.474  â†’ pred=1\n",
      "  SR-ATAD5      prob_blend=0.479  th=0.483  â†’ pred=0\n",
      "  NR-PPAR-gamma  prob_blend=0.478  th=0.427  â†’ pred=1\n",
      "  SR-HSE        prob_blend=0.460  th=0.459  â†’ pred=1\n",
      "  NR-AR         prob_blend=0.432  th=0.653  â†’ pred=0\n",
      "  NR-AR-LBD     prob_blend=0.423  th=0.621  â†’ pred=0\n",
      "  Positives: NR-AhR, NR-Aromatase, NR-ER, NR-PPAR-gamma, SR-ARE, SR-HSE, SR-MMP, SR-p53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NR-AR': {'prob_spec': 0.5297900819654926,\n",
       "  'prob_shared': 0.039244673619722704,\n",
       "  'prob_blend': 0.43168100029633866,\n",
       "  'threshold': 0.653282642364502,\n",
       "  'decision': False},\n",
       " 'NR-AR-LBD': {'prob_spec': 0.527085290472785,\n",
       "  'prob_shared': 0.007392770345504927,\n",
       "  'prob_blend': 0.42314678644732895,\n",
       "  'threshold': 0.6206690669059753,\n",
       "  'decision': False},\n",
       " 'NR-AhR': {'prob_spec': 0.6696848171077073,\n",
       "  'prob_shared': 0.8209122313828833,\n",
       "  'prob_blend': 0.6999302999627425,\n",
       "  'threshold': 0.6417197585105896,\n",
       "  'decision': True},\n",
       " 'NR-Aromatase': {'prob_spec': 0.5691290816416109,\n",
       "  'prob_shared': 0.21573499976865418,\n",
       "  'prob_blend': 0.4984502652670196,\n",
       "  'threshold': 0.4737248420715332,\n",
       "  'decision': True},\n",
       " 'NR-ER': {'prob_spec': 0.5319725845509377,\n",
       "  'prob_shared': 0.6530290641575132,\n",
       "  'prob_blend': 0.5561838804722528,\n",
       "  'threshold': 0.4802268147468567,\n",
       "  'decision': True},\n",
       " 'NR-ER-LBD': {'prob_spec': 0.5796462377733185,\n",
       "  'prob_shared': 0.17392469045136985,\n",
       "  'prob_blend': 0.49850192830892875,\n",
       "  'threshold': 0.5893745422363281,\n",
       "  'decision': False},\n",
       " 'NR-PPAR-gamma': {'prob_spec': 0.5515514734433192,\n",
       "  'prob_shared': 0.1855413765345053,\n",
       "  'prob_blend': 0.4783494540615565,\n",
       "  'threshold': 0.42748475074768066,\n",
       "  'decision': True},\n",
       " 'SR-ARE': {'prob_spec': 0.5481504678492859,\n",
       "  'prob_shared': 0.7475649174003929,\n",
       "  'prob_blend': 0.5880333577595073,\n",
       "  'threshold': 0.5278913378715515,\n",
       "  'decision': True},\n",
       " 'SR-ATAD5': {'prob_spec': 0.5547528009570648,\n",
       "  'prob_shared': 0.174235174637529,\n",
       "  'prob_blend': 0.4786492756931577,\n",
       "  'threshold': 0.4825417995452881,\n",
       "  'decision': False},\n",
       " 'SR-HSE': {'prob_spec': 0.5444436734627249,\n",
       "  'prob_shared': 0.12223593661546725,\n",
       "  'prob_blend': 0.4600021260932734,\n",
       "  'threshold': 0.4591193199157715,\n",
       "  'decision': True},\n",
       " 'SR-MMP': {'prob_spec': 0.6408550104292131,\n",
       "  'prob_shared': 0.8353312082985317,\n",
       "  'prob_blend': 0.6797502500030768,\n",
       "  'threshold': 0.589274525642395,\n",
       "  'decision': True},\n",
       " 'SR-p53': {'prob_spec': 0.560059474096727,\n",
       "  'prob_shared': 0.5128437890040615,\n",
       "  'prob_blend': 0.5506163370781939,\n",
       "  'threshold': 0.477995365858078,\n",
       "  'decision': True}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_one_blend(\"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\", mode=\"fbeta15\", topk=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9958ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMILES/SMARTS: O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1 (alpha=0.80, mode=f1)\n",
      "  NR-AhR        prob_blend=0.700  th=0.709  â†’ pred=0\n",
      "  SR-MMP        prob_blend=0.680  th=0.589  â†’ pred=1\n",
      "  SR-ARE        prob_blend=0.588  th=0.528  â†’ pred=1\n",
      "  NR-ER         prob_blend=0.556  th=0.547  â†’ pred=1\n",
      "  SR-p53        prob_blend=0.551  th=0.513  â†’ pred=1\n",
      "  NR-ER-LBD     prob_blend=0.499  th=0.589  â†’ pred=0\n",
      "  NR-Aromatase  prob_blend=0.498  th=0.564  â†’ pred=0\n",
      "  SR-ATAD5      prob_blend=0.479  th=0.483  â†’ pred=0\n",
      "  NR-PPAR-gamma  prob_blend=0.478  th=0.441  â†’ pred=1\n",
      "  SR-HSE        prob_blend=0.460  th=0.472  â†’ pred=0\n",
      "  NR-AR         prob_blend=0.432  th=0.653  â†’ pred=0\n",
      "  NR-AR-LBD     prob_blend=0.423  th=0.621  â†’ pred=0\n",
      "  Positives: NR-ER, NR-PPAR-gamma, SR-ARE, SR-MMP, SR-p53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NR-AR': {'prob_spec': 0.5297900819654926,\n",
       "  'prob_shared': 0.039244673619722704,\n",
       "  'prob_blend': 0.43168100029633866,\n",
       "  'threshold': 0.653282642364502,\n",
       "  'decision': False},\n",
       " 'NR-AR-LBD': {'prob_spec': 0.527085290472785,\n",
       "  'prob_shared': 0.007392770345504927,\n",
       "  'prob_blend': 0.42314678644732895,\n",
       "  'threshold': 0.6206690669059753,\n",
       "  'decision': False},\n",
       " 'NR-AhR': {'prob_spec': 0.6696848171077073,\n",
       "  'prob_shared': 0.8209122313828833,\n",
       "  'prob_blend': 0.6999302999627425,\n",
       "  'threshold': 0.7087583541870117,\n",
       "  'decision': False},\n",
       " 'NR-Aromatase': {'prob_spec': 0.5691290816416109,\n",
       "  'prob_shared': 0.21573499976865418,\n",
       "  'prob_blend': 0.4984502652670196,\n",
       "  'threshold': 0.5641032457351685,\n",
       "  'decision': False},\n",
       " 'NR-ER': {'prob_spec': 0.5319725845509377,\n",
       "  'prob_shared': 0.6530290641575132,\n",
       "  'prob_blend': 0.5561838804722528,\n",
       "  'threshold': 0.547207772731781,\n",
       "  'decision': True},\n",
       " 'NR-ER-LBD': {'prob_spec': 0.5796462377733185,\n",
       "  'prob_shared': 0.17392469045136985,\n",
       "  'prob_blend': 0.49850192830892875,\n",
       "  'threshold': 0.5893745422363281,\n",
       "  'decision': False},\n",
       " 'NR-PPAR-gamma': {'prob_spec': 0.5515514734433192,\n",
       "  'prob_shared': 0.1855413765345053,\n",
       "  'prob_blend': 0.4783494540615565,\n",
       "  'threshold': 0.4406646490097046,\n",
       "  'decision': True},\n",
       " 'SR-ARE': {'prob_spec': 0.5481504678492859,\n",
       "  'prob_shared': 0.7475649174003929,\n",
       "  'prob_blend': 0.5880333577595073,\n",
       "  'threshold': 0.5278913378715515,\n",
       "  'decision': True},\n",
       " 'SR-ATAD5': {'prob_spec': 0.5547528009570648,\n",
       "  'prob_shared': 0.174235174637529,\n",
       "  'prob_blend': 0.4786492756931577,\n",
       "  'threshold': 0.4825417995452881,\n",
       "  'decision': False},\n",
       " 'SR-HSE': {'prob_spec': 0.5444436734627249,\n",
       "  'prob_shared': 0.12223593661546725,\n",
       "  'prob_blend': 0.4600021260932734,\n",
       "  'threshold': 0.47199299931526184,\n",
       "  'decision': False},\n",
       " 'SR-MMP': {'prob_spec': 0.6408550104292131,\n",
       "  'prob_shared': 0.8353312082985317,\n",
       "  'prob_blend': 0.6797502500030768,\n",
       "  'threshold': 0.589274525642395,\n",
       "  'decision': True},\n",
       " 'SR-p53': {'prob_spec': 0.560059474096727,\n",
       "  'prob_shared': 0.5128437890040615,\n",
       "  'prob_blend': 0.5506163370781939,\n",
       "  'threshold': 0.5132721066474915,\n",
       "  'decision': True}}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "predict_one_blend(\"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\", mode=\"f1\", topk=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48d4411",
   "metadata": {},
   "source": [
    "## phase 6 (Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefe04b8",
   "metadata": {},
   "source": [
    "### 1: Ground truth and fused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734fcf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rebuild] v7\\data\\fused\\test_fused.npy not found â†’ recomputing test fused features...\n",
      "[Rebuild] Saved â†’ v7\\data\\fused\\test_fused.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\1952192011.py:373: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x): return 1.0/(1.0+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Evaluation complete.\n",
      " Per-label CSV  â†’ v7\\eval\\per_label_metrics.csv\n",
      " Summary JSON   â†’ v7\\eval\\summary.json\n",
      " PR curves      â†’ v7\\eval\\plots\\pr\n",
      " Reliability    â†’ v7\\eval\\plots\\reliability\n",
      "\n",
      "Global (test):\n",
      "  eval_mode: blend\n",
      "  threshold_mode: fbeta15\n",
      "  n_test: 783\n",
      "  macro_pr_auc: 0.2179368491745162\n",
      "  macro_roc_auc: 0.7544820729548566\n",
      "  micro_precision: 0.2006872852233677\n",
      "  micro_recall: 0.5793650793650794\n",
      "  micro_f1: 0.29811128126595204\n",
      "  avg_true_cardinality: 0.6436781609195402\n",
      "  avg_pred_cardinality: 1.8582375478927202\n",
      "  cardinality_error: 1.21455938697318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\1952192011.py:515: RuntimeWarning: invalid value encountered in cast\n",
      "  true_bin = Yte.copy().astype(int)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Phase 6 â€” Evaluation (robust to restarts; will rebuild fused features if missing)\n",
    "# =========================\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "# ---- Config ----\n",
    "BASE         = Path(\"v7\")\n",
    "PREP_DIR     = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR     = BASE / \"data\" / \"descriptors\"\n",
    "FUSED_DIR    = BASE / \"data\" / \"fused\"\n",
    "MODEL_DIR    = BASE / \"model\"\n",
    "CAL_DIR      = MODEL_DIR / \"calibration\"\n",
    "ENS_DIR      = MODEL_DIR / \"ensembles\"\n",
    "EVAL_DIR     = BASE / \"eval\"\n",
    "PLOT_PR_DIR  = EVAL_DIR / \"plots\" / \"pr\"\n",
    "PLOT_REL_DIR = EVAL_DIR / \"plots\" / \"reliability\"\n",
    "\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_PR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_REL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FUSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose path: \"specialist\" OR \"blend\"\n",
    "EVAL_MODE   = \"blend\"       # \"specialist\" or \"blend\"\n",
    "THRESH_MODE = \"fbeta15\"     # \"fbeta15\" or \"f1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Load manifest & test blobs ----\n",
    "mani_path = PREP_DIR / \"dataset_manifest.json\"\n",
    "assert mani_path.exists(), f\"Missing manifest: {mani_path}\"\n",
    "mani      = json.loads(mani_path.read_text())\n",
    "LABELS    = mani[\"labels\"]\n",
    "N_LABELS  = len(LABELS)\n",
    "DESC_IN_DIM = int(mani[\"n_features\"])  # 208\n",
    "\n",
    "blob_path = PREP_DIR / \"test.npz\"\n",
    "assert blob_path.exists(), f\"Missing test blob: {blob_path}\"\n",
    "blob  = np.load(blob_path, allow_pickle=True)\n",
    "smiles= [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "Yte   = blob[\"Y\"].astype(np.float32)            # (N, L)\n",
    "Mte   = blob[\"y_missing_mask\"].astype(bool)     # (N, L) True where missing\n",
    "N     = Yte.shape[0]\n",
    "\n",
    "# ---- Helper: rebuild fused features if absent ----------------------------\n",
    "def ensure_fused(split: str = \"test\") -> np.ndarray:\n",
    "    \"\"\"Return fused features for split. If missing, recompute and cache.\"\"\"\n",
    "    path = FUSED_DIR / f\"{split}_fused.npy\"\n",
    "    if path.exists():\n",
    "        return np.load(path).astype(np.float32)\n",
    "\n",
    "    print(f\"[Rebuild] {path} not found â†’ recomputing {split} fused features...\")\n",
    "\n",
    "    # 1) Load descriptor transformer\n",
    "    from joblib import load as joblib_load\n",
    "    imputer = joblib_load(DESC_DIR / \"imputer.joblib\")\n",
    "    scaler  = joblib_load(DESC_DIR / \"scaler.joblib\")\n",
    "\n",
    "    # 2) RDKit descriptor function that matches training order via feature_names.txt\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors as RDDesc\n",
    "\n",
    "    feat_list_path = DESC_DIR / \"feature_names.txt\"\n",
    "    assert feat_list_path.exists(), f\"Missing feature_names.txt at {feat_list_path}\"\n",
    "    feature_names = [ln.strip() for ln in feat_list_path.read_text().splitlines() if ln.strip()]\n",
    "    # Build callables dict for RDKit Descriptors.*\n",
    "    rd_fns = {name: getattr(RDDesc, name, None) for name in feature_names}\n",
    "\n",
    "    def compute_rdkit_descriptors_for_smiles(smiles_list: List[str]) -> np.ndarray:\n",
    "        rows = []\n",
    "        for smi in smiles_list:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                # keep row length consistent; fill with NaN\n",
    "                rows.append([np.nan]*len(feature_names))\n",
    "                continue\n",
    "            vals = []\n",
    "            for name in feature_names:\n",
    "                fn = rd_fns.get(name, None)\n",
    "                if fn is None:\n",
    "                    vals.append(np.nan)\n",
    "                    continue\n",
    "                try:\n",
    "                    v = fn(mol)\n",
    "                except Exception:\n",
    "                    v = np.nan\n",
    "                vals.append(float(v) if (v is not None and np.isfinite(v)) else np.nan)\n",
    "            rows.append(vals)\n",
    "        return np.asarray(rows, dtype=np.float32)\n",
    "\n",
    "    # 3) Build shared model (text+graph encoders + desc MLP) and load checkpoint\n",
    "    CKPT_BEST = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "    assert CKPT_BEST.exists(), f\"Missing shared checkpoint: {CKPT_BEST}\"\n",
    "\n",
    "    # -- Text encoder (ChemBERTa) --\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    class ChemBERTaEncoder(nn.Module):\n",
    "        def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "            super().__init__()\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "            self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "            self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "            self.ln = nn.LayerNorm(fusion_dim)\n",
    "        def forward(self, smiles_list: List[str], max_length=256, add_special_tokens=True):\n",
    "            enc = self.tokenizer(list(smiles_list), padding=True, truncation=True,\n",
    "                                 max_length=max_length, add_special_tokens=add_special_tokens,\n",
    "                                 return_tensors=\"pt\")\n",
    "            input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,L,H)\n",
    "            toks = self.ln(self.proj(out))  # (B,L,256)\n",
    "            return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "    # -- Graph encoder (names match checkpoint) --\n",
    "    ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "    def _one_hot(v, choices):\n",
    "        z = [0]*len(choices)\n",
    "        if v in choices: z[choices.index(v)] = 1\n",
    "        return z\n",
    "    def _bucket_oh(v, lo, hi):\n",
    "        buckets = list(range(lo, hi+1))\n",
    "        o = [0]*(len(buckets)+1)\n",
    "        idx = v - lo\n",
    "        o[idx if 0 <= idx < len(buckets) else -1] = 1\n",
    "        return o\n",
    "    def _atom_feat(atom):\n",
    "        hybs = [Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP,\n",
    "                Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3,\n",
    "                Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "        chir = [Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "                Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "                Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "                Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "        sym = atom.GetSymbol()\n",
    "        feat = _one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST+[\"other\"])\n",
    "        feat += _bucket_oh(atom.GetDegree(), 0, 5)\n",
    "        feat += _bucket_oh(atom.GetFormalCharge(), -2, 2)\n",
    "        feat += (_one_hot(atom.GetHybridization(), hybs)+[0])\n",
    "        feat += [int(atom.GetIsAromatic())]\n",
    "        feat += [int(atom.IsInRing())]\n",
    "        feat += _one_hot(atom.GetChiralTag(), chir)\n",
    "        feat += _bucket_oh(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)\n",
    "        feat += _bucket_oh(atom.GetTotalValence(), 0, 5)\n",
    "        feat += [atom.GetMass()/200.0]\n",
    "        return feat  # ~51 dims\n",
    "    def _smiles_to_graph(smi, max_nodes=128):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None or mol.GetNumAtoms() == 0:\n",
    "            return np.zeros((0,0), dtype=np.float32), np.zeros((0,0), dtype=np.float32)\n",
    "        feats = [_atom_feat(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "        x = np.asarray(feats, dtype=np.float32)\n",
    "        N = mol.GetNumAtoms()\n",
    "        adj = np.zeros((N, N), dtype=np.float32)\n",
    "        for b in mol.GetBonds():\n",
    "            i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "            adj[i, j] = 1.0; adj[j, i] = 1.0\n",
    "        if N > max_nodes:\n",
    "            x = x[:max_nodes]; adj = adj[:max_nodes, :max_nodes]\n",
    "        return x, adj\n",
    "    def _collate_graphs(smiles_batch, max_nodes=128):\n",
    "        graphs = [_smiles_to_graph(s) for s in smiles_batch]\n",
    "        Nmax = max([g[0].shape[0] for g in graphs] + [1])\n",
    "        Fnode = graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "        B = len(graphs)\n",
    "        X = np.zeros((B, Nmax, Fnode), dtype=np.float32)\n",
    "        A = np.zeros((B, Nmax, Nmax), dtype=np.float32)\n",
    "        M = np.zeros((B, Nmax), dtype=np.int64)\n",
    "        for i, (x, a) in enumerate(graphs):\n",
    "            n = x.shape[0]\n",
    "            if n == 0: continue\n",
    "            X[i, :n, :] = x\n",
    "            A[i, :n, :n] = a\n",
    "            M[i, :n] = 1\n",
    "        return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "    class GINLayer(nn.Module):\n",
    "        def __init__(self, h=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "            self.mlp = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "        def forward(self, x, adj, mask):\n",
    "            out = (1.0 + self.eps) * x + torch.matmul(adj, x)\n",
    "            out = self.mlp(out)\n",
    "            return out * mask.unsqueeze(-1).to(out.dtype)\n",
    "\n",
    "    class GraphGINEncoder(nn.Module):\n",
    "        def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.inp = nn.Sequential(nn.Linear(node_in_dim, hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "            self.layers = nn.ModuleList([GINLayer(hidden_dim, p) for _ in range(n_layers)])\n",
    "            self.out_ln = nn.LayerNorm(hidden_dim)  # name matches checkpoint\n",
    "        def forward(self, smiles_list: List[str], max_nodes=128):\n",
    "            X, A, M = _collate_graphs(smiles_list, max_nodes=max_nodes)\n",
    "            h = self.inp(X)\n",
    "            for layer in self.layers:\n",
    "                h = layer(h, A, M)\n",
    "            return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "    # -- Fusion parts --\n",
    "    def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "        mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "        denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "        return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "            self.ln  = nn.LayerNorm(dim)\n",
    "            self.do  = nn.Dropout(p)\n",
    "        def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "            Q = text_tokens.transpose(0,1); K = graph_nodes.transpose(0,1); V = graph_nodes.transpose(0,1)\n",
    "            kpm = (graph_mask == 0)\n",
    "            attn, _ = self.mha(Q, K, V, key_padding_mask=kpm)\n",
    "            attn = attn.transpose(0,1)\n",
    "            return self.ln(text_tokens + self.do(attn))\n",
    "    class DescriptorMLP(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.GELU(), nn.Dropout(p),\n",
    "                nn.Linear(hidden, out_dim), nn.GELU(), nn.Dropout(p)\n",
    "            )\n",
    "        def forward(self, x): return self.net(x)\n",
    "    class FusionClassifier(nn.Module):\n",
    "        # name 'mlp' to match checkpoint\n",
    "        def __init__(self, dim=256, n_labels=N_LABELS, p=0.1):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "                nn.Linear(dim*2, n_labels)\n",
    "            )\n",
    "        def forward(self, fused_vec): return self.mlp(fused_vec)\n",
    "    class V7FusionModel(nn.Module):\n",
    "        def __init__(self, text_encoder, graph_encoder, desc_in_dim=DESC_IN_DIM, dim=256, n_labels=N_LABELS, n_heads=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.text_encoder=text_encoder\n",
    "            self.graph_encoder=graph_encoder\n",
    "            self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "            self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "            self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "        def forward(self, smiles_list, desc_feats, return_fused=False):\n",
    "            tt, tm = self.text_encoder(smiles_list, max_length=256)\n",
    "            gn, gm = self.graph_encoder(smiles_list, max_nodes=128)\n",
    "            tta = self.cross(tt.to(device), tm.to(device), gn.to(device), gm.to(device))\n",
    "            de  = self.desc_mlp(desc_feats.to(device))\n",
    "            text_pool  = masked_mean(tta, tm.to(device), 1)\n",
    "            graph_pool = masked_mean(gn.to(device),  gm.to(device), 1)\n",
    "            fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "            logits = self.shared_head(fused)\n",
    "            return (logits, fused) if return_fused else logits\n",
    "\n",
    "    text_encoder = ChemBERTaEncoder().to(device)\n",
    "    graph_encoder= GraphGINEncoder().to(device)\n",
    "    model        = V7FusionModel(text_encoder, graph_encoder).to(device)\n",
    "    ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Compute descriptors â†’ impute/scale â†’ fused features\n",
    "    X_raw = compute_rdkit_descriptors_for_smiles(smiles)           # (N, 208 with NaNs)\n",
    "    X_imp = imputer.transform(X_raw)\n",
    "    X_std = scaler.transform(X_imp)\n",
    "    desc_t= torch.tensor(X_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    fused_list = []\n",
    "    B = 64\n",
    "    for i in range(0, N, B):\n",
    "        batch_smiles = smiles[i:i+B]\n",
    "        logits, fused = model(batch_smiles, desc_t[i:i+B], return_fused=True)\n",
    "        fused_list.append(fused.detach().cpu().numpy())\n",
    "    fused_all = np.concatenate(fused_list, axis=0).astype(np.float32)\n",
    "    np.save(path, fused_all)\n",
    "    print(f\"[Rebuild] Saved â†’ {path}\")\n",
    "    return fused_all\n",
    "\n",
    "# ---- Get fused test features (rebuild if missing) ----\n",
    "X_fused = ensure_fused(\"test\")\n",
    "X_fused_t = torch.tensor(X_fused, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---- Load calibration/thresholds ----\n",
    "temps_spec = json.loads((CAL_DIR / \"temps.json\").read_text())                # specialist temps\n",
    "if EVAL_MODE == \"specialist\":\n",
    "    thresholds_spec = json.loads((CAL_DIR / \"thresholds.json\").read_text())  # specialist thresholds\n",
    "else:\n",
    "    # Blend\n",
    "    temps_shared   = json.loads((CAL_DIR / \"temps_shared.json\").read_text())\n",
    "    blend_payload  = json.loads((CAL_DIR / \"thresholds_blend.json\").read_text())\n",
    "    ALPHA          = float(blend_payload.get(\"alpha\", 0.8))\n",
    "    thresholds_blend = blend_payload[\"thresholds\"]\n",
    "\n",
    "# ---- Define heads (specialists) ----\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    # choose seed with highest best_ap\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if mfile.exists():\n",
    "            try:\n",
    "                ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "                cands.append((ap, sd))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(ck[\"model\"], strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "\n",
    "# ---- Shared head (for blend only): load just the classifier on fused\n",
    "if EVAL_MODE == \"blend\":\n",
    "    CKPT_BEST = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "    class SharedHeadOnly(nn.Module):\n",
    "        def __init__(self, dim=256, n_labels=N_LABELS, p=0.1):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "                nn.Linear(dim*2, n_labels)\n",
    "            )\n",
    "        def forward(self, fused):\n",
    "            return self.mlp(fused)\n",
    "    shared_head = SharedHeadOnly().to(device)\n",
    "    ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "    sh_state = {k.replace(\"shared_head.\", \"\"): v for k,v in ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "    shared_head.load_state_dict(sh_state, strict=True)\n",
    "    shared_head.eval()\n",
    "\n",
    "# ---- Specialist logits on fused test (fast) ----\n",
    "with torch.no_grad():\n",
    "    spec_logits = torch.zeros((N, N_LABELS), dtype=torch.float32, device=device)\n",
    "    for j, lbl in enumerate(LABELS):\n",
    "        spec_logits[:, j] = HEADS[lbl](X_fused_t)\n",
    "spec_logits = spec_logits.cpu().numpy()\n",
    "\n",
    "# ---- Shared logits on fused test (for blend path) ----\n",
    "if EVAL_MODE == \"blend\":\n",
    "    with torch.no_grad():\n",
    "        shared_logits = shared_head(X_fused_t).cpu().numpy()\n",
    "else:\n",
    "    shared_logits = None\n",
    "\n",
    "# ---- Build probability matrix according to EVAL_MODE ----\n",
    "def sigmoid(x): return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "if EVAL_MODE == \"specialist\":\n",
    "    PROBS = np.zeros_like(spec_logits, dtype=np.float32)\n",
    "    for j, lbl in enumerate(LABELS):\n",
    "        T = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "        PROBS[:, j] = sigmoid(spec_logits[:, j] / T)\n",
    "else:\n",
    "    PROBS = np.zeros_like(spec_logits, dtype=np.float32)\n",
    "    for j, lbl in enumerate(LABELS):\n",
    "        T_spec   = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "        T_shared = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "        p_spec   = sigmoid(spec_logits[:, j]   / T_spec)\n",
    "        p_shared = sigmoid(shared_logits[:, j] / T_shared)\n",
    "        PROBS[:, j] = np.clip(ALPHA * p_spec + (1-ALPHA) * p_shared, 0.0, 1.0)\n",
    "\n",
    "# ---- Helper: ECE & reliability curve ----\n",
    "def reliability_and_ece(y_true, y_prob, n_bins=15):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    bin_ids = np.digitize(y_prob, bins) - 1\n",
    "    bin_acc, bin_conf, bin_count = [], [], []\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        mask = (bin_ids == b)\n",
    "        n = mask.sum()\n",
    "        if n == 0:\n",
    "            bin_acc.append(np.nan); bin_conf.append(np.nan); bin_count.append(0)\n",
    "            continue\n",
    "        p = y_prob[mask]; t = y_true[mask]\n",
    "        acc = t.mean(); conf = p.mean()\n",
    "        bin_acc.append(acc); bin_conf.append(conf); bin_count.append(n)\n",
    "        ece += (n/len(y_true)) * abs(acc - conf)\n",
    "    return (bins, np.array(bin_acc), np.array(bin_conf), np.array(bin_count)), float(ece)\n",
    "\n",
    "# ---- Compute metrics per label & global ----\n",
    "rows = []\n",
    "tp_micro = fp_micro = fn_micro = 0\n",
    "macro_ap_vals = []\n",
    "macro_roc_vals = []\n",
    "\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    valid = ~Mte[:, j]\n",
    "    y = Yte[valid, j].astype(int)\n",
    "    p = PROBS[valid, j]\n",
    "\n",
    "    # AUCs\n",
    "    ap = float(average_precision_score(y, p)) if valid.sum() > 0 else float(\"nan\")\n",
    "    macro_ap_vals.append(ap)\n",
    "    try:\n",
    "        roc = float(roc_auc_score(y, p))\n",
    "    except Exception:\n",
    "        roc = float(\"nan\")\n",
    "    macro_roc_vals.append(roc)\n",
    "\n",
    "    # Operating thresholds (from saved calibration)\n",
    "    if EVAL_MODE == \"specialist\":\n",
    "        th_f1  = float(json.loads((CAL_DIR / \"thresholds.json\").read_text())[lbl][\"th_f1\"])\n",
    "        th_fb  = float(json.loads((CAL_DIR / \"thresholds.json\").read_text())[lbl][\"th_fbeta15\"])\n",
    "    else:\n",
    "        th_f1  = float(thresholds_blend[lbl][\"th_f1\"])\n",
    "        th_fb  = float(thresholds_blend[lbl][\"th_fbeta15\"])\n",
    "\n",
    "    def prf_at_thresh(th):\n",
    "        pred = (p >= th).astype(int)\n",
    "        tp = int(((pred==1) & (y==1)).sum())\n",
    "        fp = int(((pred==1) & (y==0)).sum())\n",
    "        fn = int(((pred==0) & (y==1)).sum())\n",
    "        prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "        rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "        f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "        return tp, fp, fn, prec, rec, f1\n",
    "\n",
    "    tp1, fp1, fn1, pr1, rc1, f1 = prf_at_thresh(th_f1)\n",
    "    tpb, fpb, fnb, prb, rcb, fb = prf_at_thresh(th_fb)\n",
    "\n",
    "    if THRESH_MODE == \"f1\":\n",
    "        tp_micro += tp1; fp_micro += fp1; fn_micro += fn1\n",
    "    else:\n",
    "        tp_micro += tpb; fp_micro += fpb; fn_micro += fnb\n",
    "\n",
    "    # Prevalence\n",
    "    prev = float(y.mean()) if valid.sum() > 0 else float(\"nan\")\n",
    "\n",
    "    # ECE + save reliability plot\n",
    "    (bins, acc, conf, counts), ece = reliability_and_ece(y, p, n_bins=15)\n",
    "    plt.figure()\n",
    "    mask = ~np.isnan(acc)\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    if mask.any():\n",
    "        plt.plot(conf[mask], acc[mask], marker=\"o\")\n",
    "    plt.xlabel(\"Mean predicted probability\"); plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(f\"Reliability: {lbl} (ECE={ece:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_REL_DIR / f\"{lbl}.png\", dpi=160); plt.close()\n",
    "\n",
    "    # PR curve plot\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "    plt.figure()\n",
    "    plt.step(rec, prec, where=\"post\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"PR curve: {lbl} (AP={ap:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_PR_DIR / f\"{lbl}.png\", dpi=160); plt.close()\n",
    "\n",
    "    rows.append({\n",
    "        \"label\": lbl,\n",
    "        \"n_valid\": int(valid.sum()),\n",
    "        \"prevalence\": prev,\n",
    "        \"ap\": ap,\n",
    "        \"roc_auc\": roc,\n",
    "        \"th_f1\": th_f1,\n",
    "        \"prec@f1\": pr1, \"recall@f1\": rc1, \"f1\": f1,\n",
    "        \"tp@f1\": tp1, \"fp@f1\": fp1, \"fn@f1\": fn1,\n",
    "        \"th_fbeta15\": th_fb,\n",
    "        \"prec@fbeta15\": prb, \"recall@fbeta15\": rcb, \"f_beta15\": fb,\n",
    "        \"tp@fbeta15\": tpb, \"fp@fbeta15\": fpb, \"fn@fbeta15\": fnb,\n",
    "        \"ece\": ece\n",
    "    })\n",
    "\n",
    "# ---- Global summaries ----\n",
    "macro_pr_auc  = float(np.nanmean([r[\"ap\"] for r in rows]))\n",
    "macro_roc_auc = float(np.nanmean([r[\"roc_auc\"] for r in rows]))\n",
    "micro_prec = tp_micro/(tp_micro+fp_micro) if (tp_micro+fp_micro)>0 else 0.0\n",
    "micro_rec  = tp_micro/(tp_micro+fn_micro) if (tp_micro+fn_micro)>0 else 0.0\n",
    "micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec)>0 else 0.0\n",
    "\n",
    "# Cardinality (avg #positive labels per sample) â€“ true vs predicted at chosen operating mode\n",
    "if THRESH_MODE == \"f1\":\n",
    "    if EVAL_MODE == \"specialist\":\n",
    "        thobj = json.loads((CAL_DIR / \"thresholds.json\").read_text())\n",
    "        THS = np.array([float(thobj[l][\"th_f1\"]) for l in LABELS], dtype=np.float32)\n",
    "    else:\n",
    "        THS = np.array([float(thresholds_blend[l][\"th_f1\"]) for l in LABELS], dtype=np.float32)\n",
    "else:\n",
    "    if EVAL_MODE == \"specialist\":\n",
    "        thobj = json.loads((CAL_DIR / \"thresholds.json\").read_text())\n",
    "        THS = np.array([float(thobj[l][\"th_fbeta15\"]) for l in LABELS], dtype=np.float32)\n",
    "    else:\n",
    "        THS = np.array([float(thresholds_blend[l][\"th_fbeta15\"]) for l in LABELS], dtype=np.float32)\n",
    "\n",
    "pred_bin = (PROBS >= THS.reshape(1, -1)).astype(int)\n",
    "pred_bin[Mte] = 0\n",
    "true_bin = Yte.copy().astype(int)\n",
    "true_bin[Mte] = 0\n",
    "\n",
    "avg_true_card = float(true_bin.sum(axis=1).mean())\n",
    "avg_pred_card = float(pred_bin.sum(axis=1).mean())\n",
    "card_err      = float(avg_pred_card - avg_true_card)\n",
    "\n",
    "# ---- Save reports ----\n",
    "per_label_df = pd.DataFrame(rows)\n",
    "per_label_csv = EVAL_DIR / \"per_label_metrics.csv\"\n",
    "per_label_df.to_csv(per_label_csv, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"eval_mode\": EVAL_MODE,\n",
    "    \"threshold_mode\": THRESH_MODE,\n",
    "    \"n_test\": int(N),\n",
    "    \"macro_pr_auc\": macro_pr_auc,\n",
    "    \"macro_roc_auc\": macro_roc_auc,\n",
    "    \"micro_precision\": micro_prec,\n",
    "    \"micro_recall\": micro_rec,\n",
    "    \"micro_f1\": micro_f1,\n",
    "    \"avg_true_cardinality\": avg_true_card,\n",
    "    \"avg_pred_cardinality\": avg_pred_card,\n",
    "    \"cardinality_error\": card_err,\n",
    "    \"plots\": {\n",
    "        \"pr_curves_dir\": str(PLOT_PR_DIR),\n",
    "        \"reliability_dir\": str(PLOT_REL_DIR)\n",
    "    },\n",
    "}\n",
    "(EVAL_DIR / \"summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"âœ… Evaluation complete.\")\n",
    "print(f\" Per-label CSV  â†’ {per_label_csv}\")\n",
    "print(f\" Summary JSON   â†’ {EVAL_DIR / 'summary.json'}\")\n",
    "print(f\" PR curves      â†’ {PLOT_PR_DIR}\")\n",
    "print(f\" Reliability    â†’ {PLOT_REL_DIR}\")\n",
    "print(\"\\nGlobal (test):\")\n",
    "for k in [\"eval_mode\",\"threshold_mode\",\"n_test\",\"macro_pr_auc\",\"macro_roc_auc\",\"micro_precision\",\"micro_recall\",\"micro_f1\",\"avg_true_cardinality\",\"avg_pred_cardinality\",\"cardinality_error\"]:\n",
    "    print(f\"  {k}: {summary[k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05563ec",
   "metadata": {},
   "source": [
    "### 2: Specialist vs Blend comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2424ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Loaded specialist heads: 12/12\n",
      "âœ… Saved:\n",
      "  â€¢ v7\\eval\\compare_table.csv\n",
      "  â€¢ v7\\eval\\compare_summary.json\n",
      "\n",
      "Quick view:\n",
      "      path    mode  macro_pr_auc  macro_roc_auc  micro_precision  micro_recall  micro_f1  avg_true_cardinality  avg_pred_cardinality  cardinality_error\n",
      "specialist fbeta15      0.149778       0.709502         0.183659      0.503968  0.269210              0.643678              1.766284           1.122605\n",
      "     blend fbeta15      0.157441       0.727127         0.200687      0.579365  0.298111              0.643678              1.858238           1.214559\n",
      "specialist      f1      0.149778       0.709502         0.199797      0.390873  0.264430              0.643678              1.259259           0.615581\n",
      "     blend      f1      0.157441       0.727127         0.230997      0.464286  0.308504              0.643678              1.293742           0.650064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:263: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  Truth = Yte.astype(int)\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  Truth = Yte.astype(int)\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  Truth = Yte.astype(int)\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  Truth = Yte.astype(int)\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Phase 6 â€” Cell 2: Specialist vs Blend comparison (restart-proof)\n",
    "# =========================\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---- Paths & basic setup ----\n",
    "BASE         = Path(\"v7\")\n",
    "PREP_DIR     = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR     = BASE / \"data\" / \"descriptors\"\n",
    "FUSED_DIR    = BASE / \"data\" / \"fused\"\n",
    "MODEL_DIR    = BASE / \"model\"\n",
    "CAL_DIR      = MODEL_DIR / \"calibration\"\n",
    "ENS_DIR      = MODEL_DIR / \"ensembles\"\n",
    "EVAL_DIR     = BASE / \"eval\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FUSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Manifest & test blobs ----\n",
    "mani_path = PREP_DIR / \"dataset_manifest.json\"\n",
    "assert mani_path.exists(), f\"Missing manifest: {mani_path}\"\n",
    "mani = json.loads(mani_path.read_text())\n",
    "LABELS = mani[\"labels\"]\n",
    "N_LABELS = len(LABELS)\n",
    "DESC_IN_DIM = int(mani[\"n_features\"])\n",
    "\n",
    "blob_path = PREP_DIR / \"test.npz\"\n",
    "assert blob_path.exists(), f\"Missing test blob: {blob_path}\"\n",
    "blob   = np.load(blob_path, allow_pickle=True)\n",
    "smiles = [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "Yte    = blob[\"Y\"].astype(np.float32)\n",
    "Mte    = blob[\"y_missing_mask\"].astype(bool)\n",
    "N      = Yte.shape[0]\n",
    "\n",
    "# ---- Ensure fused features (rebuild if missing) ----\n",
    "def ensure_fused(split=\"test\") -> np.ndarray:\n",
    "    path = FUSED_DIR / f\"{split}_fused.npy\"\n",
    "    if path.exists():\n",
    "        return np.load(path).astype(np.float32)\n",
    "\n",
    "    print(f\"[Rebuild] {path} not found â†’ recomputing {split} fused features...\")\n",
    "\n",
    "    # Load descriptor imputer/scaler\n",
    "    from joblib import load as joblib_load\n",
    "    imp_path = DESC_DIR / \"imputer.joblib\"\n",
    "    scl_path = DESC_DIR / \"scaler.joblib\"\n",
    "    assert imp_path.exists() and scl_path.exists(), \"Missing imputer/scaler joblib files.\"\n",
    "    imputer = joblib_load(imp_path)\n",
    "    scaler  = joblib_load(scl_path)\n",
    "\n",
    "    # Prepare RDKit 208 descriptors in the SAME ORDER as training\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors as RDDesc\n",
    "    feat_names_file = DESC_DIR / \"feature_names.txt\"\n",
    "    assert feat_names_file.exists(), f\"Missing {feat_names_file}\"\n",
    "    feat_names = [ln.strip() for ln in feat_names_file.read_text().splitlines() if ln.strip()]\n",
    "    rd_fns = {name: getattr(RDDesc, name, None) for name in feat_names}\n",
    "\n",
    "    def rdkit_feats(smis: List[str]) -> np.ndarray:\n",
    "        rows = []\n",
    "        for s in smis:\n",
    "            m = Chem.MolFromSmiles(s)\n",
    "            if m is None:\n",
    "                rows.append([np.nan]*len(feat_names)); continue\n",
    "            vals = []\n",
    "            for name in feat_names:\n",
    "                fn = rd_fns.get(name, None)\n",
    "                try:\n",
    "                    v = float(fn(m)) if fn is not None else np.nan\n",
    "                except Exception:\n",
    "                    v = np.nan\n",
    "                vals.append(v if np.isfinite(v) else np.nan)\n",
    "            rows.append(vals)\n",
    "        return np.asarray(rows, dtype=np.float32)\n",
    "\n",
    "    # Build shared fusion model (text+graph+desc) & load checkpoint\n",
    "    CKPT = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "    assert CKPT.exists(), f\"Missing shared checkpoint: {CKPT}\"\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    class ChemBERTaEncoder(nn.Module):\n",
    "        def __init__(self, ckpt=\"seyonec/ChemBERTa-zinc-base-v1\", dim=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.tok = AutoTokenizer.from_pretrained(ckpt)\n",
    "            self.m   = AutoModel.from_pretrained(ckpt)\n",
    "            self.proj= nn.Sequential(nn.Dropout(p), nn.Linear(self.m.config.hidden_size, dim))\n",
    "            self.ln  = nn.LayerNorm(dim)\n",
    "        def forward(self, smis, max_length=256):\n",
    "            enc = self.tok(list(smis), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            out = self.m(input_ids=enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device)).last_hidden_state\n",
    "            return self.ln(self.proj(out)), enc[\"attention_mask\"].to(device, dtype=torch.int32)\n",
    "\n",
    "    ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "    def _one_hot(v, C): z=[0]*len(C); z[C.index(v) if v in C else -1]=1; return z\n",
    "    def _bucket(v, lo, hi):\n",
    "        buckets=list(range(lo,hi+1)); o=[0]*(len(buckets)+1); i=v-lo; o[i if 0<=i<len(buckets) else -1]=1; return o\n",
    "    def _atom_feat(a):\n",
    "        hyb=[Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "        chir=[Chem.rdchem.ChiralType.CHI_UNSPECIFIED, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW, Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "        f=_one_hot(a.GetSymbol(), ATOM_LIST+[\"other\"])\n",
    "        f+=_bucket(a.GetDegree(),0,5)+_bucket(a.GetFormalCharge(),-2,2)+(_one_hot(a.GetHybridization(),hyb)+[0])\n",
    "        f+=[int(a.GetIsAromatic()), int(a.IsInRing())]+_one_hot(a.GetChiralTag(),chir)\n",
    "        f+=_bucket(a.GetTotalNumHs(True),0,4)+_bucket(a.GetTotalValence(),0,5)+[a.GetMass()/200.0]\n",
    "        return f\n",
    "    def _smiles_to_graph(smi, max_nodes=128):\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        if m is None or m.GetNumAtoms()==0: return np.zeros((0,0),np.float32), np.zeros((0,0),np.float32)\n",
    "        X = np.asarray([_atom_feat(m.GetAtomWithIdx(i)) for i in range(m.GetNumAtoms())], np.float32)\n",
    "        N = m.GetNumAtoms(); A = np.zeros((N,N),np.float32)\n",
    "        for b in m.GetBonds(): i,j=b.GetBeginAtomIdx(), b.GetEndAtomIdx(); A[i,j]=A[j,i]=1.0\n",
    "        if N>max_nodes: X=X[:max_nodes]; A=A[:max_nodes,:max_nodes]\n",
    "        return X,A\n",
    "    def _collate(smis, max_nodes=128):\n",
    "        G=[_smiles_to_graph(s) for s in smis]\n",
    "        Nmax=max([g[0].shape[0] for g in G]+[1]); F=G[0][0].shape[1] if G[0][0].size>0 else 51; B=len(G)\n",
    "        X=np.zeros((B,Nmax,F),np.float32); A=np.zeros((B,Nmax,Nmax),np.float32); M=np.zeros((B,Nmax),np.int64)\n",
    "        for i,(x,a) in enumerate(G):\n",
    "            n=x.shape[0]\n",
    "            if n==0: continue\n",
    "            X[i,:n,:]=x; A[i,:n,:n]=a; M[i,:n]=1\n",
    "        return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "    class GINLayer(nn.Module):\n",
    "        def __init__(self,h=256,p=0.1): super().__init__(); self.eps=nn.Parameter(torch.tensor(0.0)); self.mlp=nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "        def forward(self,x,A,M): return self.mlp((1.0+self.eps)*x + torch.matmul(A,x)) * M.unsqueeze(-1).to(x.dtype)\n",
    "    class GraphGINEncoder(nn.Module):\n",
    "        def __init__(self,node_in_dim=51,h=256,L=4,p=0.1): super().__init__(); self.inp=nn.Sequential(nn.Linear(node_in_dim,h), nn.GELU(), nn.Dropout(p)); self.layers=nn.ModuleList([GINLayer(h,p) for _ in range(L)]); self.out_ln=nn.LayerNorm(h)\n",
    "        def forward(self,smis,max_nodes=128):\n",
    "            X,A,M=_collate(smis,max_nodes); h=self.inp(X)\n",
    "            for L in self.layers: h=L(h,A,M)\n",
    "            return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "    def masked_mean(x, m, dim): m=m.to(dtype=x.dtype, device=x.device); denom=m.sum(dim=dim,keepdim=True).clamp(min=1.0); return (x*m.unsqueeze(-1)).sum(dim=dim)/denom\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self,dim=256,heads=4,p=0.1): super().__init__(); self.mha=nn.MultiheadAttention(dim, heads, dropout=p, batch_first=False); self.ln=nn.LayerNorm(dim); self.do=nn.Dropout(p)\n",
    "        def forward(self,T,TM,G,GM): Q=T.transpose(0,1); K=G.transpose(0,1); V=G.transpose(0,1); kpm=(GM==0); A,_=self.mha(Q,K,V,key_padding_mask=kpm); return self.ln(T+self.do(A.transpose(0,1)))\n",
    "    class DescriptorMLP(nn.Module):\n",
    "        def __init__(self,inp,dim=256,p=0.1): super().__init__(); self.net=nn.Sequential(nn.Linear(inp,256), nn.GELU(), nn.Dropout(p), nn.Linear(256,dim), nn.GELU(), nn.Dropout(p))\n",
    "        def forward(self,x): return self.net(x)\n",
    "    class FusionClassifier(nn.Module):\n",
    "        def __init__(self,dim=256,L=N_LABELS,p=0.1): super().__init__(); self.mlp=nn.Sequential(nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p), nn.Linear(dim*2, L))\n",
    "        def forward(self,z): return self.mlp(z)\n",
    "    class V7Shared(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.text=ChemBERTaEncoder()\n",
    "            self.graph=GraphGINEncoder()\n",
    "            self.cross=CrossAttentionBlock()\n",
    "            self.desc=DescriptorMLP(DESC_IN_DIM)\n",
    "            self.head=FusionClassifier()\n",
    "        def forward(self,smis,desc,return_fused=False):\n",
    "            T,TM=self.text(smis,256); G,GM=self.graph(smis,128); Ta=self.cross(T,TM,G,GM); Dz=self.desc(desc)\n",
    "            Tp=masked_mean(Ta,TM,1); Gp=masked_mean(G,GM,1); fused=torch.cat([Tp,Gp,Dz],-1)\n",
    "            logits=self.head(fused); \n",
    "            return (logits, fused) if return_fused else logits\n",
    "\n",
    "    v7 = V7Shared().to(device)\n",
    "    v7.load_state_dict(torch.load(CKPT, map_location=device)[\"model\"], strict=True)\n",
    "    v7.eval()\n",
    "\n",
    "    # descriptors â†’ impute â†’ scale\n",
    "    X_raw = rdkit_feats(smiles)\n",
    "    X_imp = imputer.transform(X_raw)\n",
    "    X_std = scaler.transform(X_imp)\n",
    "    desc_t = torch.tensor(X_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    fused = []\n",
    "    B=64\n",
    "    for i in range(0, N, B):\n",
    "        _, f = v7(smiles[i:i+B], desc_t[i:i+B], return_fused=True)\n",
    "        fused.append(f.detach().cpu().numpy())\n",
    "    fused = np.concatenate(fused, 0).astype(np.float32)\n",
    "    np.save(path, fused)\n",
    "    print(f\"[Rebuild] Saved â†’ {path}\")\n",
    "    return fused\n",
    "\n",
    "X_fused = ensure_fused(\"test\")\n",
    "X_fused_t = torch.tensor(X_fused, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---- Load calibration artifacts ----\n",
    "temps_spec = json.loads((CAL_DIR / \"temps.json\").read_text())            # specialist temps\n",
    "thr_spec   = json.loads((CAL_DIR / \"thresholds.json\").read_text())       # specialist thresholds\n",
    "temps_sh   = json.loads((CAL_DIR / \"temps_shared.json\").read_text())     # shared temps\n",
    "blend      = json.loads((CAL_DIR / \"thresholds_blend.json\").read_text()) # blend thresholds + alpha\n",
    "ALPHA      = float(blend.get(\"alpha\", 0.8))\n",
    "thr_blend  = blend[\"thresholds\"]\n",
    "\n",
    "# ---- Robust specialist head loader (patch handles b1/b2/b3 vs block1/2/3) ----\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _remap_keys_if_needed(state_dict: dict) -> dict:\n",
    "    needs = any(k.startswith((\"b1.\", \"b2.\", \"b3.\")) for k in state_dict.keys())\n",
    "    if not needs: return state_dict\n",
    "    remap = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k2 = k.replace(\"b1.\", \"block1.\").replace(\"b2.\", \"block2.\").replace(\"b3.\", \"block3.\")\n",
    "        remap[k2] = v\n",
    "    return remap\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\"), key=lambda p: p.name):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if not mfile.exists(): continue\n",
    "        try:\n",
    "            ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "            cands.append((ap, sd))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label} under {ENS_DIR/label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    state = _remap_keys_if_needed(ck[\"model\"])\n",
    "    head.load_state_dict(state, strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "print(f\"âœ… Loaded specialist heads: {len(HEADS)}/{len(LABELS)}\")\n",
    "\n",
    "# ---- Shared head (MLP on fused) for blend path ----\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "sh = SharedHeadMLP().to(device)\n",
    "ckpt = torch.load(MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "sh_state = {k.replace(\"shared_head.\", \"\"): v for k,v in ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(sh_state, strict=True)\n",
    "sh.eval()\n",
    "\n",
    "# ---- Build probability matrices for specialist & blend ----\n",
    "with torch.no_grad():\n",
    "    spec_logits = torch.stack([HEADS[lbl](X_fused_t) for lbl in LABELS], dim=1).cpu().numpy()  # (N,L)\n",
    "    sh_logits   = sh(X_fused_t).cpu().numpy()                                                  # (N,L)\n",
    "\n",
    "sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "P_spec = np.zeros_like(spec_logits, np.float32)\n",
    "P_blnd = np.zeros_like(spec_logits, np.float32)\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    Ts = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "    Th = max(float(temps_sh.get(lbl, 1.0)),   1e-3)\n",
    "    ps = sigmoid(spec_logits[:, j] / Ts)\n",
    "    ph = sigmoid(sh_logits[:, j]   / Th)\n",
    "    P_spec[:, j] = ps\n",
    "    P_blnd[:, j] = np.clip(ALPHA*ps + (1-ALPHA)*ph, 0.0, 1.0)\n",
    "\n",
    "# ---- Scoring helper ----\n",
    "def score(PROBS: np.ndarray, thresholds_obj: Dict[str, dict], mode: str = \"fbeta15\") -> dict:\n",
    "    tp=fp=fn=0\n",
    "    ap_list=[]; roc_list=[]\n",
    "    TH = np.array([float(thresholds_obj[l][\"th_f1\" if mode==\"f1\" else \"th_fbeta15\"]) for l in LABELS], np.float32)\n",
    "    Pred = (PROBS >= TH.reshape(1, -1)).astype(int)\n",
    "    Pred[Mte] = 0\n",
    "    Truth = Yte.astype(int)\n",
    "    Truth[Mte] = 0\n",
    "\n",
    "    from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "    for j in range(N_LABELS):\n",
    "        y = Truth[:, j]; p = PROBS[:, j]\n",
    "        if y.max() != y.min():\n",
    "            try: ap_list.append(float(average_precision_score(y, p)))\n",
    "            except Exception: pass\n",
    "            try: roc_list.append(float(roc_auc_score(y, p)))\n",
    "            except Exception: pass\n",
    "        tp += int(((Pred[:, j]==1) & (y==1)).sum())\n",
    "        fp += int(((Pred[:, j]==1) & (y==0)).sum())\n",
    "        fn += int(((Pred[:, j]==0) & (y==1)).sum())\n",
    "    micro_prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    micro_rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec)>0 else 0.0\n",
    "    macro_pr   = float(np.nanmean(ap_list)) if ap_list else float(\"nan\")\n",
    "    macro_roc  = float(np.nanmean(roc_list)) if roc_list else float(\"nan\")\n",
    "    avg_true   = float(Truth.sum(1).mean())\n",
    "    avg_pred   = float(Pred.sum(1).mean())\n",
    "    return {\n",
    "        \"macro_pr_auc\": macro_pr,\n",
    "        \"macro_roc_auc\": macro_roc,\n",
    "        \"micro_precision\": micro_prec,\n",
    "        \"micro_recall\": micro_rec,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"avg_true_cardinality\": avg_true,\n",
    "        \"avg_pred_cardinality\": avg_pred,\n",
    "        \"cardinality_error\": avg_pred - avg_true\n",
    "    }\n",
    "\n",
    "# ---- Evaluate all four configurations ----\n",
    "res_spec_fb = score(P_spec, thr_spec,  mode=\"fbeta15\")\n",
    "res_blnd_fb = score(P_blnd, thr_blend, mode=\"fbeta15\")\n",
    "res_spec_f1 = score(P_spec, thr_spec,  mode=\"f1\")\n",
    "res_blnd_f1 = score(P_blnd, thr_blend, mode=\"f1\")\n",
    "\n",
    "table = pd.DataFrame([\n",
    "    {\"path\":\"specialist\",\"mode\":\"fbeta15\", **res_spec_fb},\n",
    "    {\"path\":\"blend\",     \"mode\":\"fbeta15\", **res_blnd_fb},\n",
    "    {\"path\":\"specialist\",\"mode\":\"f1\",      **res_spec_f1},\n",
    "    {\"path\":\"blend\",     \"mode\":\"f1\",      **res_blnd_f1},\n",
    "])\n",
    "table_path = EVAL_DIR / \"compare_table.csv\"\n",
    "table.to_csv(table_path, index=False)\n",
    "comp_json = {\"fbeta15\": {\"specialist\": res_spec_fb, \"blend\": res_blnd_fb},\n",
    "             \"f1\":      {\"specialist\": res_spec_f1, \"blend\": res_blnd_f1}}\n",
    "(EVAL_DIR / \"compare_summary.json\").write_text(json.dumps(comp_json, indent=2))\n",
    "\n",
    "print(\"âœ… Saved:\")\n",
    "print(\"  â€¢\", table_path)\n",
    "print(\"  â€¢\", EVAL_DIR / \"compare_summary.json\")\n",
    "print(\"\\nQuick view:\")\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8419a",
   "metadata": {},
   "source": [
    "### 3: Per-label diagnostics & policy proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17870a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Per-label policy proposed and saved.\n",
      "  â€¢ Policy JSON â†’ v7\\model\\policy\\policy.json\n",
      "  â€¢ Table CSV   â†’ v7\\eval\\policy_table.csv\n",
      "Decision counts: {'fbeta15': 5, 'precision_floor': 4, 'f1': 3}\n",
      "\n",
      "Examples:\n",
      "  NR-AR: precision_floor (th=0.653) â† val_precision_floor_0.55\n",
      "  NR-AR-LBD: precision_floor (th=0.723) â† val_precision_floor_0.55\n",
      "  NR-AhR: precision_floor (th=0.709) â† val_precision_floor_0.55\n",
      "  NR-Aromatase: fbeta15 (th=0.474) â† default_fbeta15\n"
     ]
    }
   ],
   "source": [
    "# ===============================\n",
    "# Phase 6 â€” Cell 3: Per-label diagnostics & policy proposal\n",
    "# ===============================\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# ------- Config -------\n",
    "BASE         = Path(\"v7\")\n",
    "PREP_DIR     = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR     = BASE / \"data\" / \"descriptors\"\n",
    "FUSED_DIR    = BASE / \"data\" / \"fused\"\n",
    "MODEL_DIR    = BASE / \"model\"\n",
    "ENS_DIR      = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR      = MODEL_DIR / \"calibration\"\n",
    "EVAL_DIR     = BASE / \"eval\"\n",
    "POL_DIR      = MODEL_DIR / \"policy\"\n",
    "for p in [EVAL_DIR, POL_DIR, FUSED_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# How strict should we be about precision?\n",
    "PREC_FLOOR = 0.55   # try 0.55â€“0.60; higher => fewer FPs, lower recall\n",
    "DEFAULT_ALPHA = 0.8 # specialist weight used in your saved blend calibration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------- Load manifests and artifacts -------\n",
    "mani = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABELS = mani[\"labels\"]; N_LABELS = len(LABELS)\n",
    "DESC_IN_DIM = int(mani[\"n_features\"])\n",
    "\n",
    "# Test per-label metrics (from Phase 6 Cell 1 with EVAL_MODE=\"blend\")\n",
    "test_metrics_csv = EVAL_DIR / \"per_label_metrics.csv\"\n",
    "assert test_metrics_csv.exists(), f\"Missing {test_metrics_csv}. Please run Phase 6 Cell 1 first.\"\n",
    "test_df = pd.read_csv(test_metrics_csv)\n",
    "\n",
    "# Blended calibration\n",
    "temps_spec   = json.loads((CAL_DIR / \"temps.json\").read_text())            # specialist temps\n",
    "temps_shared = json.loads((CAL_DIR / \"temps_shared.json\").read_text())     # shared temps\n",
    "blend_payload= json.loads((CAL_DIR / \"thresholds_blend.json\").read_text()) # has alpha + thresholds\n",
    "ALPHA        = float(blend_payload.get(\"alpha\", DEFAULT_ALPHA))\n",
    "thr_blend    = blend_payload[\"thresholds\"]  # per-label th_f1 / th_fbeta15\n",
    "\n",
    "# ------- Ensure fused VAL features (rebuild if missing) -------\n",
    "def ensure_fused(split: str) -> np.ndarray:\n",
    "    out = FUSED_DIR / f\"{split}_fused.npy\"\n",
    "    if out.exists():\n",
    "        return np.load(out).astype(np.float32)\n",
    "\n",
    "    print(f\"[Rebuild] {out} not found â†’ recomputing {split} fused features...\")\n",
    "\n",
    "    # Load split blob\n",
    "    blob = np.load(PREP_DIR / f\"{split}.npz\", allow_pickle=True)\n",
    "    smi  = [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "    N    = len(smi)\n",
    "\n",
    "    # Descriptor transformers\n",
    "    from joblib import load as joblib_load\n",
    "    imputer = joblib_load(DESC_DIR / \"imputer.joblib\")\n",
    "    scaler  = joblib_load(DESC_DIR / \"scaler.joblib\")\n",
    "\n",
    "    # RDKit descriptor list (same order as training)\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors as RDDesc\n",
    "    feat_names = [ln.strip() for ln in (DESC_DIR / \"feature_names.txt\").read_text().splitlines() if ln.strip()]\n",
    "    rd_fns = {name: getattr(RDDesc, name, None) for name in feat_names}\n",
    "\n",
    "    def rdkit_feats(smiles_list: List[str]) -> np.ndarray:\n",
    "        rows = []\n",
    "        for s in smiles_list:\n",
    "            m = Chem.MolFromSmiles(s)\n",
    "            if m is None:\n",
    "                rows.append([np.nan]*len(feat_names)); continue\n",
    "            vals=[]\n",
    "            for name in feat_names:\n",
    "                fn = rd_fns.get(name, None)\n",
    "                try: v = float(fn(m)) if fn is not None else np.nan\n",
    "                except Exception: v = np.nan\n",
    "                vals.append(v if np.isfinite(v) else np.nan)\n",
    "            rows.append(vals)\n",
    "        return np.asarray(rows, dtype=np.float32)\n",
    "\n",
    "    # Rebuild shared fusion model (text+graph+desc) â†’ fused\n",
    "    CKPT = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "    assert CKPT.exists(), f\"Missing shared checkpoint: {CKPT}\"\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    class ChemBERTaEncoder(nn.Module):\n",
    "        def __init__(self, ckpt=\"seyonec/ChemBERTa-zinc-base-v1\", dim=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.tok = AutoTokenizer.from_pretrained(ckpt)\n",
    "            self.m   = AutoModel.from_pretrained(ckpt)\n",
    "            self.proj= nn.Sequential(nn.Dropout(p), nn.Linear(self.m.config.hidden_size, dim))\n",
    "            self.ln  = nn.LayerNorm(dim)\n",
    "        def forward(self, smis, max_length=256):\n",
    "            enc = self.tok(list(smis), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            out = self.m(input_ids=enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device)).last_hidden_state\n",
    "            return self.ln(self.proj(out)), enc[\"attention_mask\"].to(device, dtype=torch.int32)\n",
    "\n",
    "    from rdkit import Chem\n",
    "    ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "    def _one_hot(v, C): z=[0]*len(C); z[C.index(v) if v in C else -1]=1; return z\n",
    "    def _bucket(v, lo, hi):\n",
    "        buckets=list(range(lo,hi+1)); o=[0]*(len(buckets)+1); i=v-lo; o[i if 0<=i<len(buckets) else -1]=1; return o\n",
    "    def _atom_feat(a):\n",
    "        hyb=[Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "        chir=[Chem.rdchem.ChiralType.CHI_UNSPECIFIED, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW, Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "        f=_one_hot(a.GetSymbol(), ATOM_LIST+[\"other\"])\n",
    "        f+=_bucket(a.GetDegree(),0,5)+_bucket(a.GetFormalCharge(),-2,2)+(_one_hot(a.GetHybridization(),hyb)+[0])\n",
    "        f+=[int(a.GetIsAromatic()), int(a.IsInRing())]+_one_hot(a.GetChiralTag(),chir)\n",
    "        f+=_bucket(a.GetTotalNumHs(True),0,4)+_bucket(a.GetTotalValence(),0,5)+[a.GetMass()/200.0]\n",
    "        return f\n",
    "    def _smiles_to_graph(smi, max_nodes=128):\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        if m is None or m.GetNumAtoms()==0: return np.zeros((0,0),np.float32), np.zeros((0,0),np.float32)\n",
    "        X = np.asarray([_atom_feat(m.GetAtomWithIdx(i)) for i in range(m.GetNumAtoms())], np.float32)\n",
    "        N = m.GetNumAtoms(); A = np.zeros((N,N),np.float32)\n",
    "        for b in m.GetBonds(): i,j=b.GetBeginAtomIdx(), b.GetEndAtomIdx(); A[i,j]=A[j,i]=1.0\n",
    "        if N>max_nodes: X=X[:max_nodes]; A=A[:max_nodes,:max_nodes]\n",
    "        return X,A\n",
    "    def _collate(smis, max_nodes=128):\n",
    "        G=[_smiles_to_graph(s) for s in smis]\n",
    "        Nmax=max([g[0].shape[0] for g in G]+[1]); F=G[0][0].shape[1] if G[0][0].size>0 else 51; B=len(G)\n",
    "        X=np.zeros((B,Nmax,F),np.float32); A=np.zeros((B,Nmax,Nmax),np.float32); M=np.zeros((B,Nmax),np.int64)\n",
    "        for i,(x,a) in enumerate(G):\n",
    "            n=x.shape[0]\n",
    "            if n==0: continue\n",
    "            X[i,:n,:]=x; A[i,:n,:n]=a; M[i,:n]=1\n",
    "        return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "    class GINLayer(nn.Module):\n",
    "        def __init__(self,h=256,p=0.1): super().__init__(); self.eps=nn.Parameter(torch.tensor(0.0)); self.mlp=nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "        def forward(self,x,A,M): return self.mlp((1.0+self.eps)*x + torch.matmul(A,x)) * M.unsqueeze(-1).to(x.dtype)\n",
    "    class GraphGINEncoder(nn.Module):\n",
    "        def __init__(self,node_in_dim=51,h=256,L=4,p=0.1): super().__init__(); self.inp=nn.Sequential(nn.Linear(node_in_dim,h), nn.GELU(), nn.Dropout(p)); self.layers=nn.ModuleList([GINLayer(h,p) for _ in range(L)]); self.out_ln=nn.LayerNorm(h)\n",
    "        def forward(self,smis,max_nodes=128):\n",
    "            X,A,M=_collate(smis,max_nodes); h=self.inp(X)\n",
    "            for L in self.layers: h=L(h,A,M)\n",
    "            return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "    def masked_mean(x, m, dim): m=m.to(dtype=x.dtype, device=x.device); denom=m.sum(dim=dim,keepdim=True).clamp(min=1.0); return (x*m.unsqueeze(-1)).sum(dim=dim)/denom\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self,dim=256,heads=4,p=0.1): super().__init__(); self.mha=nn.MultiheadAttention(dim, heads, dropout=p, batch_first=False); self.ln=nn.LayerNorm(dim); self.do=nn.Dropout(p)\n",
    "        def forward(self,T,TM,G,GM): Q=T.transpose(0,1); K=G.transpose(0,1); V=G.transpose(0,1); kpm=(GM==0); A,_=self.mha(Q,K,V,key_padding_mask=kpm); return self.ln(T+self.do(A.transpose(0,1)))\n",
    "    class DescriptorMLP(nn.Module):\n",
    "        def __init__(self,inp,dim=256,p=0.1): super().__init__(); self.net=nn.Sequential(nn.Linear(inp,256), nn.GELU(), nn.Dropout(p), nn.Linear(256,dim), nn.GELU(), nn.Dropout(p))\n",
    "        def forward(self,x): return self.net(x)\n",
    "    class FusionClassifier(nn.Module):\n",
    "        def __init__(self,dim=256,L=N_LABELS,p=0.1): super().__init__(); self.mlp=nn.Sequential(nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p), nn.Linear(dim*2, L))\n",
    "        def forward(self,z): return self.mlp(z)\n",
    "    class V7Shared(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.text=ChemBERTaEncoder()\n",
    "            self.graph=GraphGINEncoder()\n",
    "            self.cross=CrossAttentionBlock()\n",
    "            self.desc=DescriptorMLP(DESC_IN_DIM)\n",
    "            self.head=FusionClassifier()\n",
    "        def forward(self,smis,desc,return_fused=False):\n",
    "            T,TM=self.text(smis,256); G,GM=self.graph(smis,128); Ta=self.cross(T,TM,G,GM); Dz=self.desc(desc)\n",
    "            Tp=masked_mean(Ta,TM,1); Gp=masked_mean(G,GM,1); fused=torch.cat([Tp,Gp,Dz],-1)\n",
    "            logits=self.head(fused)\n",
    "            return (logits, fused) if return_fused else logits\n",
    "\n",
    "    # Build model & fused features\n",
    "    v7 = V7Shared().to(device)\n",
    "    v7.load_state_dict(torch.load(MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)[\"model\"], strict=True)\n",
    "    v7.eval()\n",
    "\n",
    "    # descriptors â†’ impute â†’ scale\n",
    "    X_raw = rdkit_feats(smi)\n",
    "    X_imp = imputer.transform(X_raw); X_std = scaler.transform(X_imp)\n",
    "    desc_t = torch.tensor(X_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    fused_chunks=[]\n",
    "    B=64\n",
    "    for i in range(0, N, B):\n",
    "        _, f = v7(smi[i:i+B], desc_t[i:i+B], return_fused=True)\n",
    "        fused_chunks.append(f.detach().cpu().numpy())\n",
    "    fused = np.concatenate(fused_chunks, 0).astype(np.float32)\n",
    "    np.save(out, fused)\n",
    "    print(f\"[Rebuild] Saved â†’ {out}\")\n",
    "    return fused\n",
    "\n",
    "# Get VAL fused and labels\n",
    "Xva = ensure_fused(\"val\")\n",
    "val_blob = np.load(PREP_DIR / \"val.npz\", allow_pickle=True)\n",
    "Yva = val_blob[\"Y\"].astype(np.float32)                      # (Nv, L)\n",
    "Mva = val_blob[\"y_missing_mask\"].astype(bool)               # (Nv, L)\n",
    "\n",
    "# ------- Heads: specialist + shared (MLP on fused) -------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _remap_keys_if_needed(state_dict: dict) -> dict:\n",
    "    needs = any(k.startswith((\"b1.\", \"b2.\", \"b3.\")) for k in state_dict.keys())\n",
    "    if not needs: return state_dict\n",
    "    remap = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k2 = k.replace(\"b1.\", \"block1.\").replace(\"b2.\", \"block2.\").replace(\"b3.\", \"block3.\")\n",
    "        remap[k2] = v\n",
    "    return remap\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\"), key=lambda p: p.name):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if not mfile.exists(): continue\n",
    "        try:\n",
    "            ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "            cands.append((ap, sd))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label} under {ENS_DIR/label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    state = _remap_keys_if_needed(ck[\"model\"])\n",
    "    head.load_state_dict(state, strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "sh = SharedHeadMLP().to(device)\n",
    "ckpt = torch.load(MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "sh_state = {k.replace(\"shared_head.\", \"\"): v for k,v in ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(sh_state, strict=True)\n",
    "sh.eval()\n",
    "\n",
    "# ------- Build blended probabilities on VAL -------\n",
    "sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
    "Xva_t = torch.tensor(Xva, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    spec_logits_val = torch.stack([HEADS[l](Xva_t) for l in LABELS], dim=1).cpu().numpy()\n",
    "    sh_logits_val   = sh(Xva_t).cpu().numpy()\n",
    "\n",
    "P_spec_val = np.zeros_like(spec_logits_val, np.float32)\n",
    "P_sh_val   = np.zeros_like(sh_logits_val,   np.float32)\n",
    "P_blend_val= np.zeros_like(sh_logits_val,   np.float32)\n",
    "\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    Ts = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "    Th = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "    ps = sigmoid(spec_logits_val[:, j] / Ts)\n",
    "    ph = sigmoid(sh_logits_val[:, j]   / Th)\n",
    "    P_spec_val[:, j] = ps\n",
    "    P_sh_val[:, j]   = ph\n",
    "    P_blend_val[:, j]= np.clip(ALPHA*ps + (1-ALPHA)*ph, 0.0, 1.0)\n",
    "\n",
    "# ------- Helper: precision-floor threshold on VAL -------\n",
    "def precision_floor_threshold(y_true: np.ndarray, probs: np.ndarray, floor: float) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns (threshold, precision_at_th, recall_at_th) selecting the *highest recall*\n",
    "    point on the PR curve with precision >= floor. Falls back to best F1 if none.\n",
    "    \"\"\"\n",
    "    prec, rec, th = precision_recall_curve(y_true, probs)\n",
    "    # Align thresholds with prec/rec arrays (sklearn: th has len-1)\n",
    "    th_aligned = np.concatenate([th, [1.0]]) if th.size > 0 else np.array([0.5])\n",
    "    mask = prec >= floor\n",
    "    if mask.any():\n",
    "        # Among points with precision>=floor, pick the one with highest recall\n",
    "        idx = np.argmax(rec[mask])\n",
    "        # position within masked array â†’ original index\n",
    "        candidates = np.where(mask)[0]\n",
    "        i = candidates[idx]\n",
    "        return float(th_aligned[i]), float(prec[i]), float(rec[i])\n",
    "    # Fallback: best F1\n",
    "    eps = 1e-8\n",
    "    f1 = (2*prec*rec)/np.maximum(prec+rec, eps)\n",
    "    i = int(np.nanargmax(f1))\n",
    "    return float(th_aligned[i]), float(prec[i]), float(rec[i])\n",
    "\n",
    "# ------- Build policy per label -------\n",
    "rows = []\n",
    "policy = {\n",
    "    \"alpha\": ALPHA,\n",
    "    \"precision_floor\": PREC_FLOOR,\n",
    "    \"default_mode\": \"fbeta15\",\n",
    "    \"labels\": {}\n",
    "}\n",
    "\n",
    "# Map test metrics into a dict for easy access\n",
    "test_by_label = {r[\"label\"]: r for _, r in test_df.iterrows()}\n",
    "\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    # Use only VAL rows with non-missing labels\n",
    "    valid = ~Mva[:, j]\n",
    "    yv = Yva[valid, j].astype(int)\n",
    "    pv = P_blend_val[valid, j]\n",
    "    # Degenerate label on val?\n",
    "    degenerate = (yv.max() == yv.min())\n",
    "    # Test metrics (to judge FP-ness)\n",
    "    td = test_by_label.get(lbl, {})\n",
    "    prec_f1  = float(td.get(\"prec@f1\", np.nan))\n",
    "    rec_f1   = float(td.get(\"recall@f1\", np.nan))\n",
    "    f1_test  = float(td.get(\"f1\", np.nan))\n",
    "    prec_fb  = float(td.get(\"prec@fbeta15\", np.nan))\n",
    "    rec_fb   = float(td.get(\"recall@fbeta15\", np.nan))\n",
    "    fbeta_t  = float(td.get(\"f_beta15\", np.nan))\n",
    "\n",
    "    # Start with default: keep fbeta15 threshold from blend calibration\n",
    "    th_f1  = float(thr_blend[lbl][\"th_f1\"])\n",
    "    th_fb  = float(thr_blend[lbl][\"th_fbeta15\"])\n",
    "    decision = \"fbeta15\"\n",
    "    chosen_th = th_fb\n",
    "    reason = \"default_fbeta15\"\n",
    "\n",
    "    # Heuristic 1: if F1 better than FÎ² on test *and* precision improves, prefer F1\n",
    "    if (not math.isnan(f1_test) and not math.isnan(fbeta_t)) and (f1_test >= fbeta_t) and (not math.isnan(prec_f1) and not math.isnan(prec_fb)) and (prec_f1 > prec_fb):\n",
    "        decision = \"f1\"\n",
    "        chosen_th = th_f1\n",
    "        reason = \"test_f1_better_and_more_precise\"\n",
    "\n",
    "    # Heuristic 2: if precision at FÎ² is below floor, try precision-floor on VAL\n",
    "    if (not math.isnan(prec_fb)) and (prec_fb < PREC_FLOOR) and (not degenerate):\n",
    "        th_pf, pr_pf, rc_pf = precision_floor_threshold(yv, pv, PREC_FLOOR)\n",
    "        # Only adopt if recall doesn't collapse completely\n",
    "        if rc_pf >= max(0.5*rec_fb, 0.10):  # keep at least 50% of recall@FÎ², or â‰¥0.10 absolute\n",
    "            decision = \"precision_floor\"\n",
    "            chosen_th = th_pf\n",
    "            reason = f\"val_precision_floor_{PREC_FLOOR:.2f}\"\n",
    "\n",
    "    policy[\"labels\"][lbl] = {\n",
    "        \"mode\": decision,            # \"fbeta15\" | \"f1\" | \"precision_floor\"\n",
    "        \"threshold\": float(chosen_th),\n",
    "        \"diag\": {\n",
    "            \"test_prec@f1\": prec_f1, \"test_rec@f1\": rec_f1, \"test_f1\": f1_test,\n",
    "            \"test_prec@fb\": prec_fb, \"test_rec@fb\": rec_fb, \"test_fbeta15\": fbeta_t\n",
    "        }\n",
    "    }\n",
    "\n",
    "    rows.append({\n",
    "        \"label\": lbl,\n",
    "        \"decision\": decision,\n",
    "        \"chosen_threshold\": chosen_th,\n",
    "        \"reason\": reason,\n",
    "        \"test_prec@f1\": prec_f1, \"test_rec@f1\": rec_f1, \"test_f1\": f1_test,\n",
    "        \"test_prec@fb\": prec_fb, \"test_rec@fb\": rec_fb, \"test_fbeta15\": fbeta_t\n",
    "    })\n",
    "\n",
    "# Save artifacts\n",
    "policy_path = POL_DIR / \"policy.json\"\n",
    "policy_path.write_text(json.dumps(policy, indent=2))\n",
    "pd.DataFrame(rows).to_csv(EVAL_DIR / \"policy_table.csv\", index=False)\n",
    "\n",
    "# Console summary\n",
    "dec_counts = pd.Series([r[\"decision\"] for r in rows]).value_counts().to_dict()\n",
    "print(\"âœ… Per-label policy proposed and saved.\")\n",
    "print(\"  â€¢ Policy JSON â†’\", policy_path)\n",
    "print(\"  â€¢ Table CSV   â†’\", EVAL_DIR / \"policy_table.csv\")\n",
    "print(\"Decision counts:\", dec_counts)\n",
    "print(\"\\nExamples:\")\n",
    "for r in rows[:4]:\n",
    "    print(f\"  {r['label']}: {r['decision']} (th={r['chosen_threshold']:.3f}) â† {r['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145fb5ea",
   "metadata": {},
   "source": [
    "## phase 7 (extra squeezes and testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0643212a",
   "metadata": {},
   "source": [
    "### New reg to comapre f1, fbeta15 and the new policy (6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e5ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Policy-aware tester ready.\n"
     ]
    }
   ],
   "source": [
    "# === V7 Policy-aware Single-SMILES/SMARTS Test Rig (self-contained) ===\n",
    "# Loads:\n",
    "#   v7/model/checkpoints/shared/best.pt\n",
    "#   v7/model/ensembles/<label>/seed*/best.pt\n",
    "#   v7/model/calibration/temps.json, temps_shared.json, thresholds_blend.json\n",
    "#   v7/model/policy/policy.json   (optional; for mode=\"policy\")\n",
    "\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from rdkit import Chem\n",
    "\n",
    "BASE       = Path(\"v7\")\n",
    "PREP_DIR   = BASE / \"data\" / \"prepared\"\n",
    "MODEL_DIR  = BASE / \"model\"\n",
    "ENS_DIR    = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR    = MODEL_DIR / \"calibration\"\n",
    "POL_DIR    = MODEL_DIR / \"policy\"\n",
    "CKPT_BEST  = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "\n",
    "assert CKPT_BEST.exists(), f\"Missing shared checkpoint: {CKPT_BEST}\"\n",
    "assert (PREP_DIR / \"dataset_manifest.json\").exists(), \"Missing dataset manifest.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Labels & dims ---\n",
    "ds_manifest = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABEL_NAMES: List[str] = ds_manifest[\"labels\"]\n",
    "DESC_IN_DIM = int(ds_manifest[\"n_features\"])  # 208\n",
    "\n",
    "# --- Calibration + policy ---\n",
    "temps_spec   = json.loads((CAL_DIR / \"temps.json\").read_text())            # specialist temps\n",
    "temps_shared = json.loads((CAL_DIR / \"temps_shared.json\").read_text())     # shared temps\n",
    "blend_payload= json.loads((CAL_DIR / \"thresholds_blend.json\").read_text()) # alpha + thresholds\n",
    "ALPHA        = float(blend_payload.get(\"alpha\", 0.8))\n",
    "thr_blend    = blend_payload[\"thresholds\"]                                  # per label\n",
    "POL_PATH     = POL_DIR / \"policy.json\"\n",
    "policy       = json.loads(POL_PATH.read_text()) if POL_PATH.exists() else None\n",
    "\n",
    "# --- Text encoder (ChemBERTa) ---\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "class ChemBERTaEncoder(nn.Module):\n",
    "    def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "        self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "        self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "        self.ln   = nn.LayerNorm(fusion_dim)\n",
    "    def forward(self, smiles_list: List[str], max_length=256, add_special_tokens=True):\n",
    "        enc = self.tokenizer(list(smiles_list), padding=True, truncation=True,\n",
    "                             max_length=max_length, add_special_tokens=add_special_tokens,\n",
    "                             return_tensors=\"pt\")\n",
    "        input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "        out  = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,L,H)\n",
    "        toks = self.ln(self.proj(out))  # (B,L,256)\n",
    "        return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "# --- Graph encoder (names aligned to checkpoint) ---\n",
    "ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "def _one_hot(v, choices):\n",
    "    z = [0]*len(choices); z[choices.index(v) if v in choices else -1] = 1; return z\n",
    "def _bucket_oh(v, lo, hi):\n",
    "    buckets = list(range(lo, hi+1)); o = [0]*(len(buckets)+1); idx = v - lo\n",
    "    o[idx if 0<=idx<len(buckets) else -1] = 1; return o\n",
    "def _atom_feat(atom):\n",
    "    hybs = [Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "    chir = [Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "    feat  = _one_hot(atom.GetSymbol(), ATOM_LIST+[\"other\"])\n",
    "    feat += _bucket_oh(atom.GetDegree(), 0, 5)\n",
    "    feat += _bucket_oh(atom.GetFormalCharge(), -2, 2)\n",
    "    feat += (_one_hot(atom.GetHybridization(), hybs)+[0])\n",
    "    feat += [int(atom.GetIsAromatic()), int(atom.IsInRing())]\n",
    "    feat += _one_hot(atom.GetChiralTag(), chir)\n",
    "    feat += _bucket_oh(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)\n",
    "    feat += _bucket_oh(atom.GetTotalValence(), 0, 5)\n",
    "    feat += [atom.GetMass()/200.0]\n",
    "    return feat  # ~51 dims\n",
    "def _smiles_to_graph(smi, max_nodes=128):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None or mol.GetNumAtoms()==0: return np.zeros((0,0),np.float32), np.zeros((0,0),np.float32)\n",
    "    X = np.asarray([_atom_feat(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())], np.float32)\n",
    "    N = mol.GetNumAtoms(); A = np.zeros((N,N),np.float32)\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        A[i,j] = 1.0; A[j,i] = 1.0\n",
    "    if N>max_nodes: X=X[:max_nodes]; A=A[:max_nodes,:max_nodes]\n",
    "    return X, A\n",
    "def _collate_graphs(smiles_batch, max_nodes=128):\n",
    "    graphs = [_smiles_to_graph(s) for s in smiles_batch]\n",
    "    Nmax = max([g[0].shape[0] for g in graphs] + [1])\n",
    "    Fnode = graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "    B = len(graphs)\n",
    "    X = np.zeros((B, Nmax, Fnode), np.float32)\n",
    "    A = np.zeros((B, Nmax, Nmax), np.float32)\n",
    "    M = np.zeros((B, Nmax), np.int64)\n",
    "    for i, (x, a) in enumerate(graphs):\n",
    "        n = x.shape[0]\n",
    "        if n==0: continue\n",
    "        X[i,:n,:] = x; A[i,:n,:n] = a; M[i,:n] = 1\n",
    "    return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, h=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "        self.mlp = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "    def forward(self, x, adj, mask):\n",
    "        out = (1.0 + self.eps) * x + torch.matmul(adj, x)\n",
    "        out = self.mlp(out)\n",
    "        return out * mask.unsqueeze(-1).to(out.dtype)\n",
    "\n",
    "class GraphGINEncoder(nn.Module):\n",
    "    def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(nn.Linear(node_in_dim, hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden_dim, p) for _ in range(n_layers)])\n",
    "        self.out_ln = nn.LayerNorm(hidden_dim)  # name matches checkpoint\n",
    "    def forward(self, smiles_list: List[str], max_nodes=128):\n",
    "        X, A, M = _collate_graphs(smiles_list, max_nodes=max_nodes)\n",
    "        h = self.inp(X)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, A, M)\n",
    "        return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "# --- Fusion & shared head ---\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "    denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "    return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "        self.ln  = nn.LayerNorm(dim)\n",
    "        self.do  = nn.Dropout(p)\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "        Q = text_tokens.transpose(0,1)   # (L,B,D)\n",
    "        K = graph_nodes.transpose(0,1)   # (N,B,D)\n",
    "        V = graph_nodes.transpose(0,1)\n",
    "        kpm = (graph_mask == 0)          # (B,N) 1=pad\n",
    "        attn, _ = self.mha(Q, K, V, key_padding_mask=kpm)\n",
    "        attn = attn.transpose(0,1)       # (B,L,D)\n",
    "        return self.ln(text_tokens + self.do(attn))\n",
    "\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim), nn.GELU(), nn.Dropout(p)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    # name 'mlp' matches checkpoint ('shared_head.mlp.*')\n",
    "    def __init__(self, dim=256, n_labels=12, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, fused_vec): return self.mlp(fused_vec)\n",
    "\n",
    "class V7FusionModel(nn.Module):\n",
    "    def __init__(self, text_encoder, graph_encoder, desc_in_dim=208, dim=256, n_labels=12, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.text_encoder=text_encoder\n",
    "        self.graph_encoder=graph_encoder\n",
    "        self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "        self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "        self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "    def forward(self, smiles_list, desc_feats):\n",
    "        tt, tm = self.text_encoder(smiles_list, max_length=256)\n",
    "        gn, gm = self.graph_encoder(smiles_list, max_nodes=128)\n",
    "        tta = self.cross(tt.to(device), tm.to(device), gn.to(device), gm.to(device))\n",
    "        de  = self.desc_mlp(desc_feats.to(device))\n",
    "        text_pool  = masked_mean(tta, tm.to(device), 1)\n",
    "        graph_pool = masked_mean(gn.to(device),  gm.to(device), 1)\n",
    "        fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "        logits = self.shared_head(fused)\n",
    "        return logits, fused\n",
    "\n",
    "# Build shared model\n",
    "text_encoder = ChemBERTaEncoder().to(device)\n",
    "graph_encoder= GraphGINEncoder().to(device)\n",
    "v7_shared    = V7FusionModel(text_encoder, graph_encoder, desc_in_dim=DESC_IN_DIM, n_labels=len(LABEL_NAMES)).to(device)\n",
    "ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "v7_shared.eval()\n",
    "\n",
    "# --- Specialist heads (robust loader) ---\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _remap_keys_if_needed(state_dict: dict) -> dict:\n",
    "    if not any(k.startswith((\"b1.\",\"b2.\",\"b3.\")) for k in state_dict.keys()):\n",
    "        return state_dict\n",
    "    remap = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k2 = k.replace(\"b1.\", \"block1.\").replace(\"b2.\", \"block2.\").replace(\"b3.\", \"block3.\")\n",
    "        remap[k2] = v\n",
    "    return remap\n",
    "\n",
    "def _load_best_head(label: str) -> nn.Module:\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if mfile.exists():\n",
    "            try:\n",
    "                ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "                cands.append((ap, sd))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label} under {ENS_DIR/label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    state = _remap_keys_if_needed(ck[\"model\"])\n",
    "    head.load_state_dict(state, strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS: Dict[str, nn.Module] = {lbl: _load_best_head(lbl) for lbl in LABEL_NAMES}\n",
    "\n",
    "# --- Descriptor prep for ad-hoc inputs: standardized zeros (robust & simple) ---\n",
    "def prepare_desc_matrix(smiles_list: List[str]) -> torch.Tensor:\n",
    "    Z = np.zeros((len(smiles_list), DESC_IN_DIM), dtype=np.float32)\n",
    "    return torch.tensor(Z, dtype=torch.float32, device=device)\n",
    "\n",
    "# --- Normalize SMARTSâ†’SMILES if needed ---\n",
    "def normalize_smiles_or_smarts(s: str) -> str:\n",
    "    if not isinstance(s, str): s = str(s)\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol: return Chem.MolToSmiles(mol)\n",
    "    q = Chem.MolFromSmarts(s)\n",
    "    if q:\n",
    "        try:\n",
    "            smi = Chem.MolToSmiles(q)\n",
    "            return smi if smi else s\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "@torch.no_grad()\n",
    "def fused_from_smiles(smiles_list: List[str]) -> torch.Tensor:\n",
    "    smiles_list = [normalize_smiles_or_smarts(s) for s in smiles_list]\n",
    "    desc = prepare_desc_matrix(smiles_list)\n",
    "    _, fused = v7_shared(smiles_list, desc)\n",
    "    return fused  # (B,768)\n",
    "\n",
    "# --- Inference helpers ---\n",
    "sigmoid = lambda x: 1.0/(1.0+math.e**(-x))\n",
    "\n",
    "@torch.no_grad()\n",
    "def _probs_for_one(smi: str) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Return per-label {prob_spec, prob_shared, prob_blend} for one SMILES using alpha & temps.\"\"\"\n",
    "    x = fused_from_smiles([smi])  # (1,768)\n",
    "    logits_shared = v7_shared.shared_head(x).detach().cpu().numpy()[0]  # (L,)\n",
    "    rec = {}\n",
    "    for j, lbl in enumerate(LABEL_NAMES):\n",
    "        logit_spec = HEADS[lbl](x).item()\n",
    "        p_spec   = sigmoid(logit_spec / max(float(temps_spec.get(lbl, 1.0)), 1e-3))\n",
    "        p_shared = sigmoid(float(logits_shared[j]) / max(float(temps_shared.get(lbl, 1.0)), 1e-3))\n",
    "        p_blend  = ALPHA * p_spec + (1.0 - ALPHA) * p_shared\n",
    "        rec[lbl] = {\"prob_spec\": float(p_spec), \"prob_shared\": float(p_shared), \"prob_blend\": float(p_blend)}\n",
    "    return rec\n",
    "\n",
    "def _threshold_for(lbl: str, mode: str) -> float:\n",
    "    \"\"\"Return threshold for a label under 'f1', 'fbeta15', or 'policy'.\"\"\"\n",
    "    if mode in (\"f1\", \"fbeta15\"):\n",
    "        key = \"th_f1\" if mode == \"f1\" else \"th_fbeta15\"\n",
    "        return float(thr_blend[lbl][key])\n",
    "    elif mode == \"policy\":\n",
    "        if policy is None:\n",
    "            raise RuntimeError(\"policy.json not found; run Phase 6 â€” Cell 3 to create it.\")\n",
    "        return float(policy[\"labels\"][lbl][\"threshold\"])\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'f1', 'fbeta15', or 'policy'\")\n",
    "\n",
    "def predict_one(smi: str, mode: str = \"fbeta15\", topk: int = 5, show_parts: bool = False):\n",
    "    \"\"\"\n",
    "    mode: 'f1' | 'fbeta15' | 'policy'\n",
    "    Prints top-k by blended prob and the positive set at chosen thresholds.\n",
    "    If show_parts=True, also shows p_spec / p_shared next to p_blend.\n",
    "    \"\"\"\n",
    "    assert mode in (\"f1\",\"fbeta15\",\"policy\")\n",
    "    rec = _probs_for_one(smi)\n",
    "    print(f\"\\nSMILES/SMARTS: {smi}\\n mode={mode}, alpha={ALPHA:.2f}\")\n",
    "    rows = []\n",
    "    for lbl, d in rec.items():\n",
    "        th = _threshold_for(lbl, mode)\n",
    "        rows.append((lbl, d[\"prob_blend\"], th, d[\"prob_blend\"] >= th, d[\"prob_spec\"], d[\"prob_shared\"]))\n",
    "    rows.sort(key=lambda z: z[1], reverse=True)\n",
    "    for lbl, p, th, dec, ps, ph in rows[:topk]:\n",
    "        if show_parts:\n",
    "            print(f\"  {lbl:12s}  p_spec={ps:.3f}  p_shared={ph:.3f}  p_blend={p:.3f}  th={th:.3f}  â†’ pred={int(dec)}\")\n",
    "        else:\n",
    "            print(f\"  {lbl:12s}  p_blend={p:.3f}  th={th:.3f}  â†’ pred={int(dec)}\")\n",
    "    positives = [lbl for lbl, p, th, dec, *_ in rows if dec]\n",
    "    print(\"  Positives:\", \", \".join(positives) if positives else \"none\")\n",
    "    return {lbl: {\"prob_spec\": float(ps), \"prob_shared\": float(ph), \"prob_blend\": float(p),\n",
    "                  \"threshold\": float(th), \"decision\": bool(dec)}\n",
    "            for (lbl, p, th, dec, ps, ph) in rows}\n",
    "\n",
    "def compare_modes(smi: str, modes: List[str] = (\"fbeta15\",\"f1\",\"policy\"), topk: int = 5):\n",
    "    \"\"\"Side-by-side comparison for the same SMILES.\"\"\"\n",
    "    print(\"=\"*72)\n",
    "    for m in modes:\n",
    "        predict_one(smi, mode=m, topk=topk, show_parts=False)\n",
    "        print(\"-\"*72)\n",
    "\n",
    "print(\"âœ… Policy-aware tester ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5cd0a8",
   "metadata": {},
   "source": [
    "#### test on the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b461871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved:\n",
      "  â€¢ Detailed predictions â†’ v7\\results\\inference\\Truth_labels_detailed.csv\n",
      "  â€¢ Summary metrics      â†’ v7\\results\\inference\\Truth_labels_summary.json\n",
      "\n",
      "=== Micro metrics by mode ===\n",
      "  fbeta15   P=0.625  R=0.833  F1=0.714\n",
      "  f1        P=0.750  R=0.750  F1=0.750\n",
      "  policy    P=0.667  R=0.667  F1=0.667\n",
      "\n",
      "=== Macro F1 by mode ===\n",
      "  fbeta15   macro-F1=0.783\n",
      "  f1        macro-F1=0.829\n",
      "  policy    macro-F1=0.772\n"
     ]
    }
   ],
   "source": [
    "# === Batch compare modes ('fbeta15', 'f1', 'policy') on Truth Lables.xlsx â€” FIXED NAN HANDLING ===\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# ---- prerequisites from your test rig ----\n",
    "need = ['v7_shared','HEADS','LABEL_NAMES','temps_spec','temps_shared','ALPHA','thr_blend','_probs_for_one','_threshold_for']\n",
    "for n in need:\n",
    "    assert n in globals(), f\"Missing '{n}'. Please run the self-contained test rig cell first.\"\n",
    "\n",
    "# ---- paths ----\n",
    "BASE    = Path(\"v7\")\n",
    "DATA_XL = BASE / \"data\" / \"Truth Lables.xlsx\"\n",
    "OUT_DIR = BASE / \"results\" / \"inference\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- load excel ----\n",
    "assert DATA_XL.exists(), f\"Missing file: {DATA_XL}\"\n",
    "df = pd.read_excel(DATA_XL)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# smiles column\n",
    "smiles_col = next((c for c in [\"smiles\",\"SMILES\",\"Smile\",\"smile\",\"SMILE\"] if c in df.columns), None)\n",
    "assert smiles_col is not None, f\"Could not find a SMILES column in: {list(df.columns)}\"\n",
    "\n",
    "# label columns must match training label names exactly\n",
    "LABELS = list(LABEL_NAMES)\n",
    "label_cols = [c for c in df.columns if c in LABELS]\n",
    "assert len(label_cols) == len(LABELS), \\\n",
    "    f\"Expected 12 label columns matching training names.\\nFound {len(label_cols)}: {label_cols}\\nWanted: {LABELS}\"\n",
    "\n",
    "def _to01(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    if isinstance(x, str):\n",
    "        xs = x.strip().lower()\n",
    "        if xs in {\"1\",\"true\",\"yes\",\"y\"}: return 1\n",
    "        if xs in {\"0\",\"false\",\"no\",\"n\"}: return 0\n",
    "        try:\n",
    "            xv = float(xs)\n",
    "            if np.isnan(xv): return np.nan\n",
    "            return 1 if xv >= 0.5 else 0\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    try:\n",
    "        xv = float(x)\n",
    "        if np.isnan(xv): return np.nan\n",
    "        return 1 if xv >= 0.5 else 0\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "MODES = (\"fbeta15\",\"f1\",\"policy\")\n",
    "\n",
    "# ---- iterate molecules, compute predictions once, then threshold by mode ----\n",
    "rows = []\n",
    "for idx, r in df.iterrows():\n",
    "    smi = str(r[smiles_col])\n",
    "    probs = _probs_for_one(smi)  # {lbl: {prob_spec, prob_shared, prob_blend}}\n",
    "    for lbl in LABELS:\n",
    "        prob = float(probs[lbl][\"prob_blend\"])\n",
    "        truth = _to01(r.get(lbl, np.nan))\n",
    "        for mode in MODES:\n",
    "            # skip policy if policy.json wasnâ€™t created\n",
    "            try:\n",
    "                th = _threshold_for(lbl, mode)\n",
    "            except Exception:\n",
    "                if mode == \"policy\":\n",
    "                    continue\n",
    "                raise\n",
    "            pred = int(prob >= th)\n",
    "            rows.append({\n",
    "                \"row_id\": int(idx),\n",
    "                \"smiles\": smi,\n",
    "                \"label\": lbl,\n",
    "                \"mode\": mode,\n",
    "                \"prob_blend\": prob,\n",
    "                \"threshold\": float(th),\n",
    "                \"prediction\": pred,\n",
    "                \"truth\": (np.nan if pd.isna(truth) else int(truth))\n",
    "            })\n",
    "\n",
    "detailed = pd.DataFrame(rows)\n",
    "\n",
    "# ---- metrics helpers (robust to empty/degenerate cases) ----\n",
    "def _safe_div(n, d):\n",
    "    return (n / d) if d > 0 else np.nan\n",
    "\n",
    "def _prf(tp, fp, fn):\n",
    "    prec = _safe_div(tp, tp+fp)\n",
    "    rec  = _safe_div(tp, tp+fn)\n",
    "    if np.isnan(prec) or np.isnan(rec) or (prec+rec) == 0:\n",
    "        f1 = np.nan\n",
    "    else:\n",
    "        f1 = 2 * prec * rec / (prec + rec)\n",
    "    return prec, rec, f1\n",
    "\n",
    "def _metrics(df_long: pd.DataFrame, mode: str) -> Dict:\n",
    "    dd = df_long[df_long[\"mode\"] == mode].copy()\n",
    "    dd = dd.dropna(subset=[\"truth\"])  # drop rows with unknown truth\n",
    "    if dd.empty:\n",
    "        return {\n",
    "            \"mode\": mode,\n",
    "            \"micro\": {\"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan},\n",
    "            \"macro_f1\": np.nan,\n",
    "            \"per_label\": [{ \"label\": lbl, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan } for lbl in LABELS]\n",
    "        }\n",
    "\n",
    "    y = dd[\"truth\"].astype(int).to_numpy()\n",
    "    p = dd[\"prediction\"].astype(int).to_numpy()\n",
    "    tp = int(((p==1)&(y==1)).sum())\n",
    "    fp = int(((p==1)&(y==0)).sum())\n",
    "    fn = int(((p==0)&(y==1)).sum())\n",
    "    m_prec, m_rec, m_f1 = _prf(tp, fp, fn)\n",
    "\n",
    "    per_label = []\n",
    "    for lbl in LABELS:\n",
    "        d = dd[dd[\"label\"] == lbl]\n",
    "        if d.empty:\n",
    "            per_label.append({\"label\": lbl, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan})\n",
    "            continue\n",
    "        yj = d[\"truth\"].astype(int).to_numpy()\n",
    "        pj = d[\"prediction\"].astype(int).to_numpy()\n",
    "        tpj = int(((pj==1)&(yj==1)).sum())\n",
    "        fpj = int(((pj==1)&(yj==0)).sum())\n",
    "        fnj = int(((pj==0)&(yj==1)).sum())\n",
    "        prec, rec, f1 = _prf(tpj, fpj, fnj)\n",
    "        per_label.append({\n",
    "            \"label\": lbl,\n",
    "            \"precision\": float(prec) if not np.isnan(prec) else np.nan,\n",
    "            \"recall\":    float(rec)  if not np.isnan(rec)  else np.nan,\n",
    "            \"f1\":        float(f1)   if not np.isnan(f1)   else np.nan\n",
    "        })\n",
    "\n",
    "    # macro F1 across labels (ignore NaNs)\n",
    "    macro_f1 = float(np.nanmean([pl[\"f1\"] for pl in per_label])) if len(per_label) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"micro\": {\"precision\": float(m_prec) if not np.isnan(m_prec) else np.nan,\n",
    "                  \"recall\":    float(m_rec)  if not np.isnan(m_rec)  else np.nan,\n",
    "                  \"f1\":        float(m_f1)   if not np.isnan(m_f1)   else np.nan},\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"per_label\": per_label\n",
    "    }\n",
    "\n",
    "# Only compute 'policy' summary if any policy rows exist\n",
    "modes_present = detailed[\"mode\"].unique().tolist()\n",
    "summary = {m: _metrics(detailed, m) for m in MODES if m in modes_present}\n",
    "\n",
    "# ---- save outputs ----\n",
    "csv_path  = OUT_DIR / \"Truth_labels_detailed.csv\"\n",
    "json_path = OUT_DIR / \"Truth_labels_summary.json\"\n",
    "detailed.to_csv(csv_path, index=False)\n",
    "json_path.write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"âœ… Saved:\")\n",
    "print(\"  â€¢ Detailed predictions â†’\", csv_path)\n",
    "print(\"  â€¢ Summary metrics      â†’\", json_path)\n",
    "\n",
    "# quick leaderboard\n",
    "print(\"\\n=== Micro metrics by mode ===\")\n",
    "for m, rep in summary.items():\n",
    "    pr = rep[\"micro\"][\"precision\"]; rc = rep[\"micro\"][\"recall\"]; f1 = rep[\"micro\"][\"f1\"]\n",
    "    print(f\"  {m:8s}  P={np.nan if pr is None else pr:.3f}  R={np.nan if rc is None else rc:.3f}  F1={np.nan if f1 is None else f1:.3f}\")\n",
    "print(\"\\n=== Macro F1 by mode ===\")\n",
    "for m, rep in summary.items():\n",
    "    print(f\"  {m:8s}  macro-F1={rep['macro_f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5733fc2",
   "metadata": {},
   "source": [
    "### Recompute the F1 Recompute F1 thresholds (VAL) and save as thresholds_blend_v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8ffd4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputing F1 thresholds on VAL...\n",
      "  NR-AR         old_f1=0.653 â†’ new_f1=0.660  (Î”=+0.007)  AP=0.171\n",
      "  NR-AR-LBD     old_f1=0.621 â†’ new_f1=0.621  (Î”=+0.001)  AP=0.253\n",
      "  NR-AhR        old_f1=0.709 â†’ new_f1=0.709  (Î”=+0.001)  AP=0.524\n",
      "  NR-Aromatase  old_f1=0.564 â†’ new_f1=0.565  (Î”=+0.001)  AP=0.295\n",
      "  NR-ER         old_f1=0.547 â†’ new_f1=0.547  (Î”=+0.000)  AP=0.253\n",
      "  NR-ER-LBD     old_f1=0.589 â†’ new_f1=0.594  (Î”=+0.004)  AP=0.139\n",
      "  NR-PPAR-gamma  old_f1=0.441 â†’ new_f1=0.441  (Î”=+0.000)  AP=0.063\n",
      "  SR-ARE        old_f1=0.528 â†’ new_f1=0.528  (Î”=+0.001)  AP=0.344\n",
      "  SR-ATAD5      old_f1=0.483 â†’ new_f1=0.483  (Î”=+0.001)  AP=0.171\n",
      "  SR-HSE        old_f1=0.472 â†’ new_f1=0.472  (Î”=+0.000)  AP=0.196\n",
      "  SR-MMP        old_f1=0.589 â†’ new_f1=0.591  (Î”=+0.002)  AP=0.444\n",
      "  SR-p53        old_f1=0.513 â†’ new_f1=0.513  (Î”=+0.000)  AP=0.210\n",
      "\n",
      "âœ… Saved updated thresholds â†’ v7\\model\\calibration\\thresholds_blend_v2.json\n"
     ]
    }
   ],
   "source": [
    "# === Phase 6 â€” Cell 4a: Recompute F1 thresholds (VAL) and save as thresholds_blend_v2.json ===\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "BASE      = Path(\"v7\")\n",
    "PREP_DIR  = BASE / \"data\" / \"prepared\"\n",
    "FUSED_DIR = BASE / \"data\" / \"fused\"\n",
    "MODEL_DIR = BASE / \"model\"\n",
    "CAL_DIR   = MODEL_DIR / \"calibration\"\n",
    "ENS_DIR   = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load manifest, temps, old thresholds (keep fb15 as-is) ---\n",
    "mani = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABELS = mani[\"labels\"]; N_LABELS = len(LABELS)\n",
    "DESC_IN_DIM = int(mani[\"n_features\"])\n",
    "\n",
    "temps_spec   = json.loads((CAL_DIR / \"temps.json\").read_text())\n",
    "temps_shared = json.loads((CAL_DIR / \"temps_shared.json\").read_text())\n",
    "blend_payload= json.loads((CAL_DIR / \"thresholds_blend.json\").read_text())\n",
    "ALPHA        = float(blend_payload.get(\"alpha\", 0.8))\n",
    "thr_old      = blend_payload[\"thresholds\"]  # dict[label]->{th_f1, th_fbeta15, ap_val}\n",
    "\n",
    "# --- Ensure VAL fused features ---\n",
    "Xva = np.load(FUSED_DIR / \"val_fused.npy\").astype(np.float32)\n",
    "val_blob = np.load(PREP_DIR / \"val.npz\", allow_pickle=True)\n",
    "Yva = val_blob[\"Y\"].astype(np.float32)\n",
    "Mva = val_blob[\"y_missing_mask\"].astype(bool)\n",
    "Xva_t = torch.tensor(Xva, dtype=torch.float32, device=device)\n",
    "\n",
    "# --- Shared head MLP (for logits) ---\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "sh = SharedHeadMLP().to(device)\n",
    "ckpt = torch.load(MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "sh_state = {k.replace(\"shared_head.\", \"\"): v for k,v in ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(sh_state, strict=True)\n",
    "sh.eval()\n",
    "\n",
    "# --- Specialist heads loader (best seed per label) ---\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(nn.Linear(in_dim,h1),nn.GELU(),nn.LayerNorm(h1),nn.Dropout(p))\n",
    "        self.block2=nn.Sequential(nn.Linear(h1,h2),nn.GELU(),nn.LayerNorm(h2),nn.Dropout(p))\n",
    "        self.block3=nn.Sequential(nn.Linear(h2,h3),nn.GELU(),nn.LayerNorm(h3),nn.Dropout(p))\n",
    "        self.out=nn.Linear(h3,1); self.short=nn.Linear(in_dim,h3)\n",
    "    def forward(self,x):\n",
    "        z1=self.block1(x); z2=self.block2(z1); z3=self.block3(z2)\n",
    "        return self.out(z3+self.short(x)).squeeze(-1)\n",
    "\n",
    "def _remap_keys_if_needed(sd: dict)->dict:\n",
    "    if not any(k.startswith((\"b1.\",\"b2.\",\"b3.\")) for k in sd.keys()): return sd\n",
    "    out={}; \n",
    "    for k,v in sd.items():\n",
    "        out[k.replace(\"b1.\",\"block1.\").replace(\"b2.\",\"block2.\").replace(\"b3.\",\"block3.\")] = v\n",
    "    return out\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    import math, json\n",
    "    cands=[]\n",
    "    for sd in sorted((ENS_DIR/label).glob(\"seed*/\")):\n",
    "        m=sd/\"metrics.json\"\n",
    "        if m.exists():\n",
    "            try: cands.append((float(json.loads(m.read_text()).get(\"best_ap\", float(\"nan\"))), sd))\n",
    "            except: pass\n",
    "    if not cands: raise FileNotFoundError(f\"No heads for {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best = cands[0][1]\n",
    "    ck = torch.load(best/\"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head=LabelHead(cfg[\"in_dim\"],cfg[\"h1\"],cfg[\"h2\"],cfg[\"h3\"],cfg.get(\"dropout\",0.30)).to(device)\n",
    "    state=_remap_keys_if_needed(ck[\"model\"]); head.load_state_dict(state, strict=True); head.eval(); return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "\n",
    "# --- Build blended probabilities on VAL ---\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "with torch.no_grad():\n",
    "    spec_logits = torch.stack([HEADS[l](Xva_t) for l in LABELS], dim=1).cpu().numpy()  # (Nv,L)\n",
    "    sh_logits   = sh(Xva_t).cpu().numpy()\n",
    "\n",
    "P_blend = np.zeros_like(sh_logits, np.float32)\n",
    "for j,lbl in enumerate(LABELS):\n",
    "    Ts=max(float(temps_spec.get(lbl,1.0)),1e-3); Th=max(float(temps_shared.get(lbl,1.0)),1e-3)\n",
    "    ps=sigmoid(spec_logits[:,j]/Ts); ph=sigmoid(sh_logits[:,j]/Th)\n",
    "    P_blend[:,j]=np.clip(ALPHA*ps + (1-ALPHA)*ph, 0, 1)\n",
    "\n",
    "# --- Recompute th_f1 per label (keep old th_fbeta15) ---\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def best_f1_threshold(y_true, probs):\n",
    "    prec, rec, th = precision_recall_curve(y_true, probs)\n",
    "    eps=1e-8; f1=(2*prec*rec)/np.maximum(prec+rec, eps)\n",
    "    if th.size==0: return 0.5\n",
    "    # f1 is len = len(th)+1; pick argmax ignoring the first point\n",
    "    idx = int(np.nanargmax(f1[1:]))+1\n",
    "    # align threshold array with f1: append 1.0 at end\n",
    "    th_aligned = np.concatenate([th, [1.0]])\n",
    "    return float(th_aligned[idx])\n",
    "\n",
    "new = {\"alpha\": ALPHA, \"thresholds\": {}}\n",
    "print(\"Recomputing F1 thresholds on VAL...\")\n",
    "for j,lbl in enumerate(LABELS):\n",
    "    valid = ~Mva[:,j]\n",
    "    if valid.sum()==0 or np.all(Yva[valid,j]==Yva[valid,j][0]):\n",
    "        new[\"thresholds\"][lbl] = {\n",
    "            \"th_f1\": float(thr_old[lbl][\"th_f1\"]),\n",
    "            \"th_fbeta15\": float(thr_old[lbl][\"th_fbeta15\"]),\n",
    "            \"ap_val\": float(thr_old[lbl].get(\"ap_val\", float(\"nan\")))\n",
    "        }\n",
    "        continue\n",
    "    th_f1_new = best_f1_threshold(Yva[valid,j].astype(int), P_blend[valid,j])\n",
    "    ap = float(average_precision_score(Yva[valid,j].astype(int), P_blend[valid,j]))\n",
    "    th_f1_old = float(thr_old[lbl][\"th_f1\"])\n",
    "    th_fb = float(thr_old[lbl][\"th_fbeta15\"])\n",
    "    print(f\"  {lbl:12s}  old_f1={th_f1_old:.3f} â†’ new_f1={th_f1_new:.3f}  (Î”={th_f1_new-th_f1_old:+.3f})  AP={ap:.3f}\")\n",
    "    new[\"thresholds\"][lbl] = {\"th_f1\": th_f1_new, \"th_fbeta15\": th_fb, \"ap_val\": ap}\n",
    "\n",
    "out_path = CAL_DIR / \"thresholds_blend_v2.json\"\n",
    "out_path.write_text(json.dumps(new, indent=2))\n",
    "print(\"\\nâœ… Saved updated thresholds â†’\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56494753",
   "metadata": {},
   "source": [
    "#### Test of CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e159313b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved:\n",
      "  â€¢ Detailed predictions â†’ v7\\results\\inference\\Truth_labels_detailed_v2.csv\n",
      "  â€¢ Summary metrics      â†’ v7\\results\\inference\\Truth_labels_summary_v2.json\n",
      "  â€¢ f1_soft delta = 0.040\n",
      "\n",
      "=== Micro metrics by mode ===\n",
      "  fbeta15   P=0.703  R=0.818  F1=0.756\n",
      "  f1        P=0.741  R=0.727  F1=0.734\n",
      "  f1_soft   P=0.705  R=0.782  F1=0.741\n",
      "  policy    P=0.691  R=0.691  F1=0.691\n",
      "\n",
      "=== Macro F1 by mode ===\n",
      "  fbeta15   macro-F1=0.705\n",
      "  f1        macro-F1=0.760\n",
      "  f1_soft   macro-F1=0.703\n",
      "  policy    macro-F1=0.739\n"
     ]
    }
   ],
   "source": [
    "# === Phase 6 â€” Cell 4b: Truth test with new f1_soft mode (uses thresholds_blend_v2.json if present) ===\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "BASE     = Path(\"v7\")\n",
    "DATA_XL  = BASE / \"data\" / \"Truth Lables.xlsx\"\n",
    "OUT_DIR  = BASE / \"results\" / \"inference\"\n",
    "CAL_DIR  = BASE / \"model\" / \"calibration\"\n",
    "POL_DIR  = BASE / \"model\" / \"policy\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prereqs from the self-contained rig\n",
    "need = ['v7_shared','HEADS','LABEL_NAMES','temps_spec','temps_shared','ALPHA','_probs_for_one']\n",
    "for n in need: assert n in globals(), f\"Missing {n}; run the policy-aware test rig cell first.\"\n",
    "\n",
    "# Load thresholds: prefer v2 if exists\n",
    "tb_v2 = CAL_DIR / \"thresholds_blend_v2.json\"\n",
    "tb_v1 = CAL_DIR / \"thresholds_blend.json\"\n",
    "assert tb_v1.exists(), f\"Missing {tb_v1}\"\n",
    "thr_payload = json.loads((tb_v2 if tb_v2.exists() else tb_v1).read_text())\n",
    "thr = thr_payload[\"thresholds\"]   # label -> {th_f1, th_fbeta15}\n",
    "ALPHA = float(thr_payload.get(\"alpha\", ALPHA))  # keep same alpha\n",
    "\n",
    "# Optional policy\n",
    "POL_PATH = POL_DIR / \"policy.json\"\n",
    "policy = json.loads(POL_PATH.read_text()) if POL_PATH.exists() else None\n",
    "\n",
    "# New soft band (delta)\n",
    "DELTA = 0.04\n",
    "\n",
    "def _threshold_for(lbl: str, mode: str, p_blend: float = None) -> float:\n",
    "    \"\"\"For 'f1_soft' we return the f1 threshold (decision rule adds softness outside).\"\"\"\n",
    "    if mode == \"f1\":\n",
    "        return float(thr[lbl][\"th_f1\"])\n",
    "    if mode == \"fbeta15\":\n",
    "        return float(thr[lbl][\"th_fbeta15\"])\n",
    "    if mode == \"policy\":\n",
    "        if policy is None: raise RuntimeError(\"policy.json not found\")\n",
    "        return float(policy[\"labels\"][lbl][\"threshold\"])\n",
    "    if mode == \"f1_soft\":\n",
    "        return float(thr[lbl][\"th_f1\"])\n",
    "    raise ValueError(\"mode must be one of: 'f1','fbeta15','policy','f1_soft'\")\n",
    "\n",
    "def _decide(lbl: str, mode: str, p: float) -> int:\n",
    "    if mode in (\"f1\",\"fbeta15\",\"policy\"):\n",
    "        th = _threshold_for(lbl, mode)\n",
    "        return int(p >= th)\n",
    "    if mode == \"f1_soft\":\n",
    "        th_f1 = float(thr[lbl][\"th_f1\"])\n",
    "        th_fb = float(thr[lbl][\"th_fbeta15\"])\n",
    "        if p >= th_f1: \n",
    "            return 1\n",
    "        if (p >= th_fb) and ((th_f1 - p) <= DELTA):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "# --- Load excel truth set ---\n",
    "assert DATA_XL.exists(), f\"Missing {DATA_XL}\"\n",
    "df = pd.read_excel(DATA_XL)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "smiles_col = next((c for c in [\"smiles\",\"SMILES\",\"Smile\",\"smile\",\"SMILE\"] if c in df.columns), None)\n",
    "assert smiles_col is not None, f\"No SMILES column found. Got: {list(df.columns)}\"\n",
    "LABELS = list(LABEL_NAMES)\n",
    "label_cols = [c for c in df.columns if c in LABELS]\n",
    "assert len(label_cols) == len(LABELS), f\"Label columns mismatch. Found {label_cols}, expected {LABELS}\"\n",
    "\n",
    "def _to01(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    try:\n",
    "        if isinstance(x,str):\n",
    "            xs=x.strip().lower()\n",
    "            if xs in {\"1\",\"true\",\"yes\",\"y\"}: return 1\n",
    "            if xs in {\"0\",\"false\",\"no\",\"n\"}: return 0\n",
    "            xv=float(xs); \n",
    "            if np.isnan(xv): return np.nan\n",
    "            return 1 if xv>=0.5 else 0\n",
    "        xv=float(x); \n",
    "        if np.isnan(xv): return np.nan\n",
    "        return 1 if xv>=0.5 else 0\n",
    "    except: \n",
    "        return np.nan\n",
    "\n",
    "MODES = [\"fbeta15\",\"f1\",\"f1_soft\"]\n",
    "if policy is not None: MODES.append(\"policy\")\n",
    "\n",
    "# --- Compute predictions ---\n",
    "rows=[]\n",
    "for idx, r in df.iterrows():\n",
    "    smi = str(r[smiles_col])\n",
    "    per_label = _probs_for_one(smi)  # {lbl:{prob_spec,prob_shared,prob_blend}}\n",
    "    for lbl in LABELS:\n",
    "        p = float(per_label[lbl][\"prob_blend\"])\n",
    "        truth = _to01(r.get(lbl, np.nan))\n",
    "        for m in MODES:\n",
    "            th = _threshold_for(lbl, m, p)\n",
    "            pred = _decide(lbl, m, p)\n",
    "            rows.append({\n",
    "                \"row_id\": int(idx),\n",
    "                \"smiles\": smi,\n",
    "                \"label\": lbl,\n",
    "                \"mode\": m,\n",
    "                \"prob_blend\": p,\n",
    "                \"threshold\": float(th),\n",
    "                \"prediction\": int(pred),\n",
    "                \"truth\": (np.nan if pd.isna(truth) else int(truth))\n",
    "            })\n",
    "detailed = pd.DataFrame(rows)\n",
    "\n",
    "# --- Metrics (robust) ---\n",
    "def _safe_div(n,d): return (n/d) if d>0 else np.nan\n",
    "def _prf(tp,fp,fn):\n",
    "    prec=_safe_div(tp,tp+fp); rec=_safe_div(tp,tp+fn)\n",
    "    f1=np.nan if (np.isnan(prec) or np.isnan(rec) or (prec+rec)==0) else (2*prec*rec/(prec+rec))\n",
    "    return prec,rec,f1\n",
    "\n",
    "def _metrics(df_long, mode):\n",
    "    dd=df_long[df_long[\"mode\"]==mode].dropna(subset=[\"truth\"]).copy()\n",
    "    if dd.empty:\n",
    "        return {\"mode\":mode,\"micro\":{\"precision\":np.nan,\"recall\":np.nan,\"f1\":np.nan},\"macro_f1\":np.nan,\"per_label\":[]}\n",
    "    y=dd[\"truth\"].astype(int).to_numpy(); p=dd[\"prediction\"].astype(int).to_numpy()\n",
    "    tp=int(((p==1)&(y==1)).sum()); fp=int(((p==1)&(y==0)).sum()); fn=int(((p==0)&(y==1)).sum())\n",
    "    M=_prf(tp,fp,fn)\n",
    "    per=[]\n",
    "    for lbl in LABELS:\n",
    "        d=dd[dd[\"label\"]==lbl]; \n",
    "        if d.empty: per.append({\"label\":lbl,\"precision\":np.nan,\"recall\":np.nan,\"f1\":np.nan}); continue\n",
    "        yj=d[\"truth\"].astype(int).to_numpy(); pj=d[\"prediction\"].astype(int).to_numpy()\n",
    "        tpj=int(((pj==1)&(yj==1)).sum()); fpj=int(((pj==1)&(yj==0)).sum()); fnj=int(((pj==0)&(yj==1)).sum())\n",
    "        pr,rc,f1=_prf(tpj,fpj,fnj)\n",
    "        per.append({\"label\":lbl,\"precision\":float(pr) if not np.isnan(pr) else np.nan,\n",
    "                           \"recall\":float(rc) if not np.isnan(rc) else np.nan,\n",
    "                           \"f1\":float(f1) if not np.isnan(f1) else np.nan})\n",
    "    macro_f1=float(np.nanmean([pl[\"f1\"] for pl in per])) if per else np.nan\n",
    "    return {\"mode\":mode,\"micro\":{\"precision\":float(M[0]) if not np.isnan(M[0]) else np.nan,\n",
    "                                 \"recall\":float(M[1]) if not np.isnan(M[1]) else np.nan,\n",
    "                                 \"f1\":float(M[2]) if not np.isnan(M[2]) else np.nan},\n",
    "            \"macro_f1\":macro_f1,\"per_label\":per}\n",
    "\n",
    "summary = {m:_metrics(detailed,m) for m in MODES}\n",
    "\n",
    "# --- Save v2 outputs ---\n",
    "csv_path  = OUT_DIR / \"Truth_labels_detailed_v2.csv\"\n",
    "json_path = OUT_DIR / \"Truth_labels_summary_v2.json\"\n",
    "detailed.to_csv(csv_path, index=False)\n",
    "json_path.write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"âœ… Saved:\")\n",
    "print(\"  â€¢ Detailed predictions â†’\", csv_path)\n",
    "print(\"  â€¢ Summary metrics      â†’\", json_path)\n",
    "print(f\"  â€¢ f1_soft delta = {DELTA:.3f}\")\n",
    "\n",
    "print(\"\\n=== Micro metrics by mode ===\")\n",
    "for m, rep in summary.items():\n",
    "    pr, rc, f1 = rep[\"micro\"][\"precision\"], rep[\"micro\"][\"recall\"], rep[\"micro\"][\"f1\"]\n",
    "    print(f\"  {m:8s}  P={np.nan if pr is None else pr:.3f}  R={np.nan if rc is None else rc:.3f}  F1={np.nan if f1 is None else f1:.3f}\")\n",
    "print(\"\\n=== Macro F1 by mode ===\")\n",
    "for m, rep in summary.items():\n",
    "    print(f\"  {m:8s}  macro-F1={rep['macro_f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3b284d",
   "metadata": {},
   "source": [
    "### Per-label stacking combiner (VAL), calibrate & threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28af29ef",
   "metadata": {},
   "source": [
    "#### A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00420314",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting per-label logistic stackers on VAL...\n",
      "  NR-AR       : T=0.826  AP_val=0.173  th_f1=0.966  th_fb=0.966\n",
      "  NR-AR-LBD   : T=0.602  AP_val=0.261  th_f1=0.945  th_fb=0.945\n",
      "  NR-AhR      : T=1.009  AP_val=0.518  th_f1=0.818  th_fb=0.713\n",
      "  NR-Aromatase: T=0.969  AP_val=0.293  th_f1=0.784  th_fb=0.622\n",
      "  NR-ER       : T=0.703  AP_val=0.251  th_f1=0.672  th_fb=0.504\n",
      "  NR-ER-LBD   : T=0.591  AP_val=0.116  th_f1=0.714  th_fb=0.714\n",
      "  NR-PPAR-gamma: T=1.134  AP_val=0.062  th_f1=0.513  th_fb=0.513\n",
      "  SR-ARE      : T=0.988  AP_val=0.344  th_f1=0.531  th_fb=0.524\n",
      "  SR-ATAD5    : T=0.946  AP_val=0.167  th_f1=0.709  th_fb=0.709\n",
      "  SR-HSE      : T=0.952  AP_val=0.216  th_f1=0.651  th_fb=0.651\n",
      "  SR-MMP      : T=0.953  AP_val=0.445  th_f1=0.647  th_fb=0.612\n",
      "  SR-p53      : T=1.015  AP_val=0.217  th_f1=0.625  th_fb=0.625\n",
      "\n",
      "âœ… Stacking complete.\n",
      "  â€¢ Models         â†’ v7_exp\\stacking_asl_v1\\model\\stacking_lr\n",
      "  â€¢ Temps (stack)  â†’ v7_exp\\stacking_asl_v1\\calibration\\temps_stack.json\n",
      "  â€¢ Thresholds     â†’ v7_exp\\stacking_asl_v1\\calibration\\thresholds_stack.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell A: Per-label stacking on VAL (logistic combiner of [spec_logit, shared_logit])\n",
    "# Saves under v7_exp/stacking_asl_v1/* without touching v7/*\n",
    "import json, math, os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from joblib import dump as joblib_dump\n",
    "\n",
    "# ---------- Paths (new experiment root) ----------\n",
    "ROOT_OLD = Path(\"v7\")\n",
    "ROOT_NEW = Path(\"v7_exp\") / \"stacking_asl_v1\"\n",
    "ROOT_NEW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_OLD = ROOT_OLD / \"model\"\n",
    "CAL_OLD   = MODEL_OLD / \"calibration\"\n",
    "ENS_OLD   = MODEL_OLD / \"ensembles\"\n",
    "FUSED     = ROOT_OLD / \"data\" / \"fused\"\n",
    "PREP      = ROOT_OLD / \"data\" / \"prepared\"\n",
    "\n",
    "MODEL_NEW = ROOT_NEW / \"model\"\n",
    "STACK_DIR = MODEL_NEW / \"stacking_lr\"\n",
    "CAL_NEW   = ROOT_NEW / \"calibration\"\n",
    "EVAL_NEW  = ROOT_NEW / \"eval\"\n",
    "for p in [STACK_DIR, CAL_NEW, EVAL_NEW]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load manifests / artifacts ----------\n",
    "mani = json.loads((PREP / \"dataset_manifest.json\").read_text())\n",
    "LABELS: List[str] = mani[\"labels\"]; N_LABELS = len(LABELS)\n",
    "\n",
    "# fused VAL features (B,768)\n",
    "Xv = np.load(FUSED / \"val_fused.npy\").astype(np.float32)\n",
    "val_blob = np.load(PREP / \"val.npz\", allow_pickle=True)\n",
    "Yv = val_blob[\"Y\"].astype(np.float32)                # (Nv, L)\n",
    "Mv = val_blob[\"y_missing_mask\"].astype(bool)         # True where missing\n",
    "\n",
    "# specialist temperatures and shared temperatures\n",
    "temps_spec   = json.loads((CAL_OLD / \"temps.json\").read_text())\n",
    "temps_shared = json.loads((CAL_OLD / \"temps_shared.json\").read_text())\n",
    "\n",
    "# shared-head MLP: reuse weights from v7/model/checkpoints/shared/best.pt\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sh = SharedHeadMLP().to(device)\n",
    "ck = torch.load(MODEL_OLD / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "state = {k.replace(\"shared_head.\",\"\"): v for k,v in ck[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(state, strict=True); sh.eval()\n",
    "\n",
    "# specialist heads: load best per label from v7/ensembles/*\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(nn.Linear(in_dim,h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2=nn.Sequential(nn.Linear(h1,h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3=nn.Sequential(nn.Linear(h2,h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out=nn.Linear(h3,1); self.short=nn.Linear(in_dim,h3)\n",
    "    def forward(self, x):\n",
    "        z1=self.block1(x); z2=self.block2(z1); z3=self.block3(z2)\n",
    "        return self.out(z3 + self.short(x)).squeeze(-1)\n",
    "\n",
    "def _remap(sd: dict)->dict:\n",
    "    if not any(k.startswith((\"b1.\",\"b2.\",\"b3.\")) for k in sd): return sd\n",
    "    m={}\n",
    "    for k,v in sd.items():\n",
    "        m[k.replace(\"b1.\",\"block1.\").replace(\"b2.\",\"block2.\").replace(\"b3.\",\"block3.\")] = v\n",
    "    return m\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    cands=[]\n",
    "    for sd in sorted((ENS_OLD/label).glob(\"seed*/\")):\n",
    "        m = sd/\"metrics.json\"\n",
    "        if m.exists():\n",
    "            try: cands.append((float(json.loads(m.read_text()).get(\"best_ap\", float(\"nan\"))), sd))\n",
    "            except: pass\n",
    "    if not cands: raise FileNotFoundError(f\"No heads for {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best = cands[0][1]\n",
    "    ckpt = torch.load(best/\"best.pt\", map_location=device)\n",
    "    cfg = ckpt.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(cfg[\"in_dim\"],cfg[\"h1\"],cfg[\"h2\"],cfg[\"h3\"],cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(_remap(ckpt[\"model\"]), strict=True); head.eval(); return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "\n",
    "# ---------- Build VAL logits: shared & specialist ----------\n",
    "Xv_t = torch.tensor(Xv, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    sh_logits = sh(Xv_t).cpu().numpy()                           # (Nv, L)\n",
    "    spec_logits = torch.stack([HEADS[l](Xv_t) for l in LABELS],  # -> (L, Nv)\n",
    "                               dim=1).cpu().numpy()              # -> (Nv, L)\n",
    "\n",
    "# ---------- Train per-label logistic combiner on VAL ----------\n",
    "def fit_temperature(logits: np.ndarray, y: np.ndarray, max_iter=200, lr=0.05) -> float:\n",
    "    t = torch.tensor([1.0], dtype=torch.float32, requires_grad=True, device=device)\n",
    "    x = torch.tensor(logits, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(y,      dtype=torch.float32, device=device)\n",
    "    opt = torch.optim.Adam([t], lr=lr)\n",
    "    for _ in range(max_iter):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z = x / (t.clamp(min=1e-3))\n",
    "        p = torch.sigmoid(z).clamp(1e-6, 1-1e-6)\n",
    "        loss = - (y*torch.log(p) + (1-y)*torch.log(1-p)).mean()\n",
    "        loss.backward(); opt.step()\n",
    "    return float(t.detach().cpu().item())\n",
    "\n",
    "def best_thresholds(y_true: np.ndarray, probs: np.ndarray) -> Dict[str,float]:\n",
    "    prec, rec, th = precision_recall_curve(y_true, probs)\n",
    "    eps=1e-8\n",
    "    f1  = (2*prec*rec)/np.maximum(prec+rec, eps)\n",
    "    beta=1.5\n",
    "    fb  = ((1+beta**2)*prec*rec)/np.maximum((beta**2)*prec+rec, eps)\n",
    "    th_f1 = th[np.nanargmax(f1[1:])] if th.size>0 else 0.5\n",
    "    th_fb = th[np.nanargmax(fb[1:])] if th.size>0 else 0.5\n",
    "    ap = float(average_precision_score(y_true, probs)) if (~np.isnan(y_true)).any() else float(\"nan\")\n",
    "    return {\"th_f1\": float(th_f1), \"th_fbeta15\": float(th_fb), \"ap_val\": ap}\n",
    "\n",
    "comb_meta = {}\n",
    "temps_stack = {}\n",
    "thr_stack   = {}\n",
    "\n",
    "print(\"Fitting per-label logistic stackers on VAL...\")\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    valid = ~Mv[:, j]\n",
    "    x = np.stack([spec_logits[valid, j], sh_logits[valid, j]], axis=1)   # (Nv_valid, 2)\n",
    "    y = Yv[valid, j].astype(int)\n",
    "    if x.shape[0] == 0 or y.max()==y.min():\n",
    "        print(f\"  {lbl}: degenerate label on VAL â†’ skipping (copying old thresholds).\")\n",
    "        continue\n",
    "    # Logistic combiner with class_weight='balanced' to help rare positives\n",
    "    lr = LogisticRegression(\n",
    "        penalty=\"l2\", C=1.0, solver=\"lbfgs\", max_iter=500,\n",
    "        class_weight=\"balanced\", fit_intercept=True\n",
    "    )\n",
    "    lr.fit(x, y)\n",
    "    # raw combiner logits on VAL\n",
    "    z_val = lr.decision_function(x)  # shape (Nv_valid,)\n",
    "    # temperature on combiner logits\n",
    "    T = fit_temperature(z_val, y, max_iter=300, lr=0.05)\n",
    "    temps_stack[lbl] = T\n",
    "    # calibrated probs\n",
    "    p_cal = 1.0 / (1.0 + np.exp(-(z_val / max(T,1e-3))))\n",
    "    thr = best_thresholds(y, p_cal)\n",
    "    thr_stack[lbl] = thr\n",
    "    # save model per label\n",
    "    lbl_dir = STACK_DIR / lbl\n",
    "    lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "    joblib_dump(lr, lbl_dir / \"stack_lr.joblib\")\n",
    "    json.dump({\"coef\": lr.coef_.tolist(), \"intercept\": lr.intercept_.tolist(),\n",
    "               \"n\": int(x.shape[0]), \"pos_rate\": float(y.mean())},\n",
    "              open(lbl_dir / \"meta.json\",\"w\"), indent=2)\n",
    "    print(f\"  {lbl:12s}: T={T:.3f}  AP_val={thr['ap_val']:.3f}  th_f1={thr['th_f1']:.3f}  th_fb={thr['th_fbeta15']:.3f}\")\n",
    "\n",
    "# Save global calibration/thresholds for stackers\n",
    "json.dump(temps_stack, open(CAL_NEW / \"temps_stack.json\",\"w\"), indent=2)\n",
    "json.dump({\"thresholds\": thr_stack, \"note\": \"Per-label logistic stacker on VAL\"},\n",
    "          open(CAL_NEW / \"thresholds_stack.json\",\"w\"), indent=2)\n",
    "\n",
    "print(\"\\nâœ… Stacking complete.\")\n",
    "print(\"  â€¢ Models         â†’\", STACK_DIR)\n",
    "print(\"  â€¢ Temps (stack)  â†’\", CAL_NEW / \"temps_stack.json\")\n",
    "print(\"  â€¢ Thresholds     â†’\", CAL_NEW / \"thresholds_stack.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a2f52df",
   "metadata": {},
   "source": [
    "#### B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa5f953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels selected for ASL tweak: ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ATAD5', 'SR-HSE', 'SR-p53']\n",
      "[NR-AR] epoch 1/6  train_loss=0.0878  val_AP=0.1491\n",
      "[NR-AR] epoch 2/6  train_loss=0.0643  val_AP=0.1636\n",
      "[NR-AR] epoch 3/6  train_loss=0.0616  val_AP=0.1550\n",
      "[NR-AR] epoch 4/6  train_loss=0.0575  val_AP=0.1657\n",
      "[NR-AR] epoch 5/6  train_loss=0.0562  val_AP=0.1765\n",
      "[NR-AR] epoch 6/6  train_loss=0.0555  val_AP=0.1734\n",
      "[NR-AR-LBD] epoch 1/6  train_loss=0.0808  val_AP=0.1397\n",
      "[NR-AR-LBD] epoch 2/6  train_loss=0.0486  val_AP=0.2891\n",
      "[NR-AR-LBD] epoch 3/6  train_loss=0.0467  val_AP=0.3058\n",
      "[NR-AR-LBD] epoch 4/6  train_loss=0.0472  val_AP=0.2826\n",
      "[NR-AR-LBD] epoch 5/6  train_loss=0.0460  val_AP=0.2834\n",
      "[NR-AR-LBD] epoch 6/6  train_loss=0.0447  val_AP=0.2944\n",
      "[NR-AhR] epoch 1/6  train_loss=0.2295  val_AP=0.5028\n",
      "[NR-AhR] epoch 2/6  train_loss=0.1584  val_AP=0.5323\n",
      "[NR-AhR] epoch 3/6  train_loss=0.1501  val_AP=0.5355\n",
      "[NR-AhR] epoch 4/6  train_loss=0.1470  val_AP=0.5383\n",
      "[NR-AhR] epoch 5/6  train_loss=0.1447  val_AP=0.5444\n",
      "[NR-AhR] epoch 6/6  train_loss=0.1428  val_AP=0.5274\n",
      "[NR-Aromatase] epoch 1/6  train_loss=0.1224  val_AP=0.2222\n",
      "[NR-Aromatase] epoch 2/6  train_loss=0.0850  val_AP=0.3375\n",
      "[NR-Aromatase] epoch 3/6  train_loss=0.0733  val_AP=0.3394\n",
      "[NR-Aromatase] epoch 4/6  train_loss=0.0769  val_AP=0.3256\n",
      "[NR-Aromatase] epoch 5/6  train_loss=0.0737  val_AP=0.3545\n",
      "[NR-Aromatase] epoch 6/6  train_loss=0.0707  val_AP=0.3700\n",
      "[NR-ER] epoch 1/6  train_loss=0.1365  val_AP=0.1821\n",
      "[NR-ER] epoch 2/6  train_loss=0.1085  val_AP=0.1917\n",
      "[NR-ER] epoch 3/6  train_loss=0.1035  val_AP=0.2563\n",
      "[NR-ER] epoch 4/6  train_loss=0.1026  val_AP=0.2616\n",
      "[NR-ER] epoch 5/6  train_loss=0.1029  val_AP=0.2713\n",
      "[NR-ER] epoch 6/6  train_loss=0.1009  val_AP=0.2565\n",
      "[NR-ER-LBD] epoch 1/6  train_loss=0.1267  val_AP=0.1324\n",
      "[NR-ER-LBD] epoch 2/6  train_loss=0.0871  val_AP=0.1263\n",
      "[NR-ER-LBD] epoch 3/6  train_loss=0.0831  val_AP=0.1034\n",
      "[NR-ER-LBD] epoch 4/6  train_loss=0.0794  val_AP=0.1152\n",
      "[NR-PPAR-gamma] epoch 1/6  train_loss=0.0780  val_AP=0.0804\n",
      "[NR-PPAR-gamma] epoch 2/6  train_loss=0.0479  val_AP=0.1734\n",
      "[NR-PPAR-gamma] epoch 3/6  train_loss=0.0481  val_AP=0.1624\n",
      "[NR-PPAR-gamma] epoch 4/6  train_loss=0.0473  val_AP=0.1636\n",
      "[NR-PPAR-gamma] epoch 5/6  train_loss=0.0423  val_AP=0.1663\n",
      "[SR-ARE] epoch 1/6  train_loss=0.1906  val_AP=0.3257\n",
      "[SR-ARE] epoch 2/6  train_loss=0.1229  val_AP=0.3453\n",
      "[SR-ARE] epoch 3/6  train_loss=0.1170  val_AP=0.3515\n",
      "[SR-ARE] epoch 4/6  train_loss=0.1172  val_AP=0.3627\n",
      "[SR-ARE] epoch 5/6  train_loss=0.1153  val_AP=0.3727\n",
      "[SR-ARE] epoch 6/6  train_loss=0.1141  val_AP=0.3727\n",
      "[SR-ATAD5] epoch 1/6  train_loss=0.1121  val_AP=0.1740\n",
      "[SR-ATAD5] epoch 2/6  train_loss=0.0638  val_AP=0.1723\n",
      "[SR-ATAD5] epoch 3/6  train_loss=0.0612  val_AP=0.1667\n",
      "[SR-ATAD5] epoch 4/6  train_loss=0.0599  val_AP=0.1734\n",
      "[SR-HSE] epoch 1/6  train_loss=0.1698  val_AP=0.2902\n",
      "[SR-HSE] epoch 2/6  train_loss=0.0966  val_AP=0.3549\n",
      "[SR-HSE] epoch 3/6  train_loss=0.0908  val_AP=0.4094\n",
      "[SR-HSE] epoch 4/6  train_loss=0.0908  val_AP=0.4642\n",
      "[SR-HSE] epoch 5/6  train_loss=0.0907  val_AP=0.4333\n",
      "[SR-HSE] epoch 6/6  train_loss=0.0883  val_AP=0.4870\n",
      "[SR-MMP] epoch 1/6  train_loss=0.1579  val_AP=0.4647\n",
      "[SR-MMP] epoch 2/6  train_loss=0.1096  val_AP=0.5024\n",
      "[SR-MMP] epoch 3/6  train_loss=0.1031  val_AP=0.5449\n",
      "[SR-MMP] epoch 4/6  train_loss=0.1031  val_AP=0.5558\n",
      "[SR-MMP] epoch 5/6  train_loss=0.0989  val_AP=0.5564\n",
      "[SR-MMP] epoch 6/6  train_loss=0.0956  val_AP=0.5629\n",
      "[SR-p53] epoch 1/6  train_loss=0.1320  val_AP=0.1801\n",
      "[SR-p53] epoch 2/6  train_loss=0.0974  val_AP=0.1873\n",
      "[SR-p53] epoch 3/6  train_loss=0.0940  val_AP=0.1963\n",
      "[SR-p53] epoch 4/6  train_loss=0.0918  val_AP=0.1950\n",
      "[SR-p53] epoch 5/6  train_loss=0.0931  val_AP=0.2379\n",
      "[SR-p53] epoch 6/6  train_loss=0.0918  val_AP=0.2207\n",
      "\n",
      "âœ… ASL retrain complete.\n",
      "  â€¢ New heads â†’ v7_exp\\stacking_asl_v1\\model\\ensembles_v2\n"
     ]
    }
   ],
   "source": [
    "# === Cell B: Fast specialist-head retrain with ASL tweaks (heads only, fused inputs)\n",
    "import json, math, os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# ---------- Paths ----------\n",
    "ROOT_OLD = Path(\"v7\")\n",
    "FUSED    = ROOT_OLD / \"data\" / \"fused\"\n",
    "PREP     = ROOT_OLD / \"data\" / \"prepared\"\n",
    "\n",
    "ROOT_NEW = Path(\"v7_exp\") / \"stacking_asl_v1\"\n",
    "MODEL_NEW= ROOT_NEW / \"model\"\n",
    "ENS_NEW  = MODEL_NEW / \"ensembles_v2\"\n",
    "LOG_NEW  = ROOT_NEW / \"logs\"\n",
    "for p in [ENS_NEW, LOG_NEW]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Data ----------\n",
    "# fused train/val + labels/masks\n",
    "Xtr = np.load(FUSED / \"train_fused.npy\").astype(np.float32)\n",
    "Xva = np.load(FUSED / \"val_fused.npy\").astype(np.float32)\n",
    "tr = np.load(PREP / \"train.npz\", allow_pickle=True)\n",
    "va = np.load(PREP / \"val.npz\", allow_pickle=True)\n",
    "Ytr = tr[\"Y\"].astype(np.float32); Mtr = tr[\"y_missing_mask\"].astype(bool)\n",
    "Yva = va[\"Y\"].astype(np.float32); Mva = va[\"y_missing_mask\"].astype(bool)\n",
    "\n",
    "mani = json.loads((PREP / \"dataset_manifest.json\").read_text())\n",
    "LABELS: List[str] = mani[\"labels\"]; N_LABELS=len(LABELS)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Simple dataset ----------\n",
    "class FusedDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "# ---------- Label head (same architecture as before) ----------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(nn.Linear(in_dim,h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2=nn.Sequential(nn.Linear(h1,h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3=nn.Sequential(nn.Linear(h2,h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out=nn.Linear(h3,1); self.short=nn.Linear(in_dim,h3)\n",
    "    def forward(self, x):\n",
    "        z1=self.block1(x); z2=self.block2(z1); z3=self.block3(z2)\n",
    "        return self.out(z3 + self.short(x)).squeeze(-1)\n",
    "\n",
    "# ---------- Asymmetric Loss (recall-leaning) ----------\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_pos=0.0, gamma_neg=4.0, clip=0.05, eps=1e-8, alpha_pos=1.0, alpha_neg=1.0):\n",
    "        super().__init__()\n",
    "        self.gamma_pos=gamma_pos; self.gamma_neg=gamma_neg\n",
    "        self.clip=clip; self.eps=eps; self.alpha_pos=alpha_pos; self.alpha_neg=alpha_neg\n",
    "    def forward(self, logits, y):\n",
    "        x_sigmoid = torch.sigmoid(logits)\n",
    "        xs_pos = x_sigmoid; xs_neg = 1.0 - x_sigmoid\n",
    "        if self.clip is not None and self.clip>0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1.0)\n",
    "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
    "        los_neg = (1.0 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
    "        if self.gamma_pos>0 or self.gamma_neg>0:\n",
    "            pt = torch.where(y>=0.5, xs_pos, xs_neg)\n",
    "            gamma = torch.where(y>=0.5, torch.tensor(self.gamma_pos, device=logits.device),\n",
    "                                           torch.tensor(self.gamma_neg, device=logits.device))\n",
    "            los_pos = los_pos * torch.pow(1.0 - xs_pos, self.gamma_pos)\n",
    "            los_neg = los_neg * torch.pow(1.0 - xs_neg, self.gamma_neg)\n",
    "        # class weighting\n",
    "        los = -(self.alpha_pos*los_pos + self.alpha_neg*los_neg)\n",
    "        return los.mean()\n",
    "\n",
    "# ---------- Auto-pick labels to tweak ----------\n",
    "# Use VAL prevalence and (if present) stack AP to identify \"hard/rare\" labels\n",
    "thr_stack_path = ROOT_NEW / \"calibration\" / \"thresholds_stack.json\"\n",
    "ap_hint = {}\n",
    "if thr_stack_path.exists():\n",
    "    pay = json.loads(thr_stack_path.read_text())\n",
    "    for lbl, d in pay[\"thresholds\"].items():\n",
    "        ap_hint[lbl] = float(d.get(\"ap_val\", np.nan))\n",
    "\n",
    "# prevalence on VAL\n",
    "prev_val = {LABELS[j]: float(Yva[~Mva[:,j], j].mean()) if (~Mva[:,j]).any() else np.nan for j in range(N_LABELS)}\n",
    "\n",
    "# selection rule (editable): low prevalence (<8%) OR AP hint < 0.20\n",
    "TO_TWEAK = sorted([lbl for lbl in LABELS if\n",
    "                   (not np.isnan(prev_val.get(lbl, np.nan)) and prev_val[lbl] < 0.08) or\n",
    "                   (not np.isnan(ap_hint.get(lbl, np.nan)) and ap_hint[lbl] < 0.20)])\n",
    "\n",
    "print(\"Labels selected for ASL tweak:\", TO_TWEAK)\n",
    "\n",
    "# ---------- Training hyperparams ----------\n",
    "BS = 256\n",
    "EPOCHS = 6\n",
    "LR = 1e-3\n",
    "WD = 1e-4\n",
    "PATIENCE = 3\n",
    "\n",
    "# Recall-leaning ASL defaults for tweaked labels (you can adjust)\n",
    "ASL_TWEAK = dict(gamma_neg=2.0, gamma_pos=0.0, alpha_pos=1.3, alpha_neg=1.0, clip=0.05)\n",
    "ASL_BASE  = dict(gamma_neg=4.0, gamma_pos=0.0, alpha_pos=1.0, alpha_neg=1.0, clip=0.05)\n",
    "\n",
    "# ---------- Train per label ----------\n",
    "def _val_ap(head: nn.Module, X: np.ndarray, y: np.ndarray) -> float:\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        z = head(torch.tensor(X, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "        p = 1.0/(1.0+np.exp(-z))\n",
    "    try:\n",
    "        return float(average_precision_score(y.astype(int), p))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    # slice train/val rows with known labels\n",
    "    tr_mask = ~Mtr[:, j]; va_mask = ~Mva[:, j]\n",
    "    Xtr_j, ytr_j = Xtr[tr_mask], Ytr[tr_mask, j]\n",
    "    Xva_j, yva_j = Xva[va_mask], Yva[va_mask, j]\n",
    "    if Xtr_j.shape[0] == 0 or Xva_j.shape[0] == 0:\n",
    "        print(f\"[{lbl}] no data â†’ skip.\")\n",
    "        continue\n",
    "\n",
    "    # choose ASL config\n",
    "    cfg = ASL_TWEAK if lbl in TO_TWEAK else ASL_BASE\n",
    "    loss_fn = AsymmetricLoss(**cfg)\n",
    "\n",
    "    head = LabelHead().to(device)\n",
    "    opt = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "    ds_tr = FusedDataset(Xtr_j, ytr_j)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=BS, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    best_ap = -1.0; best_state=None; bad=0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        head.train()\n",
    "        losses=[]\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = head(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward(); opt.step()\n",
    "            losses.append(float(loss.item()))\n",
    "        ap = _val_ap(head, Xva_j, yva_j)\n",
    "        print(f\"[{lbl}] epoch {epoch}/{EPOCHS}  train_loss={np.mean(losses):.4f}  val_AP={ap:.4f}\")\n",
    "        if ap > best_ap + 1e-4:\n",
    "            best_ap = ap; best_state = {k: v.detach().cpu().clone() for k, v in head.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= PATIENCE: break\n",
    "\n",
    "    # save best\n",
    "    out_dir = ENS_NEW / lbl / \"seed00\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save({\"model\": best_state if best_state is not None else head.state_dict(),\n",
    "                \"config\": {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30},\n",
    "                \"best_ap\": best_ap},\n",
    "               out_dir / \"best.pt\")\n",
    "    json.dump({\"label\": lbl, \"best_ap\": best_ap, \"asl_cfg\": cfg, \"epochs\": epoch},\n",
    "              open(out_dir / \"metrics.json\",\"w\"), indent=2)\n",
    "\n",
    "print(\"\\nâœ… ASL retrain complete.\")\n",
    "print(\"  â€¢ New heads â†’\", ENS_NEW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1306a253",
   "metadata": {},
   "source": [
    "#### C) test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21652db3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on VAL and TEST...\n",
      "\n",
      "=== VAL ===\n",
      " Old F1   : P=0.244  R=0.474  F1=0.322  | macro-F1=0.311\n",
      " Stack F1 : P=0.221  R=0.506  F1=0.308  | macro-F1=0.307\n",
      "\n",
      "=== TEST ===\n",
      " Old F1   : P=0.232  R=0.462  F1=0.309  | macro-F1=0.278\n",
      " Stack F1 : P=0.212  R=0.490  F1=0.296  | macro-F1=0.261\n",
      "\n",
      "âœ… Saved:\n",
      "  â€¢ v7_exp\\stacking_asl_v1\\eval\\compare_val.json\n",
      "  â€¢ v7_exp\\stacking_asl_v1\\eval\\compare_val.csv\n",
      "  â€¢ v7_exp\\stacking_asl_v1\\eval\\compare_test.json\n",
      "  â€¢ v7_exp\\stacking_asl_v1\\eval\\compare_test.csv\n",
      "\n",
      "(use_new_heads=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# === Compare old F1 blend vs NEW stacked F1 on VAL & TEST (self-contained) ===\n",
    "# Saves under: v7_exp/stacking_asl_v1/eval/*\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from joblib import load as joblib_load\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "ROOT_OLD   = Path(\"v7\")\n",
    "PREP       = ROOT_OLD / \"data\" / \"prepared\"\n",
    "FUSED      = ROOT_OLD / \"data\" / \"fused\"\n",
    "MODEL_OLD  = ROOT_OLD / \"model\"\n",
    "ENS_OLD    = MODEL_OLD / \"ensembles\"\n",
    "CAL_OLD    = MODEL_OLD / \"calibration\"\n",
    "\n",
    "ROOT_NEW   = Path(\"v7_exp\") / \"stacking_asl_v1\"\n",
    "MODEL_NEW  = ROOT_NEW / \"model\"\n",
    "ENS_NEW    = MODEL_NEW / \"ensembles_v2\"          # optional: ASL-tweaked heads\n",
    "STACK_DIR  = MODEL_NEW / \"stacking_lr\"\n",
    "CAL_NEW    = ROOT_NEW / \"calibration\"\n",
    "EVAL_DIR   = ROOT_NEW / \"eval\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Toggle: use ASL-tweaked heads for *both* methods (may mismatch the stackers' training)\n",
    "USE_NEW_HEADS = False   # default False (recommended)\n",
    "\n",
    "# -------------------- Manifests --------------------\n",
    "mani = json.loads((PREP / \"dataset_manifest.json\").read_text())\n",
    "LABELS: List[str] = mani[\"labels\"]; N_LABELS = len(LABELS)\n",
    "\n",
    "# Baseline (old) calibration\n",
    "thr_blend_v = CAL_OLD / \"thresholds_blend_v2.json\"\n",
    "thr_blend_1 = CAL_OLD / \"thresholds_blend.json\"\n",
    "assert thr_blend_v.exists() or thr_blend_1.exists(), \"Missing thresholds_blend file(s)\"\n",
    "blend_payload = json.loads((thr_blend_v if thr_blend_v.exists() else thr_blend_1).read_text())\n",
    "ALPHA   = float(blend_payload.get(\"alpha\", 0.8))\n",
    "THR_OLD = blend_payload[\"thresholds\"]  # label -> {th_f1, th_fbeta15, ap_val}\n",
    "temps_spec   = json.loads((CAL_OLD / \"temps.json\").read_text())\n",
    "temps_shared = json.loads((CAL_OLD / \"temps_shared.json\").read_text())\n",
    "\n",
    "# Stacked calibration\n",
    "temps_stack  = json.loads((CAL_NEW / \"temps_stack.json\").read_text())\n",
    "thr_stack    = json.loads((CAL_NEW / \"thresholds_stack.json\").read_text())[\"thresholds\"]\n",
    "\n",
    "# -------------------- Models (shared head + label heads) --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "# load shared head weights from v7 (unchanged)\n",
    "shared_ckpt = torch.load(MODEL_OLD / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "sh = SharedHeadMLP().to(device)\n",
    "sh_state = {k.replace(\"shared_head.\",\"\"): v for k,v in shared_ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(sh_state, strict=True); sh.eval()\n",
    "\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(nn.Linear(in_dim,h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2=nn.Sequential(nn.Linear(h1,h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3=nn.Sequential(nn.Linear(h2,h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out=nn.Linear(h3,1); self.short=nn.Linear(in_dim,h3)\n",
    "    def forward(self, x):\n",
    "        z1=self.block1(x); z2=self.block2(z1); z3=self.block3(z2)\n",
    "        return self.out(z3 + self.short(x)).squeeze(-1)\n",
    "\n",
    "def _remap(sd: dict)->dict:\n",
    "    if not any(k.startswith((\"b1.\",\"b2.\",\"b3.\")) for k in sd): return sd\n",
    "    m={}; \n",
    "    for k,v in sd.items():\n",
    "        m[k.replace(\"b1.\",\"block1.\").replace(\"b2.\",\"block2.\").replace(\"b3.\",\"block3.\")] = v\n",
    "    return m\n",
    "\n",
    "def load_best_head(label: str, use_new: bool=False) -> nn.Module:\n",
    "    base = ENS_NEW if use_new else ENS_OLD\n",
    "    cands=[]\n",
    "    for sd in sorted((base/label).glob(\"seed*/\")):\n",
    "        m = sd/\"metrics.json\"\n",
    "        if m.exists():\n",
    "            try: cands.append((float(json.loads(m.read_text()).get(\"best_ap\", float(\"nan\"))), sd))\n",
    "            except: pass\n",
    "    if not cands: raise FileNotFoundError(f\"No heads for {label} under {base/label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best = cands[0][1]\n",
    "    ck = torch.load(best/\"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(cfg[\"in_dim\"],cfg[\"h1\"],cfg[\"h2\"],cfg[\"h3\"],cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(_remap(ck[\"model\"]), strict=True); head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl, use_new=USE_NEW_HEADS) for lbl in LABELS}\n",
    "\n",
    "# Load per-label stackers (trained on OLD heads; if USE_NEW_HEADS=True, results may shift)\n",
    "STACKERS = {lbl: joblib_load((STACK_DIR/lbl/\"stack_lr.joblib\")) for lbl in LABELS}\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def build_logits(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute shared and specialist logits on fused features.\"\"\"\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        sh_logits   = sh(X_t).cpu().numpy()                          # (N, L)\n",
    "        spec_logits = torch.stack([HEADS[l](X_t) for l in LABELS],   # (L, N)\n",
    "                                  dim=1).cpu().numpy()               # -> (N, L)\n",
    "    return spec_logits, sh_logits\n",
    "\n",
    "def old_blend_probs(spec_logits: np.ndarray, sh_logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calibrated per-stream â†’ blended probs with ALPHA.\"\"\"\n",
    "    N, L = spec_logits.shape\n",
    "    P = np.zeros_like(spec_logits, dtype=np.float32)\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        Ts = max(float(temps_spec.get(lbl,1.0)), 1e-3)\n",
    "        Th = max(float(temps_shared.get(lbl,1.0)), 1e-3)\n",
    "        ps = sigmoid(spec_logits[:,j]/Ts)\n",
    "        ph = sigmoid(sh_logits[:,j]/Th)\n",
    "        P[:,j] = np.clip(ALPHA*ps + (1-ALPHA)*ph, 0, 1)\n",
    "    return P  # (N,L)\n",
    "\n",
    "def stack_probs(spec_logits: np.ndarray, sh_logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Per-label logistic combiner [spec,shared] â†’ logit â†’ temp â†’ prob.\"\"\"\n",
    "    N, L = spec_logits.shape\n",
    "    P = np.zeros_like(spec_logits, dtype=np.float32)\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        lr = STACKERS[lbl]\n",
    "        z  = lr.decision_function(np.stack([spec_logits[:,j], sh_logits[:,j]], axis=1))\n",
    "        T  = max(float(temps_stack.get(lbl,1.0)), 1e-3)\n",
    "        P[:,j] = sigmoid(z / T)\n",
    "    return P  # (N,L)\n",
    "\n",
    "def metric_micro_macro(Y_true: np.ndarray, M_missing: np.ndarray,\n",
    "                       P: np.ndarray, thresholds: Dict[str, Dict[str, float]], mode=\"f1\") -> Dict:\n",
    "    \"\"\"Compute micro/macro F1 given probs and per-label thresholds.\"\"\"\n",
    "    assert mode in (\"f1\",\"fbeta15\")\n",
    "    L = Y_true.shape[1]\n",
    "    valid_mask = ~M_missing\n",
    "    # predictions\n",
    "    preds = np.zeros_like(P, dtype=np.int32)\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        th = thresholds[lbl][\"th_f1\"] if mode==\"f1\" else thresholds[lbl][\"th_fbeta15\"]\n",
    "        preds[:,j] = (P[:,j] >= float(th)).astype(np.int32)\n",
    "    # micro\n",
    "    y = Y_true[valid_mask].astype(int); p = preds[valid_mask]\n",
    "    tp = int(((p==1)&(y==1)).sum()); fp = int(((p==1)&(y==0)).sum()); fn = int(((p==0)&(y==1)).sum())\n",
    "    micro_prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    micro_rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec)>0 else 0.0\n",
    "    # per-label\n",
    "    per=[]\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        mask = valid_mask[:,j]\n",
    "        if not mask.any():\n",
    "            per.append({\"label\": lbl, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan}); continue\n",
    "        yj = Y_true[mask, j].astype(int); pj = preds[mask, j]\n",
    "        tpj = int(((pj==1)&(yj==1)).sum()); fpj = int(((pj==1)&(yj==0)).sum()); fnj = int(((pj==0)&(yj==1)).sum())\n",
    "        prec = (tpj/(tpj+fpj)) if (tpj+fpj)>0 else np.nan\n",
    "        rec  = (tpj/(tpj+fnj)) if (tpj+fnj)>0 else np.nan\n",
    "        f1   = (2*prec*rec/(prec+rec)) if (not np.isnan(prec) and not np.isnan(rec) and (prec+rec)>0) else np.nan\n",
    "        per.append({\"label\": lbl, \"precision\": prec, \"recall\": rec, \"f1\": f1})\n",
    "    macro_f1 = float(np.nanmean([d[\"f1\"] for d in per]))\n",
    "    return {\"micro\": {\"precision\": micro_prec, \"recall\": micro_rec, \"f1\": micro_f1},\n",
    "            \"macro_f1\": macro_f1, \"per_label\": per, \"n_samples\": int(Y_true.shape[0])}\n",
    "\n",
    "def evaluate_split(split: str) -> Dict:\n",
    "    assert split in (\"val\",\"test\")\n",
    "    blob = np.load(PREP / f\"{split}.npz\", allow_pickle=True)\n",
    "    Y = blob[\"Y\"].astype(np.float32)\n",
    "    M = blob[\"y_missing_mask\"].astype(bool)\n",
    "    X = np.load(FUSED / f\"{split}_fused.npy\").astype(np.float32)\n",
    "\n",
    "    spec_z, sh_z = build_logits(X)\n",
    "\n",
    "    # OLD blend (F1)\n",
    "    P_old = old_blend_probs(spec_z, sh_z)\n",
    "    m_old = metric_micro_macro(Y, M, P_old, THR_OLD, mode=\"f1\")\n",
    "\n",
    "    # NEW stacked (F1 thresholds from stack)\n",
    "    P_stk = stack_probs(spec_z, sh_z)\n",
    "    m_stk = metric_micro_macro(Y, M, P_stk, thr_stack, mode=\"f1\")\n",
    "\n",
    "    # Save detailed per-label CSV\n",
    "    per_rows=[]\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        per_rows.append({\n",
    "            \"label\": lbl,\n",
    "            \"old_precision\": m_old[\"per_label\"][j][\"precision\"],\n",
    "            \"old_recall\":    m_old[\"per_label\"][j][\"recall\"],\n",
    "            \"old_f1\":        m_old[\"per_label\"][j][\"f1\"],\n",
    "            \"stack_precision\": m_stk[\"per_label\"][j][\"precision\"],\n",
    "            \"stack_recall\":    m_stk[\"per_label\"][j][\"recall\"],\n",
    "            \"stack_f1\":        m_stk[\"per_label\"][j][\"f1\"],\n",
    "        })\n",
    "    df = pd.DataFrame(per_rows)\n",
    "    df.to_csv(EVAL_DIR / f\"compare_{split}.csv\", index=False)\n",
    "\n",
    "    # Save summary JSON\n",
    "    out = {\n",
    "        \"split\": split,\n",
    "        \"use_new_heads\": USE_NEW_HEADS,\n",
    "        \"old_f1\": m_old,\n",
    "        \"stack_f1\": m_stk,\n",
    "        \"alpha_old\": ALPHA\n",
    "    }\n",
    "    (EVAL_DIR / f\"compare_{split}.json\").write_text(json.dumps(out, indent=2))\n",
    "    return out\n",
    "\n",
    "print(\"Evaluating on VAL and TEST...\")\n",
    "rep_val  = evaluate_split(\"val\")\n",
    "rep_test = evaluate_split(\"test\")\n",
    "\n",
    "def _fmt(m): \n",
    "    return f\"P={m['micro']['precision']:.3f}  R={m['micro']['recall']:.3f}  F1={m['micro']['f1']:.3f}  | macro-F1={m['macro_f1']:.3f}\"\n",
    "\n",
    "print(\"\\n=== VAL ===\")\n",
    "print(\" Old F1   :\", _fmt(rep_val[\"old_f1\"]))\n",
    "print(\" Stack F1 :\", _fmt(rep_val[\"stack_f1\"]))\n",
    "print(\"\\n=== TEST ===\")\n",
    "print(\" Old F1   :\", _fmt(rep_test[\"old_f1\"]))\n",
    "print(\" Stack F1 :\", _fmt(rep_test[\"stack_f1\"]))\n",
    "\n",
    "print(\"\\nâœ… Saved:\")\n",
    "print(\"  â€¢\", EVAL_DIR / \"compare_val.json\")\n",
    "print(\"  â€¢\", EVAL_DIR / \"compare_val.csv\")\n",
    "print(\"  â€¢\", EVAL_DIR / \"compare_test.json\")\n",
    "print(\"  â€¢\", EVAL_DIR / \"compare_test.csv\")\n",
    "print(f\"\\n(use_new_heads={USE_NEW_HEADS})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lord",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
