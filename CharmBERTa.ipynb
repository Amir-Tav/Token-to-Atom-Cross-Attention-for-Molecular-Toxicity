{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "198f0ebe",
   "metadata": {},
   "source": [
    "# The aim is to train our selected transfer learning model on all of our samples, we will use the CharmBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0ac5e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (4.53.2)\n",
      "Requirement already satisfied: torch in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (2.7.1+cu126)\n",
      "Requirement already satisfied: tqdm in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: filelock in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (0.33.4)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (0.21.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: sympy>=1.13.3 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from torch) (1.14.0)\n",
      "Requirement already satisfied: networkx in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from torch) (3.5)\n",
      "Requirement already satisfied: jinja2 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from torch) (2025.3.0)\n",
      "Requirement already satisfied: colorama in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\coding projects\\predicting-drug-response-using-multi-omics-data-with-xai\\god\\lib\\site-packages (from requests->transformers) (2025.6.15)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers torch tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136b7d55",
   "metadata": {},
   "source": [
    "## ChemBERTa Embedding + Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "106c7c7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\god\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\god\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amir\\.cache\\huggingface\\hub\\models--seyonec--ChemBERTa-zinc-base-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Load ChemBERTa model/tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "model.eval()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def extract_embeddings(smiles_list, batch_size=32):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(smiles_list), batch_size)):\n",
    "        batch = smiles_list[i:i+batch_size]\n",
    "        encodings = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
    "        input_ids = encodings['input_ids'].to(device)\n",
    "        attention_mask = encodings['attention_mask'].to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token\n",
    "            embeddings.append(cls_embeddings.cpu().numpy())\n",
    "    return np.vstack(embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c1aab",
   "metadata": {},
   "source": [
    "## Load Data + Embed + Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55fe376d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Extracting embeddings for train set (12419 molecules)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 389/389 [00:08<00:00, 45.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Extracting embeddings for val set (1518 molecules)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:00<00:00, 57.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 Extracting embeddings for test set (1260 molecules)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 40/40 [00:00<00:00, 51.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# === Load original CSVs with SMILES ===\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "datasets = {\n",
    "    \"train\": \"Data/CT-ADE-SOC/train.csv\",\n",
    "    \"val\": \"Data/CT-ADE-SOC/val.csv\",\n",
    "    \"test\": \"Data/CT-ADE-SOC/test.csv\"\n",
    "}\n",
    "\n",
    "target_cols = [col for col in pd.read_csv(datasets[\"train\"]).columns if col.startswith(\"label_\")]\n",
    "chemberta_data = {}\n",
    "\n",
    "for split, path in datasets.items():\n",
    "    df = pd.read_csv(path).dropna(subset=[\"smiles\"])  # remove invalid SMILES\n",
    "    smiles_list = df[\"smiles\"].tolist()\n",
    "    print(f\"🔬 Extracting embeddings for {split} set ({len(smiles_list)} molecules)...\")\n",
    "    \n",
    "    # Run ChemBERTa embedding\n",
    "    X = extract_embeddings(smiles_list)\n",
    "    y = df[target_cols].values\n",
    "    \n",
    "    chemberta_data[split] = (X, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff844e1a",
   "metadata": {},
   "source": [
    "## Train MLP classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35b99e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 53.7769\n",
      "Epoch 2 | Loss: 50.1690\n",
      "Epoch 3 | Loss: 49.4614\n",
      "Epoch 4 | Loss: 48.9339\n",
      "Epoch 5 | Loss: 48.2256\n",
      "Epoch 6 | Loss: 47.7185\n",
      "Epoch 7 | Loss: 47.6535\n",
      "Epoch 8 | Loss: 47.5768\n",
      "Epoch 9 | Loss: 46.8161\n",
      "Epoch 10 | Loss: 46.4474\n",
      "Epoch 11 | Loss: 46.3456\n",
      "Epoch 12 | Loss: 45.9062\n",
      "Epoch 13 | Loss: 45.7065\n",
      "Epoch 14 | Loss: 45.4276\n",
      "Epoch 15 | Loss: 45.3747\n",
      "Epoch 16 | Loss: 45.2678\n",
      "Epoch 17 | Loss: 44.8617\n",
      "Epoch 18 | Loss: 44.7204\n",
      "Epoch 19 | Loss: 44.3440\n",
      "Epoch 20 | Loss: 44.3042\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ChemDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(ChemDataset(*chemberta_data['train']), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(ChemDataset(*chemberta_data['val']), batch_size=batch_size)\n",
    "\n",
    "import torch.nn as nn\n",
    "class ChemBERTaMLP(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, output_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = ChemBERTaMLP(768, len(target_cols)).to(device)\n",
    "loss_fn = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train loop\n",
    "for epoch in range(20):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        preds = model(xb)\n",
    "        loss = loss_fn(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3775a325",
   "metadata": {},
   "source": [
    "## save inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b957a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model weights saved to Model/ChemBERta/chemberta_weights.pt\n"
     ]
    }
   ],
   "source": [
    "# Save model weights only (safe for future use)\n",
    "os.makedirs(\"Model/ChemBERta\", exist_ok=True)\n",
    "torch.save(model.state_dict(), \"Model/ChemBERta/chemberta_weights.pt\")\n",
    "print(\"✅ Model weights saved to Model/ChemBERta/chemberta_weights.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150844d8",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf476662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧪 Evaluating on Test Set...\n",
      "🔍 Classification Report (micro average):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.04      0.07       140\n",
      "           1       0.00      0.00      0.00       106\n",
      "           2       0.00      0.00      0.00         3\n",
      "           3       0.00      0.00      0.00        40\n",
      "           4       0.00      0.00      0.00        23\n",
      "           5       0.00      0.00      0.00       111\n",
      "           6       0.39      0.25      0.30       513\n",
      "           7       0.21      0.08      0.12       288\n",
      "           8       0.00      0.00      0.00        26\n",
      "           9       0.00      0.00      0.00        18\n",
      "          10       0.09      0.03      0.05       367\n",
      "          11       0.00      0.00      0.00       106\n",
      "          12       0.62      0.02      0.04       235\n",
      "          13       0.50      0.06      0.11       159\n",
      "          14       0.07      0.03      0.04       231\n",
      "          15       0.00      0.00      0.00        29\n",
      "          16       0.31      0.14      0.20       522\n",
      "          17       0.00      0.00      0.00         7\n",
      "          18       0.00      0.00      0.00       184\n",
      "          19       0.00      0.00      0.00        98\n",
      "          20       0.00      0.00      0.00        49\n",
      "          21       0.57      0.05      0.10       223\n",
      "          22       0.61      0.07      0.13       189\n",
      "          23       0.00      0.00      0.00         1\n",
      "          24       0.00      0.00      0.00        15\n",
      "          25       0.00      0.00      0.00       162\n",
      "          26       0.00      0.00      0.00         5\n",
      "\n",
      "   micro avg       0.30      0.08      0.12      3850\n",
      "   macro avg       0.15      0.03      0.04      3850\n",
      "weighted avg       0.27      0.08      0.10      3850\n",
      " samples avg       0.10      0.05      0.06      3850\n",
      "\n",
      "✅ ROC-AUC (Macro): 0.4984\n",
      "✅ ROC-AUC (Micro): 0.7660\n",
      "✅ Average Precision (Micro): 0.2596\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "# Load test data\n",
    "X_test, y_test = chemberta_data[\"test\"]\n",
    "test_loader = DataLoader(ChemDataset(X_test, y_test), batch_size=batch_size)\n",
    "\n",
    "# Reload model from weights (optional if already in memory)\n",
    "model = ChemBERTaMLP(768, len(target_cols))\n",
    "model.load_state_dict(torch.load(\"Model/ChemBERta/chemberta_weights.pt\"))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluation Function\n",
    "def evaluate(model, dataloader, y_true):\n",
    "    y_probs = []\n",
    "    with torch.no_grad():\n",
    "        for xb, _ in dataloader:\n",
    "            xb = xb.to(device)\n",
    "            probs = model(xb).cpu().numpy()\n",
    "            y_probs.append(probs)\n",
    "    \n",
    "    y_probs = np.vstack(y_probs)\n",
    "    y_pred = (y_probs > 0.5).astype(int)\n",
    "\n",
    "    print(\"🔍 Classification Report (micro average):\")\n",
    "    print(classification_report(y_true, y_pred, zero_division=0))\n",
    "\n",
    "    try:\n",
    "        roc_auc_macro = roc_auc_score(y_true, y_probs, average=\"macro\")\n",
    "        roc_auc_micro = roc_auc_score(y_true, y_probs, average=\"micro\")\n",
    "        print(f\"✅ ROC-AUC (Macro): {roc_auc_macro:.4f}\")\n",
    "        print(f\"✅ ROC-AUC (Micro): {roc_auc_micro:.4f}\")\n",
    "    except ValueError:\n",
    "        print(\"⚠️ ROC-AUC could not be calculated.\")\n",
    "\n",
    "    avg_prec = average_precision_score(y_true, y_probs, average=\"micro\")\n",
    "    print(f\"✅ Average Precision (Micro): {avg_prec:.4f}\")\n",
    "\n",
    "# Run evaluation\n",
    "print(\"🧪 Evaluating on Test Set...\")\n",
    "evaluate(model, test_loader, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092433f7",
   "metadata": {},
   "source": [
    "## SHAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "365b03ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SHAP values shape: (768, 27)\n",
      "Input features shape: (10, 768)\n"
     ]
    }
   ],
   "source": [
    "# Ensure SHAP values and feature input align\n",
    "print(\"SHAP values shape:\", shap_values[0].shape)\n",
    "print(\"Input features shape:\", X_explain.cpu().numpy().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e7885a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def explain_with_shap(model, X_tensor, y_labels, feature_names, device, frame_index=20):\n",
    "    model.eval()\n",
    "\n",
    "    # Select background and test sample\n",
    "    background = X_tensor[:200].to(device)  # (200, 768)\n",
    "    test_sample = X_tensor[frame_index:frame_index+1].to(device)  # (1, 768)\n",
    "\n",
    "    # Use GradientExplainer (DeepExplainer fails)\n",
    "    explainer = shap.GradientExplainer(model, background)\n",
    "\n",
    "    # Get predicted class with highest confidence (sigmoid output)\n",
    "    with torch.no_grad():\n",
    "        output = model(test_sample).cpu().numpy()[0]  # shape: [num_classes]\n",
    "        pred_class = np.argmax(output)\n",
    "        pred_label = y_labels[pred_class]\n",
    "        confidence = output[pred_class]\n",
    "\n",
    "    # Compute SHAP values\n",
    "    shap_values = explainer.shap_values(test_sample)\n",
    "    shap_vector = shap_values[pred_class][0]  # shape: (768,)\n",
    "    base_value = explainer.expected_value[pred_class]\n",
    "\n",
    "    # Create SHAP Explanation object\n",
    "    explanation = shap.Explanation(\n",
    "        values=shap_vector,\n",
    "        base_values=base_value,\n",
    "        data=X_tensor[frame_index].cpu().numpy(),\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "\n",
    "    # Plot waterfall\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(explanation, max_display=15, show=False)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    return fig, pred_label, confidence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6902d50d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m y_labels = target_cols  \u001b[38;5;66;03m# List of class labels (column names)\u001b[39;00m\n\u001b[32m      3\u001b[39m feature_names = [\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdim_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m768\u001b[39m)]\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m fig, pred_label, confidence = \u001b[43mexplain_with_shap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_index\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m20\u001b[39;49m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m🔍 Predicted class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpred_label\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (confidence = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfidence\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m fig.show()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mexplain_with_shap\u001b[39m\u001b[34m(model, X_tensor, y_labels, feature_names, device, frame_index)\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# Compute SHAP values\u001b[39;00m\n\u001b[32m     24\u001b[39m shap_values = explainer.shap_values(test_sample)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m shap_vector = \u001b[43mshap_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpred_class\u001b[49m\u001b[43m]\u001b[49m[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# shape: (768,)\u001b[39;00m\n\u001b[32m     26\u001b[39m base_value = explainer.expected_value[pred_class]\n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Create SHAP Explanation object\u001b[39;00m\n",
      "\u001b[31mIndexError\u001b[39m: index 6 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "X_tensor = torch.tensor(chemberta_data[\"test\"][0], dtype=torch.float32)\n",
    "y_labels = target_cols  # List of class labels (column names)\n",
    "feature_names = [f\"dim_{i}\" for i in range(768)]\n",
    "\n",
    "fig, pred_label, confidence = explain_with_shap(\n",
    "    model, X_tensor, y_labels, feature_names, device, frame_index=20\n",
    ")\n",
    "print(f\"🔍 Predicted class: {pred_label} (confidence = {confidence:.2f})\")\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "god",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
