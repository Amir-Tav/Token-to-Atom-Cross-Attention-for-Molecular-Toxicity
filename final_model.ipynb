{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b39f195",
   "metadata": {},
   "source": [
    "# V_9 model\n",
    "\n",
    "**Design Choice?**\n",
    "\n",
    "\n",
    "LightGBM per-label â€” still one binary model per endpoint, trained with the same Mordred descriptor features.\n",
    "\n",
    "Hyperparameter tuning â€” use Optuna with 5-fold stratified cross-validation per label, optimizing F1-score.\n",
    "\n",
    "GPU Acceleration â€” enable LightGBMâ€™s device_type='gpu' and gpu_platform_id, gpu_device_id so your RTX 4070 Ti is used.\n",
    "\n",
    "Reusable outputs â€” all models, tuned params, thresholds, masks, feature names saved in v9\n",
    "\n",
    "Threshold optimization â€” after tuning, apply the precisionâ€“recall threshold search from (V7_tuned) to maximize F1 (balanced recall/precision).\n",
    "\n",
    "Evaluation reporting â€” after training, output AUC, F1, accuracy per label.\n",
    "\n",
    "5-fold CV â€” better robustness than the v8â€™s 3-fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b286011",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8767efb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7831 [00:00<?, ?it/s][14:59:01] WARNING: not removing hydrogen atom without neighbors\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7831/7831 [00:00<00:00, 10389.16it/s]\n",
      "  1%|          | 70/7831 [00:02<04:40, 27.69it/s][14:59:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:59:04] WARNING: not removing hydrogen atom without neighbors\n",
      "[14:59:04] WARNING: not removing hydrogen atom without neighbors\n",
      "  1%|â–         | 114/7831 [00:04<09:07, 14.10it/s]d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:88: RuntimeWarning: overflow encountered in reduce\n",
      "  return ufunc.reduce(obj, axis, dtype, out, **passkwargs)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7831/7831 [06:25<00:00, 20.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Features shape: (7831, 1613), Labels shape: (7831, 12)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from mordred import Calculator, descriptors\n",
    "from rdkit import Chem\n",
    "from tqdm import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "# Paths\n",
    "BASE_PATH = Path(\"tox21_lightgb_pipeline\")\n",
    "V9_MODELS = BASE_PATH / \"models/v9\"\n",
    "V9_MODELS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Data path\n",
    "DATA_CSV = BASE_PATH / \"Data_v6/processed/tox21.csv\"\n",
    "\n",
    "# Load CSV with 12 labels + mol_id + smiles\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "labels = df.columns[:12]\n",
    "y_all = df[labels].values\n",
    "label_mask = ~df[labels].isna().values\n",
    "smiles_list = df[\"smiles\"].tolist()\n",
    "\n",
    "# === RECOMPUTE MORDRED DESCRIPTORS ===\n",
    "calc = Calculator(descriptors, ignore_3D=True)\n",
    "mols = df[\"smiles\"].progress_apply(Chem.MolFromSmiles)\n",
    "descs = mols.progress_apply(lambda m: calc(m) if m is not None else None)\n",
    "desc_df = pd.DataFrame([d.asdict() if d is not None else {} for d in descs])\n",
    "desc_df = desc_df.replace([np.inf, -np.inf], np.nan).fillna(-1)\n",
    "\n",
    "# Save feature names for future utils.py use\n",
    "feature_names = list(desc_df.columns)\n",
    "with open(V9_MODELS / \"feature_names.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(feature_names))\n",
    "\n",
    "X = desc_df.values.astype(np.float32)\n",
    "\n",
    "print(f\"âœ… Features shape: {X.shape}, Labels shape: {y_all.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f404e6",
   "metadata": {},
   "source": [
    "## Step 2 - Hyperparameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4d9bb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” Tuning label: NR-AR\n",
      "âœ… Best F1 for NR-AR: 0.6074\n",
      "\n",
      "ðŸ” Tuning label: NR-AR-LBD\n",
      "âœ… Best F1 for NR-AR-LBD: 0.6423\n",
      "\n",
      "ðŸ” Tuning label: NR-AhR\n",
      "âœ… Best F1 for NR-AhR: 0.5765\n",
      "\n",
      "ðŸ” Tuning label: NR-Aromatase\n",
      "âœ… Best F1 for NR-Aromatase: 0.3127\n",
      "\n",
      "ðŸ” Tuning label: NR-ER\n",
      "âœ… Best F1 for NR-ER: 0.3743\n",
      "\n",
      "ðŸ” Tuning label: NR-ER-LBD\n",
      "âœ… Best F1 for NR-ER-LBD: 0.4717\n",
      "\n",
      "ðŸ” Tuning label: NR-PPAR-gamma\n",
      "âœ… Best F1 for NR-PPAR-gamma: 0.1797\n",
      "\n",
      "ðŸ” Tuning label: SR-ARE\n",
      "âœ… Best F1 for SR-ARE: 0.4532\n",
      "\n",
      "ðŸ” Tuning label: SR-ATAD5\n",
      "âœ… Best F1 for SR-ATAD5: 0.1943\n",
      "\n",
      "ðŸ” Tuning label: SR-HSE\n",
      "âœ… Best F1 for SR-HSE: 0.2202\n",
      "\n",
      "ðŸ” Tuning label: SR-MMP\n",
      "âœ… Best F1 for SR-MMP: 0.6723\n",
      "\n",
      "ðŸ” Tuning label: SR-p53\n",
      "âœ… Best F1 for SR-p53: 0.2671\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "\n",
    "tuned_params = {}\n",
    "n_trials = 100  # balanced runtime/quality\n",
    "\n",
    "def objective(trial, X_train, y_train):\n",
    "    param_grid = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",  # required for early stopping\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"device_type\": \"gpu\",\n",
    "        \"gpu_platform_id\": 0,\n",
    "        \"gpu_device_id\": 0,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.005, 0.2, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 16, 256, log=True),\n",
    "        \"max_depth\": trial.suggest_int(\"max_depth\", 3, 12),\n",
    "        \"min_data_in_leaf\": trial.suggest_int(\"min_data_in_leaf\", 20, 500),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.6, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.6, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 10),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 0.0, 5.0),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 0.0, 5.0)\n",
    "    }\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    f1_scores = []\n",
    "    for train_idx, valid_idx in cv.split(X_train, y_train):\n",
    "        lgb_train = lgb.Dataset(X_train[train_idx], y_train[train_idx])\n",
    "        lgb_valid = lgb.Dataset(X_train[valid_idx], y_train[valid_idx])\n",
    "        model = lgb.train(\n",
    "            param_grid,\n",
    "            lgb_train,\n",
    "            valid_sets=[lgb_valid],\n",
    "            num_boost_round=500,\n",
    "            callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "        )\n",
    "        preds = (model.predict(X_train[valid_idx]) >= 0.5).astype(int)\n",
    "        f1_scores.append(f1_score(y_train[valid_idx], preds))\n",
    "    return np.mean(f1_scores)\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"\\nðŸ” Tuning label: {label}\")\n",
    "    mask = label_mask[:, i]\n",
    "    X_label = X[mask]\n",
    "    y_label = y_all[mask, i].astype(int)\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(lambda trial: objective(trial, X_label, y_label), n_trials=n_trials)\n",
    "    tuned_params[label] = study.best_params\n",
    "    tuned_params[label].update({\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\",  # keep metric for final training\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"device_type\": \"gpu\",\n",
    "        \"gpu_platform_id\": 0,\n",
    "        \"gpu_device_id\": 0,\n",
    "        \"verbosity\": -1\n",
    "    })\n",
    "    print(f\"âœ… Best F1 for {label}: {study.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0169a2cf",
   "metadata": {},
   "source": [
    "## Step 3 - Train Final Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "064f76a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training final model for NR-AR\n",
      "\n",
      "ðŸš€ Training final model for NR-AR-LBD\n",
      "\n",
      "ðŸš€ Training final model for NR-AhR\n",
      "\n",
      "ðŸš€ Training final model for NR-Aromatase\n",
      "\n",
      "ðŸš€ Training final model for NR-ER\n",
      "\n",
      "ðŸš€ Training final model for NR-ER-LBD\n",
      "\n",
      "ðŸš€ Training final model for NR-PPAR-gamma\n",
      "\n",
      "ðŸš€ Training final model for SR-ARE\n",
      "\n",
      "ðŸš€ Training final model for SR-ATAD5\n",
      "\n",
      "ðŸš€ Training final model for SR-HSE\n",
      "\n",
      "ðŸš€ Training final model for SR-MMP\n",
      "\n",
      "ðŸš€ Training final model for SR-p53\n",
      "âœ… All models saved with early stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "models = {}\n",
    "feature_masks = {}\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    print(f\"\\nðŸš€ Training final model for {label}\")\n",
    "    mask = label_mask[:, i]\n",
    "    X_label = X[mask]\n",
    "    y_label = y_all[mask, i].astype(int)\n",
    "\n",
    "    # All features are used (keep mask for utils.py)\n",
    "    feature_masks[label] = np.array([True] * X.shape[1])\n",
    "\n",
    "    # Split train/valid for early stopping\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_label, y_label, test_size=0.2, stratify=y_label, random_state=42\n",
    "    )\n",
    "\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_valid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "    # Train with tuned params and early stopping\n",
    "    model = lgb.train(\n",
    "        tuned_params[label],\n",
    "        lgb_train,\n",
    "        valid_sets=[lgb_valid],\n",
    "        num_boost_round=2000,  # allow more rounds, early stopping will cut it\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=False)]\n",
    "    )\n",
    "\n",
    "    models[label] = model\n",
    "    joblib.dump(model, V9_MODELS / f\"{label}.pkl\")\n",
    "\n",
    "# Save feature masks and tuned parameters\n",
    "joblib.dump(feature_masks, V9_MODELS / \"feature_masks.pkl\")\n",
    "joblib.dump(tuned_params, V9_MODELS / \"tuned_params.pkl\")\n",
    "np.save(V9_MODELS / \"label_mask.npy\", label_mask)\n",
    "\n",
    "print(\"âœ… All models saved with early stopping.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9d482",
   "metadata": {},
   "source": [
    "## Step 4 â€“ Threshold Optimization\n",
    "\n",
    "Threshold tuning worked\n",
    "Notice how some thresholds are far from 0.5 (e.g., 0.1583 for NR-AR-LBD).\n",
    "\n",
    "This shows that your calibrated decision boundaries are making the model more sensitive to rare positives, improving recall without killing precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "67242fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NR-AR: Best F1=0.6138 at threshold=0.4518\n",
      "NR-AR-LBD: Best F1=0.7719 at threshold=0.1583\n",
      "NR-AhR: Best F1=0.8753 at threshold=0.3931\n",
      "NR-Aromatase: Best F1=0.8186 at threshold=0.2022\n",
      "NR-ER: Best F1=0.6605 at threshold=0.1919\n",
      "NR-ER-LBD: Best F1=0.7954 at threshold=0.2101\n",
      "NR-PPAR-gamma: Best F1=0.6512 at threshold=0.1639\n",
      "SR-ARE: Best F1=0.8452 at threshold=0.3585\n",
      "SR-ATAD5: Best F1=0.7780 at threshold=0.2391\n",
      "SR-HSE: Best F1=0.7294 at threshold=0.1889\n",
      "SR-MMP: Best F1=0.9359 at threshold=0.4399\n",
      "SR-p53: Best F1=0.7895 at threshold=0.2334\n",
      "âœ… Thresholds saved.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "new_thresholds = {}\n",
    "target_f1 = True  # optimize for F1 instead of precision\n",
    "\n",
    "for i, label in enumerate(labels):\n",
    "    mask = label_mask[:, i]\n",
    "    X_label = X[mask]\n",
    "    y_label = y_all[mask, i].astype(int)\n",
    "    model = models[label]\n",
    "    probs = model.predict(X_label)\n",
    "\n",
    "    prec, rec, thr = precision_recall_curve(y_label, probs)\n",
    "    f1_scores = (2 * prec[:-1] * rec[:-1]) / (prec[:-1] + rec[:-1] + 1e-8)\n",
    "    best = np.argmax(f1_scores)\n",
    "    threshold = float(thr[best])\n",
    "    new_thresholds[label] = threshold\n",
    "    print(f\"{label}: Best F1={f1_scores[best]:.4f} at threshold={threshold:.4f}\")\n",
    "\n",
    "joblib.dump(new_thresholds, V9_MODELS / \"thresholds.pkl\")\n",
    "print(\"âœ… Thresholds saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4650b05",
   "metadata": {},
   "source": [
    "## Step 5 â€“ Evaluation\n",
    "\n",
    "1. AUC (Area Under ROC Curve)\n",
    "All AUC values are above 0.90, with several above 0.97 (NR-AhR, SR-MMP, etc.).\n",
    "\n",
    "That means the models are excellent at separating toxic vs. non-toxic cases.\n",
    "\n",
    "SR-MMP is almost perfect (0.9888 AUC).\n",
    "\n",
    "2. F1-Scores (balanced precision & recall)\n",
    "Most are above 0.75, with SR-MMP at 0.936 and NR-AhR at 0.875 â€” very strong.\n",
    "\n",
    "Lower end: NR-AR (0.613) and NR-PPAR-gamma (0.651) â€” these endpoints are typically harder because of lower prevalence in the dataset or overlapping chemical patterns.\n",
    "\n",
    "The threshold optimization clearly helped boost weaker classes (e.g., NR-ER from low baseline to 0.660).\n",
    "\n",
    "3. Accuracy\n",
    "Most are >97%, which looks impressive, but remember:\n",
    "\n",
    "Accuracy is inflated for imbalanced datasets.\n",
    "\n",
    "Thatâ€™s why your F1-scores are more telling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4ab2fea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            label       auc  accuracy  f1_score  n_samples\n",
      "0           NR-AR  0.918112  0.974535  0.613779       7265\n",
      "1       NR-AR-LBD  0.960607  0.984611  0.771930       6758\n",
      "2          NR-AhR  0.980171  0.971599  0.875335       6549\n",
      "3    NR-Aromatase  0.972503  0.981275  0.818636       5821\n",
      "4           NR-ER  0.907614  0.905377  0.660487       6193\n",
      "5       NR-ER-LBD  0.959410  0.979583  0.795389       6955\n",
      "6   NR-PPAR-gamma  0.949760  0.979070  0.651163       6450\n",
      "7          SR-ARE  0.968682  0.951132  0.845193       5832\n",
      "8        SR-ATAD5  0.971756  0.984587  0.778004       7072\n",
      "9          SR-HSE  0.959168  0.968455  0.729443       6467\n",
      "10         SR-MMP  0.988830  0.980379  0.935883       5810\n",
      "11         SR-p53  0.970532  0.973871  0.789536       6774\n"
     ]
    }
   ],
   "source": [
    "# ===========================\n",
    "# (V_9) Step 5 â€“ Evaluation\n",
    "# ===========================\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "eval_results = []\n",
    "for i, label in enumerate(labels):\n",
    "    mask = label_mask[:, i]\n",
    "    X_label = X[mask]\n",
    "    y_label = y_all[mask, i].astype(int)\n",
    "    model = models[label]\n",
    "    probs = model.predict(X_label)\n",
    "    preds = (probs >= new_thresholds[label]).astype(int)\n",
    "    auc = roc_auc_score(y_label, probs)\n",
    "    acc = accuracy_score(y_label, preds)\n",
    "    f1 = f1_score(y_label, preds)\n",
    "    eval_results.append((label, auc, acc, f1, mask.sum()))\n",
    "\n",
    "eval_df = pd.DataFrame(eval_results, columns=[\"label\", \"auc\", \"accuracy\", \"f1_score\", \"n_samples\"])\n",
    "print(eval_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lord",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
