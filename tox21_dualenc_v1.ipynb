{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa03dc51",
   "metadata": {},
   "source": [
    "# v1: Dual-Encoder Cross-Attention (ChemBERTa + GIN) for Tox21\n",
    "**Goal:** Train, from scratch, a tri-modal model that fuses:\n",
    "- SMILES view (ChemBERTa),\n",
    "- Graph view (lightweight GIN over RDKit molecule graphs),\n",
    "- Descriptor view (standardised descriptors you already prepared),\n",
    "\n",
    "…using **cross-attention** (tokens ← atoms) to align the two learned views before classification into the 12 Tox21 labels.\n",
    "\n",
    "We will:\n",
    "1) Set up folders/config (this notebook controls all outputs in `tox21_dualenc_v1`).\n",
    "2) Load splits/labels/descriptors from your v5 project.\n",
    "3) Build & cache RDKit graphs (node features + adjacency) for every molecule.\n",
    "4) Implement a minimal GIN (pure PyTorch with sparse block-diag batching).\n",
    "5) Implement the cross-attention fusion block + attention pooling.\n",
    "6) Define the full model and training stages (freeze → partial unfreeze → optional full-unfreeze).\n",
    "7) Train/evaluate on scaffold split; log & save checkpoints and plots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a73756d",
   "metadata": {},
   "source": [
    "## 1: Dual-Encoder with Cross-Attention + Descriptors (Tox21)\n",
    "**Goal:** Train a new model from scratch that fuses *SMILES (ChemBERTa)* and *molecular graphs (GIN)* with **cross-attention**, plus handcrafted **descriptors**, to predict the 12 Tox21 toxicity labels under a **scaffold split**.  \n",
    "We’ll save every artifact under `tox21_dualenc_v1/` and add sanity checks after each step.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3d11480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Workspace: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\n",
      "\n",
      "Actions:\n",
      "Copied: tox21_chemberta_v5\\data\\splits\\splits.json -> tox21_dualenc_v1\\data\\splits\\splits.json\n",
      "Copied: tox21_chemberta_v5\\data\\X_smiles.txt -> tox21_dualenc_v1\\data\\X_smiles.txt\n",
      "Copied: tox21_chemberta_v5\\data\\label_names.txt -> tox21_dualenc_v1\\data\\label_names.txt\n",
      "Copied: tox21_chemberta_v5\\data\\y.npy -> tox21_dualenc_v1\\data\\y.npy\n",
      "Copied: tox21_chemberta_v5\\data\\y_mask.npy -> tox21_dualenc_v1\\data\\y_mask.npy\n",
      "Copied: tox21_chemberta_v5\\data\\descriptors\\desc_selected.npy -> tox21_dualenc_v1\\data\\descriptors\\desc_selected.npy\n",
      "Copied: tox21_chemberta_v5\\data\\descriptors\\desc_standardizer.pkl -> tox21_dualenc_v1\\data\\descriptors\\desc_standardizer.pkl\n",
      "Copied: tox21_chemberta_v5\\data\\descriptors\\feature_names_selected.txt -> tox21_dualenc_v1\\data\\descriptors\\feature_names_selected.txt\n",
      "Copied: tox21_chemberta_v5\\data\\descriptors\\indices_selected.npy -> tox21_dualenc_v1\\data\\descriptors\\indices_selected.npy\n",
      "Copied: tox21_chemberta_v5\\model\\config_v4_snapshot.json -> tox21_dualenc_v1\\models\\config_v4_snapshot.json\n",
      "\n",
      "Inventory after setup:\n",
      " - data\\descriptors\\desc_selected.npy\n",
      " - data\\descriptors\\desc_standardizer.pkl\n",
      " - data\\descriptors\\feature_names_selected.txt\n",
      " - data\\descriptors\\indices_selected.npy\n",
      " - data\\label_names.txt\n",
      " - data\\splits\\splits.json\n",
      " - data\\X_smiles.txt\n",
      " - data\\y.npy\n",
      " - data\\y_mask.npy\n"
     ]
    }
   ],
   "source": [
    "import os, shutil, json\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "subdirs = [\n",
    "    \"data/raw\",\n",
    "    \"data/splits\",\n",
    "    \"data/descriptors\",\n",
    "    \"data/graphs\",\n",
    "    \"models/checkpoints\",\n",
    "    \"plots\",\n",
    "    \"logs\",\n",
    "    \"notebooks\",\n",
    "]\n",
    "\n",
    "for sd in subdirs:\n",
    "    (ROOT / sd).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "report = []\n",
    "\n",
    "# Try to copy seed artifacts from your v5 project if present\n",
    "V5 = Path(\"tox21_chemberta_v5\")\n",
    "if V5.exists():\n",
    "    # Splits / labels\n",
    "    for name in [\"splits/splits.json\", \"X_smiles.txt\", \"label_names.txt\", \"y.npy\", \"y_mask.npy\"]:\n",
    "        src = V5 / \"data\" / name if \"splits\" in name else V5 / \"data\" / name\n",
    "        dst = ROOT / \"data\" / (\"splits\" if \"splits\" in name else \"\") / Path(name).name\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, dst)\n",
    "            report.append(f\"Copied: {src} -> {dst}\")\n",
    "    # Descriptors\n",
    "    for name in [\"desc_selected.npy\", \"desc_standardizer.pkl\", \"feature_names_selected.txt\", \"indices_selected.npy\"]:\n",
    "        src = V5 / \"data\" / \"descriptors\" / name\n",
    "        dst = ROOT / \"data\" / \"descriptors\" / name\n",
    "        if src.exists():\n",
    "            shutil.copy2(src, dst)\n",
    "            report.append(f\"Copied: {src} -> {dst}\")\n",
    "    # Config snapshot if exists\n",
    "    cfg_src = V5 / \"model\" / \"config_v4_snapshot.json\"\n",
    "    if cfg_src.exists():\n",
    "        dst = ROOT / \"models\" / \"config_v4_snapshot.json\"\n",
    "        shutil.copy2(cfg_src, dst)\n",
    "        report.append(f\"Copied: {cfg_src} -> {dst}\")\n",
    "\n",
    "# Also copy raw CSV if available in /mnt/data\n",
    "for candidate in [\"dataset_selected.csv\", \"tox21.csv\", \"tox21_enriched.csv\"]:\n",
    "    src = Path(\"/mnt/data\") / candidate\n",
    "    if src.exists():\n",
    "        dst = ROOT / \"data\" / \"raw\" / src.name\n",
    "        shutil.copy2(src, dst)\n",
    "        report.append(f\"Copied: {src} -> {dst}\")\n",
    "\n",
    "# Write a README\n",
    "readme = ROOT / \"README.txt\"\n",
    "readme.write_text(\n",
    "\"\"\"tox21_dualenc_v1 — Dual-Encoder (ChemBERTa + GIN) with Cross-Attention + Descriptors\n",
    "\n",
    "Folders\n",
    "- data/raw/           : original CSV(s)\n",
    "- data/splits/        : scaffold splits (train/val/test indices), y.npy, y_mask.npy, X_smiles.txt, label_names.txt\n",
    "- data/descriptors/   : desc_selected.npy, desc_standardizer.pkl, feature_names_selected.txt, indices_selected.npy\n",
    "- data/graphs/        : cached graph tensors (dense padded) once built\n",
    "- models/checkpoints/ : trained model checkpoints and configs\n",
    "- plots/              : figures for the report\n",
    "- logs/               : training & evaluation logs\n",
    "\n",
    "Run the notebook cells in order. Each step includes sanity checks.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "print(\"Workspace:\", ROOT.resolve())\n",
    "print(\"\\nActions:\")\n",
    "print(*report, sep=\"\\n\")\n",
    "print(\"\\nInventory after setup:\")\n",
    "for path in sorted((ROOT / \"data\").rglob(\"*\")):\n",
    "    if path.is_file():\n",
    "        print(\" -\", path.relative_to(ROOT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adbce1",
   "metadata": {},
   "source": [
    "## 2: Environment & prerequisites\n",
    "\n",
    "We’ll train a **dual-encoder** model from scratch:\n",
    "\n",
    "- **Text branch:** `DeepChem/ChemBERTa-100M-MLM` (Transformers).\n",
    "- **Graph branch:** **GIN** (Graph Isomorphism Network) over RDKit graphs.\n",
    "- **Fusion:** a **cross-attention** block (Text ← Graph) + attention pooling.\n",
    "- **Descriptors:** 256-d MLP path (already prepared).\n",
    "\n",
    "\n",
    "This cell will:\n",
    "1) Verify your environment (versions, CUDA, RDKit).\n",
    "2) Inspect the dataset we copied into `tox21_dualenc_v1/data/`.\n",
    "3) Pick a graph backend (`pyg` or `dgl`) and save a starter **config** file under `models/config_dualenc_v1.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21bb8473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\n",
      "torch version: 2.6.0+cu124\n",
      "CUDA available: True\n",
      "CUDA device count: 1\n",
      "CUDA device 0: NVIDIA GeForce RTX 4070 Ti\n",
      "cudnn enabled: True\n",
      "transformers version: 4.43.3\n",
      "RDKit version: 2022.09.5\n",
      "Graph backend: PyTorch Geometric (torch_geometric) 2.6.1\n",
      "\n",
      "Data files present?: OK\n",
      "SMILES count: 7831\n",
      "y shape: (7831, 12) range: (-1.0, 1.0)\n",
      "y_mask shape: (7831, 12) unique: [0 1]\n",
      "labels (L): 12 ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n",
      "descriptors shape: (7831, 256)\n",
      "RDKit parse sanity: first 20 → 20 ok, 0 failed\n",
      "\n",
      "Saved config to: tox21_dualenc_v1\\models\\config_dualenc_v1.json\n",
      "\n",
      "=== SUMMARY ===\n",
      "Graph backend picked: pyg\n",
      "Descriptors dim: 256\n",
      "Num labels: 12\n",
      "CUDA: True\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, random, importlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "DATA = ROOT / \"data\"\n",
    "DESC = DATA / \"descriptors\"\n",
    "SPLITS = DATA / \"splits\"\n",
    "MODELS = ROOT / \"models\"\n",
    "\n",
    "print(\"Project root:\", ROOT.resolve())\n",
    "\n",
    "# ---- Torch / CUDA ----\n",
    "try:\n",
    "    import torch\n",
    "    print(f\"torch version: {torch.__version__}\")\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device count:\", torch.cuda.device_count())\n",
    "        print(\"CUDA device 0:\", torch.cuda.get_device_name(0))\n",
    "        print(\"cudnn enabled:\", torch.backends.cudnn.enabled)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ Torch import failed:\", repr(e))\n",
    "\n",
    "# ---- Transformers ----\n",
    "try:\n",
    "    import transformers\n",
    "    print(\"transformers version:\", transformers.__version__)\n",
    "except Exception as e:\n",
    "    print(\"⚠️ transformers import failed:\", repr(e))\n",
    "\n",
    "# ---- RDKit ----\n",
    "rdkit_ok = False\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit import RDLogger\n",
    "    from rdkit import rdBase\n",
    "    RDLogger.DisableLog(\"rdApp.*\")\n",
    "    rdkit_ok = True\n",
    "    # version (different rdkit builds expose differently)\n",
    "    try:\n",
    "        from rdkit import rdBase\n",
    "        print(\"RDKit version:\", rdBase.rdkitVersion)\n",
    "    except Exception:\n",
    "        print(\"RDKit import ok (version lookup skipped).\")\n",
    "except Exception as e:\n",
    "    print(\"⚠️ RDKit not available:\", repr(e))\n",
    "    print(\"   Install via conda (recommended): conda install -c conda-forge rdkit\")\n",
    "    print(\"   Or via pip: pip install rdkit-pypi\")\n",
    "\n",
    "# ---- Graph backend detection: PyG preferred, else DGL ----\n",
    "graph_backend = \"none\"\n",
    "pyg_ok = importlib.util.find_spec(\"torch_geometric\") is not None\n",
    "dgl_ok = importlib.util.find_spec(\"dgl\") is not None\n",
    "\n",
    "if pyg_ok:\n",
    "    import torch_geometric\n",
    "    graph_backend = \"pyg\"\n",
    "    print(\"Graph backend: PyTorch Geometric (torch_geometric)\", torch_geometric.__version__)\n",
    "elif dgl_ok:\n",
    "    import dgl\n",
    "    graph_backend = \"dgl\"\n",
    "    try:\n",
    "        print(\"Graph backend: DGL\", dgl.__version__)\n",
    "    except Exception:\n",
    "        print(\"Graph backend: DGL (version not reported)\")\n",
    "else:\n",
    "    print(\"⚠️ No graph backend detected. Install ONE of:\")\n",
    "    print(\"   PyG: see https://pytorch-geometric.readthedocs.io/en/latest/install/installation.html\")\n",
    "    print(\"   DGL (CPU): pip install dgl  |  DGL (CUDA 12.1): pip install dgl-cu121 -f https://data.dgl.ai/wheels/cu121/repo.html\")\n",
    "\n",
    "# ---- Data inventory ----\n",
    "exp_files = [\n",
    "    DATA/\"X_smiles.txt\",\n",
    "    DATA/\"y.npy\",\n",
    "    DATA/\"y_mask.npy\",\n",
    "    DATA/\"label_names.txt\",\n",
    "    SPLITS/\"splits.json\",\n",
    "    DESC/\"desc_selected.npy\",\n",
    "    DESC/\"desc_standardizer.pkl\",\n",
    "]\n",
    "missing = [str(p) for p in exp_files if not p.exists()]\n",
    "print(\"\\nData files present?:\", \"OK\" if not missing else \"MISSING\")\n",
    "if missing:\n",
    "    for m in missing: print(\" -\", m)\n",
    "\n",
    "# load small sample to sanity-check\n",
    "smiles = []\n",
    "if (DATA/\"X_smiles.txt\").exists():\n",
    "    smiles = (DATA/\"X_smiles.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "    print(\"SMILES count:\", len(smiles))\n",
    "else:\n",
    "    print(\"⚠️ X_smiles.txt missing\")\n",
    "\n",
    "if (DATA/\"y.npy\").exists():\n",
    "    y = np.load(DATA/\"y.npy\")\n",
    "    print(\"y shape:\", y.shape, \"range:\", (y.min(), y.max()))\n",
    "if (DATA/\"y_mask.npy\").exists():\n",
    "    y_mask = np.load(DATA/\"y_mask.npy\")\n",
    "    print(\"y_mask shape:\", y_mask.shape, \"unique:\", np.unique(y_mask))\n",
    "\n",
    "if (DATA/\"label_names.txt\").exists():\n",
    "    label_names = (DATA/\"label_names.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "    print(\"labels (L):\", len(label_names), label_names)\n",
    "else:\n",
    "    label_names = []\n",
    "\n",
    "desc_dim = None\n",
    "if (DESC/\"desc_selected.npy\").exists():\n",
    "    X_desc = np.load(DESC/\"desc_selected.npy\")\n",
    "    desc_dim = X_desc.shape[1]\n",
    "    print(\"descriptors shape:\", X_desc.shape)\n",
    "\n",
    "# Quick RDKit parse sanity on a few SMILES\n",
    "if rdkit_ok and smiles:\n",
    "    bad = 0\n",
    "    for s in smiles[:20]:\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is None: bad += 1\n",
    "    print(f\"RDKit parse sanity: first 20 → {20-bad} ok, {bad} failed\")\n",
    "\n",
    "# ---- Config bootstrap (save choices) ----\n",
    "CONFIG = {\n",
    "    \"text_encoder\": \"DeepChem/ChemBERTa-100M-MLM\",\n",
    "    \"max_length\": 256,\n",
    "    \"text_proj_dim\": 256,\n",
    "    \"graph_backend\": graph_backend,      # 'pyg' | 'dgl' | 'none'\n",
    "    \"graph_model\": \"GIN\",\n",
    "    \"graph_hidden\": 256,\n",
    "    \"graph_layers\": 4,\n",
    "    \"fusion_dim\": 256,\n",
    "    \"fusion_heads\": 4,\n",
    "    \"fusion_dropout\": 0.1,\n",
    "    \"attn_pool\": True,\n",
    "    \"desc_dim_in\": int(desc_dim) if desc_dim is not None else None,\n",
    "    \"desc_hidden\": 256,\n",
    "    \"head_hidden\": 512,\n",
    "    \"dropout\": 0.2,\n",
    "    \"num_labels\": int(len(label_names)) if label_names else 12,\n",
    "    \"loss\": \"BCEWithLogits\",\n",
    "    \"optimizer\": \"AdamW\",\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"batch_size\": 16,\n",
    "    \"grad_accum_steps\": 2,               # effective BS=32\n",
    "    \"epochs_max\": 40,\n",
    "    \"early_stop_patience\": 8,\n",
    "    \"seed\": 42,\n",
    "    \"stageA_freeze_text_epochs\": 4,      # freeze ChemBERTa\n",
    "    \"stageB_unfreeze_last_layers\": 4,    # unfreeze last N transformer layers\n",
    "    \"lr_text\": 2e-5,\n",
    "    \"lr_others\": 1e-3,\n",
    "    \"scheduler\": \"cosine_warmup\",\n",
    "    \"warmup_ratio\": 0.1,\n",
    "}\n",
    "\n",
    "MODELS.mkdir(parents=True, exist_ok=True)\n",
    "(MODELS/\"config_dualenc_v1.json\").write_text(json.dumps(CONFIG, indent=2))\n",
    "print(\"\\nSaved config to:\", MODELS/\"config_dualenc_v1.json\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(\"Graph backend picked:\", graph_backend)\n",
    "print(\"Descriptors dim:\", desc_dim)\n",
    "print(\"Num labels:\", len(label_names) if label_names else \"unknown\")\n",
    "print(\"CUDA:\", torch.cuda.is_available() if 'torch' in globals() else \"torch not available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204b663e",
   "metadata": {},
   "source": [
    "## 3: Build molecular graphs (RDKit → PyTorch Geometric) — sanity sample\n",
    "\n",
    "We convert each SMILES to a molecular graph:\n",
    "- **Nodes** (atoms): one-hot for atom type (C, N, O, S, F, Cl, Br, I, P, B, other), degree (0–5+), formal charge (−2..+2 + other), hybridization (sp, sp2, sp3, sp3d, sp3d2, other), flags (aromatic, ring).\n",
    "- **Edges** (bonds): one-hot for bond type (single/double/triple/aromatic), flags (conjugated, ring); edges are **bidirectional**.\n",
    "\n",
    "This cell builds **a small sample (first 200)** to validate shapes and stats before we process the full dataset. Artifacts:\n",
    "- `data/graphs/graphs_sample_pyg.pt` — list of PyG `Data` objects\n",
    "- `data/graphs/graphs_meta_sample.json` — feature dims, counts, basic stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6848732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature dim: 32, Edge feature dim: 7\n",
      "Built sample graphs: 200 / 200, failed: 0\n",
      "Avg nodes: 16.5 ± 9.4\n",
      "Avg edges: 33.9 ± 21.0\n",
      "Min/Max nodes: (2, 76)\n",
      "Min/Max edges: (0, 154)\n",
      "[0] nodes=16, edges=34, x.shape=(16, 32), edge_attr.shape=(34, 7)\n",
      "[1] nodes=15, edges=32, x.shape=(15, 32), edge_attr.shape=(32, 7)\n",
      "[2] nodes=21, edges=48, x.shape=(21, 32), edge_attr.shape=(48, 7)\n",
      "Saved: tox21_dualenc_v1\\data\\graphs\\graphs_sample_pyg.pt\n",
      "Saved: tox21_dualenc_v1\\data\\graphs\\graphs_meta_sample.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "DATA = ROOT / \"data\"\n",
    "GRAPHS = DATA / \"graphs\"\n",
    "GRAPHS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load SMILES\n",
    "smiles = (DATA/\"X_smiles.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "N_total = len(smiles)\n",
    "\n",
    "# ---- Atom & bond featurizers ----\n",
    "ATOM_LIST = [\"C\",\"N\",\"O\",\"S\",\"F\",\"Cl\",\"Br\",\"I\",\"P\",\"B\"]  # common heavy atoms; others → 'other'\n",
    "HYB_LIST = [\n",
    "    Chem.rdchem.HybridizationType.SP,\n",
    "    Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3,\n",
    "    Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2,\n",
    "]  # others → 'other'\n",
    "\n",
    "def one_hot_with_other(key, choices):\n",
    "    vec = [0]* (len(choices)+1)\n",
    "    try:\n",
    "        idx = choices.index(key)\n",
    "    except ValueError:\n",
    "        idx = None\n",
    "    if idx is None:\n",
    "        vec[-1] = 1\n",
    "    else:\n",
    "        vec[idx] = 1\n",
    "    return vec\n",
    "\n",
    "def one_hot_index(idx, size, include_other=False):\n",
    "    if include_other and (idx<0 or idx>=size-1):\n",
    "        v = [0]* (size-1) + [1]\n",
    "    else:\n",
    "        v = [0]*size\n",
    "        if 0 <= idx < size:\n",
    "            v[idx] = 1\n",
    "    return v\n",
    "\n",
    "def atom_features(atom: Chem.Atom):\n",
    "    sym = atom.GetSymbol()\n",
    "    f_type = one_hot_with_other(sym, ATOM_LIST)                         # len = len(ATOM_LIST)+1\n",
    "    deg = atom.GetTotalDegree()\n",
    "    f_deg = one_hot_index(min(deg,6), 7)                                # 0..5, 6=≥6  → len 7\n",
    "    charge = atom.GetFormalCharge()\n",
    "    charge_map = {-2:0,-1:1,0:2,1:3,2:4}\n",
    "    f_charge = one_hot_index(charge_map.get(charge,5), 6)               # (-2,-1,0,1,2,other) → len 6\n",
    "    hyb = atom.GetHybridization()\n",
    "    hyb_idx = HYB_LIST.index(hyb) if hyb in HYB_LIST else None\n",
    "    f_hyb = one_hot_with_other(hyb if hyb in HYB_LIST else \"other\", HYB_LIST)  # len = len(HYB_LIST)+1 = 6\n",
    "    f_flags = [\n",
    "        int(atom.GetIsAromatic()),\n",
    "        int(atom.IsInRing()),\n",
    "    ]  # len 2\n",
    "    # concatenate\n",
    "    feat = f_type + f_deg + f_charge + f_hyb + f_flags\n",
    "    return feat  # ~ (11 + 7 + 6 + 6 + 2) = 32 dims\n",
    "\n",
    "def bond_features(bond: Chem.Bond):\n",
    "    btype = bond.GetBondType()\n",
    "    # SINGLE, DOUBLE, TRIPLE, AROMATIC\n",
    "    types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "             Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    f_type = one_hot_with_other(btype, types)    # len 5 (incl other)\n",
    "    f_flags = [int(bond.GetIsConjugated()), int(bond.IsInRing())]  # len 2\n",
    "    return f_type + f_flags  # 7 dims\n",
    "\n",
    "NODE_DIM = len(atom_features(Chem.MolFromSmiles(\"CC\").GetAtomWithIdx(0)))\n",
    "EDGE_DIM = len(bond_features(Chem.MolFromSmiles(\"CC\").GetBondWithIdx(0)))\n",
    "print(f\"Node feature dim: {NODE_DIM}, Edge feature dim: {EDGE_DIM}\")\n",
    "\n",
    "# ---- Build a small sample of graphs for sanity ----\n",
    "SAMPLE_N = min(200, N_total)\n",
    "graphs = []\n",
    "failed = []\n",
    "\n",
    "for i, s in enumerate(smiles[:SAMPLE_N]):\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None or mol.GetNumAtoms()==0:\n",
    "        failed.append(i)\n",
    "        continue\n",
    "\n",
    "    # Nodes\n",
    "    x = []\n",
    "    for a in mol.GetAtoms():\n",
    "        x.append(atom_features(a))\n",
    "    x = torch.tensor(x, dtype=torch.float32)  # (N, NODE_DIM)\n",
    "\n",
    "    # Edges (bidirectional)\n",
    "    ei_src, ei_dst, eattr = [], [], []\n",
    "    for b in mol.GetBonds():\n",
    "        u = b.GetBeginAtomIdx()\n",
    "        v = b.GetEndAtomIdx()\n",
    "        bf = bond_features(b)\n",
    "        # u->v\n",
    "        ei_src.append(u); ei_dst.append(v); eattr.append(bf)\n",
    "        # v->u\n",
    "        ei_src.append(v); ei_dst.append(u); eattr.append(bf)\n",
    "    if len(ei_src)==0:\n",
    "        # handle molecules with no bonds: make empty edge set\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_attr  = torch.empty((0, EDGE_DIM), dtype=torch.float32)\n",
    "    else:\n",
    "        edge_index = torch.tensor([ei_src, ei_dst], dtype=torch.long)        # (2, E)\n",
    "        edge_attr  = torch.tensor(eattr, dtype=torch.float32)                # (E, EDGE_DIM)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, idx=i)\n",
    "    graphs.append(data)\n",
    "\n",
    "print(f\"Built sample graphs: {len(graphs)} / {SAMPLE_N}, failed: {len(failed)}\")\n",
    "\n",
    "# ---- Sample stats ----\n",
    "n_nodes = [g.num_nodes for g in graphs]\n",
    "n_edges = [g.num_edges for g in graphs]\n",
    "print(f\"Avg nodes: {np.mean(n_nodes):.1f} ± {np.std(n_nodes):.1f}\")\n",
    "print(f\"Avg edges: {np.mean(n_edges):.1f} ± {np.std(n_edges):.1f}\")\n",
    "print(\"Min/Max nodes:\", (int(np.min(n_nodes)), int(np.max(n_nodes))))\n",
    "print(\"Min/Max edges:\", (int(np.min(n_edges)), int(np.max(n_edges))))\n",
    "\n",
    "# Peek a few\n",
    "for k in [0, 1, 2]:\n",
    "    if k < len(graphs):\n",
    "        g = graphs[k]\n",
    "        print(f\"[{k}] nodes={g.num_nodes}, edges={g.num_edges}, x.shape={tuple(g.x.shape)}, edge_attr.shape={tuple(g.edge_attr.shape)}\")\n",
    "\n",
    "# ---- Save sample ----\n",
    "sample_path = GRAPHS / \"graphs_sample_pyg.pt\"\n",
    "torch.save(graphs, sample_path)\n",
    "meta = {\n",
    "    \"node_dim\": NODE_DIM,\n",
    "    \"edge_dim\": EDGE_DIM,\n",
    "    \"sample_size\": SAMPLE_N,\n",
    "    \"built\": len(graphs),\n",
    "    \"failed_indices\": failed,\n",
    "    \"stats\": {\n",
    "        \"avg_nodes\": float(np.mean(n_nodes)) if n_nodes else 0.0,\n",
    "        \"avg_edges\": float(np.mean(n_edges)) if n_edges else 0.0,\n",
    "    }\n",
    "}\n",
    "(GRAPHS / \"graphs_meta_sample.json\").write_text(json.dumps(meta, indent=2))\n",
    "print(\"Saved:\", sample_path)\n",
    "print(\"Saved:\", GRAPHS / \"graphs_meta_sample.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f9c5cc",
   "metadata": {},
   "source": [
    "## 4: Build full graph cache (all SMILES → PyG graphs)\n",
    "\n",
    "This step converts **all** SMILES to PyTorch Geometric `Data` objects and saves:\n",
    "- `data/graphs/graphs_all_pyg.pt` — list of `Data` (one per molecule, aligned with `X_smiles.txt` order)\n",
    "- `data/graphs/graphs_meta_all.json` — feature dims, counts, stats, and invalid counts\n",
    "\n",
    "**Robustness:** if RDKit fails to parse a SMILES (rare), we create a **1-node dummy** graph (zeros) to preserve alignment with labels/splits, and mark `data.invalid = 1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96deab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node feature dim: 32, Edge feature dim: 7\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e29f581ddb40719dd547bfd7b392bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building graphs:   0%|          | 0/7831 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Built graphs: 7831 (expected 7831)\n",
      "Invalid (dummy) graphs: 0\n",
      "Molecules with zero bonds (edge-empty): 19\n",
      "Avg nodes: 18.57 ± 11.35\n",
      "Avg edges: 38.59 ± 25.05\n",
      "Min/Max nodes: (1, 132)\n",
      "Min/Max edges: (0, 290)\n",
      "Saved: tox21_dualenc_v1\\data\\graphs\\graphs_all_pyg.pt\n",
      "Saved: tox21_dualenc_v1\\data\\graphs\\graphs_meta_all.json\n",
      "Updated config: tox21_dualenc_v1\\models\\config_dualenc_v1.json\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json, math\n",
    "import numpy as np\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "DATA = ROOT / \"data\"\n",
    "GRAPHS = DATA / \"graphs\"\n",
    "GRAPHS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load SMILES & config\n",
    "smiles = (DATA/\"X_smiles.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "N_total = len(smiles)\n",
    "CONFIG_PATH = ROOT / \"models\" / \"config_dualenc_v1.json\"\n",
    "CONFIG = json.loads(CONFIG_PATH.read_text())\n",
    "\n",
    "# Reuse featurizers from Cell 3 (if not in scope, redefine quickly)\n",
    "ATOM_LIST = [\"C\",\"N\",\"O\",\"S\",\"F\",\"Cl\",\"Br\",\"I\",\"P\",\"B\"]\n",
    "HYB_LIST = [\n",
    "    Chem.rdchem.HybridizationType.SP,\n",
    "    Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3,\n",
    "    Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2,\n",
    "]\n",
    "def one_hot_with_other(key, choices):\n",
    "    vec = [0]* (len(choices)+1)\n",
    "    try:\n",
    "        idx = choices.index(key)\n",
    "    except ValueError:\n",
    "        idx = None\n",
    "    if idx is None:\n",
    "        vec[-1] = 1\n",
    "    else:\n",
    "        vec[idx] = 1\n",
    "    return vec\n",
    "def one_hot_index(idx, size, include_other=False):\n",
    "    if include_other and (idx<0 or idx>=size-1):\n",
    "        v = [0]* (size-1) + [1]\n",
    "    else:\n",
    "        v = [0]*size\n",
    "        if 0 <= idx < size:\n",
    "            v[idx] = 1\n",
    "    return v\n",
    "def atom_features(atom: Chem.Atom):\n",
    "    sym = atom.GetSymbol()\n",
    "    f_type = one_hot_with_other(sym, ATOM_LIST)            # 11\n",
    "    deg = atom.GetTotalDegree()\n",
    "    f_deg = one_hot_index(min(deg,6), 7)                   # 7\n",
    "    charge = atom.GetFormalCharge()\n",
    "    charge_map = {-2:0,-1:1,0:2,1:3,2:4}\n",
    "    f_charge = one_hot_index(charge_map.get(charge,5), 6)  # 6\n",
    "    hyb = atom.GetHybridization()\n",
    "    f_hyb = one_hot_with_other(hyb if hyb in HYB_LIST else \"other\", HYB_LIST)  # 6\n",
    "    f_flags = [int(atom.GetIsAromatic()), int(atom.IsInRing())]  # 2\n",
    "    return f_type + f_deg + f_charge + f_hyb + f_flags           # total = 32\n",
    "def bond_features(bond: Chem.Bond):\n",
    "    types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "             Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    f_type = one_hot_with_other(bond.GetBondType(), types)       # 5\n",
    "    f_flags = [int(bond.GetIsConjugated()), int(bond.IsInRing())]# 2\n",
    "    return f_type + f_flags                                      # total = 7\n",
    "\n",
    "NODE_DIM = len(atom_features(Chem.MolFromSmiles(\"CC\").GetAtomWithIdx(0)))\n",
    "EDGE_DIM = len(bond_features(Chem.MolFromSmiles(\"CC\").GetBondWithIdx(0)))\n",
    "print(f\"Node feature dim: {NODE_DIM}, Edge feature dim: {EDGE_DIM}\")\n",
    "\n",
    "graphs = []\n",
    "invalid = 0\n",
    "degenerate_edges = 0\n",
    "\n",
    "for i, s in enumerate(tqdm(smiles, desc=\"Building graphs\", total=N_total)):\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if (mol is None) or (mol.GetNumAtoms()==0):\n",
    "        # Fallback: 1-node dummy graph to preserve alignment\n",
    "        x = torch.zeros((1, NODE_DIM), dtype=torch.float32)\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_attr  = torch.empty((0, EDGE_DIM), dtype=torch.float32)\n",
    "        data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, idx=i, invalid=1)\n",
    "        graphs.append(data)\n",
    "        invalid += 1\n",
    "        continue\n",
    "\n",
    "    # Nodes\n",
    "    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float32)\n",
    "\n",
    "    # Edges (bidirectional)\n",
    "    ei_src, ei_dst, eattr = [], [], []\n",
    "    for b in mol.GetBonds():\n",
    "        u = b.GetBeginAtomIdx(); v = b.GetEndAtomIdx()\n",
    "        bf = bond_features(b)\n",
    "        # u->v and v->u\n",
    "        ei_src.extend([u,v]); ei_dst.extend([v,u]); eattr.extend([bf,bf])\n",
    "\n",
    "    if len(ei_src)==0:\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_attr  = torch.empty((0, EDGE_DIM), dtype=torch.float32)\n",
    "        degenerate_edges += 1\n",
    "    else:\n",
    "        edge_index = torch.tensor([ei_src, ei_dst], dtype=torch.long)\n",
    "        edge_attr  = torch.tensor(eattr, dtype=torch.float32)\n",
    "\n",
    "    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, idx=i, invalid=0)\n",
    "    graphs.append(data)\n",
    "\n",
    "print(f\"Built graphs: {len(graphs)} (expected {N_total})\")\n",
    "print(f\"Invalid (dummy) graphs: {invalid}\")\n",
    "print(f\"Molecules with zero bonds (edge-empty): {degenerate_edges}\")\n",
    "\n",
    "# Stats\n",
    "n_nodes = [g.num_nodes for g in graphs]\n",
    "n_edges = [g.num_edges for g in graphs]\n",
    "print(f\"Avg nodes: {np.mean(n_nodes):.2f} ± {np.std(n_nodes):.2f}\")\n",
    "print(f\"Avg edges: {np.mean(n_edges):.2f} ± {np.std(n_edges):.2f}\")\n",
    "print(\"Min/Max nodes:\", (int(np.min(n_nodes)), int(np.max(n_nodes))))\n",
    "print(\"Min/Max edges:\", (int(np.min(n_edges)), int(np.max(n_edges))))\n",
    "\n",
    "# Save\n",
    "all_path = GRAPHS / \"graphs_all_pyg.pt\"\n",
    "torch.save(graphs, all_path)\n",
    "meta = {\n",
    "    \"node_dim\": NODE_DIM,\n",
    "    \"edge_dim\": EDGE_DIM,\n",
    "    \"N_total\": N_total,\n",
    "    \"built\": len(graphs),\n",
    "    \"invalid\": invalid,\n",
    "    \"degenerate_edges\": degenerate_edges,\n",
    "    \"stats\": {\n",
    "        \"avg_nodes\": float(np.mean(n_nodes)) if n_nodes else 0.0,\n",
    "        \"avg_edges\": float(np.mean(n_edges)) if n_edges else 0.0,\n",
    "        \"min_nodes\": int(np.min(n_nodes)) if n_nodes else 0,\n",
    "        \"max_nodes\": int(np.max(n_nodes)) if n_nodes else 0,\n",
    "        \"min_edges\": int(np.min(n_edges)) if n_edges else 0,\n",
    "        \"max_edges\": int(np.max(n_edges)) if n_edges else 0,\n",
    "    }\n",
    "}\n",
    "(GRAPHS / \"graphs_meta_all.json\").write_text(json.dumps(meta, indent=2))\n",
    "print(\"Saved:\", all_path)\n",
    "print(\"Saved:\", GRAPHS / \"graphs_meta_all.json\")\n",
    "\n",
    "# Update config with dims\n",
    "CONFIG[\"node_dim\"] = NODE_DIM\n",
    "CONFIG[\"edge_dim\"] = EDGE_DIM\n",
    "CONFIG_PATH.write_text(json.dumps(CONFIG, indent=2))\n",
    "print(\"Updated config:\", CONFIG_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28630b4",
   "metadata": {},
   "source": [
    "## 5: Unified dataset & dataloaders (SMILES + Graph + Descriptors)\n",
    "\n",
    "We build a single `DualEncDataset` that, for each index:\n",
    "- returns the **RDKit→PyG graph**,\n",
    "- the **raw SMILES** (we’ll tokenize in the collate function),\n",
    "- the **descriptor vector** (256-d),\n",
    "- the **labels** `y[i]` and **mask** `y_mask[i]`.\n",
    "\n",
    "A custom `collate` will:\n",
    "- batch the graphs via `torch_geometric.data.Batch.from_data_list`,\n",
    "- tokenize the SMILES with `transformers` (pad/truncate to `max_length`),\n",
    "- stack descriptors/labels/masks into tensors.\n",
    "\n",
    "We’ll also compute **per-label `pos_weight`** from the **train split**:\n",
    "\\[\n",
    "\\text{pos\\_weight}_\\ell = \\frac{\\#\\{\\text{negatives}\\}}{\\#\\{\\text{positives}\\}}\n",
    "\\]\n",
    "(on labeled entries only), which helps with class imbalance for BCEWithLogits.\n",
    "Artifacts:\n",
    "- `models/pos_weight.npy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049641f7",
   "metadata": {},
   "source": [
    "### 5a) Fix graph loading (PyTorch 2.6) & create a safe cache\n",
    "\n",
    "PyTorch 2.6 defaults `torch.load(..., weights_only=True)`, which blocks loading custom\n",
    "classes (like PyG’s `Data`). We will:\n",
    "\n",
    "1) Try to load `graphs_all_pyg.pt` with an allowlist or `weights_only=False` (trusted local file).\n",
    "2) Immediately convert the list of `Data` objects into a **safe list of plain dicts** and save\n",
    "   as `graphs_all_pyg_safe.pt`. This format loads cleanly under the new defaults.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7164867f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First load attempt failed: UnpicklingError('Weights only load failed. This file can still be loaded, to do so you have two options, \\x1b[1mdo those steps only if you trust the source of the checkpoint\\x1b[0m. \\n\\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\\n\\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\\n\\tWeightsUnpickler error: Unsupported global: GLOBAL torch_geometric.data.data.DataEdgeAttr was not an allowed global by default. Please use `torch.serialization.add_safe_globals([DataEdgeAttr])` or the `torch.serialization.safe_globals([DataEdgeAttr])` context manager to allowlist this global if you trust this class/function.\\n\\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.')\n",
      "Retrying with weights_only=False (trusted local file).\n",
      "Loaded PyG graphs: 7831\n",
      "Saved safe graph cache to: tox21_dualenc_v1\\data\\graphs\\graphs_all_pyg_safe.pt\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "GRAPHS = ROOT / \"data\" / \"graphs\"\n",
    "\n",
    "raw_path  = GRAPHS / \"graphs_all_pyg.pt\"\n",
    "safe_path = GRAPHS / \"graphs_all_pyg_safe.pt\"\n",
    "\n",
    "def robust_load_pyg_list(path: Path):\n",
    "    \"\"\"\n",
    "    Try to load a list of PyG Data objects saved with torch.save.\n",
    "    Works around PyTorch 2.6 weights_only default by:\n",
    "      - attempting a normal load\n",
    "      - if it fails, retry with weights_only=False (safe because it's your own file)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return torch.load(path)  # may fail under 2.6 with weights_only=True\n",
    "    except Exception as e:\n",
    "        print(\"First load attempt failed:\", repr(e))\n",
    "        print(\"Retrying with weights_only=False (trusted local file).\")\n",
    "        return torch.load(path, weights_only=False)\n",
    "\n",
    "def to_safe_dict_list(graphs):\n",
    "    \"\"\"\n",
    "    Convert list[Data] -> list[dict of tensors/primitives] to avoid pickling PyG classes.\n",
    "    \"\"\"\n",
    "    safe = []\n",
    "    for g in graphs:\n",
    "        d = {\n",
    "            \"x\": g.x,                               # (N, node_dim)\n",
    "            \"edge_index\": g.edge_index,             # (2, E)\n",
    "            \"edge_attr\": g.edge_attr,               # (E, edge_dim)\n",
    "            \"idx\": int(getattr(g, \"idx\", -1)),\n",
    "            \"invalid\": int(getattr(g, \"invalid\", 0)),\n",
    "            # You can add more keys if you attach them later\n",
    "        }\n",
    "        safe.append(d)\n",
    "    return safe\n",
    "\n",
    "if not raw_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing {raw_path} — please re-run Cell 4 to build graphs.\")\n",
    "\n",
    "graphs = robust_load_pyg_list(raw_path)\n",
    "print(f\"Loaded PyG graphs: {len(graphs)}\")\n",
    "\n",
    "# Convert & save safe cache\n",
    "safe_graphs = to_safe_dict_list(graphs)\n",
    "torch.save(safe_graphs, safe_path)\n",
    "print(\"Saved safe graph cache to:\", safe_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3d5210",
   "metadata": {},
   "source": [
    "### 5b) Unified dataset & dataloaders (using safe graph cache)\n",
    "\n",
    "We now load `graphs_all_pyg_safe.pt` (plain dicts) and reconstruct PyG `Data` objects in memory.\n",
    "Then we build:\n",
    "- `DualEncDataset` (returns graph, SMILES, descriptors, labels, mask)\n",
    "- dataloaders for train/val/test\n",
    "- `pos_weight` from the train split (for BCEWithLogits)\n",
    "\n",
    "Includes a one-batch sanity print.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd607cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded assets:\n",
      "  N=7831 L=12\n",
      "  train/val/test = 6265/783/783\n",
      "Batches:\n",
      "  train=392\n",
      "  val=49\n",
      "  test=49\n",
      "\n",
      "Sanity (one train batch):\n",
      "  input_ids: (16, 69) attention_mask: (16, 69)\n",
      "  graph batch: DataBatch(x=[245, 32], edge_index=[2, 490], edge_attr=[490, 7], idx=[16], invalid=[16], batch=[245], ptr=[17])\n",
      "   - x shape: (245, 32) edge_index: (2, 490) edge_attr: (490, 7)\n",
      "   - num_graphs: 16\n",
      "  desc: (16, 256)  y: (16, 12)  mask: (16, 12)\n",
      "  mask unique: tensor([0., 1.])\n",
      "\n",
      "pos_weight saved to: tox21_dualenc_v1\\models\\pos_weight.npy\n",
      "pos counts: [261, 210, 633, 235, 668, 302, 141, 741, 208, 293, 758, 311]\n",
      "neg counts: [5555, 5233, 4623, 4464, 4352, 5280, 5076, 4010, 5476, 4926, 3946, 5119]\n",
      "pos_weight (first 6): [21.28352483 24.91904751  7.30331753 18.9957446   6.51497005 17.48344365]\n"
     ]
    }
   ],
   "source": [
    "import json, math, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data, Batch as GeometricBatch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "DATA = ROOT / \"data\"\n",
    "SPLITS = DATA / \"splits\"\n",
    "DESC = DATA / \"descriptors\"\n",
    "GRAPHS = DATA / \"graphs\"\n",
    "MODELS = ROOT / \"models\"\n",
    "\n",
    "CONFIG = json.loads((MODELS/\"config_dualenc_v1.json\").read_text())\n",
    "MAX_LEN = int(CONFIG.get(\"max_length\", 256))\n",
    "L = int(CONFIG.get(\"num_labels\", 12))\n",
    "\n",
    "# ---- Load arrays & splits\n",
    "smiles = (DATA/\"X_smiles.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "y       = np.load(DATA/\"y.npy\").astype(np.float32)\n",
    "y_mask  = np.load(DATA/\"y_mask.npy\").astype(np.float32)\n",
    "X_desc  = np.load(DESC/\"desc_selected.npy\").astype(np.float32)\n",
    "label_names = (DATA/\"label_names.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "splits = json.loads((SPLITS/\"splits.json\").read_text())\n",
    "train_idx = np.array(splits[\"train\"], dtype=int)\n",
    "val_idx   = np.array(splits[\"val\"], dtype=int)\n",
    "test_idx  = np.array(splits[\"test\"], dtype=int)\n",
    "\n",
    "# ---- Load safe cache and reconstruct Data objects\n",
    "safe_path = GRAPHS / \"graphs_all_pyg_safe.pt\"\n",
    "if not safe_path.exists():\n",
    "    raise FileNotFoundError(f\"Missing {safe_path}. Run Cell 5a first.\")\n",
    "safe_graphs = torch.load(safe_path)  # list of dicts\n",
    "\n",
    "def dict_to_data(d):\n",
    "    return Data(\n",
    "        x=d[\"x\"],\n",
    "        edge_index=d[\"edge_index\"],\n",
    "        edge_attr=d[\"edge_attr\"],\n",
    "        idx=int(d.get(\"idx\", -1)),\n",
    "        invalid=int(d.get(\"invalid\", 0)),\n",
    "    )\n",
    "\n",
    "graphs = [dict_to_data(d) for d in safe_graphs]\n",
    "\n",
    "# ---- Alignment checks\n",
    "assert len(graphs) == len(smiles) == y.shape[0] == X_desc.shape[0], \"Alignment mismatch among graphs/smiles/y/desc\"\n",
    "print(\"Loaded assets:\",\n",
    "      f\"N={len(smiles)} L={L}\",\n",
    "      f\"train/val/test = {len(train_idx)}/{len(val_idx)}/{len(test_idx)}\",\n",
    "      sep=\"\\n  \")\n",
    "\n",
    "# ---- Tokenizer\n",
    "tok = AutoTokenizer.from_pretrained(CONFIG[\"text_encoder\"])\n",
    "\n",
    "# ---- Dataset\n",
    "class DualEncDataset(Dataset):\n",
    "    def __init__(self, indices):\n",
    "        self.idx = np.asarray(indices, dtype=int)\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idx[i])\n",
    "        return {\n",
    "            \"graph\": graphs[j],\n",
    "            \"smiles\": smiles[j],\n",
    "            \"desc\":  X_desc[j],\n",
    "            \"y\":     y[j],\n",
    "            \"mask\":  y_mask[j],\n",
    "            \"idx\":   j,\n",
    "        }\n",
    "\n",
    "# ---- Collate\n",
    "def collate_fn(batch):\n",
    "    g_batch = GeometricBatch.from_data_list([b[\"graph\"] for b in batch])\n",
    "    toks = tok(\n",
    "        [b[\"smiles\"] for b in batch],\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    desc = torch.tensor(np.stack([b[\"desc\"] for b in batch]), dtype=torch.float32)\n",
    "    yb   = torch.tensor(np.stack([b[\"y\"]    for b in batch]), dtype=torch.float32)\n",
    "    mb   = torch.tensor(np.stack([b[\"mask\"] for b in batch]), dtype=torch.float32)\n",
    "    idxs = torch.tensor([b[\"idx\"] for b in batch], dtype=torch.long)\n",
    "    return {\"graph\": g_batch, \"tok\": toks, \"desc\": desc, \"y\": yb, \"mask\": mb, \"idx\": idxs}\n",
    "\n",
    "# ---- Dataloaders\n",
    "BS = int(CONFIG.get(\"batch_size\", 16))\n",
    "def make_loader(which):\n",
    "    if which==\"train\":\n",
    "        ds = DualEncDataset(train_idx)\n",
    "        return DataLoader(ds, batch_size=BS, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "    elif which==\"val\":\n",
    "        ds = DualEncDataset(val_idx)\n",
    "        return DataLoader(ds, batch_size=BS, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "    elif which==\"test\":\n",
    "        ds = DualEncDataset(test_idx)\n",
    "        return DataLoader(ds, batch_size=BS, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "    else:\n",
    "        raise ValueError(\"which must be train|val|test\")\n",
    "\n",
    "train_loader = make_loader(\"train\")\n",
    "val_loader   = make_loader(\"val\")\n",
    "test_loader  = make_loader(\"test\")\n",
    "\n",
    "print(\"Batches:\",\n",
    "      f\"train={len(train_loader)}\",\n",
    "      f\"val={len(val_loader)}\",\n",
    "      f\"test={len(test_loader)}\", sep=\"\\n  \")\n",
    "\n",
    "# ---- One-batch sanity\n",
    "batch = next(iter(train_loader))\n",
    "g = batch[\"graph\"]; t = batch[\"tok\"]\n",
    "print(\"\\nSanity (one train batch):\")\n",
    "print(\"  input_ids:\", tuple(t[\"input_ids\"].shape), \"attention_mask:\", tuple(t[\"attention_mask\"].shape))\n",
    "print(\"  graph batch:\", g)\n",
    "print(\"   - x shape:\", tuple(g.x.shape), \"edge_index:\", tuple(g.edge_index.shape), \"edge_attr:\", tuple(g.edge_attr.shape))\n",
    "print(\"   - num_graphs:\", g.num_graphs)\n",
    "print(\"  desc:\", tuple(batch[\"desc\"].shape), \" y:\", tuple(batch[\"y\"].shape), \" mask:\", tuple(batch[\"mask\"].shape))\n",
    "print(\"  mask unique:\", torch.unique(batch['mask']))\n",
    "\n",
    "# ---- pos_weight from TRAIN (labeled only)\n",
    "pos = ((y[train_idx]==1) & (y_mask[train_idx]==1)).sum(axis=0)\n",
    "neg = ((y[train_idx]==0) & (y_mask[train_idx]==1)).sum(axis=0)\n",
    "eps = 1e-6\n",
    "pos_weight = (neg + eps) / (pos + eps)\n",
    "np.save(MODELS/\"pos_weight.npy\", pos_weight.astype(np.float32))\n",
    "print(\"\\npos_weight saved to:\", MODELS/\"pos_weight.npy\")\n",
    "print(\"pos counts:\", pos.tolist())\n",
    "print(\"neg counts:\", neg.tolist())\n",
    "print(\"pos_weight (first 6):\", pos_weight[:6])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d82510",
   "metadata": {},
   "source": [
    "## 6: Model definition: Dual-Encoder with Cross-Attention + Descriptors\n",
    "\n",
    "**Text branch (SMILES):** ChemBERTa encoder → project 768→256.  \n",
    "**Graph branch:** GINE (edge-aware GIN) with 4 layers @ 256 hidden → node embeddings + global mean pool.  \n",
    "**Fusion:** Cross-attention (Text ← Graph) with 4 heads, model dim 256, dropout 0.1, + residual/FFN.  \n",
    "**Pooling:** Attention pooling over fused text tokens (masking padded tokens).  \n",
    "**Descriptors:** 256-d MLP.  \n",
    "**Head:** concat([z_text, z_graph, z_desc]) ∈ ℝ⁷⁶⁸ → MLP(512) → 12 logits.\n",
    "\n",
    "This cell:\n",
    "1) Builds the modules.\n",
    "2) Prints parameter counts.\n",
    "3) Runs a **dry-run** forward pass on one batch (no grad) to verify shapes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b628306",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: total=94.06M, trainable=94.06M\n",
      "Text hidden size: 768 → proj to 256\n",
      "Dry-run logits shape: (16, 12)  (expect (B, L) = (16, 12) )\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "import torch, torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "MODELS = ROOT / \"models\"\n",
    "CONFIG = json.loads((MODELS/\"config_dualenc_v1.json\").read_text())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# --- Small utility: parameter counting\n",
    "def count_params(model):\n",
    "    total = sum(p.numel() for p in model.parameters())\n",
    "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "# --- Attention pooling with optional mask (mask=True for real tokens)\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.score = nn.Linear(dim, 1)\n",
    "    def forward(self, X, mask=None):   # X: (B,L,D), mask: (B,L) bool or 0/1\n",
    "        s = self.score(X).squeeze(-1)  # (B,L)\n",
    "        if mask is not None:\n",
    "            mask = mask.bool()\n",
    "            s = s.masked_fill(~mask, -1e9)\n",
    "        a = torch.softmax(s, dim=1)    # (B,L)\n",
    "        return torch.bmm(a.unsqueeze(1), X).squeeze(1)  # (B,D)\n",
    "\n",
    "# --- Cross-attention block: Text <- Graph\n",
    "class CrossAttnBlock(nn.Module):\n",
    "    def __init__(self, dim=256, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(dim, 4*dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(4*dim, dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, Q_text, K_graph, V_graph, key_padding_mask=None):\n",
    "        # Q_text: (B,T,D), K_graph/V_graph: (B,N,D), key_padding_mask: (B,N) True=PAD\n",
    "        attn_out, _ = self.attn(Q_text, K_graph, V_graph, key_padding_mask=key_padding_mask)\n",
    "        x = self.norm1(Q_text + self.drop(attn_out))\n",
    "        ff = self.ffn(x)\n",
    "        y = self.norm2(x + self.drop(ff))\n",
    "        return y\n",
    "\n",
    "# --- GNN encoder: GINE stack (edge-aware)\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden=256, layers=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        in_dim = node_dim\n",
    "        for _ in range(layers):\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "            )\n",
    "            conv = GINEConv(mlp, edge_dim=edge_dim)  # learns a Linear(edge_attr) internally\n",
    "            self.layers.append(conv)\n",
    "            self.bns.append(nn.BatchNorm1d(hidden))\n",
    "            in_dim = hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden = hidden\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch_index):\n",
    "        # x: (sumN, node_dim); edge_index: (2,E); edge_attr: (E, edge_dim); batch_index: (sumN,)\n",
    "        h = x\n",
    "        for conv, bn in zip(self.layers, self.bns):\n",
    "            h = conv(h, edge_index, edge_attr)\n",
    "            h = bn(h)\n",
    "            h = F.relu(h)\n",
    "            h = self.dropout(h)\n",
    "        # Node embeddings (sumN, hidden)\n",
    "        # Graph pooled (B, hidden)\n",
    "        z_graph = global_mean_pool(h, batch_index)\n",
    "        return h, z_graph\n",
    "\n",
    "# --- Full model\n",
    "class DualEncoderXAttn(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "        # Text encoder\n",
    "        self.tok = AutoTokenizer.from_pretrained(config[\"text_encoder\"])\n",
    "        self.text = AutoModel.from_pretrained(config[\"text_encoder\"])\n",
    "        self.text_proj = nn.Linear(self.text.config.hidden_size, config[\"text_proj_dim\"])\n",
    "\n",
    "        # Graph encoder\n",
    "        self.gnn = GINEncoder(\n",
    "            node_dim=int(config[\"node_dim\"]),\n",
    "            edge_dim=int(config[\"edge_dim\"]),\n",
    "            hidden=int(config[\"graph_hidden\"]),\n",
    "            layers=int(config[\"graph_layers\"]),\n",
    "            dropout=float(config[\"dropout\"]),\n",
    "        )\n",
    "\n",
    "        # Cross-attn (Text <- Graph)\n",
    "        self.xattn = CrossAttnBlock(\n",
    "            dim=int(config[\"fusion_dim\"]),\n",
    "            heads=int(config[\"fusion_heads\"]),\n",
    "            dropout=float(config[\"fusion_dropout\"]),\n",
    "        )\n",
    "\n",
    "        # Pools & descriptor MLP\n",
    "        self.tpool = AttentionPool(dim=int(config[\"fusion_dim\"]))\n",
    "        self.dmlp = nn.Sequential(\n",
    "            nn.Linear(int(config[\"desc_dim_in\"]), int(config[\"desc_hidden\"])),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(float(config[\"dropout\"])),\n",
    "        )\n",
    "\n",
    "        # Head\n",
    "        fused_in = int(config[\"fusion_dim\"]) + int(config[\"graph_hidden\"]) + int(config[\"desc_hidden\"])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_in, int(config[\"head_hidden\"])),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(float(config[\"dropout\"])),\n",
    "            nn.Linear(int(config[\"head_hidden\"]), int(config[\"num_labels\"])),\n",
    "        )\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, graph_batch, desc):\n",
    "        # Text\n",
    "        t_out = self.text(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,T,768)\n",
    "        t_proj = self.text_proj(t_out)  # (B,T,256)\n",
    "\n",
    "        # Graph\n",
    "        g = graph_batch\n",
    "        node_h, z_graph = self.gnn(g.x, g.edge_index, g.edge_attr, g.batch)  # node_h: (sumN,256), z_graph: (B,256)\n",
    "\n",
    "        # Dense pack nodes per graph for cross-attn keys/values\n",
    "        node_dense, node_mask = to_dense_batch(node_h, g.batch)  # (B,Nmax,256), (B,Nmax) True=valid nodes\n",
    "        key_padding_mask = ~node_mask  # True where PAD → mask out\n",
    "\n",
    "        # Cross-attn: Text queries nodes\n",
    "        fused_text = self.xattn(t_proj, node_dense, node_dense, key_padding_mask=key_padding_mask)  # (B,T,256)\n",
    "\n",
    "        # Attention pool over tokens (mask pads)\n",
    "        text_mask_bool = attention_mask.bool()\n",
    "        z_text = self.tpool(fused_text, mask=text_mask_bool)  # (B,256)\n",
    "\n",
    "        # Descriptors\n",
    "        z_desc = self.dmlp(desc)  # (B,256)\n",
    "\n",
    "        # Head\n",
    "        z = torch.cat([z_text, z_graph, z_desc], dim=-1)  # (B, 256+256+256=768)\n",
    "        logits = self.head(z)  # (B,L)\n",
    "        return logits\n",
    "\n",
    "# --- Instantiate & dry-run on one batch\n",
    "model = DualEncoderXAttn(CONFIG).to(device)\n",
    "total, trainable = count_params(model)\n",
    "print(f\"Model params: total={total/1e6:.2f}M, trainable={trainable/1e6:.2f}M\")\n",
    "print(\"Text hidden size:\", model.text.config.hidden_size, \"→ proj to\", CONFIG[\"text_proj_dim\"])\n",
    "\n",
    "# Dry-run\n",
    "model.eval()\n",
    "batch = next(iter(train_loader))\n",
    "with torch.no_grad():\n",
    "    ids = batch[\"tok\"][\"input_ids\"].to(device)\n",
    "    att = batch[\"tok\"][\"attention_mask\"].to(device)\n",
    "    gbt = batch[\"graph\"].to(device)\n",
    "    dsc = batch[\"desc\"].to(device)\n",
    "    out = model(ids, att, gbt, dsc)\n",
    "print(\"Dry-run logits shape:\", tuple(out.shape), \" (expect (B, L) =\", (ids.shape[0], CONFIG[\"num_labels\"]), \")\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7679fa0e",
   "metadata": {},
   "source": [
    "## 7: Training loop (Stage A: freeze text → Stage B: partial unfreeze)\n",
    "\n",
    "**Objective.** Train the dual-encoder from scratch (new GNN/fusion/head) while *fine-tuning* ChemBERTa safely.\n",
    "\n",
    "**Stages**\n",
    "- **Stage A (warm-start):** *Freeze ChemBERTa*; train GNN + cross-attn + head + descriptors for a few epochs (default 4).\n",
    "- **Stage B (main):** *Unfreeze the last N transformer layers* (default 4) with a *small LR*; continue training with early stopping on **macro ROC-AUC (val)**.\n",
    "\n",
    "**Details**\n",
    "- Optimizer: AdamW with two parameter groups (text vs others).\n",
    "- Scheduler: cosine with warm-up (10% of steps).\n",
    "- Loss: masked **BCEWithLogits** with **per-label pos_weight**.\n",
    "- Mixed precision (AMP), gradient accumulation, gradient clipping.\n",
    "- Checkpoints:\n",
    "  - `models/checkpoints/dualenc_best.pt` (best val macro ROC-AUC)\n",
    "  - `models/checkpoints/dualenc_last.pt` (last epoch)\n",
    "- Logs:\n",
    "  - `logs/train_log.jsonl` (one JSON per epoch with train/val metrics)\n",
    "  - `models/run_summary.json` (final operating point)\n",
    "\n",
    "We’ll print epoch summaries and sanity checks as we go.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb16cd15",
   "metadata": {},
   "source": [
    "### 7a) Hotfix: AttentionPool under AMP/float16\n",
    "\n",
    "`masked_fill(-1e9)` overflows in fp16. We’ll:\n",
    "1) compute the attention scores in **float32**,\n",
    "2) apply the mask with a large negative (−1e9) safely,\n",
    "3) softmax in float32, then cast weights back to the original dtype.\n",
    "\n",
    "After patching, we sanity-run one forward pass under AMP to confirm no errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cd0803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity after patch — logits: (16, 12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_21812\\552480316.py:25: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), autocast(dtype=torch.float16):\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.cuda.amp import autocast\n",
    "\n",
    "# New AMP-safe attention pool\n",
    "class AttentionPoolAMP(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.score = nn.Linear(dim, 1)\n",
    "    def forward(self, X, mask=None):\n",
    "        # compute scores in float32 for numerical stability\n",
    "        s = self.score(X).squeeze(-1).float()   # (B,L) in fp32\n",
    "        if mask is not None:\n",
    "            s = s.masked_fill(~mask.bool(), -1e9)  # safe in fp32\n",
    "        a = torch.softmax(s, dim=1)                  # fp32\n",
    "        a = a.to(X.dtype)                            # match X (fp16 under AMP)\n",
    "        return torch.bmm(a.unsqueeze(1), X).squeeze(1)\n",
    "\n",
    "# Swap into the existing model\n",
    "model.tpool = AttentionPoolAMP(dim=int(CONFIG[\"fusion_dim\"])).to(next(model.parameters()).device)\n",
    "\n",
    "# Quick sanity forward under AMP\n",
    "model.eval()\n",
    "with torch.no_grad(), autocast(dtype=torch.float16):\n",
    "    batch = next(iter(train_loader))\n",
    "    ids = batch[\"tok\"][\"input_ids\"].to(device)\n",
    "    att = batch[\"tok\"][\"attention_mask\"].to(device)\n",
    "    gbt = batch[\"graph\"].to(device)\n",
    "    dsc = batch[\"desc\"].to(device)\n",
    "    out = model(ids, att, gbt, dsc)\n",
    "print(\"Sanity after patch — logits:\", tuple(out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "666c1f3e",
   "metadata": {},
   "source": [
    "### 7b) Training loop (AMP-safe): Stage A (freeze text) → Stage B (unfreeze last N)\n",
    "\n",
    "This cell re-runs the full training with:\n",
    "- `torch.amp.autocast('cuda', dtype=torch.float16)` (no deprecation warning),\n",
    "- our AMP-safe `AttentionPool`,\n",
    "- early stopping on **val macro ROC-AUC**,\n",
    "- checkpoints for **best** and **last**,\n",
    "- JSON logs per epoch.\n",
    "\n",
    "Artifacts saved under:\n",
    "- `tox21_dualenc_v1/models/checkpoints/dualenc_best.pt`\n",
    "- `tox21_dualenc_v1/models/checkpoints/dualenc_last.pt`\n",
    "- `tox21_dualenc_v1/logs/train_log.jsonl`\n",
    "- `tox21_dualenc_v1/models/run_summary.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1506d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Stage A] Trainable params: 1.93M (text frozen)\n",
      "[A:01] loss=1.1117  val_auc=0.7719  time=8.0s\n",
      "[A:02] loss=0.9189  val_auc=0.7883  time=7.6s\n",
      "[A:03] loss=0.7797  val_auc=0.7925  time=7.5s\n",
      "[A:04] loss=0.7038  val_auc=0.7932  time=7.3s\n",
      "[Stage B] Unfroze last 4 text layers → trainable params: 30.28M\n",
      "[B:01] loss=0.6901  val_auc=0.7921  time=10.7s\n",
      "  ↳ saved new BEST to tox21_dualenc_v1\\models\\checkpoints\\dualenc_best.pt (val_macro_auc=0.7921)\n",
      "[B:02] loss=0.6921  val_auc=0.7801  time=10.7s\n",
      "[B:03] loss=0.6719  val_auc=0.7655  time=10.7s\n",
      "[B:04] loss=0.6935  val_auc=0.7565  time=10.6s\n",
      "[B:05] loss=0.6914  val_auc=0.7402  time=10.7s\n",
      "[B:06] loss=0.5794  val_auc=0.7391  time=10.7s\n",
      "[B:07] loss=0.5044  val_auc=0.7298  time=10.7s\n",
      "[B:08] loss=0.4420  val_auc=0.7446  time=10.7s\n",
      "[B:09] loss=0.3965  val_auc=0.7480  time=10.7s\n",
      "Early stopping: no improvement for 8 epochs (best @ epoch 1 = 0.7921)\n",
      "Saved summary to tox21_dualenc_v1\\models\\run_summary.json\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, json, math, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch, torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.optim import AdamW\n",
    "from torch import amp\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "LOGS = ROOT / \"logs\"\n",
    "CKPT = ROOT / \"models\" / \"checkpoints\"\n",
    "CKPT.mkdir(parents=True, exist_ok=True)\n",
    "LOGS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- Utilities ----\n",
    "def set_seed(seed: int):\n",
    "    import random, numpy as np, torch\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def bce_masked_logits_loss(logits, y, mask, pos_weight=None):\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\", pos_weight=pos_weight)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss = (loss * mask).sum() / (mask.sum() + 1e-6)\n",
    "    return loss\n",
    "\n",
    "def eval_macro_auc(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_y, all_m = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            ids = batch[\"tok\"][\"input_ids\"].to(device)\n",
    "            att = batch[\"tok\"][\"attention_mask\"].to(device)\n",
    "            gbt = batch[\"graph\"].to(device)\n",
    "            dsc = batch[\"desc\"].to(device)\n",
    "            logits = model(ids, att, gbt, dsc)\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            y01 = (batch[\"y\"] == 1).float().cpu()\n",
    "            all_y.append(y01)\n",
    "            all_m.append(batch[\"mask\"].float().cpu())\n",
    "    logits = torch.cat(all_logits).numpy()\n",
    "    y = torch.cat(all_y).numpy()\n",
    "    m = torch.cat(all_m).numpy()\n",
    "    prob = 1.0 / (1.0 + np.exp(-logits))\n",
    "    aucs = []\n",
    "    for l in range(prob.shape[1]):\n",
    "        mask_l = m[:, l] == 1\n",
    "        y_l = y[:, l][mask_l]\n",
    "        p_l = prob[:, l][mask_l]\n",
    "        valid = (mask_l.sum() > 0) and (y_l.min() < 1) and (y_l.max() > 0)\n",
    "        if valid:\n",
    "            try: aucs.append(roc_auc_score(y_l, p_l))\n",
    "            except Exception: aucs.append(np.nan)\n",
    "        else:\n",
    "            aucs.append(np.nan)\n",
    "    macro_auc = float(np.nanmean(aucs))\n",
    "    return macro_auc, aucs\n",
    "\n",
    "def count_trainable_params(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def freeze_text(model):\n",
    "    for p in model.text.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_last_n_text_layers(model, n_last=4):\n",
    "    freeze_text(model)\n",
    "    enc = getattr(model.text, \"encoder\", None)\n",
    "    if enc is None and hasattr(model.text, \"roberta\"):\n",
    "        enc = model.text.roberta.encoder\n",
    "    layers = getattr(enc, \"layer\", None)\n",
    "    if layers is None:\n",
    "        for p in model.text.parameters(): p.requires_grad = True\n",
    "        return\n",
    "    L = len(layers)\n",
    "    keep_frozen = max(L - int(n_last), 0)\n",
    "    for li in range(keep_frozen, L):\n",
    "        for p in layers[li].parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "def build_optim_sched(model, steps_per_epoch, total_epochs, lr_text, lr_others, weight_decay, warmup_ratio=0.1):\n",
    "    text_params = [p for n,p in model.named_parameters() if n.startswith(\"text.\") and p.requires_grad]\n",
    "    other_params = [p for n,p in model.named_parameters() if (not n.startswith(\"text.\")) and p.requires_grad]\n",
    "    opt = AdamW([\n",
    "        {\"params\": other_params, \"lr\": lr_others, \"weight_decay\": weight_decay},\n",
    "        {\"params\": text_params,   \"lr\": lr_text,   \"weight_decay\": weight_decay},\n",
    "    ])\n",
    "    from transformers import get_cosine_schedule_with_warmup\n",
    "    total_steps = steps_per_epoch * total_epochs\n",
    "    warmup_steps = max(1, int(total_steps * warmup_ratio))\n",
    "    sch = get_cosine_schedule_with_warmup(opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    return opt, sch\n",
    "\n",
    "# ---- Load pos_weight (from file, in case var was lost)\n",
    "pos_weight = torch.tensor(\n",
    "    np.load(MODELS/\"pos_weight.npy\"),\n",
    "    dtype=torch.float32, device=device\n",
    ")\n",
    "\n",
    "# ---- Seeds\n",
    "set_seed(int(CONFIG.get(\"seed\", 42)))\n",
    "\n",
    "# ---- Accumulation & AMP\n",
    "accum = int(CONFIG.get(\"grad_accum_steps\", 1))\n",
    "scaler = amp.GradScaler(enabled=True)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, batch in enumerate(loader, start=1):\n",
    "        ids = batch[\"tok\"][\"input_ids\"].to(device)\n",
    "        att = batch[\"tok\"][\"attention_mask\"].to(device)\n",
    "        gbt = batch[\"graph\"].to(device)\n",
    "        dsc = batch[\"desc\"].to(device)\n",
    "        y01 = (batch[\"y\"] == 1).float().to(device)\n",
    "        msk = batch[\"mask\"].float().to(device)\n",
    "\n",
    "        with amp.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "            logits = model(ids, att, gbt, dsc)\n",
    "            loss = bce_masked_logits_loss(logits, y01, msk, pos_weight=pos_weight)\n",
    "\n",
    "        loss_scaled = loss / accum\n",
    "        scaler.scale(loss_scaled).backward()\n",
    "\n",
    "        if step % accum == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "\n",
    "        running_loss += float(loss.item())\n",
    "\n",
    "    return running_loss / max(len(loader), 1)\n",
    "\n",
    "def log_epoch(epoch, stage, tr_loss, val_auc, val_by_label, path):\n",
    "    rec = {\n",
    "        \"epoch\": int(epoch),\n",
    "        \"stage\": stage,\n",
    "        \"train_loss\": float(tr_loss) if tr_loss is not None else None,\n",
    "        \"val_macro_auc\": float(val_auc) if val_auc is not None else None,\n",
    "        \"val_per_label\": {label_names[i]: (None if np.isnan(val_by_label[i]) else float(val_by_label[i]))\n",
    "                          for i in range(len(label_names))}\n",
    "    }\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    return rec\n",
    "\n",
    "# ===== Stage A: freeze text =====\n",
    "freeze_text(model)\n",
    "print(f\"[Stage A] Trainable params: {count_trainable_params(model)/1e6:.2f}M (text frozen)\")\n",
    "\n",
    "steps_per_epoch_A = len(train_loader) // max(accum,1)\n",
    "optA, schA = build_optim_sched(\n",
    "    model, steps_per_epoch=steps_per_epoch_A,\n",
    "    total_epochs=int(CONFIG[\"stageA_freeze_text_epochs\"]),\n",
    "    lr_text=0.0,\n",
    "    lr_others=float(CONFIG[\"lr_others\"]),\n",
    "    weight_decay=float(CONFIG[\"weight_decay\"]),\n",
    "    warmup_ratio=float(CONFIG[\"warmup_ratio\"])\n",
    ")\n",
    "\n",
    "history_path = LOGS/\"train_log.jsonl\"\n",
    "if history_path.exists(): history_path.unlink()\n",
    "\n",
    "for epoch in range(1, int(CONFIG[\"stageA_freeze_text_epochs\"])+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(model, train_loader, optA, schA)\n",
    "    val_auc, val_by_label = eval_macro_auc(model, val_loader)\n",
    "    _ = log_epoch(epoch, \"A\", tr_loss, val_auc, val_by_label, history_path)\n",
    "    print(f\"[A:{epoch:02d}] loss={tr_loss:.4f}  val_auc={val_auc:.4f}  time={time.time()-t0:.1f}s\")\n",
    "\n",
    "# ===== Stage B: unfreeze last N layers & early stop =====\n",
    "n_last = int(CONFIG.get(\"stageB_unfreeze_last_layers\", 4))\n",
    "unfreeze_last_n_text_layers(model, n_last=n_last)\n",
    "print(f\"[Stage B] Unfroze last {n_last} text layers → trainable params: {count_trainable_params(model)/1e6:.2f}M\")\n",
    "\n",
    "epochs_max = int(CONFIG.get(\"epochs_max\", 40))\n",
    "patience   = int(CONFIG.get(\"early_stop_patience\", 8))\n",
    "best_auc   = -1.0\n",
    "best_epoch = -1\n",
    "no_imp     = 0\n",
    "\n",
    "steps_per_epoch_B = len(train_loader) // max(accum,1)\n",
    "optB, schB = build_optim_sched(\n",
    "    model, steps_per_epoch=steps_per_epoch_B,\n",
    "    total_epochs=epochs_max,\n",
    "    lr_text=float(CONFIG[\"lr_text\"]),\n",
    "    lr_others=float(CONFIG[\"lr_others\"]),\n",
    "    weight_decay=float(CONFIG[\"weight_decay\"]),\n",
    "    warmup_ratio=float(CONFIG[\"warmup_ratio\"])\n",
    ")\n",
    "\n",
    "for epoch in range(1, epochs_max+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(model, train_loader, optB, schB)\n",
    "    val_auc, val_by_label = eval_macro_auc(model, val_loader)\n",
    "    _ = log_epoch(epoch, \"B\", tr_loss, val_auc, val_by_label, history_path)\n",
    "    print(f\"[B:{epoch:02d}] loss={tr_loss:.4f}  val_auc={val_auc:.4f}  time={time.time()-t0:.1f}s\")\n",
    "\n",
    "    improved = val_auc > best_auc + 1e-5\n",
    "    if improved:\n",
    "        best_auc, best_epoch, no_imp = val_auc, epoch, 0\n",
    "        torch.save({\n",
    "            \"config\": CONFIG,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"val_macro_auc\": best_auc,\n",
    "            \"epoch\": epoch,\n",
    "        }, CKPT/\"dualenc_best.pt\")\n",
    "        print(f\"  ↳ saved new BEST to {CKPT/'dualenc_best.pt'} (val_macro_auc={best_auc:.4f})\")\n",
    "    else:\n",
    "        no_imp += 1\n",
    "\n",
    "    torch.save({\n",
    "        \"config\": CONFIG,\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"val_macro_auc\": val_auc,\n",
    "        \"epoch\": epoch,\n",
    "    }, CKPT/\"dualenc_last.pt\")\n",
    "\n",
    "    if no_imp >= patience:\n",
    "        print(f\"Early stopping: no improvement for {patience} epochs (best @ epoch {best_epoch} = {best_auc:.4f})\")\n",
    "        break\n",
    "\n",
    "# ---- Save run summary\n",
    "summary = {\"best_val_macro_auc\": best_auc, \"best_epoch\": best_epoch, \"config\": CONFIG}\n",
    "(Path(MODELS)/\"run_summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "print(\"Saved summary to\", Path(MODELS)/\"run_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fe718d",
   "metadata": {},
   "source": [
    "## 8: v1 Results: Evaluate best checkpoint on TEST + save artifacts\n",
    "\n",
    "We will:\n",
    "1) Load `models/checkpoints/dualenc_best.pt`.\n",
    "2) Run TEST inference and compute:\n",
    "   - Macro ROC-AUC (primary)\n",
    "   - Macro PR-AUC\n",
    "   - Per-label ROC-AUC and PR-AUC\n",
    "3) Save everything under `tox21_dualenc_v1/results/v1/`:\n",
    "   - `metrics.json` (macro + per-label)\n",
    "   - `per_label_metrics.csv`\n",
    "   - `test_logits.npy`, `test_prob.npy`, `test_y.npy`, `test_mask.npy`, `test_indices.npy`\n",
    "   - Plots: `per_label_rocauc_bar.png`, `per_label_prauc_bar.png`\n",
    "\n",
    "This locks in **v1** so we can compare to future runs (v2/v3…).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51084ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best checkpoint from: tox21_dualenc_v1\\models\\checkpoints\\dualenc_best.pt (val_macro_auc=0.7921)\n",
      "TEST macro ROC-AUC = 0.8024 | macro PR-AUC = 0.3617\n",
      "Saved arrays and metrics to: tox21_dualenc_v1\\results\\v1\n",
      "Saved: tox21_dualenc_v1\\results\\v1\\per_label_rocauc_bar.png\n",
      "Saved: tox21_dualenc_v1\\results\\v1\\per_label_prauc_bar.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "ROOT   = Path(\"tox21_dualenc_v1\")\n",
    "MODELS = ROOT / \"models\"\n",
    "CKPT   = MODELS / \"checkpoints\"\n",
    "DATA   = ROOT / \"data\"\n",
    "RES    = ROOT / \"results\" / \"v1\"\n",
    "RES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reuse: model, device, test_loader, label_names, CONFIG\n",
    "# Load best checkpoint weights into the existing model\n",
    "best_path = CKPT / \"dualenc_best.pt\"\n",
    "state = torch.load(best_path, map_location=device)\n",
    "model.load_state_dict(state[\"state_dict\"])\n",
    "model.eval()\n",
    "print(f\"Loaded best checkpoint from: {best_path} (val_macro_auc={state.get('val_macro_auc'):.4f})\")\n",
    "\n",
    "# Collect TEST logits/probs/labels/masks/indices\n",
    "all_logits, all_y, all_m, all_idx = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        ids = batch[\"tok\"][\"input_ids\"].to(device)\n",
    "        att = batch[\"tok\"][\"attention_mask\"].to(device)\n",
    "        gbt = batch[\"graph\"].to(device)\n",
    "        dsc = batch[\"desc\"].to(device)\n",
    "        logits = model(ids, att, gbt, dsc).detach().cpu().numpy()\n",
    "        all_logits.append(logits)\n",
    "        all_y.append((batch[\"y\"]==1).float().cpu().numpy())\n",
    "        all_m.append(batch[\"mask\"].float().cpu().numpy())\n",
    "        all_idx.append(batch[\"idx\"].cpu().numpy())\n",
    "\n",
    "logits = np.concatenate(all_logits, axis=0)       # (Ntest, L)\n",
    "prob   = 1.0 / (1.0 + np.exp(-logits))            # sigmoid\n",
    "y01    = np.concatenate(all_y, axis=0).astype(np.float32)\n",
    "mask   = np.concatenate(all_m, axis=0).astype(np.float32)\n",
    "idxs   = np.concatenate(all_idx, axis=0).astype(np.int64)\n",
    "\n",
    "# Compute metrics\n",
    "L = prob.shape[1]\n",
    "roc_list, pr_list = [], []\n",
    "for l in range(L):\n",
    "    m = mask[:, l] == 1\n",
    "    y = y01[:, l][m]\n",
    "    p = prob[:, l][m]\n",
    "    if m.sum() > 0 and (y.min() < 1) and (y.max() > 0):\n",
    "        try:\n",
    "            roc = roc_auc_score(y, p)\n",
    "        except Exception:\n",
    "            roc = np.nan\n",
    "        try:\n",
    "            pr = average_precision_score(y, p)\n",
    "        except Exception:\n",
    "            pr = np.nan\n",
    "    else:\n",
    "        roc, pr = np.nan, np.nan\n",
    "    roc_list.append(roc)\n",
    "    pr_list.append(pr)\n",
    "\n",
    "macro_roc = float(np.nanmean(roc_list))\n",
    "macro_pr  = float(np.nanmean(pr_list))\n",
    "print(f\"TEST macro ROC-AUC = {macro_roc:.4f} | macro PR-AUC = {macro_pr:.4f}\")\n",
    "\n",
    "# Save arrays for reproducibility\n",
    "np.save(RES/\"test_logits.npy\", logits)\n",
    "np.save(RES/\"test_prob.npy\", prob)\n",
    "np.save(RES/\"test_y.npy\", y01)\n",
    "np.save(RES/\"test_mask.npy\", mask)\n",
    "np.save(RES/\"test_indices.npy\", idxs)\n",
    "\n",
    "# Save per-label table + metrics.json\n",
    "df = pd.DataFrame({\n",
    "    \"label\": label_names,\n",
    "    \"roc_auc\": roc_list,\n",
    "    \"pr_auc\": pr_list,\n",
    "})\n",
    "df.to_csv(RES/\"per_label_metrics.csv\", index=False)\n",
    "\n",
    "metrics = {\n",
    "    \"macro_roc_auc\": macro_roc,\n",
    "    \"macro_pr_auc\": macro_pr,\n",
    "    \"per_label\": {label_names[i]: {\"roc_auc\": (None if np.isnan(roc_list[i]) else float(roc_list[i])),\n",
    "                                   \"pr_auc\":  (None if np.isnan(pr_list[i])  else float(pr_list[i]))}\n",
    "                  for i in range(L)},\n",
    "    \"config\": CONFIG,\n",
    "    \"best_val_macro_auc\": float(state.get(\"val_macro_auc\", float(\"nan\"))),\n",
    "}\n",
    "(Path(RES/\"metrics.json\")).write_text(json.dumps(metrics, indent=2))\n",
    "print(\"Saved arrays and metrics to:\", RES)\n",
    "\n",
    "# Plots: per-label ROC-AUC and PR-AUC bars\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(df[\"label\"], df[\"roc_auc\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"ROC-AUC (TEST)\")\n",
    "plt.title(\"Per-label ROC-AUC (v1)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(RES/\"per_label_rocauc_bar.png\", dpi=200)\n",
    "print(\"Saved:\", RES/\"per_label_rocauc_bar.png\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(df[\"label\"], df[\"pr_auc\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"PR-AUC (TEST)\")\n",
    "plt.title(\"Per-label PR-AUC (v1)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(RES/\"per_label_prauc_bar.png\", dpi=200)\n",
    "print(\"Saved:\", RES/\"per_label_prauc_bar.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dfcc92",
   "metadata": {},
   "source": [
    "## 9: Quick inference on custom SMILES (v1 checkpoint)\n",
    "\n",
    "- Input: a Python list `custom_smiles` (edit in the code cell).\n",
    "- The graph is built with the same RDKit→PyG featurization as training.\n",
    "- Descriptors: use the **train-set mean** descriptor vector per molecule (good default).\n",
    "- Output:\n",
    "  - A tidy Pandas table of per-label probabilities for each SMILES.\n",
    "  - A compact \"top labels\" column using a display threshold (default 0.5).\n",
    "  - CSV saved under `tox21_dualenc_v1/results/v1/infer_*.csv`.\n",
    "\n",
    "> If any SMILES is invalid (RDKit can’t parse), it’s marked and skipped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a03ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions (probabilities):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>NR-AR</th>\n",
       "      <th>NR-AR-LBD</th>\n",
       "      <th>NR-AhR</th>\n",
       "      <th>NR-Aromatase</th>\n",
       "      <th>NR-ER</th>\n",
       "      <th>NR-ER-LBD</th>\n",
       "      <th>NR-PPAR-gamma</th>\n",
       "      <th>SR-ARE</th>\n",
       "      <th>SR-ATAD5</th>\n",
       "      <th>SR-HSE</th>\n",
       "      <th>SR-MMP</th>\n",
       "      <th>SR-p53</th>\n",
       "      <th>top(≥0.50)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOc1ccc2nc(S(N)(=O)=O)sc2c1</td>\n",
       "      <td>0.298675</td>\n",
       "      <td>0.061473</td>\n",
       "      <td>0.523068</td>\n",
       "      <td>0.211713</td>\n",
       "      <td>0.383640</td>\n",
       "      <td>0.104908</td>\n",
       "      <td>0.107308</td>\n",
       "      <td>0.338421</td>\n",
       "      <td>0.133352</td>\n",
       "      <td>0.155401</td>\n",
       "      <td>0.147968</td>\n",
       "      <td>0.257838</td>\n",
       "      <td>NR-AhR 0.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCN1C(=O)NC(c2ccccc2)C1=O</td>\n",
       "      <td>0.120189</td>\n",
       "      <td>0.016753</td>\n",
       "      <td>0.168321</td>\n",
       "      <td>0.030036</td>\n",
       "      <td>0.258730</td>\n",
       "      <td>0.060593</td>\n",
       "      <td>0.043702</td>\n",
       "      <td>0.106603</td>\n",
       "      <td>0.026243</td>\n",
       "      <td>0.109143</td>\n",
       "      <td>0.030710</td>\n",
       "      <td>0.021889</td>\n",
       "      <td>NR-ER 0.26, NR-AhR 0.17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1</td>\n",
       "      <td>0.414409</td>\n",
       "      <td>0.073200</td>\n",
       "      <td>0.487879</td>\n",
       "      <td>0.361990</td>\n",
       "      <td>0.570323</td>\n",
       "      <td>0.340692</td>\n",
       "      <td>0.501555</td>\n",
       "      <td>0.397276</td>\n",
       "      <td>0.116313</td>\n",
       "      <td>0.285753</td>\n",
       "      <td>0.485444</td>\n",
       "      <td>0.362100</td>\n",
       "      <td>NR-ER 0.57, NR-PPAR-gamma 0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCCCCCCCCOCC(O)CN</td>\n",
       "      <td>0.090146</td>\n",
       "      <td>0.005924</td>\n",
       "      <td>0.016008</td>\n",
       "      <td>0.019051</td>\n",
       "      <td>0.235007</td>\n",
       "      <td>0.035460</td>\n",
       "      <td>0.048690</td>\n",
       "      <td>0.055867</td>\n",
       "      <td>0.001675</td>\n",
       "      <td>0.060946</td>\n",
       "      <td>0.011783</td>\n",
       "      <td>0.012380</td>\n",
       "      <td>NR-ER 0.24, NR-AR 0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nc1ccc([N+](=O)[O-])cc1N</td>\n",
       "      <td>0.150438</td>\n",
       "      <td>0.022171</td>\n",
       "      <td>0.639579</td>\n",
       "      <td>0.066463</td>\n",
       "      <td>0.300498</td>\n",
       "      <td>0.054996</td>\n",
       "      <td>0.062462</td>\n",
       "      <td>0.229143</td>\n",
       "      <td>0.040020</td>\n",
       "      <td>0.128588</td>\n",
       "      <td>0.168752</td>\n",
       "      <td>0.029191</td>\n",
       "      <td>NR-AhR 0.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   SMILES     NR-AR  NR-AR-LBD    NR-AhR  \\\n",
       "0            CCOc1ccc2nc(S(N)(=O)=O)sc2c1  0.298675   0.061473  0.523068   \n",
       "1               CCN1C(=O)NC(c2ccccc2)C1=O  0.120189   0.016753  0.168321   \n",
       "2  O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1  0.414409   0.073200  0.487879   \n",
       "3                      CCCCCCCCCCOCC(O)CN  0.090146   0.005924  0.016008   \n",
       "4                Nc1ccc([N+](=O)[O-])cc1N  0.150438   0.022171  0.639579   \n",
       "\n",
       "   NR-Aromatase     NR-ER  NR-ER-LBD  NR-PPAR-gamma    SR-ARE  SR-ATAD5  \\\n",
       "0      0.211713  0.383640   0.104908       0.107308  0.338421  0.133352   \n",
       "1      0.030036  0.258730   0.060593       0.043702  0.106603  0.026243   \n",
       "2      0.361990  0.570323   0.340692       0.501555  0.397276  0.116313   \n",
       "3      0.019051  0.235007   0.035460       0.048690  0.055867  0.001675   \n",
       "4      0.066463  0.300498   0.054996       0.062462  0.229143  0.040020   \n",
       "\n",
       "     SR-HSE    SR-MMP    SR-p53                      top(≥0.50)  \n",
       "0  0.155401  0.147968  0.257838                     NR-AhR 0.52  \n",
       "1  0.109143  0.030710  0.021889         NR-ER 0.26, NR-AhR 0.17  \n",
       "2  0.285753  0.485444  0.362100  NR-ER 0.57, NR-PPAR-gamma 0.50  \n",
       "3  0.060946  0.011783  0.012380          NR-ER 0.24, NR-AR 0.09  \n",
       "4  0.128588  0.168752  0.029191                     NR-AhR 0.64  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tox21_dualenc_v1\\results\\v1\\infer_20250901_190058.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, torch, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data, Batch as GeometricBatch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "ROOT   = Path(\"tox21_dualenc_v1\")\n",
    "DATA   = ROOT / \"data\"\n",
    "DESC   = DATA / \"descriptors\"\n",
    "SPLITS = DATA / \"splits\"\n",
    "MODELS = ROOT / \"models\"\n",
    "CKPT   = MODELS / \"checkpoints\"\n",
    "RES    = ROOT / \"results\" / \"v1\"\n",
    "RES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = json.loads((MODELS/\"config_dualenc_v1.json\").read_text())\n",
    "label_names = (DATA/\"label_names.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load descriptor mean (train-set)\n",
    "X_desc  = np.load(DESC/\"desc_selected.npy\").astype(np.float32)\n",
    "splits  = json.loads((SPLITS/\"splits.json\").read_text())\n",
    "train_idx = np.array(splits[\"train\"], dtype=int)\n",
    "desc_mean = X_desc[train_idx].mean(axis=0).astype(np.float32)  # (256,)\n",
    "\n",
    "# --- Featurizers (same as training)\n",
    "ATOM_LIST = [\"C\",\"N\",\"O\",\"S\",\"F\",\"Cl\",\"Br\",\"I\",\"P\",\"B\"]\n",
    "from rdkit import Chem\n",
    "HYB_LIST = [\n",
    "    Chem.rdchem.HybridizationType.SP,\n",
    "    Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3,\n",
    "    Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2,\n",
    "]\n",
    "def one_hot_with_other(key, choices):\n",
    "    vec = [0]* (len(choices)+1)\n",
    "    try: idx = choices.index(key)\n",
    "    except ValueError: idx = None\n",
    "    if idx is None: vec[-1] = 1\n",
    "    else: vec[idx] = 1\n",
    "    return vec\n",
    "def one_hot_index(idx, size):\n",
    "    v = [0]*size\n",
    "    if 0 <= idx < size: v[idx] = 1\n",
    "    return v\n",
    "def atom_features(atom: Chem.Atom):\n",
    "    f_type   = one_hot_with_other(atom.GetSymbol(), ATOM_LIST)            # 11\n",
    "    f_deg    = one_hot_index(min(atom.GetTotalDegree(),6), 7)             # 7\n",
    "    charge_map = {-2:0,-1:1,0:2,1:3,2:4}\n",
    "    f_charge = one_hot_index({**charge_map}.get(atom.GetFormalCharge(),5), 6) # 6\n",
    "    hyb = atom.GetHybridization()\n",
    "    f_hyb   = one_hot_with_other(hyb if hyb in HYB_LIST else \"other\", HYB_LIST) # 6\n",
    "    f_flags = [int(atom.GetIsAromatic()), int(atom.IsInRing())]            # 2\n",
    "    return f_type + f_deg + f_charge + f_hyb + f_flags                     # 32\n",
    "def bond_features(bond: Chem.Bond):\n",
    "    types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "             Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    f_type  = one_hot_with_other(bond.GetBondType(), types)                # 5\n",
    "    f_flags = [int(bond.GetIsConjugated()), int(bond.IsInRing())]          # 2\n",
    "    return f_type + f_flags                                                # 7\n",
    "\n",
    "NODE_DIM = 32\n",
    "EDGE_DIM = 7\n",
    "\n",
    "def build_graph_from_smiles(s):\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None or mol.GetNumAtoms()==0:\n",
    "        return None\n",
    "    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float32)\n",
    "    ei_src, ei_dst, eattr = [], [], []\n",
    "    for b in mol.GetBonds():\n",
    "        u, v = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bf = bond_features(b)\n",
    "        ei_src.extend([u,v]); ei_dst.extend([v,u]); eattr.extend([bf,bf])\n",
    "    if len(ei_src)==0:\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_attr  = torch.empty((0, EDGE_DIM), dtype=torch.float32)\n",
    "    else:\n",
    "        edge_index = torch.tensor([ei_src, ei_dst], dtype=torch.long)\n",
    "        edge_attr  = torch.tensor(eattr, dtype=torch.float32)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, invalid=0)\n",
    "\n",
    "# --- Load model checkpoint if needed\n",
    "# Reuse 'model' from earlier if it exists; otherwise instantiate and load.\n",
    "try:\n",
    "    model\n",
    "except NameError:\n",
    "    # Recreate model class inline if needed (imports came from earlier cells)\n",
    "    from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "    from torch_geometric.utils import to_dense_batch\n",
    "    import torch.nn as nn, torch.nn.functional as F\n",
    "    class AttentionPoolAMP(nn.Module):\n",
    "        def __init__(self, dim):\n",
    "            super().__init__()\n",
    "            self.score = nn.Linear(dim, 1)\n",
    "        def forward(self, X, mask=None):\n",
    "            s = self.score(X).squeeze(-1).float()\n",
    "            if mask is not None:\n",
    "                s = s.masked_fill(~mask.bool(), -1e9)\n",
    "            a = torch.softmax(s, dim=1)\n",
    "            a = a.to(X.dtype)\n",
    "            return torch.bmm(a.unsqueeze(1), X).squeeze(1)\n",
    "    class CrossAttnBlock(nn.Module):\n",
    "        def __init__(self, dim=256, heads=4, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "            self.norm1 = nn.LayerNorm(dim)\n",
    "            self.ffn = nn.Sequential(nn.Linear(dim, 4*dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(4*dim, dim))\n",
    "            self.norm2 = nn.LayerNorm(dim)\n",
    "            self.drop = nn.Dropout(dropout)\n",
    "        def forward(self, Q_text, K_graph, V_graph, key_padding_mask=None):\n",
    "            attn_out, _ = self.attn(Q_text, K_graph, V_graph, key_padding_mask=key_padding_mask)\n",
    "            x = self.norm1(Q_text + self.drop(attn_out))\n",
    "            ff = self.ffn(x)\n",
    "            y = self.norm2(x + self.drop(ff))\n",
    "            return y\n",
    "    class GINEncoder(nn.Module):\n",
    "        def __init__(self, node_dim, edge_dim, hidden=256, layers=4, dropout=0.2):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList(); self.bns = nn.ModuleList()\n",
    "            in_dim = node_dim\n",
    "            for _ in range(layers):\n",
    "                mlp = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden))\n",
    "                conv = GINEConv(mlp, edge_dim=edge_dim)\n",
    "                self.layers.append(conv); self.bns.append(nn.BatchNorm1d(hidden)); in_dim = hidden\n",
    "            self.dropout = nn.Dropout(dropout); self.hidden = hidden\n",
    "        def forward(self, x, edge_index, edge_attr, batch_index):\n",
    "            h = x\n",
    "            for conv, bn in zip(self.layers, self.bns):\n",
    "                h = conv(h, edge_index, edge_attr); h = bn(h); h = F.relu(h); h = self.dropout(h)\n",
    "            z_graph = global_mean_pool(h, batch_index)\n",
    "            return h, z_graph\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    class DualEncoderXAttn(nn.Module):\n",
    "        def __init__(self, config):\n",
    "            super().__init__()\n",
    "            self.cfg = config\n",
    "            self.tok   = AutoTokenizer.from_pretrained(config[\"text_encoder\"])\n",
    "            self.text  = AutoModel.from_pretrained(config[\"text_encoder\"])\n",
    "            self.text_proj = nn.Linear(self.text.config.hidden_size, config[\"text_proj_dim\"])\n",
    "            self.gnn   = GINEncoder(node_dim=int(config[\"node_dim\"]), edge_dim=int(config[\"edge_dim\"]),\n",
    "                                    hidden=int(config[\"graph_hidden\"]), layers=int(config[\"graph_layers\"]),\n",
    "                                    dropout=float(config[\"dropout\"]))\n",
    "            self.xattn = CrossAttnBlock(dim=int(config[\"fusion_dim\"]), heads=int(config[\"fusion_heads\"]),\n",
    "                                        dropout=float(config[\"fusion_dropout\"]))\n",
    "            self.tpool = AttentionPoolAMP(dim=int(config[\"fusion_dim\"]))\n",
    "            self.dmlp  = nn.Sequential(nn.Linear(int(config[\"desc_dim_in\"]), int(config[\"desc_hidden\"])),\n",
    "                                       nn.ReLU(), nn.Dropout(float(config[\"dropout\"])))\n",
    "            fused_in = int(config[\"fusion_dim\"]) + int(config[\"graph_hidden\"]) + int(config[\"desc_hidden\"])\n",
    "            self.head = nn.Sequential(nn.Linear(fused_in, int(config[\"head_hidden\"])),\n",
    "                                      nn.ReLU(), nn.Dropout(float(config[\"dropout\"])),\n",
    "                                      nn.Linear(int(config[\"head_hidden\"]), int(config[\"num_labels\"])))\n",
    "        def forward(self, input_ids, attention_mask, graph_batch, desc):\n",
    "            t_out  = self.text(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "            t_proj = self.text_proj(t_out)\n",
    "            g = graph_batch\n",
    "            from torch_geometric.utils import to_dense_batch\n",
    "            node_h, z_graph = self.gnn(g.x, g.edge_index, g.edge_attr, g.batch)\n",
    "            node_dense, node_mask = to_dense_batch(node_h, g.batch)  # (B,Nmax,256),(B,Nmax)\n",
    "            fused_text = self.xattn(t_proj, node_dense, node_dense, key_padding_mask=~node_mask)\n",
    "            z_text = self.tpool(fused_text, mask=attention_mask.bool())\n",
    "            z_desc = self.dmlp(desc)\n",
    "            z = torch.cat([z_text, z_graph, z_desc], dim=-1)\n",
    "            return self.head(z)\n",
    "    model = DualEncoderXAttn(CONFIG).to(device)\n",
    "    state = torch.load(CKPT/\"dualenc_best.pt\", map_location=device)\n",
    "    model.load_state_dict(state[\"state_dict\"])\n",
    "    model.eval()\n",
    "    print(\"Model reloaded from checkpoint.\")\n",
    "\n",
    "tok = model.tok  # tokenizer already attached\n",
    "\n",
    "# --- EDIT THIS LIST with your SMILES ---\n",
    "custom_smiles = [\n",
    "    \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",                 #1st \n",
    "    \"CCN1C(=O)NC(c2ccccc2)C1=O\",                    #2nd\n",
    "    \"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\",       #9th\n",
    "    \"CCCCCCCCCCOCC(O)CN\",                           #18th\n",
    "    \"Nc1ccc([N+](=O)[O-])cc1N\"                      #96th\n",
    "]\n",
    "\n",
    "# --- Build graphs and descriptors for the custom list\n",
    "graphs_list, ok_smiles = [], []\n",
    "for s in custom_smiles:\n",
    "    g = build_graph_from_smiles(s)\n",
    "    if g is None:\n",
    "        print(f\"⚠️ Invalid SMILES skipped: {s}\")\n",
    "        continue\n",
    "    graphs_list.append(g)\n",
    "    ok_smiles.append(s)\n",
    "\n",
    "if not graphs_list:\n",
    "    raise ValueError(\"No valid SMILES provided.\")\n",
    "\n",
    "# Batch graphs\n",
    "g_batch = GeometricBatch.from_data_list(graphs_list).to(device)\n",
    "\n",
    "# Tokenize\n",
    "toks = tok(ok_smiles, padding=True, truncation=True, max_length=int(CONFIG[\"max_length\"]), return_tensors=\"pt\")\n",
    "ids  = toks[\"input_ids\"].to(device)\n",
    "att  = toks[\"attention_mask\"].to(device)\n",
    "\n",
    "# Descriptors: use train mean replicated\n",
    "desc = torch.tensor(np.tile(desc_mean, (len(ok_smiles), 1)), dtype=torch.float32, device=device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    logits = model(ids, att, g_batch, desc).detach().cpu().numpy()\n",
    "prob = 1.0 / (1.0 + np.exp(-logits))  # sigmoid\n",
    "\n",
    "# Build result table\n",
    "df = pd.DataFrame(prob, columns=label_names, index=ok_smiles)\n",
    "display_threshold = 0.50\n",
    "top_labels = []\n",
    "for row in prob:\n",
    "    idx = np.where(row >= display_threshold)[0]\n",
    "    if idx.size == 0:\n",
    "        # show top-2 anyway\n",
    "        top2 = np.argsort(-row)[:2]\n",
    "        top_labels.append(\", \".join([f\"{label_names[i]} {row[i]:.2f}\" for i in top2]))\n",
    "    else:\n",
    "        top_labels.append(\", \".join([f\"{label_names[i]} {row[i]:.2f}\" for i in idx]))\n",
    "df_out = df.copy()\n",
    "df_out.insert(0, \"SMILES\", df_out.index)\n",
    "df_out[\"top(≥{:.2f})\".format(display_threshold)] = top_labels\n",
    "df_out = df_out.reset_index(drop=True)\n",
    "\n",
    "print(\"Predictions (probabilities):\")\n",
    "display(df_out)\n",
    "\n",
    "# Save CSV\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_csv = RES / f\"infer_{timestamp}.csv\"\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0762a",
   "metadata": {},
   "source": [
    "# V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ebc58c",
   "metadata": {},
   "source": [
    "## 1: v2 setup: reset folders, refresh config, sanity-check data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111ebb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archived: tox21_dualenc_v1\\models\\checkpoints_v2 -> tox21_dualenc_v1\\archive\\v2_20250901_200305\\checkpoints_v2\n",
      "Archived: tox21_dualenc_v1\\logs\\v2 -> tox21_dualenc_v1\\archive\\v2_20250901_200305\\v2\n",
      "Archived: tox21_dualenc_v1\\results\\v2 -> tox21_dualenc_v1\\archive\\v2_20250901_200305\\v2\n",
      "Saved refreshed v2 config to: tox21_dualenc_v1\\models\\config_dualenc_v2.json\n",
      "\n",
      "=== v2 Setup Summary ===\n",
      "Workspace: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\n",
      "CUDA available: True | device count: 1\n",
      "GPU 0: NVIDIA GeForce RTX 4070 Ti\n",
      "N=7831, L=12 labels\n",
      "Splits train/val/test = 6265 783 783\n",
      "y shape: (7831, 12) mask shape: (7831, 12) descriptors: (7831, 256)\n",
      "graphs (safe) count: 7831\n",
      "\n",
      "Fresh v2 folders ready:\n",
      " - tox21_dualenc_v1\\models\\checkpoints_v2\n",
      " - tox21_dualenc_v1\\logs\\v2\n",
      " - tox21_dualenc_v1\\results\\v2\n"
     ]
    }
   ],
   "source": [
    "import os, json, shutil, time, numpy as np, torch\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "DATA = ROOT / \"data\"\n",
    "DESC = DATA / \"descriptors\"\n",
    "SPL  = DATA / \"splits\"\n",
    "GRPH = DATA / \"graphs\"\n",
    "MODELS = ROOT / \"models\"\n",
    "LOGS   = ROOT / \"logs\"\n",
    "RES    = ROOT / \"results\"\n",
    "\n",
    "# v2 folders (fresh)\n",
    "CKPT_V2 = MODELS / \"checkpoints_v2\"\n",
    "LOGS_V2 = LOGS / \"v2\"\n",
    "RES_V2  = RES / \"v2\"\n",
    "\n",
    "# --- 1) Archive existing v2 artifacts (if any) ---\n",
    "archive_root = ROOT / \"archive\"\n",
    "archive_root.mkdir(parents=True, exist_ok=True)\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "arch_dir = archive_root / f\"v2_{ts}\"\n",
    "\n",
    "def archive_if_exists(p: Path):\n",
    "    if p.exists():\n",
    "        arch_dir.mkdir(parents=True, exist_ok=True)\n",
    "        dest = arch_dir / p.name\n",
    "        if dest.exists():\n",
    "            shutil.rmtree(dest)\n",
    "        shutil.move(str(p), sstr(dest))\n",
    "        print(f\"Archived: {p} -> {dest}\")\n",
    "\n",
    "for p in [CKPT_V2, LOGS_V2, RES_V2]:\n",
    "    archive_if_exists(p)\n",
    "\n",
    "# Recreate clean folders\n",
    "for p in [CKPT_V2, LOGS_V2, RES_V2]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- 2) Build refreshed v2 config ---\n",
    "# Start from v1 config if present, else assemble minimal dict\n",
    "cfg_v1_path = MODELS / \"config_dualenc_v1.json\"\n",
    "if cfg_v1_path.exists():\n",
    "    base = json.loads(cfg_v1_path.read_text())\n",
    "else:\n",
    "    # sensible defaults if v1 config is missing for any reason\n",
    "    base = dict(\n",
    "        text_encoder=\"DeepChem/ChemBERTa-100M-MLM\",\n",
    "        num_labels=12, max_length=256,\n",
    "        node_dim=32, edge_dim=7,\n",
    "        graph_hidden=256, graph_layers=4,\n",
    "        fusion_dim=256, fusion_heads=4, fusion_dropout=0.1,\n",
    "        text_proj_dim=256,\n",
    "        desc_dim_in=256, desc_hidden=256,\n",
    "        head_hidden=512,\n",
    "        dropout=0.3,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "cfg_v2 = base.copy()\n",
    "cfg_v2.update({\n",
    "    # training knobs\n",
    "    \"batch_size\": 24,\n",
    "    \"grad_accum_steps\": 2,          # effective ≈ 48\n",
    "    \"stageA_freeze_text_epochs\": 10,\n",
    "    \"stageB_unfreeze_last_layers\": 2,\n",
    "    \"epochs_max\": 100,\n",
    "    \"early_stop_patience\": 20,\n",
    "    \"lr_text\": 1e-5,\n",
    "    \"lr_others\": 1e-3,\n",
    "    \"warmup_ratio\": 0.20,\n",
    "    \"dropout\": 0.30,\n",
    "    \"weight_decay\": 2e-4,\n",
    "    # layer-wise lr decay multipliers for the two unfrozen text layers (penultimate, last)\n",
    "    \"text_llrd\": [0.7, 1.0],\n",
    "    # bookkeeping\n",
    "    \"checkpoint_dir\": str(CKPT_V2),\n",
    "    \"log_path\": str(LOGS_V2 / \"train_log.jsonl\"),\n",
    "    \"results_dir\": str(RES_V2),\n",
    "})\n",
    "\n",
    "cfg_v2_path = MODELS / \"config_dualenc_v2.json\"\n",
    "cfg_v2_path.write_text(json.dumps(cfg_v2, indent=2))\n",
    "print(\"Saved refreshed v2 config to:\", cfg_v2_path)\n",
    "\n",
    "# --- 3) Sanity-check data assets ---\n",
    "req_files = [\n",
    "    DATA/\"X_smiles.txt\",\n",
    "    DATA/\"y.npy\",\n",
    "    DATA/\"y_mask.npy\",\n",
    "    DESC/\"desc_selected.npy\",\n",
    "    SPL/\"splits.json\",\n",
    "    DATA/\"label_names.txt\",\n",
    "    GRPH/\"graphs_all_pyg_safe.pt\",\n",
    "]\n",
    "missing = [str(p) for p in req_files if not p.exists()]\n",
    "\n",
    "if missing:\n",
    "    print(\"⚠️ Missing files:\\n - \" + \"\\n - \".join(missing))\n",
    "else:\n",
    "    # load and print shapes\n",
    "    smiles = (DATA/\"X_smiles.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "    y      = np.load(DATA/\"y.npy\").astype(np.float32)\n",
    "    y_mask = np.load(DATA/\"y_mask.npy\").astype(np.float32)\n",
    "    X_desc = np.load(DESC/\"desc_selected.npy\").astype(np.float32)\n",
    "    splits = json.loads((SPL/\"splits.json\").read_text())\n",
    "    train_idx = np.array(splits[\"train\"], dtype=int)\n",
    "    val_idx   = np.array(splits[\"val\"], dtype=int)\n",
    "    test_idx  = np.array(splits[\"test\"], dtype=int)\n",
    "\n",
    "    # safe load graphs\n",
    "    safe_graphs = torch.load(GRPH/\"graphs_all_pyg_safe.pt\")\n",
    "    N_graphs = len(safe_graphs)\n",
    "\n",
    "    print(\"\\n=== v2 Setup Summary ===\")\n",
    "    print(\"Workspace:\", ROOT.resolve())\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()} | device count: {torch.cuda.device_count()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU 0:\", torch.cuda.get_device_name(0))\n",
    "    print(f\"N={len(smiles)}, L={y.shape[1]} labels\")\n",
    "    print(\"Splits train/val/test =\", len(train_idx), len(val_idx), len(test_idx))\n",
    "    print(\"y shape:\", y.shape, \"mask shape:\", y_mask.shape, \"descriptors:\", X_desc.shape)\n",
    "    print(\"graphs (safe) count:\", N_graphs)\n",
    "\n",
    "# Inventory of (fresh) v2 dirs\n",
    "print(\"\\nFresh v2 folders ready:\")\n",
    "print(\" -\", CKPT_V2)\n",
    "print(\" -\", LOGS_V2)\n",
    "print(\" -\", RES_V2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46db1aa",
   "metadata": {},
   "source": [
    "## 2: Instantiate model_v2 (grad checkpointing) + rebuild loaders + dry-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b57cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2 model params: total=94.06M, trainable=94.06M\n",
      "Text hidden → proj: 768 → 256\n",
      "Dropout: 0.3 | Fusion heads: 4\n",
      "v2 Batches: train/val/test = 262 33 33\n",
      "Dry-run logits shape: (24, 12) | expected: (24, 12)\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, torch, torch.nn as nn, torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.data import Data, Batch as GeometricBatch\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "# ---------- config & device ----------\n",
    "ROOT   = Path(\"tox21_dualenc_v1\")\n",
    "DATA   = ROOT / \"data\"\n",
    "DESC   = DATA / \"descriptors\"\n",
    "SPLITS = DATA / \"splits\"\n",
    "GRAPHS = DATA / \"graphs\"\n",
    "MODELS = ROOT / \"models\"\n",
    "\n",
    "CONFIG_V2 = json.loads((MODELS/\"config_dualenc_v2.json\").read_text())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "try:\n",
    "    torch.set_float32_matmul_precision(\"high\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---------- model definition ----------\n",
    "class AttentionPoolAMP(nn.Module):\n",
    "    def __init__(self, dim): \n",
    "        super().__init__(); self.score = nn.Linear(dim, 1)\n",
    "    def forward(self, X, mask=None):\n",
    "        s = self.score(X).squeeze(-1).float()           # softmax in fp32 for stability\n",
    "        if mask is not None: s = s.masked_fill(~mask.bool(), -1e9)\n",
    "        a = torch.softmax(s, dim=1).to(X.dtype)         # cast back to amp dtype\n",
    "        return torch.bmm(a.unsqueeze(1), X).squeeze(1)\n",
    "\n",
    "class CrossAttnBlock(nn.Module):\n",
    "    def __init__(self, dim=256, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn  = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ffn   = nn.Sequential(nn.Linear(dim, 4*dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(4*dim, dim))\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "    def forward(self, Q_text, K_graph, V_graph, key_padding_mask=None):\n",
    "        attn_out, _ = self.attn(Q_text, K_graph, V_graph, key_padding_mask=key_padding_mask)\n",
    "        x  = self.norm1(Q_text + self.drop(attn_out))\n",
    "        ff = self.ffn(x)\n",
    "        y  = self.norm2(x + self.drop(ff))\n",
    "        return y\n",
    "\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden=256, layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(); self.bns = nn.ModuleList()\n",
    "        in_dim = node_dim\n",
    "        for _ in range(layers):\n",
    "            mlp  = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden))\n",
    "            conv = GINEConv(mlp, edge_dim=edge_dim)\n",
    "            self.layers.append(conv); self.bns.append(nn.BatchNorm1d(hidden))\n",
    "            in_dim = hidden\n",
    "        self.dropout = nn.Dropout(dropout); self.hidden = hidden\n",
    "    def forward(self, x, edge_index, edge_attr, batch_index):\n",
    "        h = x\n",
    "        for conv, bn in zip(self.layers, self.bns):\n",
    "            h = conv(h, edge_index, edge_attr); h = bn(h); h = F.relu(h); h = self.dropout(h)\n",
    "        z_graph = global_mean_pool(h, batch_index)\n",
    "        return h, z_graph\n",
    "\n",
    "class DualEncoderXAttn(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        # Text encoder (+ gradient checkpointing)\n",
    "        self.tok  = AutoTokenizer.from_pretrained(cfg[\"text_encoder\"])\n",
    "        self.text = AutoModel.from_pretrained(cfg[\"text_encoder\"])\n",
    "        try:\n",
    "            self.text.gradient_checkpointing_enable()\n",
    "        except Exception:\n",
    "            pass\n",
    "        if hasattr(self.text, \"config\") and hasattr(self.text.config, \"use_cache\"):\n",
    "            self.text.config.use_cache = False\n",
    "        self.text_proj = nn.Linear(self.text.config.hidden_size, cfg[\"text_proj_dim\"])\n",
    "        # Graph encoder\n",
    "        self.gnn = GINEncoder(\n",
    "            node_dim=int(cfg[\"node_dim\"]), edge_dim=int(cfg[\"edge_dim\"]),\n",
    "            hidden=int(cfg[\"graph_hidden\"]), layers=int(cfg[\"graph_layers\"]),\n",
    "            dropout=float(cfg[\"dropout\"])\n",
    "        )\n",
    "        # Cross-attn + pooling + descriptor MLP\n",
    "        self.xattn = CrossAttnBlock(\n",
    "            dim=int(cfg[\"fusion_dim\"]), heads=int(cfg[\"fusion_heads\"]),\n",
    "            dropout=float(cfg[\"fusion_dropout\"])\n",
    "        )\n",
    "        self.tpool = AttentionPoolAMP(int(cfg[\"fusion_dim\"]))\n",
    "        self.dmlp  = nn.Sequential(\n",
    "            nn.Linear(int(cfg[\"desc_dim_in\"]), int(cfg[\"desc_hidden\"])),\n",
    "            nn.ReLU(), nn.Dropout(float(cfg[\"dropout\"]))\n",
    "        )\n",
    "        # Head\n",
    "        fused_in = int(cfg[\"fusion_dim\"]) + int(cfg[\"graph_hidden\"]) + int(cfg[\"desc_hidden\"])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_in, int(cfg[\"head_hidden\"])),\n",
    "            nn.ReLU(), nn.Dropout(float(cfg[\"dropout\"])),\n",
    "            nn.Linear(int(cfg[\"head_hidden\"]), int(cfg[\"num_labels\"]))\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask, graph_batch, desc):\n",
    "        # Text\n",
    "        t_out  = self.text(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        t_proj = self.text_proj(t_out)                                   # (B,T,256)\n",
    "        # Graph\n",
    "        node_h, z_graph = self.gnn(graph_batch.x, graph_batch.edge_index,\n",
    "                                   graph_batch.edge_attr, graph_batch.batch)  # (ΣN,256),(B,256)\n",
    "        node_dense, node_mask = to_dense_batch(node_h, graph_batch.batch)     # (B,Nmax,256),(B,Nmax)\n",
    "        # Cross-attn & fuse\n",
    "        fused_text = self.xattn(t_proj, node_dense, node_dense, key_padding_mask=~node_mask)\n",
    "        z_text = self.tpool(fused_text, mask=attention_mask.bool())      # (B,256)\n",
    "        z_desc = self.dmlp(desc)                                         # (B,256)\n",
    "        z = torch.cat([z_text, z_graph, z_desc], dim=-1)                 # (B,768)\n",
    "        return self.head(z)                                              # (B,L)\n",
    "\n",
    "# Instantiate model_v2\n",
    "model_v2 = DualEncoderXAttn(CONFIG_V2).to(device)\n",
    "total = sum(p.numel() for p in model_v2.parameters())\n",
    "trainable = sum(p.numel() for p in model_v2.parameters() if p.requires_grad)\n",
    "print(f\"v2 model params: total={total/1e6:.2f}M, trainable={trainable/1e6:.2f}M\")\n",
    "print(\"Text hidden → proj:\", model_v2.text.config.hidden_size, \"→\", CONFIG_V2[\"text_proj_dim\"])\n",
    "print(\"Dropout:\", CONFIG_V2[\"dropout\"], \"| Fusion heads:\", CONFIG_V2[\"fusion_heads\"])\n",
    "\n",
    "# ---------- v2 dataset & loaders ----------\n",
    "# Load assets\n",
    "smiles = (DATA/\"X_smiles.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "y      = np.load(DATA/\"y.npy\").astype(np.float32)\n",
    "y_mask = np.load(DATA/\"y_mask.npy\").astype(np.float32)\n",
    "X_desc = np.load(DESC/\"desc_selected.npy\").astype(np.float32)\n",
    "splits = json.loads((SPLITS/\"splits.json\").read_text())\n",
    "train_idx = np.array(splits[\"train\"], dtype=int)\n",
    "val_idx   = np.array(splits[\"val\"], dtype=int)\n",
    "test_idx  = np.array(splits[\"test\"], dtype=int)\n",
    "\n",
    "safe_graphs = torch.load(GRAPHS/\"graphs_all_pyg_safe.pt\")\n",
    "def dict_to_data(d):\n",
    "    return Data(x=d[\"x\"], edge_index=d[\"edge_index\"], edge_attr=d[\"edge_attr\"],\n",
    "                idx=int(d.get(\"idx\",-1)), invalid=int(d.get(\"invalid\",0)))\n",
    "graphs = [dict_to_data(d) for d in safe_graphs]\n",
    "\n",
    "tok = model_v2.tok\n",
    "MAX_LEN = int(CONFIG_V2.get(\"max_length\", 256))\n",
    "BS      = int(CONFIG_V2.get(\"batch_size\", 24))\n",
    "\n",
    "class DualEncDataset(Dataset):\n",
    "    def __init__(self, indices, smiles, y, y_mask, X_desc, graphs):\n",
    "        self.idx = np.asarray(indices, dtype=int)\n",
    "        self.smiles, self.y, self.y_mask, self.X_desc, self.graphs = smiles, y, y_mask, X_desc, graphs\n",
    "    def __len__(self): return len(self.idx)\n",
    "    def __getitem__(self, i):\n",
    "        j = int(self.idx[i])\n",
    "        return {\"graph\": self.graphs[j], \"smiles\": self.smiles[j], \"desc\": self.X_desc[j],\n",
    "                \"y\": self.y[j], \"mask\": self.y_mask[j], \"idx\": j}\n",
    "\n",
    "def collate_fn(batch):\n",
    "    g_batch = GeometricBatch.from_data_list([b[\"graph\"] for b in batch])\n",
    "    toks = tok([b[\"smiles\"] for b in batch], padding=True, truncation=True,\n",
    "               max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    desc = torch.tensor(np.stack([b[\"desc\"] for b in batch]), dtype=torch.float32)\n",
    "    yb   = torch.tensor(np.stack([b[\"y\"]    for b in batch]), dtype=torch.float32)\n",
    "    mb   = torch.tensor(np.stack([b[\"mask\"] for b in batch]), dtype=torch.float32)\n",
    "    idxs = torch.tensor([b[\"idx\"] for b in batch], dtype=torch.long)\n",
    "    return {\"graph\": g_batch, \"tok\": toks, \"desc\": desc, \"y\": yb, \"mask\": mb, \"idx\": idxs}\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "train_loader_v2 = DataLoader(DualEncDataset(train_idx, smiles, y, y_mask, X_desc, graphs),\n",
    "                             batch_size=BS, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader_v2   = DataLoader(DualEncDataset(val_idx, smiles, y, y_mask, X_desc, graphs),\n",
    "                             batch_size=BS, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "test_loader_v2  = DataLoader(DualEncDataset(test_idx, smiles, y, y_mask, X_desc, graphs),\n",
    "                             batch_size=BS, shuffle=False, num_workers=0, collate_fn=collate_fn)\n",
    "\n",
    "print(\"v2 Batches: train/val/test =\", len(train_loader_v2), len(val_loader_v2), len(test_loader_v2))\n",
    "\n",
    "# ---------- dry-run ----------\n",
    "b = next(iter(train_loader_v2))\n",
    "ids = b[\"tok\"][\"input_ids\"].to(device)\n",
    "att = b[\"tok\"][\"attention_mask\"].to(device)\n",
    "gbt = b[\"graph\"].to(device)\n",
    "dsc = b[\"desc\"].to(device)\n",
    "with torch.no_grad():\n",
    "    out = model_v2(ids, att, gbt, dsc)\n",
    "print(\"Dry-run logits shape:\", tuple(out.shape), \"| expected:\", (ids.shape[0], CONFIG_V2[\"num_labels\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494597e5",
   "metadata": {},
   "source": [
    "## 3: Train v2 (refresh): Stage A (freeze text) → Stage B (unfreeze last 2, LLRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e767b132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pos_weight_v2.npy. First 6: [21.28352483 24.91904751  7.30331753 18.9957446   6.51497005 17.48344365]\n",
      "[v2 Stage A] Trainable params: 1.93M (text frozen)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_15324\\2522521797.py:180: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:01] loss=1.1615  val_auc=0.7572  time=7.1s\n",
      "  ↳ saved new BEST to tox21_dualenc_v1\\models\\checkpoints_v2\\dualenc_best_v2.pt (val_macro_auc=0.7572)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:02] loss=1.0031  val_auc=0.7517  time=6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:03] loss=0.9213  val_auc=0.7957  time=6.7s\n",
      "  ↳ saved new BEST to tox21_dualenc_v1\\models\\checkpoints_v2\\dualenc_best_v2.pt (val_macro_auc=0.7957)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:04] loss=0.8164  val_auc=0.7849  time=6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:05] loss=0.7223  val_auc=0.7917  time=6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:06] loss=0.6487  val_auc=0.7775  time=7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:07] loss=0.5909  val_auc=0.7717  time=7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:08] loss=0.5455  val_auc=0.7651  time=7.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:09] loss=0.5187  val_auc=0.7672  time=6.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 A:10] loss=0.4846  val_auc=0.7655  time=6.7s\n",
      "[v2 Stage B] Unfrozen text layers: [10, 11] → Trainable params: 16.11M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:01] loss=0.4836  val_auc=0.7689  time=6.7s\n",
      "  ↳ saved new BEST to tox21_dualenc_v1\\models\\checkpoints_v2\\dualenc_best_v2.pt (val_macro_auc=0.7689)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:02] loss=0.4806  val_auc=0.7672  time=6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:03] loss=0.4826  val_auc=0.7603  time=6.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:04] loss=0.4822  val_auc=0.7730  time=6.5s\n",
      "  ↳ saved new BEST to tox21_dualenc_v1\\models\\checkpoints_v2\\dualenc_best_v2.pt (val_macro_auc=0.7730)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:05] loss=0.4796  val_auc=0.7653  time=6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:06] loss=0.4741  val_auc=0.7552  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:07] loss=0.4579  val_auc=0.7568  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:08] loss=0.4449  val_auc=0.7565  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:09] loss=0.4391  val_auc=0.7594  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:10] loss=0.4279  val_auc=0.7533  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:11] loss=0.4162  val_auc=0.7625  time=6.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:12] loss=0.4109  val_auc=0.7579  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:13] loss=0.3940  val_auc=0.7578  time=6.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:14] loss=0.3708  val_auc=0.7571  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:15] loss=0.3351  val_auc=0.7487  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:16] loss=0.3331  val_auc=0.7506  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:17] loss=0.3189  val_auc=0.7520  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:18] loss=0.3234  val_auc=0.7488  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:19] loss=0.3211  val_auc=0.7485  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:20] loss=0.3164  val_auc=0.7348  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:21] loss=0.3063  val_auc=0.7632  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:22] loss=0.3013  val_auc=0.7420  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:23] loss=0.3211  val_auc=0.7439  time=6.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[v2 B:24] loss=0.3716  val_auc=0.7435  time=6.5s\n",
      "Early stopping: no improvement for 20 epochs (best @ epoch 4 = 0.7730)\n",
      "Saved summary to tox21_dualenc_v1\\models\\run_summary_v2.json\n"
     ]
    }
   ],
   "source": [
    "import json, time, numpy as np, torch, torch.nn as nn\n",
    "from pathlib import Path\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from contextlib import nullcontext\n",
    "\n",
    "ROOT     = Path(\"tox21_dualenc_v1\")\n",
    "MODELS   = ROOT / \"models\"\n",
    "CKPT_V2  = MODELS / \"checkpoints_v2\"\n",
    "LOGS_V2  = ROOT / \"logs\" / \"v2\"\n",
    "RES_V2   = ROOT / \"results\" / \"v2\"\n",
    "CKPT_V2.mkdir(parents=True, exist_ok=True)\n",
    "LOGS_V2.mkdir(parents=True, exist_ok=True)\n",
    "RES_V2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG_V2 = json.loads((MODELS/\"config_dualenc_v2.json\").read_text())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reuse: model_v2, train_loader_v2, val_loader_v2 from Cell 2\n",
    "assert 'model_v2' in globals(), \"Run Cell 2 first to instantiate model_v2 and loaders.\"\n",
    "assert 'train_loader_v2' in globals() and 'val_loader_v2' in globals(), \"Run Cell 2 to build loaders.\"\n",
    "\n",
    "# ----- helpers -----\n",
    "def set_seed(seed: int):\n",
    "    import random\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def bce_masked_logits_loss(logits, y, mask, pos_weight=None):\n",
    "    loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\", pos_weight=pos_weight)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss = (loss * mask).sum() / (mask.sum() + 1e-6)\n",
    "    return loss\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_macro_auc(model, loader):\n",
    "    model.eval()\n",
    "    all_logits, all_y, all_m = [], [], []\n",
    "    for batch in loader:\n",
    "        ids = batch[\"tok\"][\"input_ids\"].to(device)\n",
    "        att = batch[\"tok\"][\"attention_mask\"].to(device)\n",
    "        gbt = batch[\"graph\"].to(device)\n",
    "        dsc = batch[\"desc\"].to(device)\n",
    "        logits = model(ids, att, gbt, dsc)\n",
    "        all_logits.append(logits.detach().cpu())\n",
    "        all_y.append((batch[\"y\"]==1).float().cpu())\n",
    "        all_m.append(batch[\"mask\"].float().cpu())\n",
    "    logits = torch.cat(all_logits).numpy()\n",
    "    y = torch.cat(all_y).numpy()\n",
    "    m = torch.cat(all_m).numpy()\n",
    "    prob = 1.0 / (1.0 + np.exp(-logits))\n",
    "    aucs = []\n",
    "    for l in range(prob.shape[1]):\n",
    "        ml = m[:,l]==1\n",
    "        yl = y[:,l][ml]; pl = prob[:,l][ml]\n",
    "        valid = (ml.sum()>0) and (yl.min()<1) and (yl.max()>0)\n",
    "        if valid:\n",
    "            try: aucs.append(roc_auc_score(yl, pl))\n",
    "            except Exception: aucs.append(np.nan)\n",
    "        else:\n",
    "            aucs.append(np.nan)\n",
    "    return float(np.nanmean(aucs)), aucs\n",
    "\n",
    "def count_trainable(model): \n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def freeze_text(m):\n",
    "    for p in m.text.parameters(): p.requires_grad = False\n",
    "\n",
    "def unfreeze_last_n_text_layers(m, n_last=2):\n",
    "    # Works for Roberta/BERT style encoders\n",
    "    enc = getattr(m.text, \"encoder\", None)\n",
    "    if enc is None and hasattr(m.text, \"roberta\"):\n",
    "        enc = m.text.roberta.encoder\n",
    "    layers = getattr(enc, \"layer\", None)\n",
    "    if layers is None:\n",
    "        # Fallback: unfreeze entire text if structure not found\n",
    "        for p in m.text.parameters(): p.requires_grad = True\n",
    "        return []\n",
    "    L = len(layers)\n",
    "    start = max(L - n_last, 0)\n",
    "    unfrozen = []\n",
    "    # freeze all first\n",
    "    for p in m.text.parameters(): p.requires_grad = False\n",
    "    # unfreeze last n\n",
    "    for li in range(start, L):\n",
    "        for p in layers[li].parameters(): \n",
    "            p.requires_grad = True\n",
    "        unfrozen.append(li)\n",
    "    return unfrozen  # list of layer indices\n",
    "\n",
    "def get_last_n_layer_modules(m, n_last=2):\n",
    "    enc = getattr(m.text, \"encoder\", None)\n",
    "    if enc is None and hasattr(m.text, \"roberta\"):\n",
    "        enc = m.text.roberta.encoder\n",
    "    layers = getattr(enc, \"layer\", None)\n",
    "    if layers is None: \n",
    "        return []\n",
    "    L = len(layers)\n",
    "    start = max(L - n_last, 0)\n",
    "    return [layers[i] for i in range(start, L)]\n",
    "\n",
    "def build_optimizer_with_llrd(model, lr_text, lr_others, weight_decay, llrd_mults):\n",
    "    # Collect non-text (and projection) params\n",
    "    other_params = []\n",
    "    text_groups  = []\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad: \n",
    "            continue\n",
    "        if n.startswith(\"text.\"):\n",
    "            # will be assigned per-layer below\n",
    "            continue\n",
    "        other_params.append(p)\n",
    "\n",
    "    # Per-layer groups for last-N\n",
    "    last_layers = get_last_n_layer_modules(model, n_last=len(llrd_mults))\n",
    "    assert len(last_layers) == len(llrd_mults), \"Mismatch in LLRD layers vs multipliers.\"\n",
    "\n",
    "    for mod, mult in zip(last_layers, llrd_mults):\n",
    "        params = [p for p in mod.parameters() if p.requires_grad]\n",
    "        if params:\n",
    "            text_groups.append({\"params\": params, \"lr\": lr_text*float(mult), \"weight_decay\": weight_decay})\n",
    "\n",
    "    groups = []\n",
    "    if other_params:\n",
    "        groups.append({\"params\": other_params, \"lr\": lr_others, \"weight_decay\": weight_decay})\n",
    "    groups.extend(text_groups)\n",
    "    opt = AdamW(groups)\n",
    "    return opt\n",
    "\n",
    "def build_scheduler(opt, steps_per_epoch, total_epochs, warmup_ratio):\n",
    "    from transformers import get_cosine_schedule_with_warmup\n",
    "    total_steps  = max(1, steps_per_epoch * total_epochs)\n",
    "    warmup_steps = max(1, int(total_steps * warmup_ratio))\n",
    "    sch = get_cosine_schedule_with_warmup(opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    return sch\n",
    "\n",
    "def log_epoch(path, epoch, stage, tr_loss, val_auc, per_label):\n",
    "    rec = {\n",
    "        \"epoch\": int(epoch), \"stage\": stage,\n",
    "        \"train_loss\": float(tr_loss) if tr_loss is not None else None,\n",
    "        \"val_macro_auc\": float(val_auc) if val_auc is not None else None,\n",
    "        \"val_per_label\": {str(i): (None if np.isnan(per_label[i]) else float(per_label[i]))\n",
    "                          for i in range(len(per_label))}\n",
    "    }\n",
    "    with open(path, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "    return rec\n",
    "\n",
    "# ----- seed -----\n",
    "set_seed(int(CONFIG_V2.get(\"seed\", 42)))\n",
    "\n",
    "# ----- pos_weight from v2 train split -----\n",
    "# Reuse arrays from Cell 2 scope\n",
    "# (We can rebuild quickly to be robust)\n",
    "from json import loads\n",
    "from pathlib import Path\n",
    "DATA   = ROOT / \"data\"\n",
    "DESC   = DATA / \"descriptors\"\n",
    "SPLITS = DATA / \"splits\"\n",
    "\n",
    "y_all      = np.load(DATA/\"y.npy\").astype(np.float32)\n",
    "y_mask_all = np.load(DATA/\"y_mask.npy\").astype(np.float32)\n",
    "splits     = loads((SPLITS/\"splits.json\").read_text())\n",
    "train_idx  = np.array(splits[\"train\"], dtype=int)\n",
    "\n",
    "pos = ((y_all[train_idx]==1) & (y_mask_all[train_idx]==1)).sum(axis=0)\n",
    "neg = ((y_all[train_idx]==0) & (y_mask_all[train_idx]==1)).sum(axis=0)\n",
    "eps = 1e-6\n",
    "pos_weight_v2 = (neg + eps) / (pos + eps)\n",
    "np.save(MODELS/\"pos_weight_v2.npy\", pos_weight_v2.astype(np.float32))\n",
    "pos_weight = torch.tensor(pos_weight_v2, dtype=torch.float32, device=device)\n",
    "print(\"Saved pos_weight_v2.npy. First 6:\", pos_weight_v2[:6])\n",
    "\n",
    "# ----- AMP / grad accum -----\n",
    "accum = int(CONFIG_V2.get(\"grad_accum_steps\", 2))\n",
    "amp_enabled = torch.cuda.is_available()\n",
    "autocast_ctx = torch.amp.autocast(device_type=\"cuda\", dtype=torch.float16, enabled=amp_enabled)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
    "\n",
    "def train_one_epoch(model, loader, optimizer, scheduler):\n",
    "    model.train()\n",
    "    running = 0.0\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    for step, batch in enumerate(loader, start=1):\n",
    "        ids = batch[\"tok\"][\"input_ids\"].to(device)\n",
    "        att = batch[\"tok\"][\"attention_mask\"].to(device)\n",
    "        gbt = batch[\"graph\"].to(device)\n",
    "        dsc = batch[\"desc\"].to(device)\n",
    "        y01 = (batch[\"y\"]==1).float().to(device)\n",
    "        msk = batch[\"mask\"].float().to(device)\n",
    "        with autocast_ctx:\n",
    "            logits = model(ids, att, gbt, dsc)\n",
    "            loss = bce_masked_logits_loss(logits, y01, msk, pos_weight=pos_weight)\n",
    "        loss_scaled = loss / max(accum,1)\n",
    "        if amp_enabled:\n",
    "            scaler.scale(loss_scaled).backward()\n",
    "        else:\n",
    "            loss_scaled.backward()\n",
    "        if step % accum == 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            if amp_enabled:\n",
    "                scaler.step(optimizer); scaler.update()\n",
    "            else:\n",
    "                optimizer.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            if scheduler is not None: scheduler.step()\n",
    "        running += float(loss.item())\n",
    "    return running / max(len(loader), 1)\n",
    "\n",
    "# ----- Stage A: freeze text -----\n",
    "freeze_text(model_v2)\n",
    "print(f\"[v2 Stage A] Trainable params: {count_trainable(model_v2)/1e6:.2f}M (text frozen)\")\n",
    "\n",
    "steps_A = len(train_loader_v2) // max(accum,1)\n",
    "optA = AdamW([\n",
    "    {\"params\": [p for n,p in model_v2.named_parameters() if p.requires_grad and not n.startswith(\"text.\")],\n",
    "     \"lr\": float(CONFIG_V2[\"lr_others\"]), \"weight_decay\": float(CONFIG_V2[\"weight_decay\"])}\n",
    "])\n",
    "schA = build_scheduler(optA, steps_per_epoch=steps_A, total_epochs=int(CONFIG_V2[\"stageA_freeze_text_epochs\"]),\n",
    "                       warmup_ratio=float(CONFIG_V2[\"warmup_ratio\"]))\n",
    "\n",
    "best_auc, best_epoch = -1.0, -1\n",
    "history_path = LOGS_V2 / \"train_log.jsonl\"\n",
    "if history_path.exists(): history_path.unlink()\n",
    "\n",
    "for epoch in range(1, int(CONFIG_V2[\"stageA_freeze_text_epochs\"])+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(model_v2, train_loader_v2, optA, schA)\n",
    "    val_auc, per_label = eval_macro_auc(model_v2, val_loader_v2)\n",
    "    log_epoch(history_path, epoch, \"A\", tr_loss, val_auc, per_label)\n",
    "    print(f\"[v2 A:{epoch:02d}] loss={tr_loss:.4f}  val_auc={val_auc:.4f}  time={time.time()-t0:.1f}s\")\n",
    "    if val_auc > best_auc + 1e-5:\n",
    "        best_auc, best_epoch = val_auc, f\"A{epoch}\"\n",
    "        torch.save({\"config\": CONFIG_V2, \"state_dict\": model_v2.state_dict(),\n",
    "                    \"val_macro_auc\": best_auc, \"epoch\": epoch, \"stage\": \"A\"},\n",
    "                   CKPT_V2/\"dualenc_best_v2.pt\")\n",
    "        print(f\"  ↳ saved new BEST to {CKPT_V2/'dualenc_best_v2.pt'} (val_macro_auc={best_auc:.4f})\")\n",
    "\n",
    "# ----- Stage B: unfreeze last 2 with LLRD -----\n",
    "unfrozen_idx = unfreeze_last_n_text_layers(model_v2, n_last=int(CONFIG_V2[\"stageB_unfreeze_last_layers\"]))\n",
    "print(f\"[v2 Stage B] Unfrozen text layers: {unfrozen_idx} → Trainable params: {count_trainable(model_v2)/1e6:.2f}M\")\n",
    "\n",
    "optB = build_optimizer_with_llrd(\n",
    "    model_v2,\n",
    "    lr_text=float(CONFIG_V2[\"lr_text\"]),\n",
    "    lr_others=float(CONFIG_V2[\"lr_others\"]),\n",
    "    weight_decay=float(CONFIG_V2[\"weight_decay\"]),\n",
    "    llrd_mults=list(CONFIG_V2.get(\"text_llrd\", [0.7, 1.0])),\n",
    ")\n",
    "steps_B = len(train_loader_v2) // max(accum,1)\n",
    "schB = build_scheduler(optB, steps_per_epoch=steps_B, total_epochs=int(CONFIG_V2[\"epochs_max\"]),\n",
    "                       warmup_ratio=float(CONFIG_V2[\"warmup_ratio\"]))\n",
    "\n",
    "patience = int(CONFIG_V2.get(\"early_stop_patience\", 20))\n",
    "no_imp = 0\n",
    "best_auc_num = -1.0\n",
    "best_epoch_num = -1\n",
    "\n",
    "for epoch in range(1, int(CONFIG_V2[\"epochs_max\"])+1):\n",
    "    t0 = time.time()\n",
    "    tr_loss = train_one_epoch(model_v2, train_loader_v2, optB, schB)\n",
    "    val_auc, per_label = eval_macro_auc(model_v2, val_loader_v2)\n",
    "    log_epoch(history_path, epoch, \"B\", tr_loss, val_auc, per_label)\n",
    "    print(f\"[v2 B:{epoch:02d}] loss={tr_loss:.4f}  val_auc={val_auc:.4f}  time={time.time()-t0:.1f}s\")\n",
    "\n",
    "    improved = val_auc > best_auc_num + 1e-5\n",
    "    if improved:\n",
    "        best_auc_num, best_epoch_num, no_imp = val_auc, epoch, 0\n",
    "        torch.save({\"config\": CONFIG_V2, \"state_dict\": model_v2.state_dict(),\n",
    "                    \"val_macro_auc\": best_auc_num, \"epoch\": epoch, \"stage\": \"B\"},\n",
    "                   CKPT_V2/\"dualenc_best_v2.pt\")\n",
    "        print(f\"  ↳ saved new BEST to {CKPT_V2/'dualenc_best_v2.pt'} (val_macro_auc={best_auc_num:.4f})\")\n",
    "    else:\n",
    "        no_imp += 1\n",
    "\n",
    "    # always save last\n",
    "    torch.save({\"config\": CONFIG_V2, \"state_dict\": model_v2.state_dict(),\n",
    "                \"val_macro_auc\": val_auc, \"epoch\": epoch, \"stage\": \"B\"},\n",
    "               CKPT_V2/\"dualenc_last_v2.pt\")\n",
    "\n",
    "    if no_imp >= patience:\n",
    "        print(f\"Early stopping: no improvement for {patience} epochs (best @ epoch {best_epoch_num} = {best_auc_num:.4f})\")\n",
    "        break\n",
    "\n",
    "# ----- Save run summary -----\n",
    "summary = {\n",
    "    \"best_val_macro_auc\": float(max(best_auc, best_auc_num)),\n",
    "    \"best_epoch\": best_epoch if best_auc >= best_auc_num else f\"B{best_epoch_num}\",\n",
    "    \"config\": CONFIG_V2\n",
    "}\n",
    "(MODELS/\"run_summary_v2.json\").write_text(json.dumps(summary, indent=2))\n",
    "print(\"Saved summary to\", MODELS/\"run_summary_v2.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8f31a7",
   "metadata": {},
   "source": [
    "## 4: Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115357f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded v2 best checkpoint: dualenc_best_v2.pt (val_macro_auc=0.7730, stage=B)\n",
      "Shapes — logits: (783, 12) | prob: (783, 12) | y/mask: (783, 12) (783, 12)\n",
      "TEST macro ROC-AUC = 0.8008 | macro PR-AUC = 0.3697\n",
      "Saved arrays & metrics to: tox21_dualenc_v1\\results\\v2\n",
      "Saved: tox21_dualenc_v1\\results\\v2\\per_label_rocauc_bar.png\n",
      "Saved: tox21_dualenc_v1\\results\\v2\\per_label_prauc_bar.png\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os, json, numpy as np, pandas as pd, torch, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "ROOT     = Path(\"tox21_dualenc_v1\")\n",
    "MODELS   = ROOT / \"models\"\n",
    "CKPT_V2  = MODELS / \"checkpoints_v2\"\n",
    "DATA     = ROOT / \"data\"\n",
    "RES_V2   = ROOT / \"results\" / \"v2\"\n",
    "RES_V2.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Expect these from earlier cells\n",
    "assert 'model_v2' in globals(), \"Please run v2 — Cell 2 to build the model.\"\n",
    "assert 'test_loader_v2' in globals(), \"Please run v2 — Cell 2 to build loaders.\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load best checkpoint weights into the in-memory model\n",
    "best_path = CKPT_V2 / \"dualenc_best_v2.pt\"\n",
    "state = torch.load(best_path, map_location=device)\n",
    "model_v2.load_state_dict(state[\"state_dict\"])\n",
    "model_v2.eval()\n",
    "print(f\"Loaded v2 best checkpoint: {best_path.name} (val_macro_auc={state.get('val_macro_auc'):.4f}, stage={state.get('stage')})\")\n",
    "\n",
    "label_names = (DATA/\"label_names.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "\n",
    "# --- Run TEST inference\n",
    "all_logits, all_y, all_m, all_idx = [], [], [], []\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader_v2:\n",
    "        ids = batch[\"tok\"][\"input_ids\"].to(device)\n",
    "        att = batch[\"tok\"][\"attention_mask\"].to(device)\n",
    "        gbt = batch[\"graph\"].to(device)\n",
    "        dsc = batch[\"desc\"].to(device)\n",
    "        logits = model_v2(ids, att, gbt, dsc).detach().cpu().numpy()\n",
    "        all_logits.append(logits)\n",
    "        all_y.append((batch[\"y\"]==1).float().cpu().numpy())\n",
    "        all_m.append(batch[\"mask\"].float().cpu().numpy())\n",
    "        all_idx.append(batch[\"idx\"].cpu().numpy())\n",
    "\n",
    "logits = np.concatenate(all_logits, axis=0)       # (Ntest, L)\n",
    "prob   = 1.0 / (1.0 + np.exp(-logits))            # sigmoid\n",
    "y01    = np.concatenate(all_y, axis=0).astype(np.float32)\n",
    "mask   = np.concatenate(all_m, axis=0).astype(np.float32)\n",
    "idxs   = np.concatenate(all_idx, axis=0).astype(np.int64)\n",
    "\n",
    "print(\"Shapes — logits:\", logits.shape, \"| prob:\", prob.shape, \"| y/mask:\", y01.shape, mask.shape)\n",
    "\n",
    "# --- Metrics\n",
    "L = prob.shape[1]\n",
    "roc_list, pr_list = [], []\n",
    "for l in range(L):\n",
    "    m = mask[:, l] == 1\n",
    "    y = y01[:, l][m]\n",
    "    p = prob[:, l][m]\n",
    "    if m.sum() > 0 and (y.min() < 1) and (y.max() > 0):\n",
    "        try: roc = roc_auc_score(y, p)\n",
    "        except Exception: roc = np.nan\n",
    "        try: pr = average_precision_score(y, p)\n",
    "        except Exception: pr = np.nan\n",
    "    else:\n",
    "        roc, pr = np.nan, np.nan\n",
    "    roc_list.append(roc); pr_list.append(pr)\n",
    "\n",
    "macro_roc = float(np.nanmean(roc_list))\n",
    "macro_pr  = float(np.nanmean(pr_list))\n",
    "print(f\"TEST macro ROC-AUC = {macro_roc:.4f} | macro PR-AUC = {macro_pr:.4f}\")\n",
    "\n",
    "# --- Save arrays & tables\n",
    "np.save(RES_V2/\"test_logits.npy\", logits)\n",
    "np.save(RES_V2/\"test_prob.npy\", prob)\n",
    "np.save(RES_V2/\"test_y.npy\", y01)\n",
    "np.save(RES_V2/\"test_mask.npy\", mask)\n",
    "np.save(RES_V2/\"test_indices.npy\", idxs)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"label\": label_names,\n",
    "    \"roc_auc\": roc_list,\n",
    "    \"pr_auc\": pr_list,\n",
    "})\n",
    "df.to_csv(RES_V2/\"per_label_metrics.csv\", index=False)\n",
    "\n",
    "metrics = {\n",
    "    \"macro_roc_auc\": macro_roc,\n",
    "    \"macro_pr_auc\": macro_pr,\n",
    "    \"per_label\": {label_names[i]: {\"roc_auc\": (None if np.isnan(roc_list[i]) else float(roc_list[i])),\n",
    "                                   \"pr_auc\":  (None if np.isnan(pr_list[i])  else float(pr_list[i]))}\n",
    "                  for i in range(L)},\n",
    "    \"best_val_macro_auc\": float(state.get(\"val_macro_auc\", float(\"nan\"))),\n",
    "    \"best_stage\": state.get(\"stage\"),\n",
    "}\n",
    "(RES_V2/\"metrics.json\").write_text(json.dumps(metrics, indent=2))\n",
    "print(\"Saved arrays & metrics to:\", RES_V2)\n",
    "\n",
    "# --- Plots\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(df[\"label\"], df[\"roc_auc\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"ROC-AUC (TEST)\")\n",
    "plt.title(\"Per-label ROC-AUC (v2 refreshed)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(RES_V2/\"per_label_rocauc_bar.png\", dpi=200)\n",
    "print(\"Saved:\", RES_V2/\"per_label_rocauc_bar.png\")\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.bar(df[\"label\"], df[\"pr_auc\"])\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.ylabel(\"PR-AUC (TEST)\")\n",
    "plt.title(\"Per-label PR-AUC (v2 refreshed)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(RES_V2/\"per_label_prauc_bar.png\", dpi=200)\n",
    "print(\"Saved:\", RES_V2/\"per_label_prauc_bar.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb38e64",
   "metadata": {},
   "source": [
    "## 5: Dynamic thresholds from train+val (per-label) with robust fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09c6487a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dynamic thresholds to: tox21_dualenc_v1\\models\\thresholds_v2_dynamic.json\n",
      "Lowest 6 thresholds: [('NR-ER', 0.6777247875928872, 'best_f1_clipped'), ('SR-ARE', 0.7474570083618164, 'best_f1_clipped'), ('SR-MMP', 0.798002764582634, 'best_f1_clipped'), ('SR-HSE', 0.8113327467441558, 'best_f1_clipped'), ('NR-AhR', 0.8182927966117857, 'best_f1_clipped'), ('NR-ER-LBD', 0.8382711422443387, 'best_f1_clipped')]\n"
     ]
    }
   ],
   "source": [
    "import json, numpy as np, torch, time\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "ROOT     = Path(\"tox21_dualenc_v1\")\n",
    "MODELS   = ROOT / \"models\"\n",
    "CKPT_V2  = MODELS / \"checkpoints_v2\"\n",
    "DATA     = ROOT / \"data\"\n",
    "\n",
    "assert 'model_v2' in globals(), \"Run v2 — Cell 2 (model + loaders) first.\"\n",
    "assert 'train_loader_v2' in globals() and 'val_loader_v2' in globals(), \"Run v2 — Cell 2 first.\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load best weights\n",
    "st = torch.load(CKPT_V2/\"dualenc_best_v2.pt\", map_location=device)\n",
    "model_v2.load_state_dict(st[\"state_dict\"])\n",
    "model_v2.eval()\n",
    "\n",
    "label_names = (DATA/\"label_names.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "L = len(label_names)\n",
    "\n",
    "def collect(loader):\n",
    "    probs, ys, ms = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            ids = b[\"tok\"][\"input_ids\"].to(device)\n",
    "            att = b[\"tok\"][\"attention_mask\"].to(device)\n",
    "            gbt = b[\"graph\"].to(device)\n",
    "            dsc = b[\"desc\"].to(device)\n",
    "            logits = model_v2(ids, att, gbt, dsc)\n",
    "            p = torch.sigmoid(logits).cpu().numpy()\n",
    "            y = (b[\"y\"]==1).float().cpu().numpy()\n",
    "            m = b[\"mask\"].float().cpu().numpy()\n",
    "            probs.append(p); ys.append(y); ms.append(m)\n",
    "    return np.concatenate(probs), np.concatenate(ys), np.concatenate(ms)\n",
    "\n",
    "prob_tr, y_tr, m_tr = collect(train_loader_v2)\n",
    "prob_va, y_va, m_va = collect(val_loader_v2)\n",
    "\n",
    "# Use ALL labeled non-test: train + val\n",
    "prob_tv = np.concatenate([prob_tr, prob_va], axis=0)\n",
    "y_tv    = np.concatenate([y_tr,    y_va   ], axis=0)\n",
    "m_tv    = np.concatenate([m_tr,    m_va   ], axis=0)\n",
    "\n",
    "def safe_scores(y_true, y_pred):\n",
    "    return {\n",
    "        \"f1\":  f1_score(y_true, y_pred, zero_division=0),\n",
    "        \"pr\":  precision_score(y_true, y_pred, zero_division=0),\n",
    "        \"rc\":  recall_score(y_true, y_pred, zero_division=0),\n",
    "    }\n",
    "\n",
    "# Threshold search params\n",
    "FLOOR = 0.25   # never go below this unless using top-1 fallback at inference\n",
    "CEIL  = 0.90\n",
    "MIN_POS_FRAC = 0.01  # if predicted positives < 1% on train+val, consider too conservative\n",
    "\n",
    "results = {\"meta\": {\n",
    "    \"computed_at\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    \"ckpt_val_macro_auc\": float(st.get(\"val_macro_auc\", float(\"nan\"))),\n",
    "    \"ckpt_stage\": st.get(\"stage\", \"B\"),\n",
    "    \"splits_used\": \"train+val\",\n",
    "    \"floor\": FLOOR, \"ceil\": CEIL, \"min_pos_frac\": MIN_POS_FRAC\n",
    "}}\n",
    "\n",
    "per_label = {}\n",
    "\n",
    "for j, name in enumerate(label_names):\n",
    "    mask = m_tv[:, j] == 1\n",
    "    if mask.sum() == 0:\n",
    "        per_label[name] = {\"thr\": 0.50, \"reason\": \"no_labeled_data\", \"f1\": None, \"pr\": None, \"rc\": None}\n",
    "        continue\n",
    "\n",
    "    y = y_tv[:, j][mask].astype(int)\n",
    "    p = prob_tv[:, j][mask]\n",
    "\n",
    "    # prevalence-matched quantile (positive rate target)\n",
    "    prev = y.mean()  # fraction positives in true labels\n",
    "    # target predicted positive fraction ~= prev → threshold at (1 - prev) quantile\n",
    "    thr_quant = float(np.quantile(p, 1.0 - prev)) if prev < 1.0 else CEIL\n",
    "\n",
    "    # build candidate thresholds from percentiles of this label's probabilities\n",
    "    qs = np.linspace(0.05, 0.95, 181)\n",
    "    cand = np.unique(np.quantile(p, qs))\n",
    "    # ensure floor & ceil included\n",
    "    cand = np.unique(np.clip(np.concatenate([cand, [FLOOR, CEIL]]), 0.0, 1.0))\n",
    "\n",
    "    # evaluate candidates on F1\n",
    "    best = {\"thr\": 0.50, \"f1\": -1.0, \"pr\": 0.0, \"rc\": 0.0}\n",
    "    for t in cand:\n",
    "        yhat = (p >= t).astype(int)\n",
    "        sc = safe_scores(y, yhat)\n",
    "        if (sc[\"f1\"] > best[\"f1\"]) or (np.isclose(sc[\"f1\"], best[\"f1\"]) and t < best[\"thr\"]):\n",
    "            best = {\"thr\": float(t), **sc}\n",
    "\n",
    "    # guardrails\n",
    "    pos_frac = (p >= best[\"thr\"]).mean()\n",
    "    if np.isnan(best[\"f1\"]) or pos_frac < MIN_POS_FRAC:\n",
    "        thr_final = max(thr_quant, FLOOR)\n",
    "        reason = f\"fallback_quantile (pos_frac={pos_frac:.3f})\"\n",
    "        # compute scores at fallback (for logging only, uses ground truth)\n",
    "        yhat_fb = (p >= thr_final).astype(int)\n",
    "        sc_fb = safe_scores(y, yhat_fb)\n",
    "        per_label[name] = {\"thr\": float(thr_final), \"reason\": reason,\n",
    "                           \"f1\": float(sc_fb[\"f1\"]), \"pr\": float(sc_fb[\"pr\"]), \"rc\": float(sc_fb[\"rc\"]),\n",
    "                           \"thr_f1\": best[\"thr\"], \"f1_f1\": float(best[\"f1\"]),\n",
    "                           \"thr_quant\": float(thr_quant)}\n",
    "    else:\n",
    "        thr_final = float(np.clip(best[\"thr\"], FLOOR, CEIL))\n",
    "        per_label[name] = {\"thr\": thr_final, \"reason\": \"best_f1_clipped\",\n",
    "                           \"f1\": float(best[\"f1\"]), \"pr\": float(best[\"pr\"]), \"rc\": float(best[\"rc\"]),\n",
    "                           \"thr_quant\": float(thr_quant)}\n",
    "\n",
    "results[\"per_label\"] = per_label\n",
    "\n",
    "# Also compute a reference global threshold on val only (not used by default)\n",
    "from sklearn.metrics import f1_score\n",
    "def macro_f1_at(t, prob, y, m):\n",
    "    f1s = []\n",
    "    for j in range(prob.shape[1]):\n",
    "        mask = m[:, j]==1\n",
    "        if mask.sum()==0: continue\n",
    "        yy = y[:, j][mask].astype(int)\n",
    "        pp = prob[:, j][mask]\n",
    "        f1s.append(f1_score(yy, (pp>=t).astype(int), zero_division=0))\n",
    "    return float(np.mean(f1s)) if f1s else 0.0\n",
    "\n",
    "grid = np.linspace(0.05, 0.95, 181)\n",
    "best_t, best_f = 0.5, -1.0\n",
    "for t in grid:\n",
    "    f = macro_f1_at(t, prob_va, y_va, m_va)\n",
    "    if f > best_f or (np.isclose(f,best_f) and t < best_t):\n",
    "        best_t, best_f = float(t), float(f)\n",
    "results[\"global\"] = {\"thr\": best_t, \"val_macro_f1\": best_f}\n",
    "\n",
    "# Save\n",
    "out_path = MODELS/\"thresholds_v2_dynamic.json\"\n",
    "out_path.write_text(json.dumps(results, indent=2))\n",
    "print(\"Saved dynamic thresholds to:\", out_path)\n",
    "# quick peek\n",
    "kpeek = sorted([(k, v[\"thr\"], v[\"reason\"]) for k,v in per_label.items()], key=lambda x: x[1])[:6]\n",
    "print(\"Lowest 6 thresholds:\", kpeek)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0500db1c",
   "metadata": {},
   "source": [
    "## 6: New test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f290687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "v2 Predictions using *dynamic* thresholds (train+val) with top-1 fallback:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>NR-AR</th>\n",
       "      <th>NR-AR-LBD</th>\n",
       "      <th>NR-AhR</th>\n",
       "      <th>NR-Aromatase</th>\n",
       "      <th>NR-ER</th>\n",
       "      <th>NR-ER-LBD</th>\n",
       "      <th>NR-PPAR-gamma</th>\n",
       "      <th>SR-ARE</th>\n",
       "      <th>SR-ATAD5</th>\n",
       "      <th>SR-HSE</th>\n",
       "      <th>SR-MMP</th>\n",
       "      <th>SR-p53</th>\n",
       "      <th>predicted (+ dyn)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOc1ccc2nc(S(N)(=O)=O)sc2c1</td>\n",
       "      <td>0.170309</td>\n",
       "      <td>0.028467</td>\n",
       "      <td>0.591774</td>\n",
       "      <td>0.123696</td>\n",
       "      <td>0.501389</td>\n",
       "      <td>0.102438</td>\n",
       "      <td>0.014603</td>\n",
       "      <td>0.400861</td>\n",
       "      <td>0.053786</td>\n",
       "      <td>0.081905</td>\n",
       "      <td>0.219491</td>\n",
       "      <td>0.121179</td>\n",
       "      <td>NR-AhR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCN1C(=O)NC(c2ccccc2)C1=O</td>\n",
       "      <td>0.044805</td>\n",
       "      <td>0.002296</td>\n",
       "      <td>0.250795</td>\n",
       "      <td>0.012563</td>\n",
       "      <td>0.297068</td>\n",
       "      <td>0.043581</td>\n",
       "      <td>0.011215</td>\n",
       "      <td>0.114922</td>\n",
       "      <td>0.004707</td>\n",
       "      <td>0.050002</td>\n",
       "      <td>0.067577</td>\n",
       "      <td>0.005852</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1</td>\n",
       "      <td>0.160081</td>\n",
       "      <td>0.020800</td>\n",
       "      <td>0.486615</td>\n",
       "      <td>0.132796</td>\n",
       "      <td>0.378834</td>\n",
       "      <td>0.122802</td>\n",
       "      <td>0.167773</td>\n",
       "      <td>0.428976</td>\n",
       "      <td>0.037286</td>\n",
       "      <td>0.390943</td>\n",
       "      <td>0.499104</td>\n",
       "      <td>0.453635</td>\n",
       "      <td>SR-MMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCCCCCCCCOCC(O)CN</td>\n",
       "      <td>0.091298</td>\n",
       "      <td>0.005698</td>\n",
       "      <td>0.029006</td>\n",
       "      <td>0.023343</td>\n",
       "      <td>0.210743</td>\n",
       "      <td>0.016582</td>\n",
       "      <td>0.051413</td>\n",
       "      <td>0.071533</td>\n",
       "      <td>0.001126</td>\n",
       "      <td>0.110368</td>\n",
       "      <td>0.038713</td>\n",
       "      <td>0.016474</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nc1ccc([N+](=O)[O-])cc1N</td>\n",
       "      <td>0.390982</td>\n",
       "      <td>0.290442</td>\n",
       "      <td>0.836103</td>\n",
       "      <td>0.104454</td>\n",
       "      <td>0.408156</td>\n",
       "      <td>0.479435</td>\n",
       "      <td>0.650302</td>\n",
       "      <td>0.719225</td>\n",
       "      <td>0.347424</td>\n",
       "      <td>0.440683</td>\n",
       "      <td>0.692822</td>\n",
       "      <td>0.522807</td>\n",
       "      <td>NR-AhR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   SMILES     NR-AR  NR-AR-LBD    NR-AhR  \\\n",
       "0            CCOc1ccc2nc(S(N)(=O)=O)sc2c1  0.170309   0.028467  0.591774   \n",
       "1               CCN1C(=O)NC(c2ccccc2)C1=O  0.044805   0.002296  0.250795   \n",
       "2  O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1  0.160081   0.020800  0.486615   \n",
       "3                      CCCCCCCCCCOCC(O)CN  0.091298   0.005698  0.029006   \n",
       "4                Nc1ccc([N+](=O)[O-])cc1N  0.390982   0.290442  0.836103   \n",
       "\n",
       "   NR-Aromatase     NR-ER  NR-ER-LBD  NR-PPAR-gamma    SR-ARE  SR-ATAD5  \\\n",
       "0      0.123696  0.501389   0.102438       0.014603  0.400861  0.053786   \n",
       "1      0.012563  0.297068   0.043581       0.011215  0.114922  0.004707   \n",
       "2      0.132796  0.378834   0.122802       0.167773  0.428976  0.037286   \n",
       "3      0.023343  0.210743   0.016582       0.051413  0.071533  0.001126   \n",
       "4      0.104454  0.408156   0.479435       0.650302  0.719225  0.347424   \n",
       "\n",
       "     SR-HSE    SR-MMP    SR-p53 predicted (+ dyn)  \n",
       "0  0.081905  0.219491  0.121179            NR-AhR  \n",
       "1  0.050002  0.067577  0.005852                 —  \n",
       "2  0.390943  0.499104  0.453635            SR-MMP  \n",
       "3  0.110368  0.038713  0.016474                 —  \n",
       "4  0.440683  0.692822  0.522807            NR-AhR  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tox21_dualenc_v1\\results\\v2\\infer_v2_dynamic_20250901_203227.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, torch, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data, Batch as GeometricBatch\n",
    "\n",
    "ROOT   = Path(\"tox21_dualenc_v1\")\n",
    "DATA   = ROOT / \"data\"\n",
    "DESC   = DATA / \"descriptors\"\n",
    "SPLITS = DATA / \"splits\"\n",
    "MODELS = ROOT / \"models\"\n",
    "CKPT_V2 = MODELS / \"checkpoints_v2\"\n",
    "RES_V2  = ROOT / \"results\" / \"v2\"\n",
    "RES_V2.mkdir(parents=True, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "CONFIG_V2 = json.loads((MODELS/\"config_dualenc_v2.json\").read_text())\n",
    "label_names = (DATA/\"label_names.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "thr_conf = json.loads((MODELS/\"thresholds_v2_dynamic.json\").read_text())\n",
    "applied_thr = {lbl: float(thr_conf[\"per_label\"][lbl][\"thr\"]) for lbl in label_names}\n",
    "\n",
    "TOP1_FALLBACK = True     # set False to disable\n",
    "TOP1_FLOOR    = 0.30     # only trigger fallback if top-1 prob ≥ this floor\n",
    "\n",
    "# --- Descriptor train-mean\n",
    "X_desc  = np.load(DESC/\"desc_selected.npy\").astype(np.float32)\n",
    "splits  = json.loads((SPLITS/\"splits.json\").read_text())\n",
    "train_idx = np.array(splits[\"train\"], dtype=int)\n",
    "desc_mean = X_desc[train_idx].mean(axis=0).astype(np.float32)\n",
    "\n",
    "# --- Featurizers (same as training)\n",
    "ATOM_LIST = [\"C\",\"N\",\"O\",\"S\",\"F\",\"Cl\",\"Br\",\"I\",\"P\",\"B\"]\n",
    "HYB_LIST = [\n",
    "    Chem.rdchem.HybridizationType.SP,\n",
    "    Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3,\n",
    "    Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2,\n",
    "]\n",
    "def one_hot_with_other(key, choices):\n",
    "    v = [0]*(len(choices)+1)\n",
    "    try: idx = choices.index(key)\n",
    "    except ValueError: idx = None\n",
    "    v[-1 if idx is None else idx] = 1\n",
    "    return v\n",
    "def one_hot_index(idx, size):\n",
    "    v = [0]*size\n",
    "    if 0 <= idx < size: v[idx] = 1\n",
    "    return v\n",
    "def atom_features(atom: Chem.Atom):\n",
    "    f_type   = one_hot_with_other(atom.GetSymbol(), ATOM_LIST)\n",
    "    f_deg    = one_hot_index(min(atom.GetTotalDegree(),6), 7)\n",
    "    charge_map = {-2:0,-1:1,0:2,1:3,2:4}\n",
    "    f_charge = one_hot_index(charge_map.get(atom.GetFormalCharge(),5), 6)\n",
    "    hyb = atom.GetHybridization()\n",
    "    f_hyb   = one_hot_with_other(hyb if hyb in HYB_LIST else \"other\", HYB_LIST)\n",
    "    f_flags = [int(atom.GetIsAromatic()), int(atom.IsInRing())]\n",
    "    return f_type + f_deg + f_charge + f_hyb + f_flags\n",
    "def bond_features(bond: Chem.Bond):\n",
    "    types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "             Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    f_type  = one_hot_with_other(bond.GetBondType(), types)\n",
    "    f_flags = [int(bond.GetIsConjugated()), int(bond.IsInRing())]\n",
    "    return f_type + f_flags\n",
    "\n",
    "NODE_DIM, EDGE_DIM = 32, 7\n",
    "def build_graph_from_smiles(s):\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None or mol.GetNumAtoms()==0:\n",
    "        return None\n",
    "    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float32)\n",
    "    ei_src, ei_dst, eattr = [], [], []\n",
    "    for b in mol.GetBonds():\n",
    "        u, v = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bf = bond_features(b)\n",
    "        ei_src.extend([u,v]); ei_dst.extend([v,u]); eattr.extend([bf,bf])\n",
    "    if len(ei_src)==0:\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_attr  = torch.empty((0, EDGE_DIM), dtype=torch.float32)\n",
    "    else:\n",
    "        edge_index = torch.tensor([ei_src, ei_dst], dtype=torch.long)\n",
    "        edge_attr  = torch.tensor(eattr, dtype=torch.float32)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, invalid=0)\n",
    "\n",
    "# Load model (reuse if present)\n",
    "try:\n",
    "    model_v2\n",
    "    tok = model_v2.tok\n",
    "except NameError:\n",
    "    import torch.nn as nn, torch.nn.functional as F\n",
    "    from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "    from torch_geometric.utils import to_dense_batch\n",
    "    from transformers import AutoModel, AutoTokenizer\n",
    "    class AttentionPoolAMP(nn.Module):\n",
    "        def __init__(self, dim): super().__init__(); self.score = nn.Linear(dim, 1)\n",
    "        def forward(self, X, mask=None):\n",
    "            s = self.score(X).squeeze(-1).float()\n",
    "            if mask is not None: s = s.masked_fill(~mask.bool(), -1e9)\n",
    "            a = torch.softmax(s, dim=1).to(X.dtype)\n",
    "            return torch.bmm(a.unsqueeze(1), X).squeeze(1)\n",
    "    class CrossAttnBlock(nn.Module):\n",
    "        def __init__(self, dim=256, heads=4, dropout=0.1):\n",
    "            super().__init__()\n",
    "            self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "            self.norm1 = nn.LayerNorm(dim)\n",
    "            self.ffn = nn.Sequential(nn.Linear(dim, 4*dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(4*dim, dim))\n",
    "            self.norm2 = nn.LayerNorm(dim); self.drop = nn.Dropout(dropout)\n",
    "        def forward(self, Q_text, K_graph, V_graph, key_padding_mask=None):\n",
    "            attn_out,_ = self.attn(Q_text, K_graph, V_graph, key_padding_mask=key_padding_mask)\n",
    "            x = self.norm1(Q_text + self.drop(attn_out)); ff = self.ffn(x)\n",
    "            return self.norm2(x + self.drop(ff))\n",
    "    class GINEncoder(nn.Module):\n",
    "        def __init__(self, node_dim, edge_dim, hidden=256, layers=4, dropout=0.3):\n",
    "            super().__init__()\n",
    "            self.layers = nn.ModuleList(); self.bns = nn.ModuleList()\n",
    "            in_dim = node_dim\n",
    "            for _ in range(layers):\n",
    "                mlp  = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden))\n",
    "                conv = GINEConv(mlp, edge_dim=edge_dim)\n",
    "                self.layers.append(conv); self.bns.append(nn.BatchNorm1d(hidden)); in_dim = hidden\n",
    "            self.dropout = nn.Dropout(dropout); self.hidden = hidden\n",
    "        def forward(self, x, edge_index, edge_attr, batch_index):\n",
    "            h = x\n",
    "            for conv, bn in zip(self.layers, self.bns):\n",
    "                h = conv(h, edge_index, edge_attr); h = bn(h); h = torch.relu(h); h = self.dropout(h)\n",
    "            z_graph = global_mean_pool(h, batch_index); return h, z_graph\n",
    "    class DualEncoderXAttn(nn.Module):\n",
    "        def __init__(self, cfg):\n",
    "            super().__init__()\n",
    "            self.cfg = cfg\n",
    "            self.tok  = AutoTokenizer.from_pretrained(cfg[\"text_encoder\"])\n",
    "            self.text = AutoModel.from_pretrained(cfg[\"text_encoder\"])\n",
    "            self.text_proj = nn.Linear(self.text.config.hidden_size, cfg[\"text_proj_dim\"])\n",
    "            self.gnn  = GINEncoder(int(cfg[\"node_dim\"]), int(cfg[\"edge_dim\"]),\n",
    "                                   int(cfg[\"graph_hidden\"]), int(cfg[\"graph_layers\"]),\n",
    "                                   float(cfg[\"dropout\"]))\n",
    "            self.xattn = CrossAttnBlock(int(cfg[\"fusion_dim\"]), int(cfg[\"fusion_heads\"]),\n",
    "                                        float(cfg[\"fusion_dropout\"]))\n",
    "            self.tpool = AttentionPoolAMP(int(cfg[\"fusion_dim\"]))\n",
    "            self.dmlp  = nn.Sequential(\n",
    "                torch.nn.Linear(int(cfg[\"desc_dim_in\"]), int(cfg[\"desc_hidden\"])),\n",
    "                torch.nn.ReLU(), torch.nn.Dropout(float(cfg[\"dropout\"]))\n",
    "            )\n",
    "            fused_in = int(cfg[\"fusion_dim\"]) + int(cfg[\"graph_hidden\"]) + int(cfg[\"desc_hidden\"])\n",
    "            self.head = nn.Sequential(torch.nn.Linear(fused_in, int(cfg[\"head_hidden\"])),\n",
    "                                      torch.nn.ReLU(), torch.nn.Dropout(float(cfg[\"dropout\"])),\n",
    "                                      torch.nn.Linear(int(cfg[\"head_hidden\"]), int(cfg[\"num_labels\"])))\n",
    "        def forward(self, input_ids, attention_mask, graph_batch, desc):\n",
    "            t_out  = self.text(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "            t_proj = self.text_proj(t_out)\n",
    "            node_h, z_graph = self.gnn(graph_batch.x, graph_batch.edge_index, graph_batch.edge_attr, graph_batch.batch)\n",
    "            from torch_geometric.utils import to_dense_batch\n",
    "            node_dense, node_mask = to_dense_batch(node_h, graph_batch.batch)\n",
    "            fused_text = self.xattn(t_proj, node_dense, node_dense, key_padding_mask=~node_mask)\n",
    "            z_text = self.tpool(fused_text, mask=attention_mask.bool())\n",
    "            z_desc = self.dmlp(desc)\n",
    "            z = torch.cat([z_text, z_graph, z_desc], dim=-1)\n",
    "            return self.head(z)\n",
    "    model_v2 = DualEncoderXAttn(CONFIG_V2).to(device)\n",
    "    st = torch.load(CKPT_V2/\"dualenc_best_v2.pt\", map_location=device)\n",
    "    model_v2.load_state_dict(st[\"state_dict\"]); model_v2.eval()\n",
    "    tok = model_v2.tok\n",
    "\n",
    "# --- Your SMILES list ---\n",
    "custom_smiles = [\n",
    "    \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",\n",
    "    \"CCN1C(=O)NC(c2ccccc2)C1=O\",\n",
    "    \"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\",\n",
    "    \"CCCCCCCCCCOCC(O)CN\",\n",
    "    \"Nc1ccc([N+](=O)[O-])cc1N\",\n",
    "]\n",
    "\n",
    "# Build graphs & tokenize\n",
    "def build_graph_from_smiles(s):\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None or mol.GetNumAtoms()==0: return None\n",
    "    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float32)\n",
    "    ei_src, ei_dst, eattr = [], [], []\n",
    "    for b in mol.GetBonds():\n",
    "        u, v = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bf = bond_features(b)\n",
    "        ei_src.extend([u,v]); ei_dst.extend([v,u]); eattr.extend([bf,bf])\n",
    "    if len(ei_src)==0:\n",
    "        edge_index = torch.empty((2,0), dtype=torch.long)\n",
    "        edge_attr  = torch.empty((0, 7), dtype=torch.float32)\n",
    "    else:\n",
    "        edge_index = torch.tensor([ei_src, ei_dst], dtype=torch.long)\n",
    "        edge_attr  = torch.tensor(eattr, dtype=torch.float32)\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, invalid=0)\n",
    "\n",
    "graphs_list, ok_smiles = [], []\n",
    "for s in custom_smiles:\n",
    "    g = build_graph_from_smiles(s)\n",
    "    if g is None: \n",
    "        print(\"⚠️ Invalid SMILES skipped:\", s); \n",
    "        continue\n",
    "    graphs_list.append(g); ok_smiles.append(s)\n",
    "\n",
    "g_batch = GeometricBatch.from_data_list(graphs_list).to(device)\n",
    "toks = tok(ok_smiles, padding=True, truncation=True,\n",
    "           max_length=int(CONFIG_V2[\"max_length\"]), return_tensors=\"pt\")\n",
    "ids, att = toks[\"input_ids\"].to(device), toks[\"attention_mask\"].to(device)\n",
    "\n",
    "# descriptors = train-mean\n",
    "X_desc  = np.load(DESC/\"desc_selected.npy\").astype(np.float32)\n",
    "train_idx = np.array(json.loads((SPLITS/\"splits.json\").read_text())[\"train\"], dtype=int)\n",
    "desc_mean = X_desc[train_idx].mean(axis=0).astype(np.float32)\n",
    "desc = torch.tensor(np.tile(desc_mean, (len(ok_smiles), 1)), dtype=torch.float32, device=device)\n",
    "\n",
    "# Predict\n",
    "with torch.no_grad():\n",
    "    prob = torch.sigmoid(model_v2(ids, att, g_batch, desc)).cpu().numpy()  # (B,L)\n",
    "\n",
    "# Apply per-label thresholds\n",
    "thr_vec = np.array([applied_thr[lbl] for lbl in label_names], dtype=float)\n",
    "pred_bin = (prob >= thr_vec[None, :]).astype(int)\n",
    "\n",
    "# Optional top-1 fallback per molecule\n",
    "if TOP1_FALLBACK:\n",
    "    for i in range(pred_bin.shape[0]):\n",
    "        if pred_bin[i].sum() == 0:\n",
    "            j = int(np.argmax(prob[i]))\n",
    "            if prob[i, j] >= TOP1_FLOOR:\n",
    "                pred_bin[i, j] = 1  # force top-1 call\n",
    "\n",
    "# Build output\n",
    "df_prob = pd.DataFrame(prob, columns=label_names, index=ok_smiles)\n",
    "calls = []\n",
    "for i, smi in enumerate(ok_smiles):\n",
    "    pos = [label_names[j] for j in np.where(pred_bin[i] == 1)[0]]\n",
    "    calls.append(\", \".join(pos) if pos else \"—\")\n",
    "df_out = df_prob.copy()\n",
    "df_out.insert(0, \"SMILES\", df_out.index)\n",
    "df_out[\"predicted (+ dyn)\"] = calls\n",
    "df_out = df_out.reset_index(drop=True)\n",
    "\n",
    "print(\"v2 Predictions using *dynamic* thresholds (train+val) with top-1 fallback:\")\n",
    "display(df_out)\n",
    "\n",
    "# Save\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_csv = RES_V2 / f\"infer_v2_dynamic_{timestamp}.csv\"\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56702e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1db2e8cf",
   "metadata": {},
   "source": [
    "# V3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21070cc3",
   "metadata": {},
   "source": [
    "## 1: Setup & Dynamic Config (paths, env, dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f469361c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V3] Device: cuda | torch=2.6.0+cu124\n",
      "[V3] Checkpoints: tox21_dualenc_v1\\models\\checkpoints_v3\n",
      "[V3] Results    : tox21_dualenc_v1\\results\\v3\n",
      "[V3] Logs       : tox21_dualenc_v1\\logs\\v3\\train_log.jsonl\n",
      "[V3] Seeds      : [42, 43, 44]\n"
     ]
    }
   ],
   "source": [
    "import os, json, random\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "DATA = ROOT / \"data\"\n",
    "DESC = DATA / \"descriptors\"\n",
    "SPL  = DATA / \"splits\"\n",
    "\n",
    "CKPT = ROOT / \"models\" / \"checkpoints_v3\"\n",
    "RES  = ROOT / \"results\" / \"v3\"\n",
    "LOGS = ROOT / \"logs\" / \"v3\"\n",
    "for p in [CKPT, RES, LOGS]: p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- seeds\n",
    "SEEDS = [42, 43, 44]\n",
    "def set_seed(s=42):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); \n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(s)\n",
    "set_seed(SEEDS[0])\n",
    "\n",
    "# ---- config (tuned)\n",
    "CONFIG = {\n",
    "    \"text_encoder\": \"DeepChem/ChemBERTa-100M-MLM\",\n",
    "    \"max_length\": 256,\n",
    "    \"node_dim\": 34, \"edge_dim\": 7,\n",
    "    \"graph_hidden\": 256, \"graph_layers\": 4,\n",
    "    \"desc_dim_in\": 256, \"desc_hidden\": 256,\n",
    "    \"fusion_dim\": 256, \"text_proj_dim\": 256,\n",
    "    \"fusion_heads\": 4, \"fusion_dropout\": 0.10,\n",
    "    \"head_hidden\": 512,\n",
    "    \"dropout\": 0.20,\n",
    "    \"num_labels\": 12,\n",
    "\n",
    "    # training\n",
    "    \"batch_size\": 24, \"grad_accum_steps\": 2,\n",
    "    \"lr_others\": 1e-3, \"lr_text\": 2e-6,         # gentler text LR\n",
    "    \"weight_decay\": 2.5e-4,\n",
    "    \"epochs_max\": 120, \"stageA_freeze_text_epochs\": 8,\n",
    "    \"stageB_unfreeze_last_layers\": 2,           # gentler unfreeze\n",
    "    \"text_llrd\": [0.4, 1.0],                    # LLRD multipliers last-2\n",
    "    \"warmup_ratio\": 0.12,\n",
    "    \"early_stop_patience\": 20,\n",
    "    \"use_ema\": True, \"stageC_epochs\": 2,\n",
    "\n",
    "    # augmentation / tta\n",
    "    \"train_smiles_enum_p\": 0.3,\n",
    "    \"tta_enum_n\": 5,\n",
    "\n",
    "    # I/O\n",
    "    \"log_path\": str(LOGS / \"train_log.jsonl\")\n",
    "}\n",
    "\n",
    "# save config\n",
    "(CKPT / \"config_dualenc_v3.json\").write_text(json.dumps(CONFIG, indent=2))\n",
    "print(f\"[V3] Device: {'cuda' if torch.cuda.is_available() else 'cpu'} | torch={torch.__version__}\")\n",
    "print(f\"[V3] Checkpoints: {CKPT}\")\n",
    "print(f\"[V3] Results    : {RES}\")\n",
    "print(f\"[V3] Logs       : {CONFIG['log_path']}\")\n",
    "print(f\"[V3] Seeds      : {SEEDS}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01be3ff7",
   "metadata": {},
   "source": [
    "## 2: Data & Featurization (SMILES → RDKit → PyG, Tokenizer, Enumeration, Loaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3379885",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'tox21_dualenc_v1\\\\data\\\\labels.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     20\u001b[39m X_desc  = np.load(DESC/\u001b[33m\"\u001b[39m\u001b[33mdesc_selected.npy\u001b[39m\u001b[33m\"\u001b[39m).astype(np.float32)\n\u001b[32m     21\u001b[39m splits  = json.loads((SPL/\u001b[33m\"\u001b[39m\u001b[33msplits.json\u001b[39m\u001b[33m\"\u001b[39m).read_text())\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m labels  = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA\u001b[49m\u001b[43m/\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlabels.npy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m.astype(np.float32)         \u001b[38;5;66;03m# shape (N,12)\u001b[39;00m\n\u001b[32m     23\u001b[39m mask    = np.load(DATA/\u001b[33m\"\u001b[39m\u001b[33mmask.npy\u001b[39m\u001b[33m\"\u001b[39m).astype(np.float32)           \u001b[38;5;66;03m# 1 if label present else 0\u001b[39;00m\n\u001b[32m     24\u001b[39m smiles  = (DATA/\u001b[33m\"\u001b[39m\u001b[33msmiles_all.txt\u001b[39m\u001b[33m\"\u001b[39m).read_text(encoding=\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m).splitlines()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\lord\\Lib\\site-packages\\numpy\\lib\\npyio.py:427\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[39m\n\u001b[32m    425\u001b[39m     own_fid = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m427\u001b[39m     fid = stack.enter_context(\u001b[38;5;28mopen\u001b[39m(os_fspath(file), \u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m    428\u001b[39m     own_fid = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'tox21_dualenc_v1\\\\data\\\\labels.npy'"
     ]
    }
   ],
   "source": [
    "import json, os, warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict\n",
    "\n",
    "from rdkit import Chem\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Data as GeoData, Batch as GeoBatch\n",
    "\n",
    "ROOT = Path(\"tox21_dualenc_v1\")\n",
    "DATA = ROOT / \"data\"\n",
    "DESC = DATA / \"descriptors\"\n",
    "SPL  = DATA / \"splits\"\n",
    "CKPT = ROOT / \"models\" / \"checkpoints_v3\"\n",
    "\n",
    "# ---- load core arrays prepared earlier in your repo\n",
    "X_desc  = np.load(DESC/\"desc_selected.npy\").astype(np.float32)\n",
    "splits  = json.loads((SPL/\"splits.json\").read_text())\n",
    "labels  = np.load(DATA/\"labels.npy\").astype(np.float32)         # shape (N,12)\n",
    "mask    = np.load(DATA/\"mask.npy\").astype(np.float32)           # 1 if label present else 0\n",
    "smiles  = (DATA/\"smiles_all.txt\").read_text(encoding=\"utf-8\").splitlines()\n",
    "assert len(smiles) == labels.shape[0] == mask.shape[0] == X_desc.shape[0]\n",
    "\n",
    "N, L = labels.shape\n",
    "print(f\"[V3] Loaded: N={N}, L={L}, desc_dim={X_desc.shape[1]}, smiles={len(smiles)}\")\n",
    "\n",
    "# build atom/bond featurizers (34 / 7)\n",
    "ATOM_LIST = [\"H\",\"B\",\"C\",\"N\",\"O\",\"F\",\"Si\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "HYB_LIST  = [Chem.rdchem.HybridizationType.SP,\n",
    "             Chem.rdchem.HybridizationType.SP2,\n",
    "             Chem.rdchem.HybridizationType.SP3,\n",
    "             Chem.rdchem.HybridizationType.SP3D,\n",
    "             Chem.rdchem.HybridizationType.SP3D2]\n",
    "def one_hot_with_other(x, choices):\n",
    "    return [1 if x == c else 0 for c in choices] + [0 if x in choices else 1]\n",
    "def one_hot_index(idx, size):\n",
    "    v = [0]*size\n",
    "    if 0 <= idx < size: v[idx] = 1\n",
    "    return v\n",
    "def atom_features(atom: Chem.Atom):\n",
    "    sym      = atom.GetSymbol()\n",
    "    f_type   = one_hot_with_other(sym, ATOM_LIST)           # 13\n",
    "    deg      = atom.GetTotalDegree()\n",
    "    f_deg    = one_hot_index(min(deg,6), 7)                 # 7\n",
    "    charge   = atom.GetFormalCharge()\n",
    "    charge_map = {-2:0,-1:1,0:2,1:3,2:4}\n",
    "    f_charge = one_hot_index(charge_map.get(charge,5), 6)   # 6\n",
    "    hyb      = atom.GetHybridization()\n",
    "    f_hyb    = one_hot_with_other(hyb, HYB_LIST)            # 6\n",
    "    f_flags  = [int(atom.GetIsAromatic()), int(atom.IsInRing())]  # 2\n",
    "    return f_type + f_deg + f_charge + f_hyb + f_flags      # 34\n",
    "def bond_features(bond: Chem.Bond):\n",
    "    types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "             Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    f_type  = one_hot_with_other(bond.GetBondType(), types)   # 5\n",
    "    f_flags = [int(bond.GetIsConjugated()), int(bond.IsInRing())]  # 2\n",
    "    return f_type + f_flags                                   # 7\n",
    "\n",
    "def build_graph(s):\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None or m.GetNumAtoms()==0: \n",
    "        return None\n",
    "    x = torch.tensor([atom_features(a) for a in m.GetAtoms()], dtype=torch.float32)\n",
    "    ei_src, ei_dst, eattr = [], [], []\n",
    "    for b in m.GetBonds():\n",
    "        u, v = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        f = bond_features(b)\n",
    "        ei_src += [u,v]; ei_dst += [v,u]; eattr += [f,f]\n",
    "    if len(ei_src)==0:\n",
    "        edge_index = torch.zeros((2,1), dtype=torch.long)\n",
    "        edge_attr  = torch.zeros((1,7), dtype=torch.float32)\n",
    "    else:\n",
    "        edge_index = torch.tensor([ei_src, ei_dst], dtype=torch.long)\n",
    "        edge_attr  = torch.tensor(eattr, dtype=torch.float32)\n",
    "    return GeoData(x=x, edge_index=edge_index, edge_attr=edge_attr, invalid=0)\n",
    "\n",
    "# cache graphs\n",
    "GRAPH_DIR = DATA / \"graphs\"\n",
    "GRAPH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "graph_pt = GRAPH_DIR / \"graphs_all_pyg_safe.pt\"\n",
    "if graph_pt.exists():\n",
    "    graphs = torch.load(graph_pt)\n",
    "else:\n",
    "    graphs = []\n",
    "    bad = 0\n",
    "    for s in smiles:\n",
    "        g = build_graph(s)\n",
    "        if g is None: bad += 1\n",
    "        graphs.append(g if g is not None else GeoData(\n",
    "            x=torch.zeros((1,34), dtype=torch.float32),\n",
    "            edge_index=torch.zeros((2,1), dtype=torch.long),\n",
    "            edge_attr=torch.zeros((1,7), dtype=torch.float32),\n",
    "            invalid=1\n",
    "        ))\n",
    "    torch.save(graphs, graph_pt)\n",
    "    print(f\"[V3] Built graphs. Bad SMILES: {bad}. Saved → {graph_pt}\")\n",
    "\n",
    "# pos_weight\n",
    "train_idx = np.array(splits[\"train\"], dtype=int)\n",
    "Yt = labels[train_idx]; Mt = mask[train_idx]\n",
    "pos = (Yt*Mt).sum(axis=0); neg = (Mt - Yt*Mt).sum(axis=0)\n",
    "pos_weight = np.where(pos>0, neg/np.clip(pos, 1, None), 1.0).astype(np.float32)\n",
    "np.save(CKPT / \"pos_weight.npy\", pos_weight)\n",
    "print(f\"[V3] Saved pos_weight.npy → {CKPT/'pos_weight.npy'}\")\n",
    "\n",
    "# dataset\n",
    "@dataclass\n",
    "class Row:\n",
    "    idx: int\n",
    "    smiles: str\n",
    "    desc: np.ndarray\n",
    "    y: np.ndarray\n",
    "    m: np.ndarray\n",
    "    g: GeoData\n",
    "\n",
    "class ToxDS(Dataset):\n",
    "    def __init__(self, rows: List[Row]): self.rows = rows\n",
    "    def __len__(self): return len(self.rows)\n",
    "    def __getitem__(self, i):\n",
    "        r = self.rows[i]\n",
    "        return {\"idx\": r.idx, \"smiles\": r.smiles, \"desc\": torch.from_numpy(r.desc).float(),\n",
    "                \"y\": torch.from_numpy(r.y).float(), \"mask\": torch.from_numpy(r.m).float(),\n",
    "                \"graph\": r.g}\n",
    "\n",
    "# build splits\n",
    "def rows_from_idx(idxs):\n",
    "    return [Row(i, smiles[i], X_desc[i], labels[i], mask[i], graphs[i]) for i in idxs]\n",
    "\n",
    "ds_train = ToxDS(rows_from_idx(train_idx))\n",
    "ds_val   = ToxDS(rows_from_idx(np.array(splits[\"val\"], dtype=int)))\n",
    "ds_test  = ToxDS(rows_from_idx(np.array(splits[\"test\"], dtype=int)))\n",
    "\n",
    "# collate with tokenizer (created later in Cell 3; for now we only check shapes here)\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-100M-MLM\")\n",
    "\n",
    "def collate(batch):\n",
    "    sms = [b[\"smiles\"] for b in batch]\n",
    "    toks = tok(sms, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "    g_batch = GeoBatch.from_data_list([b[\"graph\"] for b in batch])\n",
    "    return {\"idx\": torch.tensor([b[\"idx\"] for b in batch]),\n",
    "            \"smiles\": sms,\n",
    "            \"tok\": {\"input_ids\": toks[\"input_ids\"], \"attention_mask\": toks[\"attention_mask\"]},\n",
    "            \"graph\": g_batch,\n",
    "            \"desc\": torch.stack([b[\"desc\"] for b in batch]),\n",
    "            \"y\": torch.stack([b[\"y\"] for b in batch]),\n",
    "            \"mask\": torch.stack([b[\"mask\"] for b in batch])}\n",
    "\n",
    "def make_loader(ds, shuffle):\n",
    "    return DataLoader(ds, batch_size=CONFIG[\"batch_size\"], shuffle=shuffle,\n",
    "                      num_workers=0, collate_fn=collate, pin_memory=True)\n",
    "\n",
    "train_loader = make_loader(ds_train, True)\n",
    "val_loader   = make_loader(ds_val, False)\n",
    "test_loader  = make_loader(ds_test, False)\n",
    "\n",
    "# quick sanity\n",
    "b = next(iter(train_loader))\n",
    "print(f\"[V3] ===== Dataloader Sanity =====\")\n",
    "print(\"train/val/test sizes :\", len(ds_train), len(ds_val), len(ds_test))\n",
    "print(\"Tok shapes           :\", b[\"tok\"][\"input_ids\"].shape, b[\"tok\"][\"attention_mask\"].shape)\n",
    "print(\"Graph shapes         :\", b[\"graph\"].x.shape, b[\"graph\"].edge_index.shape, b[\"graph\"].edge_attr.shape)\n",
    "print(\"Desc/labels          :\", b[\"desc\"].shape, b[\"y\"].shape, b[\"mask\"].shape)\n",
    "\n",
    "# save smiles index (for inference descriptor lookup later)\n",
    "(DATA / \"smiles_all.txt\").write_text(\"\\n\".join(smiles), encoding=\"utf-8\")\n",
    "with open(DATA / \"smiles_index.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({sm: i for i, sm in enumerate(smiles)}, f)\n",
    "print(f\"[V3] Saved smiles_all.txt and smiles_index.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0468fe71",
   "metadata": {},
   "source": [
    "## 3: Model Modules (GIN(E), Cross-Attn, AttentionPool, DualEncoderXAttn, EMA) + Dry-Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67167d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V3] Instantiating DualEncoderXAttn…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V3] Params — total: 94.06M | trainable now: 94.06M\n",
      "[V3] Dry-run OK — logits shape: (24, 12) (expect (B, 12))\n",
      "[V3] Saved initial checkpoint → tox21_dualenc_v1\\models\\checkpoints_v3\\init_state.pt\n",
      "[V3] Saved config copy → tox21_dualenc_v1\\models\\checkpoints_v3\\config_dualenc_v3.copy.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional, List\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "ROOT     = Path(\"tox21_dualenc_v1\")\n",
    "CKPT_DIR = ROOT / \"models\" / \"checkpoints_v3\"\n",
    "CFG_PATH = CKPT_DIR / \"config_dualenc_v3.json\"\n",
    "CONFIG   = json.loads(CFG_PATH.read_text())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----- Attention Pool (tokens)\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.score = nn.Linear(dim, 1)\n",
    "    def forward(self, X, mask=None):   # X: (B,L,D), mask: (B,L) bool or 0/1\n",
    "        s = self.score(X).squeeze(-1)  # (B,L)\n",
    "        if mask is not None:\n",
    "            mask = mask.bool()\n",
    "            s = s.masked_fill(~mask, -1e9)\n",
    "        a = torch.softmax(s, dim=1)    # (B,L)\n",
    "        return torch.bmm(a.unsqueeze(1), X).squeeze(1)  # (B,D)\n",
    "\n",
    "# ----- Cross-Attention Block (Text <- Graph)\n",
    "class CrossAttnBlock(nn.Module):\n",
    "    def __init__(self, dim=256, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn  = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ffn   = nn.Sequential(\n",
    "            nn.Linear(dim, 4*dim), nn.GELU(), nn.Dropout(dropout),\n",
    "            nn.Linear(4*dim, dim),\n",
    "        )\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.drop  = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, Q_text, K_graph, V_graph, key_padding_mask=None):\n",
    "        # Q_text: (B,T,D); K/V_graph: (B,N,D); key_padding_mask: (B,N) True=PAD\n",
    "        attn_out, _ = self.attn(Q_text, K_graph, V_graph, key_padding_mask=key_padding_mask)\n",
    "        x  = self.norm1(Q_text + self.drop(attn_out))\n",
    "        ff = self.ffn(x)\n",
    "        y  = self.norm2(x + self.drop(ff))\n",
    "        return y  # (B,T,D)\n",
    "\n",
    "# ----- GIN(E) Encoder\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden=256, layers=4, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.bns    = nn.ModuleList()\n",
    "        in_dim = node_dim\n",
    "        for _ in range(layers):\n",
    "            mlp = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(hidden, hidden),\n",
    "            )\n",
    "            conv = GINEConv(mlp, edge_dim=edge_dim)  # learns an edge MLP internally\n",
    "            self.layers.append(conv)\n",
    "            self.bns.append(nn.BatchNorm1d(hidden))\n",
    "            in_dim = hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr, batch_index):\n",
    "        # x: (sumN, node_dim); edge_index: (2,E); edge_attr: (E, edge_dim); batch_index: (sumN,)\n",
    "        h = x\n",
    "        for conv, bn in zip(self.layers, self.bns):\n",
    "            h = conv(h, edge_index, edge_attr)  # (sumN, hidden)\n",
    "            h = bn(h)\n",
    "            h = F.relu(h)\n",
    "            h = self.dropout(h)\n",
    "        z_graph = global_mean_pool(h, batch_index)  # (B, hidden)\n",
    "        return h, z_graph\n",
    "\n",
    "# ----- Dual-Encoder with Cross-Attn + Descriptor branch + Head\n",
    "class DualEncoderXAttn(nn.Module):\n",
    "    def __init__(self, config: dict):\n",
    "        super().__init__()\n",
    "        self.cfg = config\n",
    "\n",
    "        # Text encoder (ChemBERTa) + projection to fusion_dim\n",
    "        self.text = AutoModel.from_pretrained(config[\"text_encoder\"])\n",
    "        self.text_proj = nn.Linear(self.text.config.hidden_size, int(config[\"text_proj_dim\"]))\n",
    "\n",
    "        # Graph encoder (GIN(E))\n",
    "        self.gnn = GINEncoder(\n",
    "            node_dim=int(config[\"node_dim\"]),\n",
    "            edge_dim=int(config[\"edge_dim\"]),\n",
    "            hidden=int(config[\"graph_hidden\"]),\n",
    "            layers=int(config[\"graph_layers\"]),\n",
    "            dropout=float(config[\"dropout\"]),\n",
    "        )\n",
    "\n",
    "        # Cross-attn: Text queries Graph\n",
    "        self.xattn = CrossAttnBlock(\n",
    "            dim=int(config[\"fusion_dim\"]),\n",
    "            heads=int(config[\"fusion_heads\"]),\n",
    "            dropout=float(config[\"fusion_dropout\"]),\n",
    "        )\n",
    "\n",
    "        # Pools & descriptor MLP\n",
    "        self.tpool = AttentionPool(dim=int(config[\"fusion_dim\"]))\n",
    "        self.dmlp  = nn.Sequential(\n",
    "            nn.Linear(int(config[\"desc_dim_in\"]), int(config[\"desc_hidden\"])),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(float(config[\"dropout\"])),\n",
    "        )\n",
    "\n",
    "        fused_in = int(config[\"fusion_dim\"]) + int(config[\"graph_hidden\"]) + int(config[\"desc_hidden\"])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(fused_in, int(config[\"head_hidden\"])),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(float(config[\"dropout\"])),\n",
    "            nn.Linear(int(config[\"head_hidden\"]), int(config[\"num_labels\"])),\n",
    "        )\n",
    "\n",
    "        # Basic sanity on dims\n",
    "        assert int(config[\"text_proj_dim\"]) == int(config[\"fusion_dim\"]), \"text_proj_dim must equal fusion_dim for cross-attn\"\n",
    "        assert fused_in == int(config[\"fusion_dim\"]) + int(config[\"graph_hidden\"]) + int(config[\"desc_hidden\"]), \"fused dim mismatch\"\n",
    "        assert int(config[\"num_labels\"]) > 0, \"num_labels must be > 0\"\n",
    "\n",
    "    def forward(self, batch):\n",
    "        # batch: dict from our collate_fn\n",
    "        g = batch[\"graph\"]              # PyG Batch\n",
    "        tok = batch[\"tok\"]              # dict: input_ids, attention_mask\n",
    "        desc = batch[\"desc\"]            # (B, D_desc)\n",
    "\n",
    "        # Text encoder\n",
    "        out = self.text(input_ids=tok[\"input_ids\"].to(self.text.device),\n",
    "                        attention_mask=tok[\"attention_mask\"].to(self.text.device))\n",
    "        # hidden states (B,T,H); use last hidden state\n",
    "        H_text = out.last_hidden_state\n",
    "        t_proj = self.text_proj(H_text)                        # (B,T, fusion_dim)\n",
    "\n",
    "        # Graph encoder\n",
    "        node_h, z_graph = self.gnn(g.x, g.edge_index, g.edge_attr, g.batch)  # (sumN,hidden), (B,hidden)\n",
    "        # Dense pack nodes per graph for cross-attn\n",
    "        node_dense, node_mask = to_dense_batch(node_h, g.batch)              # (B,Nmax,hidden), (B,Nmax) True=valid\n",
    "        key_padding_mask = ~node_mask                                        # True where PAD\n",
    "\n",
    "        # Cross-attn: Text (Q) ← Graph (K/V)\n",
    "        fused_text = self.xattn(t_proj, node_dense, node_dense, key_padding_mask=key_padding_mask)  # (B,T,fusion_dim)\n",
    "\n",
    "        # Attention-pool text (mask pads from tokenizer)\n",
    "        text_mask_bool = batch[\"tok\"][\"attention_mask\"].bool().to(fused_text.device)\n",
    "        z_text = self.tpool(fused_text, mask=text_mask_bool)   # (B, fusion_dim)\n",
    "\n",
    "        # Descriptors branch\n",
    "        z_desc = self.dmlp(desc.to(fused_text.device))         # (B, desc_hidden)\n",
    "\n",
    "        # Concatenate and head\n",
    "        z = torch.cat([z_text, z_graph, z_desc], dim=-1)       # (B, fusion+graph+desc) = (B, 256+256+256)\n",
    "        logits = self.head(z)                                  # (B, num_labels)\n",
    "        return logits\n",
    "\n",
    "# ----- EMA utility (for Stage C polish)\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.detach().clone()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        for name, param in model.named_parameters():\n",
    "            if not param.requires_grad: \n",
    "                continue\n",
    "            assert name in self.shadow\n",
    "            new_avg = (1.0 - self.decay) * param.detach() + self.decay * self.shadow[name]\n",
    "            self.shadow[name] = new_avg.clone()\n",
    "\n",
    "    def apply_shadow(self, model: nn.Module):\n",
    "        self.backup = {}\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.shadow:\n",
    "                self.backup[name] = param.detach().clone()\n",
    "                param.data.copy_(self.shadow[name].data)\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        for name, param in model.named_parameters():\n",
    "            if name in self.backup:\n",
    "                param.data.copy_(self.backup[name].data)\n",
    "        self.backup = {}\n",
    "\n",
    "# -----------------------------\n",
    "# Instantiate model & sanity\n",
    "# -----------------------------\n",
    "print(\"[V3] Instantiating DualEncoderXAttn…\")\n",
    "model = DualEncoderXAttn(CONFIG).to(device)\n",
    "\n",
    "# Freeze sanity for Stage A later; for now just count params\n",
    "def count_params(m):\n",
    "    total = sum(p.numel() for p in m.parameters())\n",
    "    trainable = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "    return total, trainable\n",
    "\n",
    "total_p, trainable_p = count_params(model)\n",
    "print(f\"[V3] Params — total: {total_p/1e6:.2f}M | trainable now: {trainable_p/1e6:.2f}M\")\n",
    "\n",
    "# -----------------------------\n",
    "# Dry-run on one batch\n",
    "# -----------------------------\n",
    "try:\n",
    "    b = next(iter(train_loader))\n",
    "except NameError as e:\n",
    "    raise RuntimeError(\"train_loader not found. Please run Cell 2 first.\") from e\n",
    "\n",
    "# Move batch tensors that are used directly\n",
    "b[\"desc\"] = b[\"desc\"].to(device)\n",
    "b[\"y\"]    = b[\"y\"].to(device)\n",
    "b[\"mask\"] = b[\"mask\"].to(device)\n",
    "b[\"graph\"] = b[\"graph\"].to(device)\n",
    "b[\"tok\"][\"input_ids\"] = b[\"tok\"][\"input_ids\"].to(device)\n",
    "b[\"tok\"][\"attention_mask\"] = b[\"tok\"][\"attention_mask\"].to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(b)\n",
    "print(f\"[V3] Dry-run OK — logits shape: {tuple(logits.shape)} (expect (B, {CONFIG['num_labels']}))\")\n",
    "\n",
    "# Numerical sanity\n",
    "if torch.isnan(logits).any():\n",
    "    raise ValueError(\"Found NaNs in logits during dry-run\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save initial weights & config copy\n",
    "# -----------------------------\n",
    "init_ckpt = CKPT_DIR / \"init_state.pt\"\n",
    "torch.save({\"state_dict\": model.state_dict(), \"config\": CONFIG}, init_ckpt)\n",
    "print(f\"[V3] Saved initial checkpoint → {init_ckpt}\")\n",
    "\n",
    "cfg_copy = CKPT_DIR / \"config_dualenc_v3.copy.json\"\n",
    "with open(cfg_copy, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "print(f\"[V3] Saved config copy → {cfg_copy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27521628",
   "metadata": {},
   "source": [
    "## 4: Training Loop (Stage A/B/C, LLRD, Cosine Warmup, Early Stop, EMA) + Checkpointing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4dd776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V3] Patched: torch.cuda.amp → torch.amp (torch 2.6 compatible).\n",
      "[V3] Patched: AttentionPool.forward AMP-safe masking.\n",
      "\n",
      "[V3] ===== Stage A: Freeze text =====\n",
      "[Stage A] Trainable params: 1.93M\n",
      "[StageA][1/8] loss=1.1353 | val ROC=0.7502 PR=0.2638 | best=0.7502 (epoch 1)\n",
      "[StageA][2/8] loss=1.0099 | val ROC=0.7910 PR=0.3067 | best=0.7910 (epoch 2)\n",
      "[StageA][3/8] loss=0.8605 | val ROC=0.8037 PR=0.3461 | best=0.8037 (epoch 3)\n",
      "[StageA][4/8] loss=0.7555 | val ROC=0.7942 PR=0.3444 | best=0.8037 (epoch 3)\n",
      "[StageA][5/8] loss=0.6802 | val ROC=0.7938 PR=0.3537 | best=0.8037 (epoch 3)\n",
      "[StageA][6/8] loss=0.6191 | val ROC=0.7882 PR=0.3479 | best=0.8037 (epoch 3)\n",
      "[StageA][7/8] loss=0.5629 | val ROC=0.7883 PR=0.3561 | best=0.8037 (epoch 3)\n",
      "[StageA][8/8] loss=0.5443 | val ROC=0.7880 PR=0.3564 | best=0.8037 (epoch 3)\n",
      "\n",
      "[V3] ===== Stage B: Unfreeze last-N with LLRD =====\n",
      "[Stage B] Unfrozen text layers: 4\n",
      "[Stage B] Trainable params: 30.28M\n",
      "[StageB] Unfrozen last-4 text layers\n",
      "[StageB][1/120] loss=0.5445 | val ROC=0.7884 PR=0.3559 | best=0.7884 (epoch 1)\n",
      "[StageB][2/120] loss=0.5598 | val ROC=0.7797 PR=0.3540 | best=0.7884 (epoch 1)\n",
      "[StageB][3/120] loss=0.5435 | val ROC=0.7780 PR=0.3582 | best=0.7884 (epoch 1)\n",
      "[StageB][4/120] loss=0.5276 | val ROC=0.7760 PR=0.3548 | best=0.7884 (epoch 1)\n",
      "[StageB][5/120] loss=0.5178 | val ROC=0.7789 PR=0.3672 | best=0.7884 (epoch 1)\n",
      "[StageB][6/120] loss=0.5140 | val ROC=0.7710 PR=0.3535 | best=0.7884 (epoch 1)\n",
      "[StageB][7/120] loss=0.5004 | val ROC=0.7820 PR=0.3531 | best=0.7884 (epoch 1)\n",
      "[StageB][8/120] loss=0.4796 | val ROC=0.7745 PR=0.3526 | best=0.7884 (epoch 1)\n",
      "[StageB][9/120] loss=0.4624 | val ROC=0.7603 PR=0.3445 | best=0.7884 (epoch 1)\n",
      "[StageB][10/120] loss=0.4480 | val ROC=0.7686 PR=0.3526 | best=0.7884 (epoch 1)\n",
      "[StageB][11/120] loss=0.4379 | val ROC=0.7677 PR=0.3475 | best=0.7884 (epoch 1)\n",
      "[StageB][12/120] loss=0.4316 | val ROC=0.7426 PR=0.3417 | best=0.7884 (epoch 1)\n",
      "[StageB][13/120] loss=0.4254 | val ROC=0.7624 PR=0.3534 | best=0.7884 (epoch 1)\n",
      "[StageB][14/120] loss=0.4066 | val ROC=0.7497 PR=0.3531 | best=0.7884 (epoch 1)\n",
      "[StageB][15/120] loss=0.3991 | val ROC=0.7547 PR=0.3411 | best=0.7884 (epoch 1)\n",
      "[StageB][16/120] loss=0.3844 | val ROC=0.7498 PR=0.3547 | best=0.7884 (epoch 1)\n",
      "[StageB][17/120] loss=0.3597 | val ROC=0.7559 PR=0.3361 | best=0.7884 (epoch 1)\n",
      "[StageB][18/120] loss=0.3441 | val ROC=0.7501 PR=0.3517 | best=0.7884 (epoch 1)\n",
      "[StageB][19/120] loss=0.3383 | val ROC=0.7561 PR=0.3572 | best=0.7884 (epoch 1)\n",
      "[StageB][20/120] loss=0.3305 | val ROC=0.7444 PR=0.3664 | best=0.7884 (epoch 1)\n",
      "[StageB][21/120] loss=0.2982 | val ROC=0.7594 PR=0.3671 | best=0.7884 (epoch 1)\n",
      "[StageB] Early stopping: no improvement for 20 epochs (best @ epoch 1 = 0.7884)\n",
      "\n",
      "[V3] ===== Stage C: EMA polish =====\n",
      "[StageC-EMA] Unfrozen last-4 text layers\n",
      "[StageC-EMA][1/2] loss=0.2911 | val ROC=0.7499 PR=0.3482 | best=0.7499 (epoch 1)\n",
      "[StageC-EMA][2/2] loss=0.2419 | val ROC=0.7505 PR=0.3545 | best=0.7505 (epoch 2)\n",
      "\n",
      "[V3] ===== Training complete =====\n",
      "{\n",
      "  \"bestA\": 0.8037017371631987,\n",
      "  \"bestB\": 0.7883954684945326,\n",
      "  \"bestC\": 0.7504528252545786,\n",
      "  \"final_val_macro_roc_auc\": 0.7504528252545786,\n",
      "  \"final_val_macro_pr_auc\": 0.3544651199993074\n",
      "}\n",
      "Saved: tox21_dualenc_v1\\models\\checkpoints_v3\\best.pt, tox21_dualenc_v1\\models\\checkpoints_v3\\last.pt, tox21_dualenc_v1\\models\\checkpoints_v3\\best_summary.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time, warnings\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.cuda.amp as old_amp  # we'll map to torch.amp below\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from transformers import get_cosine_schedule_with_warmup, AutoConfig\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & config (extend training a bit)\n",
    "# -----------------------------\n",
    "ROOT     = Path(\"tox21_dualenc_v1\")\n",
    "CKPT_DIR = ROOT / \"models\" / \"checkpoints_v3\"\n",
    "RESULTS  = ROOT / \"results\" / \"v3\"\n",
    "\n",
    "CFG_PATH = CKPT_DIR / \"config_dualenc_v3.json\"\n",
    "CONFIG   = json.loads(CFG_PATH.read_text())\n",
    "\n",
    "# extend Stage B and adjust patience/warmup\n",
    "CONFIG[\"epochs_max\"] = 120\n",
    "CONFIG[\"early_stop_patience\"] = 20\n",
    "CONFIG[\"warmup_ratio\"] = 0.12\n",
    "CFG_PATH.write_text(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "LOG_PATH = Path(CONFIG[\"log_path\"])\n",
    "LOG_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Guards: require earlier cells\n",
    "# -----------------------------\n",
    "for name in [\"train_loader\", \"val_loader\", \"test_loader\", \"DualEncoderXAttn\", \"AttentionPool\", \"GINEncoder\", \"CrossAttnBlock\", \"EMA\"]:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"{name} not found — please run Cells 2 and 3 first.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Patch 1: AMP deprecation (torch 2.6 compatible)\n",
    "# -----------------------------\n",
    "from torch.amp import GradScaler as NewGradScaler, autocast as new_autocast\n",
    "\n",
    "def _GradScaler(enabled=True):\n",
    "    # torch 2.6: GradScaler(enabled=...) is valid; no device_type kwarg\n",
    "    return NewGradScaler(enabled=enabled)\n",
    "\n",
    "def _autocast(enabled=True):\n",
    "    # torch 2.6: autocast(device_type, enabled=...) signature; pass \"cuda\"\n",
    "    return new_autocast(\"cuda\", enabled=enabled)\n",
    "\n",
    "old_amp.GradScaler = _GradScaler\n",
    "old_amp.autocast  = _autocast\n",
    "print(\"[V3] Patched: torch.cuda.amp → torch.amp (torch 2.6 compatible).\")\n",
    "\n",
    "# -----------------------------\n",
    "# Patch 2: AMP-safe masking in AttentionPool (avoid fp16 overflow)\n",
    "# -----------------------------\n",
    "def _attnpool_forward_amp_safe(self, X, mask=None):  # X: (B,L,D), mask: (B,L)\n",
    "    s = self.score(X).squeeze(-1)  # (B,L)\n",
    "    if mask is not None:\n",
    "        mask = mask.to(dtype=torch.bool, device=s.device)\n",
    "        neg_large = torch.tensor(-1e4 if s.dtype == torch.float16 else -1e9, dtype=s.dtype, device=s.device)\n",
    "        s = s.masked_fill(~mask, neg_large)\n",
    "    a = torch.softmax(s, dim=1)\n",
    "    return torch.bmm(a.unsqueeze(1), X).squeeze(1)\n",
    "\n",
    "AttentionPool.forward = _attnpool_forward_amp_safe\n",
    "print(\"[V3] Patched: AttentionPool.forward AMP-safe masking.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Patch 3: silence pooler warning by forcing add_pooling_layer=False in model init\n",
    "# -----------------------------\n",
    "orig_init = DualEncoderXAttn.__init__\n",
    "\n",
    "def patched_init(self, config: dict):\n",
    "    nn.Module.__init__(self)\n",
    "    self.cfg = config\n",
    "\n",
    "    # Text encoder without pooler\n",
    "    txt_cfg = AutoConfig.from_pretrained(config[\"text_encoder\"])\n",
    "    if hasattr(txt_cfg, \"add_pooling_layer\"):\n",
    "        txt_cfg.add_pooling_layer = False\n",
    "    from transformers import AutoModel\n",
    "    self.text = AutoModel.from_pretrained(config[\"text_encoder\"], config=txt_cfg)\n",
    "    self.text_proj = nn.Linear(self.text.config.hidden_size, int(config[\"text_proj_dim\"]))\n",
    "\n",
    "    # Graph encoder\n",
    "    self.gnn = GINEncoder(\n",
    "        node_dim=int(config[\"node_dim\"]),\n",
    "        edge_dim=int(config[\"edge_dim\"]),\n",
    "        hidden=int(config[\"graph_hidden\"]),\n",
    "        layers=int(config[\"graph_layers\"]),\n",
    "        dropout=float(config[\"dropout\"]),\n",
    "    )\n",
    "\n",
    "    # Cross-attn\n",
    "    self.xattn = CrossAttnBlock(\n",
    "        dim=int(config[\"fusion_dim\"]),\n",
    "        heads=int(config[\"fusion_heads\"]),\n",
    "        dropout=float(config[\"fusion_dropout\"]),\n",
    "    )\n",
    "\n",
    "    # Pools & descriptor MLP\n",
    "    self.tpool = AttentionPool(dim=int(config[\"fusion_dim\"]))\n",
    "    self.dmlp  = nn.Sequential(\n",
    "        nn.Linear(int(config[\"desc_dim_in\"]), int(config[\"desc_hidden\"])),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(float(config[\"dropout\"])),\n",
    "    )\n",
    "\n",
    "    fused_in = int(config[\"fusion_dim\"]) + int(config[\"graph_hidden\"]) + int(config[\"desc_hidden\"])\n",
    "    self.head = nn.Sequential(\n",
    "        nn.Linear(fused_in, int(config[\"head_hidden\"])),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(float(config[\"dropout\"])),\n",
    "        nn.Linear(int(config[\"head_hidden\"]), int(config[\"num_labels\"])),\n",
    "    )\n",
    "\n",
    "    assert int(config[\"text_proj_dim\"]) == int(config[\"fusion_dim\"]), \"text_proj_dim must equal fusion_dim\"\n",
    "    assert int(config[\"num_labels\"]) > 0, \"num_labels must be > 0\"\n",
    "\n",
    "DualEncoderXAttn.__init__ = patched_init\n",
    "model = DualEncoderXAttn(CONFIG).to(device)\n",
    "DualEncoderXAttn.__init__ = orig_init  # restore\n",
    "\n",
    "# -----------------------------\n",
    "# Loss with class imbalance & mask\n",
    "# -----------------------------\n",
    "pos_weight = torch.tensor(np.load(CKPT_DIR / \"pos_weight.npy\"), dtype=torch.float32, device=device)\n",
    "assert pos_weight.numel() == int(CONFIG[\"num_labels\"]), f\"pos_weight size mismatch ({pos_weight.numel()} vs {CONFIG['num_labels']})\"\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "\n",
    "def loss_with_mask(logits, y, mask):\n",
    "    # logits,y,mask: (B,L)\n",
    "    base = criterion(logits, y)         # (B,L)\n",
    "    pw = pos_weight.view(1, -1)         # (1,L)\n",
    "    base = (y * base * pw) + ((1 - y) * base)  # reweight positives only\n",
    "    base = base * mask                  # ignore missing labels\n",
    "    denom = torch.clamp(mask.sum(), min=1.0)\n",
    "    return base.sum() / denom\n",
    "\n",
    "# -----------------------------\n",
    "# Freeze/unfreeze helpers\n",
    "# -----------------------------\n",
    "def freeze_text(m):\n",
    "    for p in m.text.parameters():\n",
    "        p.requires_grad = False\n",
    "\n",
    "def unfreeze_last_n_text_layers(m, n_last: int):\n",
    "    enc = getattr(m.text, \"encoder\", None)\n",
    "    if enc is None and hasattr(m.text, \"roberta\"):\n",
    "        enc = m.text.roberta.encoder\n",
    "    layers = getattr(enc, \"layer\", None)\n",
    "    if layers is None:\n",
    "        return 0\n",
    "    L = len(layers)\n",
    "    start = max(L - n_last, 0)\n",
    "    unfrozen = 0\n",
    "    for li in range(start, L):\n",
    "        for p in layers[li].parameters():\n",
    "            p.requires_grad = True\n",
    "        unfrozen += 1\n",
    "    return unfrozen\n",
    "\n",
    "def set_all_text_requires_grad(m, flag: bool):\n",
    "    for p in m.text.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "def count_trainable(m):\n",
    "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
    "\n",
    "# -----------------------------\n",
    "# Optimizers & schedulers\n",
    "# -----------------------------\n",
    "def build_optimizer_stageA(m):\n",
    "    other_params = [p for n,p in m.named_parameters() if p.requires_grad and not n.startswith(\"text.\")]\n",
    "    return AdamW(other_params, lr=float(CONFIG[\"lr_others\"]), weight_decay=float(CONFIG[\"weight_decay\"]))\n",
    "\n",
    "def build_optimizer_stageB_llrd(m):\n",
    "    groups = []\n",
    "    others = [p for n,p in m.named_parameters() if p.requires_grad and not n.startswith(\"text.\")]\n",
    "    if others:\n",
    "        groups.append({\"params\": others, \"lr\": float(CONFIG[\"lr_others\"]), \"weight_decay\": float(CONFIG[\"weight_decay\"])})\n",
    "\n",
    "    enc = getattr(m.text, \"encoder\", None)\n",
    "    if enc is None and hasattr(m.text, \"roberta\"):\n",
    "        enc = m.text.roberta.encoder\n",
    "    layers = getattr(enc, \"layer\", None)\n",
    "    assert layers is not None, \"Cannot access text encoder layers for LLRD\"\n",
    "    L = len(layers)\n",
    "    n_last = int(CONFIG[\"stageB_unfreeze_last_layers\"])\n",
    "    start = max(L - n_last, 0)\n",
    "    mults = list(CONFIG.get(\"text_llrd\", [0.5, 0.7, 0.9, 1.0]))\n",
    "    assert len(mults) == (L - start), \"LLRD multipliers mismatch with unfrozen layers\"\n",
    "\n",
    "    base = float(CONFIG[\"lr_text\"])\n",
    "    for li, mult in zip(range(start, L), mults):\n",
    "        params = [p for p in layers[li].parameters() if p.requires_grad]\n",
    "        if params:\n",
    "            groups.append({\"params\": params, \"lr\": base*float(mult), \"weight_decay\": float(CONFIG[\"weight_decay\"])})\n",
    "\n",
    "    return AdamW(groups)\n",
    "\n",
    "def build_scheduler(opt, steps_per_epoch, epochs, warmup_ratio):\n",
    "    total_steps  = max(1, steps_per_epoch * epochs)\n",
    "    warmup_steps = max(1, int(total_steps * warmup_ratio))\n",
    "    return get_cosine_schedule_with_warmup(opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "\n",
    "# -----------------------------\n",
    "# Evaluation\n",
    "# -----------------------------\n",
    "def evaluate(model, loader, msg=\"val\"):\n",
    "    model.eval()\n",
    "    all_logits, all_y, all_m = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            g = batch[\"graph\"].to(device)\n",
    "            tok = {k: v.to(device) for k,v in batch[\"tok\"].items()}\n",
    "            desc = batch[\"desc\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "            m = batch[\"mask\"].to(device)\n",
    "\n",
    "            logits = model({\"graph\": g, \"tok\": tok, \"desc\": desc})\n",
    "            all_logits.append(torch.sigmoid(logits).detach().cpu().numpy())\n",
    "            all_y.append(y.detach().cpu().numpy())\n",
    "            all_m.append(m.detach().cpu().numpy())\n",
    "\n",
    "    P = np.concatenate(all_logits, axis=0)\n",
    "    Y = np.concatenate(all_y, axis=0)\n",
    "    M = np.concatenate(all_m, axis=0)\n",
    "\n",
    "    per_roc, per_pr = [], []\n",
    "    for k in range(P.shape[1]):\n",
    "        valid = M[:,k] > 0.5\n",
    "        if valid.sum() < 2 or np.unique(Y[valid, k]).size < 2:\n",
    "            per_roc.append(np.nan); per_pr.append(np.nan); continue\n",
    "        try:\n",
    "            per_roc.append(roc_auc_score(Y[valid, k], P[valid, k]))\n",
    "        except:\n",
    "            per_roc.append(np.nan)\n",
    "        try:\n",
    "            per_pr.append(average_precision_score(Y[valid, k], P[valid, k]))\n",
    "        except:\n",
    "            per_pr.append(np.nan)\n",
    "\n",
    "    macro_roc = np.nanmean(per_roc)\n",
    "    macro_pr  = np.nanmean(per_pr)\n",
    "    return {\n",
    "        \"macro_roc_auc\": float(macro_roc),\n",
    "        \"macro_pr_auc\": float(macro_pr),\n",
    "        \"per_label_roc_auc\": per_roc,\n",
    "        \"per_label_pr_auc\": per_pr,\n",
    "        \"n_samples\": int(P.shape[0]),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# One training stage\n",
    "# -----------------------------\n",
    "def train_stage(model, stage_name, epochs, freeze_text_flag=None, unfreeze_last_n=None, use_llrd=False, ema=None):\n",
    "    # Freeze/unfreeze policy\n",
    "    if freeze_text_flag is True:\n",
    "        freeze_text(model)\n",
    "    elif freeze_text_flag is False and unfreeze_last_n is not None:\n",
    "        set_all_text_requires_grad(model, False)\n",
    "        unf = unfreeze_last_n_text_layers(model, unfreeze_last_n)\n",
    "        print(f\"[{stage_name}] Unfrozen last-{unf} text layers\")\n",
    "\n",
    "    # Optimizer/scheduler\n",
    "    steps_per_epoch = max(1, len(train_loader) // max(1, int(CONFIG[\"grad_accum_steps\"])))\n",
    "    opt = build_optimizer_stageB_llrd(model) if use_llrd else build_optimizer_stageA(model)\n",
    "    sch = build_scheduler(opt, steps_per_epoch, epochs, float(CONFIG[\"warmup_ratio\"]))\n",
    "\n",
    "    scaler = old_amp.GradScaler(enabled=True)\n",
    "    best_metric, best_epoch, no_imp = -1.0, -1, 0\n",
    "    patience = int(CONFIG[\"early_stop_patience\"])\n",
    "\n",
    "    for epoch in range(1, epochs+1):\n",
    "        model.train()\n",
    "        running = 0.0\n",
    "        t0 = time.time()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        for step, batch in enumerate(train_loader, start=1):\n",
    "            g = batch[\"graph\"].to(device)\n",
    "            tok = {k: v.to(device) for k,v in batch[\"tok\"].items()}\n",
    "            desc = batch[\"desc\"].to(device)\n",
    "            y = batch[\"y\"].to(device)\n",
    "            m = batch[\"mask\"].to(device)\n",
    "\n",
    "            with old_amp.autocast(enabled=True):\n",
    "                logits = model({\"graph\": g, \"tok\": tok, \"desc\": desc})\n",
    "                loss = loss_with_mask(logits, y, m)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if step % int(CONFIG[\"grad_accum_steps\"]) == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                scaler.step(opt)\n",
    "                scaler.update()\n",
    "                opt.zero_grad(set_to_none=True)\n",
    "                sch.step()\n",
    "\n",
    "            running += loss.item()\n",
    "\n",
    "            if ema is not None:\n",
    "                with torch.no_grad():\n",
    "                    ema.update(model)\n",
    "\n",
    "        # Validate\n",
    "        val_metrics = evaluate(model, val_loader, msg=\"val\")\n",
    "        macro_roc = val_metrics[\"macro_roc_auc\"]\n",
    "        macro_pr  = val_metrics[\"macro_pr_auc\"]\n",
    "\n",
    "        # Early stopping on macro_roc\n",
    "        if macro_roc > best_metric + 1e-6:\n",
    "            best_metric = macro_roc\n",
    "            best_epoch  = epoch\n",
    "            no_imp = 0\n",
    "            # Save best\n",
    "            torch.save(\n",
    "                {\"state_dict\": model.state_dict(), \"config\": CONFIG, \"val_macro_auc\": best_metric, \"stage\": stage_name, \"epoch\": epoch},\n",
    "                CKPT_DIR / \"best.pt\",\n",
    "            )\n",
    "        else:\n",
    "            no_imp += 1\n",
    "\n",
    "        # Save last\n",
    "        torch.save(\n",
    "            {\"state_dict\": model.state_dict(), \"config\": CONFIG, \"val_macro_auc\": macro_roc, \"stage\": stage_name, \"epoch\": epoch},\n",
    "            CKPT_DIR / \"last.pt\",\n",
    "        )\n",
    "\n",
    "        # Log epoch\n",
    "        rec = {\n",
    "            \"stage\": stage_name,\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": running / max(1, len(train_loader)),\n",
    "            \"val_macro_roc_auc\": macro_roc,\n",
    "            \"val_macro_pr_auc\": macro_pr,\n",
    "            \"best_metric_so_far\": best_metric,\n",
    "            \"time_sec\": round(time.time() - t0, 2),\n",
    "        }\n",
    "        with open(LOG_PATH, \"a\", encoding=\"utf-8\") as f:\n",
    "            f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "        print(f\"[{stage_name}][{epoch}/{epochs}] loss={rec['train_loss']:.4f} | val ROC={macro_roc:.4f} PR={macro_pr:.4f} | best={best_metric:.4f} (epoch {best_epoch})\")\n",
    "\n",
    "        if no_imp >= patience:\n",
    "            print(f\"[{stage_name}] Early stopping: no improvement for {patience} epochs (best @ epoch {best_epoch} = {best_metric:.4f})\")\n",
    "            break\n",
    "\n",
    "    return best_metric, best_epoch\n",
    "\n",
    "# -----------------------------\n",
    "# Stage A\n",
    "# -----------------------------\n",
    "print(\"\\n[V3] ===== Stage A: Freeze text =====\")\n",
    "freeze_text(model)\n",
    "print(f\"[Stage A] Trainable params: {count_trainable(model)/1e6:.2f}M\")\n",
    "bestA, bestA_ep = train_stage(\n",
    "    model, \"StageA\",\n",
    "    epochs=int(CONFIG[\"stageA_freeze_text_epochs\"]),\n",
    "    freeze_text_flag=True, unfreeze_last_n=None, use_llrd=False, ema=None\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Stage B\n",
    "# -----------------------------\n",
    "print(\"\\n[V3] ===== Stage B: Unfreeze last-N with LLRD =====\")\n",
    "set_all_text_requires_grad(model, False)\n",
    "unf = unfreeze_last_n_text_layers(model, int(CONFIG[\"stageB_unfreeze_last_layers\"]))\n",
    "print(f\"[Stage B] Unfrozen text layers: {unf}\")\n",
    "print(f\"[Stage B] Trainable params: {count_trainable(model)/1e6:.2f}M\")\n",
    "bestB, bestB_ep = train_stage(\n",
    "    model, \"StageB\",\n",
    "    epochs=int(CONFIG[\"epochs_max\"]),\n",
    "    freeze_text_flag=False, unfreeze_last_n=int(CONFIG[\"stageB_unfreeze_last_layers\"]),\n",
    "    use_llrd=True, ema=None\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Stage C (EMA polish)\n",
    "# -----------------------------\n",
    "bestC = None\n",
    "if bool(CONFIG[\"use_ema\"]) and int(CONFIG[\"stageC_epochs\"]) > 0:\n",
    "    print(\"\\n[V3] ===== Stage C: EMA polish =====\")\n",
    "    ema = EMA(model, decay=0.999)\n",
    "    bestC, bestC_ep = train_stage(\n",
    "        model, \"StageC-EMA\",\n",
    "        epochs=int(CONFIG[\"stageC_epochs\"]),\n",
    "        freeze_text_flag=False, unfreeze_last_n=int(CONFIG[\"stageB_unfreeze_last_layers\"]),\n",
    "        use_llrd=True, ema=ema\n",
    "    )\n",
    "\n",
    "# -----------------------------\n",
    "# Final summary\n",
    "# -----------------------------\n",
    "final_val = evaluate(model, val_loader, msg=\"val_final\")\n",
    "summary = {\n",
    "    \"bestA\": bestA,\n",
    "    \"bestB\": bestB,\n",
    "    \"bestC\": bestC,\n",
    "    \"final_val_macro_roc_auc\": final_val[\"macro_roc_auc\"],\n",
    "    \"final_val_macro_pr_auc\": final_val[\"macro_pr_auc\"],\n",
    "}\n",
    "with open(CKPT_DIR / \"best_summary.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n[V3] ===== Training complete =====\")\n",
    "print(json.dumps(summary, indent=2))\n",
    "print(f\"Saved: {CKPT_DIR/'best.pt'}, {CKPT_DIR/'last.pt'}, {CKPT_DIR/'best_summary.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea08b8a",
   "metadata": {},
   "source": [
    "## 5: Evaluation — TTA, Threshold Calibration, Full Metrics & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afd000d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[V3][Eval] Running validation predictions with TTA= 5\n",
      "[V3][Eval] val shapes: P=(783, 12), Y=(783, 12), M=(783, 12)\n",
      "[V3][Eval] Saved thresholds → tox21_dualenc_v1\\models\\checkpoints_v3\\thresholds_v3.npy\n",
      "[V3][Eval] Saved → tox21_dualenc_v1\\results\\v3\\val_metrics.json\n",
      "[V3][Eval] Running TEST predictions with TTA= 5\n",
      "[V3][Eval] test shapes: P=(783, 12), Y=(783, 12), M=(783, 12)\n",
      "[V3][Eval] Saved → tox21_dualenc_v1\\results\\v3\\test_metrics.json\n",
      "[V3][Eval] Saved → tox21_dualenc_v1\\results\\v3\\test_predictions.csv\n",
      "[V3][Eval] Saved → tox21_dualenc_v1\\results\\v3\\per_label_metrics.csv\n",
      "\n",
      "[V3][Eval] ===== Summary =====\n",
      "VAL:  ROC-AUC=0.7499 | PR-AUC=0.3482 | F1=0.4157 | P=0.4522 | R=0.4177\n",
      "TEST: ROC-AUC=0.7678 | PR-AUC=0.3777 | F1=0.3727 | P=0.4251 | R=0.3500\n",
      "\n",
      "Artifacts saved to: tox21_dualenc_v1/results/v3\n",
      " - val_metrics.json, test_metrics.json\n",
      " - thresholds_v3.npy/json (in checkpoints_v3)\n",
      " - test_predictions.npy / test_predictions.csv\n",
      " - per_label_metrics.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time, warnings, csv\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_score, recall_score\n",
    "\n",
    "# Reuse items from previous cells\n",
    "ROOT      = Path(\"tox21_dualenc_v1\")\n",
    "CKPT_DIR  = ROOT / \"models\" / \"checkpoints_v3\"\n",
    "RESULTS   = ROOT / \"results\" / \"v3\"\n",
    "RESULTS.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG = json.loads((CKPT_DIR / \"config_dualenc_v3.json\").read_text())\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Guard checks\n",
    "for name in [\"test_loader\", \"val_loader\", \"DualEncoderXAttn\", \"AttentionPool\", \"GINEncoder\", \"CrossAttnBlock\"]:\n",
    "    if name not in globals():\n",
    "        raise RuntimeError(f\"{name} not found — please run Cells 2 & 3 first.\")\n",
    "\n",
    "# --- Silenced pooler rebuild (consistent with training)\n",
    "from transformers import AutoConfig\n",
    "orig_init = DualEncoderXAttn.__init__\n",
    "def patched_init(self, config: dict):\n",
    "    nn.Module.__init__(self)\n",
    "    self.cfg = config\n",
    "    txt_cfg = AutoConfig.from_pretrained(config[\"text_encoder\"])\n",
    "    if hasattr(txt_cfg, \"add_pooling_layer\"): txt_cfg.add_pooling_layer = False\n",
    "    from transformers import AutoModel\n",
    "    self.text = AutoModel.from_pretrained(config[\"text_encoder\"], config=txt_cfg)\n",
    "    self.text_proj = nn.Linear(self.text.config.hidden_size, int(config[\"text_proj_dim\"]))\n",
    "    self.gnn = GINEncoder(\n",
    "        node_dim=int(config[\"node_dim\"]), edge_dim=int(config[\"edge_dim\"]),\n",
    "        hidden=int(config[\"graph_hidden\"]), layers=int(config[\"graph_layers\"]),\n",
    "        dropout=float(config[\"dropout\"]),\n",
    "    )\n",
    "    self.xattn = CrossAttnBlock(dim=int(config[\"fusion_dim\"]),\n",
    "                                heads=int(config[\"fusion_heads\"]),\n",
    "                                dropout=float(config[\"fusion_dropout\"]))\n",
    "    self.tpool = AttentionPool(dim=int(config[\"fusion_dim\"]))\n",
    "    self.dmlp  = nn.Sequential(\n",
    "        nn.Linear(int(config[\"desc_dim_in\"]), int(config[\"desc_hidden\"])),\n",
    "        nn.ReLU(), nn.Dropout(float(config[\"dropout\"])),\n",
    "    )\n",
    "    fused_in = int(config[\"fusion_dim\"]) + int(config[\"graph_hidden\"]) + int(config[\"desc_hidden\"])\n",
    "    self.head = nn.Sequential(\n",
    "        nn.Linear(fused_in, int(config[\"head_hidden\"])),\n",
    "        nn.ReLU(), nn.Dropout(float(config[\"dropout\"])),\n",
    "        nn.Linear(int(config[\"head_hidden\"]), int(config[\"num_labels\"])),\n",
    "    )\n",
    "    assert int(config[\"text_proj_dim\"]) == int(config[\"fusion_dim\"])\n",
    "DualEncoderXAttn.__init__ = patched_init\n",
    "\n",
    "# Load model from best checkpoint\n",
    "ckpt_path = CKPT_DIR / \"best.pt\"\n",
    "assert ckpt_path.exists(), f\"best.pt not found at {ckpt_path}\"\n",
    "model = DualEncoderXAttn(CONFIG).to(device)\n",
    "sd = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "model.load_state_dict(sd, strict=True)\n",
    "model.eval()\n",
    "DualEncoderXAttn.__init__ = orig_init  # restore\n",
    "\n",
    "# Tokenizer & TTA enumeration (from Cell 2)\n",
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(CONFIG[\"text_encoder\"])\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"RDKit required for SMILES enumeration in TTA.\") from e\n",
    "\n",
    "def smiles_randomize(s: str):\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None: \n",
    "        return s\n",
    "    return Chem.MolToSmiles(m, doRandom=True, canonical=False)\n",
    "\n",
    "TTA_N = int(CONFIG.get(\"tta_enum_n\", 5))\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_with_tta(loader, tta_n: int = TTA_N):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      P: (N,L) probabilities,\n",
    "      Y: (N,L) labels,\n",
    "      M: (N,L) mask,\n",
    "      order_idx: (N,) original indices to ensure alignment\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_probs, all_y, all_m, all_idx = [], [], [], []\n",
    "    for batch in loader:\n",
    "        # common tensors\n",
    "        g = batch[\"graph\"].to(device)\n",
    "        desc = batch[\"desc\"].to(device)\n",
    "        y = batch[\"y\"].to(device)\n",
    "        m = batch[\"mask\"].to(device)\n",
    "        idxs = batch[\"idx\"].cpu().numpy()\n",
    "\n",
    "        # base tokenization (no enum)\n",
    "        tok_base = {k: v.to(device) for k, v in batch[\"tok\"].items()}\n",
    "        logits_acc = torch.sigmoid(model({\"graph\": g, \"tok\": tok_base, \"desc\": desc}))\n",
    "\n",
    "        # TTA loops (SMILES enumeration on-the-fly)\n",
    "        if tta_n > 0:\n",
    "            sm_list = batch[\"smiles\"]\n",
    "            for _ in range(tta_n):\n",
    "                sm_enum = [smiles_randomize(s) for s in sm_list]\n",
    "                tok_enum = tok(sm_enum, padding=True, truncation=True, max_length=int(CONFIG[\"max_length\"]), return_tensors=\"pt\")\n",
    "                tok_enum = {k: v.to(device) for k, v in tok_enum.items()}\n",
    "                logits_enum = torch.sigmoid(model({\"graph\": g, \"tok\": tok_enum, \"desc\": desc}))\n",
    "                logits_acc += logits_enum\n",
    "            logits_acc = logits_acc / (tta_n + 1)\n",
    "\n",
    "        all_probs.append(logits_acc.detach().cpu().numpy())\n",
    "        all_y.append(y.detach().cpu().numpy())\n",
    "        all_m.append(m.detach().cpu().numpy())\n",
    "        all_idx.append(idxs)\n",
    "\n",
    "    P = np.concatenate(all_probs, axis=0)\n",
    "    Y = np.concatenate(all_y, axis=0)\n",
    "    M = np.concatenate(all_m, axis=0)\n",
    "    order_idx = np.concatenate(all_idx, axis=0)\n",
    "    return P, Y, M, order_idx\n",
    "\n",
    "def masked_metric_bin(y_true, y_prob, y_mask, thresholds):\n",
    "    \"\"\"Compute macro F1/precision/recall and per-label metrics with mask-aware thresholds.\"\"\"\n",
    "    L = y_true.shape[1]\n",
    "    y_pred = (y_prob >= thresholds.reshape(1, L)).astype(np.float32)\n",
    "\n",
    "    per = []\n",
    "    for k in range(L):\n",
    "        valid = y_mask[:, k] > 0.5\n",
    "        if valid.sum() == 0 or np.unique(y_true[valid, k]).size < 2:\n",
    "            per.append({\"f1\": np.nan, \"precision\": np.nan, \"recall\": np.nan,\n",
    "                        \"support_pos\": int((y_true[valid, k] >= 0.5).sum()),\n",
    "                        \"support_total\": int(valid.sum())})\n",
    "            continue\n",
    "        f1  = f1_score(       y_true[valid, k], y_pred[valid, k])\n",
    "        pr  = precision_score(y_true[valid, k], y_pred[valid, k], zero_division=0)\n",
    "        rc  = recall_score(   y_true[valid, k], y_pred[valid, k])\n",
    "        per.append({\"f1\": float(f1), \"precision\": float(pr), \"recall\": float(rc),\n",
    "                    \"support_pos\": int((y_true[valid, k] >= 0.5).sum()),\n",
    "                    \"support_total\": int(valid.sum())})\n",
    "    macro = {\n",
    "        \"macro_f1\":  float(np.nanmean([d[\"f1\"] for d in per])),\n",
    "        \"macro_pr\":  float(np.nanmean([d[\"precision\"] for d in per])),\n",
    "        \"macro_rc\":  float(np.nanmean([d[\"recall\"] for d in per])),\n",
    "    }\n",
    "    return macro, per, y_pred\n",
    "\n",
    "def masked_auc_ap(y_true, y_prob, y_mask):\n",
    "    \"\"\"Compute mask-aware per-label ROC-AUC and PR-AUC.\"\"\"\n",
    "    L = y_true.shape[1]\n",
    "    roc_list, pr_list = [], []\n",
    "    for k in range(L):\n",
    "        valid = y_mask[:, k] > 0.5\n",
    "        if valid.sum() < 2 or np.unique(y_true[valid, k]).size < 2:\n",
    "            roc_list.append(np.nan); pr_list.append(np.nan); continue\n",
    "        try:\n",
    "            roc_list.append(roc_auc_score(y_true[valid, k], y_prob[valid, k]))\n",
    "        except:\n",
    "            roc_list.append(np.nan)\n",
    "        try:\n",
    "            pr_list.append(average_precision_score(y_true[valid, k], y_prob[valid, k]))\n",
    "        except:\n",
    "            pr_list.append(np.nan)\n",
    "    return float(np.nanmean(roc_list)), float(np.nanmean(pr_list)), roc_list, pr_list\n",
    "\n",
    "def calibrate_thresholds(y_true_val, y_prob_val, y_mask_val, grid=None):\n",
    "    \"\"\"Per-label threshold selection by maximizing F1 on validation.\"\"\"\n",
    "    if grid is None:\n",
    "        grid = np.linspace(0.05, 0.95, 19)\n",
    "    L = y_true_val.shape[1]\n",
    "    best_thr = np.full(L, 0.5, dtype=np.float32)\n",
    "    for k in range(L):\n",
    "        valid = y_mask_val[:, k] > 0.5\n",
    "        if valid.sum() == 0 or np.unique(y_true_val[valid, k]).size < 2:\n",
    "            best_thr[k] = 0.5\n",
    "            continue\n",
    "        yk = y_true_val[valid, k]\n",
    "        pk = y_prob_val[valid, k]\n",
    "        f1s = []\n",
    "        for t in grid:\n",
    "            yhat = (pk >= t).astype(np.float32)\n",
    "            f1s.append(f1_score(yk, yhat))\n",
    "        t_opt = float(grid[int(np.argmax(f1s))])\n",
    "        best_thr[k] = t_opt\n",
    "    return best_thr\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Validation predictions with TTA\n",
    "# -----------------------------\n",
    "print(\"[V3][Eval] Running validation predictions with TTA=\", TTA_N)\n",
    "P_val, Y_val, M_val, idx_val = predict_with_tta(val_loader, tta_n=TTA_N)\n",
    "print(f\"[V3][Eval] val shapes: P={P_val.shape}, Y={Y_val.shape}, M={M_val.shape}\")\n",
    "\n",
    "# 2) Calibrate thresholds on validation (maximize F1)\n",
    "thr = calibrate_thresholds(Y_val, P_val, M_val, grid=np.linspace(0.05, 0.95, 19))\n",
    "np.save(CKPT_DIR / \"thresholds_v3.npy\", thr)\n",
    "with open(CKPT_DIR / \"thresholds_v3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"thresholds\": [float(x) for x in thr]}, f, indent=2)\n",
    "print(\"[V3][Eval] Saved thresholds →\", CKPT_DIR / \"thresholds_v3.npy\")\n",
    "\n",
    "# 3) Validation metrics summary\n",
    "val_roc_macro, val_pr_macro, val_roc_per, val_pr_per = masked_auc_ap(Y_val, P_val, M_val)\n",
    "val_macro_bin, val_per_bin, _ = masked_metric_bin(Y_val, P_val, M_val, thr)\n",
    "val_metrics = {\n",
    "    \"macro_roc_auc\": val_roc_macro,\n",
    "    \"macro_pr_auc\":  val_pr_macro,\n",
    "    **val_macro_bin,\n",
    "    \"per_label_roc_auc\": val_roc_per,\n",
    "    \"per_label_pr_auc\":  val_pr_per,\n",
    "    \"thresholds\": [float(x) for x in thr],\n",
    "}\n",
    "with open(RESULTS / \"val_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(val_metrics, f, indent=2)\n",
    "print(\"[V3][Eval] Saved →\", RESULTS / \"val_metrics.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Test predictions with TTA\n",
    "# -----------------------------\n",
    "print(\"[V3][Eval] Running TEST predictions with TTA=\", TTA_N)\n",
    "P_test, Y_test, M_test, idx_test = predict_with_tta(test_loader, tta_n=TTA_N)\n",
    "print(f\"[V3][Eval] test shapes: P={P_test.shape}, Y={Y_test.shape}, M={M_test.shape}\")\n",
    "\n",
    "# 5) Test metrics using calibrated thresholds\n",
    "test_roc_macro, test_pr_macro, test_roc_per, test_pr_per = masked_auc_ap(Y_test, P_test, M_test)\n",
    "test_macro_bin, test_per_bin, Yhat_test = masked_metric_bin(Y_test, P_test, M_test, thr)\n",
    "\n",
    "test_metrics = {\n",
    "    \"macro_roc_auc\": test_roc_macro,\n",
    "    \"macro_pr_auc\":  test_pr_macro,\n",
    "    **test_macro_bin,\n",
    "    \"per_label_roc_auc\": test_roc_per,\n",
    "    \"per_label_pr_auc\":  test_pr_per,\n",
    "}\n",
    "\n",
    "# Save test metrics\n",
    "with open(RESULTS / \"test_metrics.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(test_metrics, f, indent=2)\n",
    "print(\"[V3][Eval] Saved →\", RESULTS / \"test_metrics.json\")\n",
    "\n",
    "# 6) Save raw predictions (npy + csv with thresholds)\n",
    "np.save(RESULTS / \"test_predictions.npy\", P_test)\n",
    "\n",
    "# Label names (if available from earlier)\n",
    "label_names_path = ROOT / \"data\" / \"label_names.txt\"\n",
    "if label_names_path.exists():\n",
    "    label_names = label_names_path.read_text(encoding=\"utf-8\").splitlines()\n",
    "else:\n",
    "    label_names = [f\"label_{i}\" for i in range(P_test.shape[1])]\n",
    "\n",
    "csv_path = RESULTS / \"test_predictions.csv\"\n",
    "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    header = [\"index\"] + [f\"{n}_prob\" for n in label_names] + [f\"{n}_bin\" for n in label_names]\n",
    "    writer.writerow(header)\n",
    "    for idx, probs, bins in zip(idx_test, P_test, Yhat_test):\n",
    "        writer.writerow([int(idx)] + [f\"{p:.6f}\" for p in probs.tolist()] + [int(b) for b in bins.tolist()])\n",
    "print(\"[V3][Eval] Saved →\", csv_path)\n",
    "\n",
    "# 7) Save per-label metrics CSV (F1/Precision/Recall + ROC/PR AUC)\n",
    "per_csv = RESULTS / \"per_label_metrics.csv\"\n",
    "with open(per_csv, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"label\", \"val_roc_auc\", \"val_pr_auc\", \"test_roc_auc\", \"test_pr_auc\",\n",
    "                     \"val_f1\", \"val_precision\", \"val_recall\", \"val_support_pos\", \"val_support_total\",\n",
    "                     \"thr\"])\n",
    "    # Compute val per-label F1/PR/RC to match thresholding\n",
    "    _, val_per_bin2, _ = masked_metric_bin(Y_val, P_val, M_val, thr)\n",
    "    for i, name in enumerate(label_names):\n",
    "        writer.writerow([\n",
    "            name,\n",
    "            f\"{val_roc_per[i]:.6f}\" if not np.isnan(val_roc_per[i]) else \"\",\n",
    "            f\"{val_pr_per[i]:.6f}\" if not np.isnan(val_pr_per[i]) else \"\",\n",
    "            f\"{test_roc_per[i]:.6f}\" if not np.isnan(test_roc_per[i]) else \"\",\n",
    "            f\"{test_pr_per[i]:.6f}\" if not np.isnan(test_pr_per[i]) else \"\",\n",
    "            f\"{val_per_bin2[i]['f1']:.6f}\" if not np.isnan(val_per_bin2[i]['f1']) else \"\",\n",
    "            f\"{val_per_bin2[i]['precision']:.6f}\" if not np.isnan(val_per_bin2[i]['precision']) else \"\",\n",
    "            f\"{val_per_bin2[i]['recall']:.6f}\" if not np.isnan(val_per_bin2[i]['recall']) else \"\",\n",
    "            val_per_bin2[i][\"support_pos\"],\n",
    "            val_per_bin2[i][\"support_total\"],\n",
    "            f\"{thr[i]:.3f}\",\n",
    "        ])\n",
    "print(\"[V3][Eval] Saved →\", per_csv)\n",
    "\n",
    "# 8) Final on-screen summary\n",
    "print(\"\\n[V3][Eval] ===== Summary =====\")\n",
    "print(f\"VAL:  ROC-AUC={val_metrics['macro_roc_auc']:.4f} | PR-AUC={val_metrics['macro_pr_auc']:.4f} | \"\n",
    "      f\"F1={val_metrics['macro_f1']:.4f} | P={val_metrics['macro_pr']:.4f} | R={val_metrics['macro_rc']:.4f}\")\n",
    "print(f\"TEST: ROC-AUC={test_metrics['macro_roc_auc']:.4f} | PR-AUC={test_metrics['macro_pr_auc']:.4f} | \"\n",
    "      f\"F1={test_macro_bin['macro_f1']:.4f} | P={test_macro_bin['macro_pr']:.4f} | R={test_macro_bin['macro_rc']:.4f}\")\n",
    "\n",
    "print(\"\\nArtifacts saved to:\", RESULTS.as_posix())\n",
    "print(\" - val_metrics.json, test_metrics.json\")\n",
    "print(\" - thresholds_v3.npy/json (in checkpoints_v3)\")\n",
    "print(\" - test_predictions.npy / test_predictions.csv\")\n",
    "print(\" - per_label_metrics.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2ee791",
   "metadata": {},
   "source": [
    "## 6: Custom test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577695a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V3 Predictions using DEFAULT thresholds (0.5) with TTA and optional top-1 fallback:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SMILES</th>\n",
       "      <th>NR-AR</th>\n",
       "      <th>NR-AR-LBD</th>\n",
       "      <th>NR-AhR</th>\n",
       "      <th>NR-Aromatase</th>\n",
       "      <th>NR-ER</th>\n",
       "      <th>NR-ER-LBD</th>\n",
       "      <th>NR-PPAR-gamma</th>\n",
       "      <th>SR-ARE</th>\n",
       "      <th>SR-ATAD5</th>\n",
       "      <th>SR-HSE</th>\n",
       "      <th>SR-MMP</th>\n",
       "      <th>SR-p53</th>\n",
       "      <th>predicted (V3 @ 0.5)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CCOc1ccc2nc(S(N)(=O)=O)sc2c1</td>\n",
       "      <td>0.026445</td>\n",
       "      <td>0.002039</td>\n",
       "      <td>0.284742</td>\n",
       "      <td>0.020950</td>\n",
       "      <td>0.297778</td>\n",
       "      <td>0.064517</td>\n",
       "      <td>0.014526</td>\n",
       "      <td>0.325764</td>\n",
       "      <td>0.031588</td>\n",
       "      <td>0.195467</td>\n",
       "      <td>0.352094</td>\n",
       "      <td>0.153829</td>\n",
       "      <td>SR-MMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CCN1C(=O)NC(c2ccccc2)C1=O</td>\n",
       "      <td>0.005738</td>\n",
       "      <td>0.000050</td>\n",
       "      <td>0.421162</td>\n",
       "      <td>0.004822</td>\n",
       "      <td>0.182672</td>\n",
       "      <td>0.006225</td>\n",
       "      <td>0.000407</td>\n",
       "      <td>0.060547</td>\n",
       "      <td>0.002258</td>\n",
       "      <td>0.008692</td>\n",
       "      <td>0.022797</td>\n",
       "      <td>0.004604</td>\n",
       "      <td>NR-AhR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1</td>\n",
       "      <td>0.114900</td>\n",
       "      <td>0.014576</td>\n",
       "      <td>0.307331</td>\n",
       "      <td>0.074902</td>\n",
       "      <td>0.542951</td>\n",
       "      <td>0.230346</td>\n",
       "      <td>0.027670</td>\n",
       "      <td>0.316693</td>\n",
       "      <td>0.005549</td>\n",
       "      <td>0.478462</td>\n",
       "      <td>0.549441</td>\n",
       "      <td>0.400057</td>\n",
       "      <td>NR-ER, SR-MMP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CCCCCCCCCCOCC(O)CN</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.000046</td>\n",
       "      <td>0.013636</td>\n",
       "      <td>0.000786</td>\n",
       "      <td>0.187622</td>\n",
       "      <td>0.011747</td>\n",
       "      <td>0.000715</td>\n",
       "      <td>0.025681</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.015662</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.000547</td>\n",
       "      <td>—</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nc1ccc([N+](=O)[O-])cc1N</td>\n",
       "      <td>0.008537</td>\n",
       "      <td>0.000303</td>\n",
       "      <td>0.315043</td>\n",
       "      <td>0.001750</td>\n",
       "      <td>0.232611</td>\n",
       "      <td>0.029604</td>\n",
       "      <td>0.003713</td>\n",
       "      <td>0.193489</td>\n",
       "      <td>0.008741</td>\n",
       "      <td>0.106936</td>\n",
       "      <td>0.151615</td>\n",
       "      <td>0.017604</td>\n",
       "      <td>NR-AhR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   SMILES     NR-AR  NR-AR-LBD    NR-AhR  \\\n",
       "0            CCOc1ccc2nc(S(N)(=O)=O)sc2c1  0.026445   0.002039  0.284742   \n",
       "1               CCN1C(=O)NC(c2ccccc2)C1=O  0.005738   0.000050  0.421162   \n",
       "2  O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1  0.114900   0.014576  0.307331   \n",
       "3                      CCCCCCCCCCOCC(O)CN  0.004184   0.000046  0.013636   \n",
       "4                Nc1ccc([N+](=O)[O-])cc1N  0.008537   0.000303  0.315043   \n",
       "\n",
       "   NR-Aromatase     NR-ER  NR-ER-LBD  NR-PPAR-gamma    SR-ARE  SR-ATAD5  \\\n",
       "0      0.020950  0.297778   0.064517       0.014526  0.325764  0.031588   \n",
       "1      0.004822  0.182672   0.006225       0.000407  0.060547  0.002258   \n",
       "2      0.074902  0.542951   0.230346       0.027670  0.316693  0.005549   \n",
       "3      0.000786  0.187622   0.011747       0.000715  0.025681  0.000036   \n",
       "4      0.001750  0.232611   0.029604       0.003713  0.193489  0.008741   \n",
       "\n",
       "     SR-HSE    SR-MMP    SR-p53 predicted (V3 @ 0.5)  \n",
       "0  0.195467  0.352094  0.153829               SR-MMP  \n",
       "1  0.008692  0.022797  0.004604               NR-AhR  \n",
       "2  0.478462  0.549441  0.400057        NR-ER, SR-MMP  \n",
       "3  0.015662  0.009282  0.000547                    —  \n",
       "4  0.106936  0.151615  0.017604               NR-AhR  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: tox21_dualenc_v1/results/v3/infer_v3_defaultthr_20250901_221530.csv\n"
     ]
    }
   ],
   "source": [
    "import os, json, numpy as np, torch, pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Paths\n",
    "ROOT   = Path(\"tox21_dualenc_v1\")\n",
    "DATA   = ROOT / \"data\"\n",
    "DESC   = DATA / \"descriptors\"\n",
    "SPLITS = DATA / \"splits\"\n",
    "MODELS = ROOT / \"models\"\n",
    "CKPT_V3 = MODELS / \"checkpoints_v3\"\n",
    "RES_V3  = ROOT / \"results\" / \"v3\"\n",
    "RES_V3.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load V3 config & label names\n",
    "CONFIG_V3 = json.loads((CKPT_V3/\"config_dualenc_v3.json\").read_text())\n",
    "NUM_LABELS = int(CONFIG_V3[\"num_labels\"])\n",
    "label_names = (DATA/\"label_names.txt\").read_text(encoding=\"utf-8\").splitlines() \\\n",
    "             if (DATA/\"label_names.txt\").exists() else [f\"label_{i}\" for i in range(NUM_LABELS)]\n",
    "\n",
    "# === DEFAULT THRESHOLDS ===\n",
    "thr_vec = np.full(NUM_LABELS, 0.5, dtype=np.float32)  # model default: 0.5\n",
    "\n",
    "# Inference options\n",
    "TTA_N          = int(CONFIG_V3.get(\"tta_enum_n\", 5))   # you can set to 0 to disable\n",
    "TOP1_FALLBACK  = True\n",
    "TOP1_FLOOR     = 0.30\n",
    "\n",
    "# --- Descriptor train-mean for custom inference\n",
    "X_desc   = np.load(DESC/\"desc_selected.npy\").astype(np.float32)\n",
    "splits   = json.loads((SPLITS/\"splits.json\").read_text())\n",
    "train_idx = np.array(splits[\"train\"], dtype=int)\n",
    "DESC_MEAN = X_desc[train_idx].mean(axis=0).astype(np.float32)\n",
    "assert DESC_MEAN.shape[0] == CONFIG_V3[\"desc_dim_in\"], \"Descriptor dim mismatch vs config\"\n",
    "\n",
    "# --- Featurizers: EXACTLY as V3 training (34/7 dims)\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data, Batch as GeometricBatch\n",
    "\n",
    "ATOM_LIST = [\"H\",\"B\",\"C\",\"N\",\"O\",\"F\",\"Si\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "HYB_LIST  = [Chem.rdchem.HybridizationType.SP,\n",
    "             Chem.rdchem.HybridizationType.SP2,\n",
    "             Chem.rdchem.HybridizationType.SP3,\n",
    "             Chem.rdchem.HybridizationType.SP3D,\n",
    "             Chem.rdchem.HybridizationType.SP3D2]\n",
    "\n",
    "def one_hot_with_other(x, choices):\n",
    "    return [1 if x == c else 0 for c in choices] + [0 if x in choices else 1]\n",
    "\n",
    "def one_hot_index(idx, size):\n",
    "    v = [0]*size\n",
    "    if 0 <= idx < size: v[idx] = 1\n",
    "    return v\n",
    "\n",
    "def atom_features(atom: Chem.Atom):\n",
    "    sym      = atom.GetSymbol()\n",
    "    f_type   = one_hot_with_other(sym, ATOM_LIST)           # 13\n",
    "    deg      = atom.GetTotalDegree()\n",
    "    f_deg    = one_hot_index(min(deg,6), 7)                 # 7\n",
    "    charge   = atom.GetFormalCharge()\n",
    "    charge_map = {-2:0,-1:1,0:2,1:3,2:4}\n",
    "    f_charge = one_hot_index(charge_map.get(charge,5), 6)   # 6\n",
    "    hyb      = atom.GetHybridization()\n",
    "    f_hyb    = one_hot_with_other(hyb, HYB_LIST)            # 6\n",
    "    f_flags  = [int(atom.GetIsAromatic()), int(atom.IsInRing())]  # 2\n",
    "    return f_type + f_deg + f_charge + f_hyb + f_flags      # total 34\n",
    "\n",
    "def bond_features(bond: Chem.Bond):\n",
    "    types = [Chem.rdchem.BondType.SINGLE, Chem.rdchem.BondType.DOUBLE,\n",
    "             Chem.rdchem.BondType.TRIPLE, Chem.rdchem.BondType.AROMATIC]\n",
    "    f_type  = one_hot_with_other(bond.GetBondType(), types)   # 5\n",
    "    f_flags = [int(bond.GetIsConjugated()), int(bond.IsInRing())]  # 2\n",
    "    return f_type + f_flags                                   # total 7\n",
    "\n",
    "NODE_DIM, EDGE_DIM = CONFIG_V3[\"node_dim\"], CONFIG_V3[\"edge_dim\"]\n",
    "\n",
    "def build_graph_from_smiles(s):\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None or mol.GetNumAtoms()==0:\n",
    "        return None\n",
    "    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float32)\n",
    "    assert x.size(1) == NODE_DIM, f\"node_dim mismatch: {x.size(1)} vs {NODE_DIM}\"\n",
    "\n",
    "    ei_src, ei_dst, eattr = [], [], []\n",
    "    for b in mol.GetBonds():\n",
    "        u, v = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bf = bond_features(b)\n",
    "        ei_src.extend([u,v]); ei_dst.extend([v,u]); eattr.extend([bf,bf])\n",
    "\n",
    "    if len(ei_src)==0:\n",
    "        edge_index = torch.zeros((2,1), dtype=torch.long)\n",
    "        edge_attr  = torch.zeros((1, EDGE_DIM), dtype=torch.float32)\n",
    "    else:\n",
    "        edge_index = torch.tensor([ei_src, ei_dst], dtype=torch.long)\n",
    "        edge_attr  = torch.tensor(eattr, dtype=torch.float32)\n",
    "\n",
    "    assert edge_attr.size(1) == EDGE_DIM, f\"edge_dim mismatch: {edge_attr.size(1)} vs {EDGE_DIM}\"\n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr, invalid=0)\n",
    "\n",
    "# --- Minimal inference model (same architecture wiring; pooler disabled)\n",
    "import torch.nn as nn\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "from torch_geometric.nn import GINEConv, global_mean_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "class AttentionPool(nn.Module):\n",
    "    def __init__(self, dim): super().__init__(); self.score = nn.Linear(dim, 1)\n",
    "    def forward(self, X, mask=None):\n",
    "        s = self.score(X).squeeze(-1)\n",
    "        if mask is not None:\n",
    "            mask = mask.to(dtype=torch.bool, device=s.device)\n",
    "            neg_large = torch.tensor(-1e4 if s.dtype==torch.float16 else -1e9, dtype=s.dtype, device=s.device)\n",
    "            s = s.masked_fill(~mask, neg_large)\n",
    "        a = torch.softmax(s, dim=1)\n",
    "        return torch.bmm(a.unsqueeze(1), X).squeeze(1)\n",
    "\n",
    "class CrossAttnBlock(nn.Module):\n",
    "    def __init__(self, dim=256, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn  = nn.MultiheadAttention(dim, heads, dropout=dropout, batch_first=True)\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.ffn   = nn.Sequential(nn.Linear(dim, 4*dim), nn.GELU(), nn.Dropout(dropout), nn.Linear(4*dim, dim))\n",
    "        self.norm2 = nn.LayerNorm(dim); self.drop = nn.Dropout(dropout)\n",
    "    def forward(self, Q_text, K_graph, V_graph, key_padding_mask=None):\n",
    "        attn_out,_ = self.attn(Q_text, K_graph, V_graph, key_padding_mask=key_padding_mask)\n",
    "        x  = self.norm1(Q_text + self.drop(attn_out))\n",
    "        ff = self.ffn(x)\n",
    "        return self.norm2(x + self.drop(ff))\n",
    "\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, node_dim, edge_dim, hidden=256, layers=4, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(); self.bns = nn.ModuleList()\n",
    "        in_dim = node_dim\n",
    "        for _ in range(layers):\n",
    "            mlp  = nn.Sequential(nn.Linear(in_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden))\n",
    "            conv = GINEConv(mlp, edge_dim=edge_dim)\n",
    "            self.layers.append(conv); self.bns.append(nn.BatchNorm1d(hidden)); in_dim = hidden\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.hidden = hidden\n",
    "    def forward(self, x, edge_index, edge_attr, batch_index):\n",
    "        h = x\n",
    "        for conv, bn in zip(self.layers, self.bns):\n",
    "            h = conv(h, edge_index, edge_attr); h = bn(h); h = torch.relu(h); h = self.dropout(h)\n",
    "        z_graph = global_mean_pool(h, batch_index)\n",
    "        return h, z_graph\n",
    "\n",
    "class DualEncoderXAttn_Infer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        txt_cfg = AutoConfig.from_pretrained(cfg[\"text_encoder\"])\n",
    "        if hasattr(txt_cfg, \"add_pooling_layer\"):\n",
    "            txt_cfg.add_pooling_layer = False\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(cfg[\"text_encoder\"])\n",
    "        self.text = AutoModel.from_pretrained(cfg[\"text_encoder\"], config=txt_cfg)\n",
    "        self.text_proj = nn.Linear(self.text.config.hidden_size, int(cfg[\"text_proj_dim\"]))\n",
    "        self.gnn  = GINEncoder(int(cfg[\"node_dim\"]), int(cfg[\"edge_dim\"]),\n",
    "                               int(cfg[\"graph_hidden\"]), int(cfg[\"graph_layers\"]),\n",
    "                               float(cfg[\"dropout\"]))\n",
    "        self.xattn = CrossAttnBlock(int(cfg[\"fusion_dim\"]), int(cfg[\"fusion_heads\"]),\n",
    "                                    float(cfg[\"fusion_dropout\"]))\n",
    "        self.tpool = AttentionPool(int(cfg[\"fusion_dim\"]))\n",
    "        self.dmlp  = nn.Sequential(\n",
    "            nn.Linear(int(cfg[\"desc_dim_in\"]), int(cfg[\"desc_hidden\"])),\n",
    "            nn.ReLU(), nn.Dropout(float(cfg[\"dropout\"]))\n",
    "        )\n",
    "        fused_in = int(cfg[\"fusion_dim\"]) + int(cfg[\"graph_hidden\"]) + int(cfg[\"desc_hidden\"])\n",
    "        self.head = nn.Sequential(nn.Linear(fused_in, int(cfg[\"head_hidden\"])),\n",
    "                                  nn.ReLU(), nn.Dropout(float(cfg[\"dropout\"])),\n",
    "                                  nn.Linear(int(cfg[\"head_hidden\"]), int(cfg[\"num_labels\"])))\n",
    "    def forward(self, smiles_list, g_batch, desc, max_length=None):\n",
    "        max_length = max_length or int(self.cfg[\"max_length\"])\n",
    "        toks = self.tokenizer(smiles_list, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "        ids, att = toks[\"input_ids\"].to(desc.device), toks[\"attention_mask\"].to(desc.device)\n",
    "        H = self.text(input_ids=ids, attention_mask=att).last_hidden_state\n",
    "        t_proj = self.text_proj(H)  # (B,T,256)\n",
    "        node_h, z_graph = self.gnn(g_batch.x, g_batch.edge_index, g_batch.edge_attr, g_batch.batch)\n",
    "        node_dense, node_mask = to_dense_batch(node_h, g_batch.batch)\n",
    "        fused = self.xattn(t_proj, node_dense, node_dense, key_padding_mask=~node_mask)\n",
    "        z_text = self.tpool(fused, mask=att.bool())\n",
    "        z_desc = self.dmlp(desc)\n",
    "        z = torch.cat([z_text, z_graph, z_desc], dim=-1)\n",
    "        return self.head(z)\n",
    "\n",
    "# Load checkpoint\n",
    "ckpt_path = CKPT_V3 / \"best.pt\"\n",
    "assert ckpt_path.exists(), f\"Missing checkpoint: {ckpt_path}\"\n",
    "infer_model = DualEncoderXAttn_Infer(CONFIG_V3).to(device)\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")[\"state_dict\"]\n",
    "infer_model.load_state_dict(state, strict=False)\n",
    "infer_model.eval()\n",
    "\n",
    "# --- Helpers\n",
    "def smiles_randomize(s: str):\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None: return s\n",
    "    return Chem.MolToSmiles(m, doRandom=True, canonical=False)\n",
    "\n",
    "def predict_batch(smiles_list, tta_n=TTA_N):\n",
    "    graphs, ok_smiles = [], []\n",
    "    for s in smiles_list:\n",
    "        g = build_graph_from_smiles(s)\n",
    "        if g is None:\n",
    "            print(\"⚠️ Invalid SMILES skipped:\", s)\n",
    "            continue\n",
    "        graphs.append(g); ok_smiles.append(s)\n",
    "    if len(ok_smiles)==0:\n",
    "        raise ValueError(\"All provided SMILES were invalid.\")\n",
    "    g_batch = GeometricBatch.from_data_list(graphs).to(device)\n",
    "    desc = torch.tensor(np.tile(DESC_MEAN, (len(ok_smiles), 1)), dtype=torch.float32, device=device)\n",
    "\n",
    "    infer_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_acc = torch.sigmoid(infer_model(ok_smiles, g_batch, desc)).detach()\n",
    "        if tta_n > 0:\n",
    "            for _ in range(tta_n):\n",
    "                s_enum = [smiles_randomize(s) for s in ok_smiles]\n",
    "                logits_acc += torch.sigmoid(infer_model(s_enum, g_batch, desc)).detach()\n",
    "            logits_acc = logits_acc / (tta_n + 1)\n",
    "    P = logits_acc.cpu().numpy()\n",
    "    return ok_smiles, P\n",
    "\n",
    "# --- Your SMILES list (edit here)\n",
    "custom_smiles = [\n",
    "    \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",                     #1st\n",
    "    \"CCN1C(=O)NC(c2ccccc2)C1=O\",                        #2nd \n",
    "    \"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\",           #9th\n",
    "    \"CCCCCCCCCCOCC(O)CN\",                               #18th \n",
    "    \"Nc1ccc([N+](=O)[O-])cc1N\",                         #\n",
    "]\n",
    "\n",
    "ok_smiles, prob = predict_batch(custom_smiles, tta_n=TTA_N)  # (B,L)\n",
    "\n",
    "# Apply DEFAULT thresholds (0.5) + optional top-1 fallback\n",
    "pred_bin = (prob >= thr_vec.reshape(1, -1)).astype(np.int32)\n",
    "if TOP1_FALLBACK:\n",
    "    for i in range(pred_bin.shape[0]):\n",
    "        if pred_bin[i].sum() == 0:\n",
    "            j = int(np.argmax(prob[i]))\n",
    "            if prob[i, j] >= TOP1_FLOOR:\n",
    "                pred_bin[i, j] = 1\n",
    "\n",
    "# Output table\n",
    "df_prob = pd.DataFrame(prob, columns=label_names, index=ok_smiles)\n",
    "calls = []\n",
    "for i, smi in enumerate(ok_smiles):\n",
    "    pos = [label_names[j] for j in np.where(pred_bin[i] == 1)[0]]\n",
    "    calls.append(\", \".join(pos) if pos else \"—\")\n",
    "\n",
    "df_out = df_prob.copy()\n",
    "df_out.insert(0, \"SMILES\", df_out.index)\n",
    "df_out[\"predicted (V3 @ 0.5)\"] = calls\n",
    "df_out = df_out.reset_index(drop=True)\n",
    "\n",
    "print(\"V3 Predictions using DEFAULT thresholds (0.5) with TTA and optional top-1 fallback:\")\n",
    "display(df_out)\n",
    "\n",
    "# Save\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "out_csv = RES_V3 / f\"infer_v3_defaultthr_{timestamp}.csv\"\n",
    "df_out.to_csv(out_csv, index=False)\n",
    "print(\"Saved:\", out_csv.as_posix())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34a1bdc",
   "metadata": {},
   "source": [
    "# v4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a58ba85",
   "metadata": {},
   "source": [
    "## 1: Environment, Paths, Logging, Seeds, and Sanity Checks\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a62d805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] === V4 • Cell 1: Environment & Sanity Checks ===\n",
      "[INFO] Random seeds set (GLOBAL_SEED=42).\n",
      "\n",
      "----- SANITY CHECKS -----\n",
      "Run ID:            v4_20250902_165736\n",
      "Python:            3.11.9\n",
      "PyTorch:           2.6.0+cu124\n",
      "Transformers:      4.43.3\n",
      "RDKit:             available\n",
      "PyG:               2.6.1 (scatter: missing (No module named 'torch_scatter'), sparse: missing (No module named 'torch_sparse'), cluster: missing (No module named 'torch_cluster'))\n",
      "NumPy / Pandas:    1.26.4 / 2.3.1\n",
      "Device:            cuda — NVIDIA GeForce RTX 4070 Ti\n",
      "CUDA CC:           (8, 9)\n",
      "AMP dtype:         bf16 (bf16_ok=True, fp16_ok=True)\n",
      "Device compute OK: True\n",
      "Write CKPT dir:    True (tox21_dualenc_v1\\models\\checkpoints_v4\\write_test.txt)\n",
      "Write RESULTS dir: True  (tox21_dualenc_v1\\results\\v4\\write_test.txt)\n",
      "Log file:          D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v4\\v4_run.log\n",
      "-------------------------\n",
      "\n",
      "[INFO] Environment & sanity checks complete. Proceed to dataset/config setup in Cell 2.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, sys, json, time, random, platform, warnings, textwrap, shutil, math, subprocess\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# -----------------------\n",
    "# 1) Paths & Directories\n",
    "# -----------------------\n",
    "CHECKPOINTS_DIR = Path(\"tox21_dualenc_v1/models/checkpoints_v4\")\n",
    "RESULTS_DIR     = Path(\"tox21_dualenc_v1/results/v4\")\n",
    "LOG_FILE        = RESULTS_DIR / \"v4_run.log\"\n",
    "ENV_REPORT_FILE = RESULTS_DIR / \"env_report.json\"\n",
    "\n",
    "for p in [CHECKPOINTS_DIR, RESULTS_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# 2) Basic Logging Setup\n",
    "# -----------------------\n",
    "import logging\n",
    "\n",
    "def _make_logger(name: str = \"v4\"):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(logging.DEBUG)\n",
    "\n",
    "    # Avoid duplicate handlers if cell re-run\n",
    "    for h in list(logger.handlers):\n",
    "        logger.removeHandler(h)\n",
    "\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setLevel(logging.INFO)\n",
    "    ch_fmt = logging.Formatter(\"[%(levelname)s] %(message)s\")\n",
    "    ch.setFormatter(ch_fmt)\n",
    "\n",
    "    fh = logging.FileHandler(LOG_FILE, mode=\"a\", encoding=\"utf-8\")\n",
    "    fh.setLevel(logging.DEBUG)\n",
    "    fh_fmt = logging.Formatter(\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\")\n",
    "    fh.setFormatter(fh_fmt)\n",
    "\n",
    "    logger.addHandler(ch)\n",
    "    logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "logger = _make_logger(\"v4\")\n",
    "\n",
    "logger.info(\"=== V4 • Cell 1: Environment & Sanity Checks ===\")\n",
    "\n",
    "# -----------------------\n",
    "# 3) Seed & Determinism\n",
    "# -----------------------\n",
    "def set_global_seed(seed: int = 42):\n",
    "    import numpy as np\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    try:\n",
    "        import torch\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        # Full determinism can slow GNNs; keep cudnn benchmark off but avoid forcing full determinism.\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        # Do NOT set torch.use_deterministic_algorithms(True) due to some ops used in GNNs.\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not set torch seeds: {e}\")\n",
    "\n",
    "GLOBAL_SEED = 42\n",
    "set_global_seed(GLOBAL_SEED)\n",
    "logger.info(f\"Random seeds set (GLOBAL_SEED={GLOBAL_SEED}).\")\n",
    "\n",
    "# -----------------------\n",
    "# 4) Import Key Libraries\n",
    "# -----------------------\n",
    "lib = {\n",
    "    \"torch\": None,\n",
    "    \"transformers\": None,\n",
    "    \"rdkit\": None,\n",
    "    \"torch_geometric\": None,\n",
    "    \"numpy\": None,\n",
    "    \"pandas\": None,\n",
    "    \"torch_scatter\": None,\n",
    "    \"torch_sparse\": None,\n",
    "    \"torch_cluster\": None,\n",
    "}\n",
    "\n",
    "# Import with graceful fallbacks (we only error when actually needed later)\n",
    "try:\n",
    "    import torch\n",
    "    lib[\"torch\"] = torch.__version__\n",
    "except Exception as e:\n",
    "    logger.error(f\"PyTorch not importable: {e}\")\n",
    "\n",
    "try:\n",
    "    import transformers\n",
    "    lib[\"transformers\"] = transformers.__version__\n",
    "except Exception as e:\n",
    "    logger.warning(f\"transformers not importable (ok for now): {e}\")\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    lib[\"numpy\"] = np.__version__\n",
    "except Exception as e:\n",
    "    logger.error(f\"numpy not importable: {e}\")\n",
    "\n",
    "try:\n",
    "    import pandas as pd\n",
    "    lib[\"pandas\"] = pd.__version__\n",
    "except Exception as e:\n",
    "    logger.warning(f\"pandas not importable (ok for now): {e}\")\n",
    "\n",
    "# RDKit\n",
    "try:\n",
    "    from rdkit import Chem     # noqa: F401\n",
    "    from rdkit.Chem import AllChem  # noqa: F401\n",
    "    lib[\"rdkit\"] = \"available\"\n",
    "except Exception as e:\n",
    "    lib[\"rdkit\"] = f\"missing ({e})\"\n",
    "    logger.warning(f\"RDKit not importable (we'll handle later if needed): {e}\")\n",
    "\n",
    "# PyTorch Geometric & friends\n",
    "try:\n",
    "    import torch_geometric\n",
    "    lib[\"torch_geometric\"] = torch_geometric.__version__\n",
    "    # Optional satellites:\n",
    "    try:\n",
    "        import torch_scatter\n",
    "        lib[\"torch_scatter\"] = torch_scatter.__version__\n",
    "    except Exception as e:\n",
    "        lib[\"torch_scatter\"] = f\"missing ({e})\"\n",
    "\n",
    "    try:\n",
    "        import torch_sparse\n",
    "        lib[\"torch_sparse\"] = torch_sparse.__version__\n",
    "    except Exception as e:\n",
    "        lib[\"torch_sparse\"] = f\"missing ({e})\"\n",
    "\n",
    "    try:\n",
    "        import torch_cluster\n",
    "        lib[\"torch_cluster\"] = torch_cluster.__version__\n",
    "    except Exception as e:\n",
    "        lib[\"torch_cluster\"] = f\"missing ({e})\"\n",
    "except Exception as e:\n",
    "    logger.warning(f\"torch_geometric not importable (ok for now): {e}\")\n",
    "\n",
    "# -----------------------\n",
    "# 5) Device & AMP Probes\n",
    "# -----------------------\n",
    "device_kind = \"cpu\"\n",
    "device_name = \"CPU\"\n",
    "cuda_cc = None\n",
    "bf16_ok = False\n",
    "fp16_ok = False\n",
    "\n",
    "if lib[\"torch\"] is not None and torch.cuda.is_available():\n",
    "    device_kind = \"cuda\"\n",
    "    try:\n",
    "        idx = torch.cuda.current_device()\n",
    "        device_name = torch.cuda.get_device_name(idx)\n",
    "        cuda_cc = torch.cuda.get_device_capability(idx)  # (major, minor)\n",
    "        bf16_ok = torch.cuda.is_bf16_supported()\n",
    "        # Simple FP16 check: most CUDA GPUs support fp16 math\n",
    "        fp16_ok = True\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"Could not probe CUDA device: {e}\")\n",
    "else:\n",
    "    logger.info(\"CUDA not available; defaulting to CPU.\")\n",
    "\n",
    "# Decide default AMP dtype preference\n",
    "AMP_DTYPE = \"bf16\" if bf16_ok else (\"fp16\" if fp16_ok else \"float32\")\n",
    "\n",
    "# Quick device write test\n",
    "torch_ok = (lib[\"torch\"] is not None)\n",
    "if torch_ok:\n",
    "    try:\n",
    "        _dev = torch.device(device_kind)\n",
    "        x = torch.randn(1024, 1024, device=_dev)\n",
    "        y = torch.matmul(x, x.T)\n",
    "        _ = y.mean().item()\n",
    "        dev_ok = True\n",
    "    except Exception as e:\n",
    "        dev_ok = False\n",
    "        logger.error(f\"Device compute test failed: {e}\")\n",
    "else:\n",
    "    dev_ok = False\n",
    "\n",
    "# -----------------------\n",
    "# 6) Write-permission tests\n",
    "# -----------------------\n",
    "def _write_test(path: Path, fname: str, payload: dict | str) -> tuple[bool, str]:\n",
    "    try:\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "        f = path / fname\n",
    "        if isinstance(payload, dict):\n",
    "            f.write_text(json.dumps(payload, indent=2))\n",
    "        else:\n",
    "            f.write_text(str(payload))\n",
    "        return True, str(f)\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "ok_ckpt, msg_ckpt = _write_test(CHECKPOINTS_DIR, \"write_test.txt\", \"ok\\n\")\n",
    "ok_res,  msg_res  = _write_test(RESULTS_DIR, \"write_test.txt\", \"ok\\n\")\n",
    "\n",
    "# -----------------------\n",
    "# 7) Run metadata & config stub\n",
    "# -----------------------\n",
    "RUN_ID = datetime.now().strftime(\"v4_%Y%m%d_%H%M%S\")\n",
    "\n",
    "CONFIG = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"seed\": GLOBAL_SEED,\n",
    "    \"paths\": {\n",
    "        \"checkpoints_dir\": str(CHECKPOINTS_DIR.resolve()),\n",
    "        \"results_dir\": str(RESULTS_DIR.resolve()),\n",
    "        \"log_file\": str(LOG_FILE.resolve()),\n",
    "    },\n",
    "    \"device\": {\n",
    "        \"kind\": device_kind,\n",
    "        \"name\": device_name,\n",
    "        \"cuda_compute_capability\": cuda_cc,\n",
    "        \"amp_preferred_dtype\": AMP_DTYPE,\n",
    "        \"bf16_supported\": bool(bf16_ok),\n",
    "        \"fp16_supported\": bool(fp16_ok),\n",
    "    },\n",
    "    \"libraries\": lib,\n",
    "    # Model choices will be finalized in later cells; we pin high-level intent here.\n",
    "    \"model_plan\": {\n",
    "        \"text_encoder\": \"ChemBERTa-100M-MLM\",\n",
    "        \"graph_encoder\": \"GIN\",   # <- per your request (not GINE)\n",
    "        \"use_virtual_node\": True,\n",
    "        \"fusion\": \"Descriptor-Gated Bi-Directional Cross-Attention\",\n",
    "        \"aux_losses\": {\"nce\": 0.1, \"attn_l1\": 1e-4}\n",
    "    },\n",
    "    \"notes\": \"Initial V4 environment/config stub. Details to be concretized in subsequent cells.\"\n",
    "}\n",
    "\n",
    "# Save env report & config stub\n",
    "env_report = {\n",
    "    \"platform\": {\n",
    "        \"python\": sys.version.split()[0],\n",
    "        \"platform\": platform.platform(),\n",
    "        \"executable\": sys.executable,\n",
    "        \"cwd\": str(Path.cwd())\n",
    "    },\n",
    "    \"env\": {\n",
    "        \"CUDA_VISIBLE_DEVICES\": os.environ.get(\"CUDA_VISIBLE_DEVICES\", None),\n",
    "        \"OMP_NUM_THREADS\": os.environ.get(\"OMP_NUM_THREADS\", None),\n",
    "    },\n",
    "    \"libraries\": lib,\n",
    "    \"device\": CONFIG[\"device\"],\n",
    "    \"paths\": CONFIG[\"paths\"],\n",
    "    \"checks\": {\n",
    "        \"device_compute_ok\": dev_ok,\n",
    "        \"write_checkpoints_ok\": ok_ckpt,\n",
    "        \"write_results_ok\": ok_res\n",
    "    }\n",
    "}\n",
    "\n",
    "# Write files\n",
    "try:\n",
    "    (RESULTS_DIR / f\"{RUN_ID}_config.json\").write_text(json.dumps(CONFIG, indent=2))\n",
    "    ENV_REPORT_FILE.write_text(json.dumps(env_report, indent=2))\n",
    "except Exception as e:\n",
    "    logger.error(f\"Failed to write env/config files: {e}\")\n",
    "\n",
    "# -----------------------\n",
    "# 8) Surface a concise sanity summary\n",
    "# -----------------------\n",
    "summary_lines = [\n",
    "    \"\",\n",
    "    \"----- SANITY CHECKS -----\",\n",
    "    f\"Run ID:            {RUN_ID}\",\n",
    "    f\"Python:            {env_report['platform']['python']}\",\n",
    "    f\"PyTorch:           {lib['torch']}\",\n",
    "    f\"Transformers:      {lib['transformers']}\",\n",
    "    f\"RDKit:             {lib['rdkit']}\",\n",
    "    f\"PyG:               {lib['torch_geometric']} (scatter: {lib['torch_scatter']}, sparse: {lib['torch_sparse']}, cluster: {lib['torch_cluster']})\",\n",
    "    f\"NumPy / Pandas:    {lib['numpy']} / {lib['pandas']}\",\n",
    "    f\"Device:            {device_kind} — {device_name}\",\n",
    "    f\"CUDA CC:           {cuda_cc}\",\n",
    "    f\"AMP dtype:         {AMP_DTYPE} (bf16_ok={bf16_ok}, fp16_ok={fp16_ok})\",\n",
    "    f\"Device compute OK: {dev_ok}\",\n",
    "    f\"Write CKPT dir:    {ok_ckpt} ({msg_ckpt})\",\n",
    "    f\"Write RESULTS dir: {ok_res}  ({msg_res})\",\n",
    "    f\"Log file:          {LOG_FILE.resolve()}\",\n",
    "    \"-------------------------\",\n",
    "    \"\"\n",
    "]\n",
    "print(\"\\n\".join(summary_lines))\n",
    "\n",
    "logger.info(\"Environment & sanity checks complete. Proceed to dataset/config setup in Cell 2.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8d5a89",
   "metadata": {},
   "source": [
    "## 2: Data Config, Auto-Discovery, Schema Validation, RDKit & PyG Probes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01afe231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Discovered in tox21_dualenc_v1/data/raw: {'tox21': 'tox21.csv', 'dataset_selected': 'dataset_selected.csv', 'dataset_selected_train': 'dataset_selected_train.csv', 'dataset_selected_val': 'dataset_selected_val.csv', 'dataset_selected_test': 'dataset_selected_test.csv'}\n",
      "\n",
      "----- SANITY SUMMARY -----\n",
      "Base dir:               D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\data\\raw\n",
      "Discovered:             {'tox21': 'tox21.csv', 'dataset_selected': 'dataset_selected.csv', 'dataset_selected_train': 'dataset_selected_train.csv', 'dataset_selected_val': 'dataset_selected_val.csv', 'dataset_selected_test': 'dataset_selected_test.csv'}\n",
      "Primary dataset:        dataset_selected\n",
      "Shape (main):           [7831, 271]\n",
      "SMILES column:          smiles\n",
      "ID column:              None\n",
      "Label cols (n=12): ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']\n",
      "Extra numeric features: 257\n",
      "RDKit parse (n=200): 200 OK, 0 FAIL\n",
      "Pre-splits present:     True\n",
      "  Shapes (train/val/test): {'train': [6265, 271], 'val': [783, 271], 'test': [783, 271]}\n",
      "  Columns match:        True\n",
      "  Leakage (overlaps):   {'overlap_train_val': 0, 'overlap_train_test': 0, 'overlap_val_test': 0}\n",
      "PyG GIN forward probe:  True (OK)\n",
      "--------------------------\n",
      "\n",
      "=== COPY-PASTE DEBUG BLOCK ===\n",
      "{\n",
      "  \"which_main\": \"dataset_selected\",\n",
      "  \"shape_main\": [\n",
      "    7831,\n",
      "    271\n",
      "  ],\n",
      "  \"smiles_col\": \"smiles\",\n",
      "  \"id_col\": null,\n",
      "  \"labels\": [\n",
      "    \"NR-AR\",\n",
      "    \"NR-AR-LBD\",\n",
      "    \"NR-AhR\",\n",
      "    \"NR-Aromatase\",\n",
      "    \"NR-ER\",\n",
      "    \"NR-ER-LBD\",\n",
      "    \"NR-PPAR-gamma\",\n",
      "    \"SR-ARE\",\n",
      "    \"SR-ATAD5\",\n",
      "    \"SR-HSE\",\n",
      "    \"SR-MMP\",\n",
      "    \"SR-p53\"\n",
      "  ],\n",
      "  \"n_extra_numeric_features\": 257,\n",
      "  \"using_presplits\": true,\n",
      "  \"presplit_shapes\": {\n",
      "    \"train\": [\n",
      "      6265,\n",
      "      271\n",
      "    ],\n",
      "    \"val\": [\n",
      "      783,\n",
      "      271\n",
      "    ],\n",
      "    \"test\": [\n",
      "      783,\n",
      "      271\n",
      "    ]\n",
      "  },\n",
      "  \"presplit_leak\": {\n",
      "    \"overlap_train_val\": 0,\n",
      "    \"overlap_train_test\": 0,\n",
      "    \"overlap_val_test\": 0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, re, random\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "logger = logging.getLogger(\"v4\")\n",
    "\n",
    "RAW_DIR = Path(\"tox21_dualenc_v1/data/raw\")\n",
    "RESULT_JSON = RESULTS_DIR / f\"{RUN_ID}_dataset_report.json\"\n",
    "\n",
    "RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Canonical Tox21 labels\n",
    "CANON_LABELS = [\n",
    "    \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n",
    "    \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-gamma\",\n",
    "    \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n",
    "]\n",
    "\n",
    "def find_csvs(base: Path) -> Dict[str, Path]:\n",
    "    \"\"\"Return discovered CSVs keyed by simple names.\"\"\"\n",
    "    found = {}\n",
    "    # main raw\n",
    "    p = base / \"tox21.csv\"\n",
    "    if p.exists(): found[\"tox21\"] = p\n",
    "    # enriched set(s)\n",
    "    for name in [\"dataset_selected.csv\", \"dataset_selected_train.csv\",\n",
    "                 \"dataset_selected_val.csv\", \"dataset_selected_test.csv\"]:\n",
    "        q = base / name\n",
    "        if q.exists(): found[name.replace(\".csv\",\"\")] = q\n",
    "    return found\n",
    "\n",
    "def detect_cols(df: pd.DataFrame) -> Tuple[Optional[str], Optional[str], List[str], List[str]]:\n",
    "    \"\"\"Return (smiles_col, id_col, label_cols, extra_numeric_feature_cols).\"\"\"\n",
    "    cols = list(df.columns)\n",
    "    # SMILES guess\n",
    "    smiles_col = None\n",
    "    for cand in [\"smiles\",\"SMILES\",\"Smiles\",\"canonical_smiles\",\"smiles_str\"]:\n",
    "        if cand in cols:\n",
    "            smiles_col = cand; break\n",
    "    if smiles_col is None:\n",
    "        for c in cols:\n",
    "            if \"smile\" in c.lower():\n",
    "                smiles_col = c; break\n",
    "    # ID guess\n",
    "    id_col = None\n",
    "    for cand in [\"mol_id\",\"molecule_id\",\"id\",\"compound_id\"]:\n",
    "        if cand in cols:\n",
    "            id_col = cand; break\n",
    "    # Labels\n",
    "    if all(lbl in cols for lbl in CANON_LABELS):\n",
    "        labels = CANON_LABELS.copy()\n",
    "    else:\n",
    "        # Infer binary columns (quick sample)\n",
    "        bin_candidates = []\n",
    "        for c in cols:\n",
    "            if c in {smiles_col, id_col}: continue\n",
    "            s = df[c]\n",
    "            if not pd.api.types.is_numeric_dtype(s): \n",
    "                continue\n",
    "            v = s.dropna()\n",
    "            if v.empty: \n",
    "                continue\n",
    "            u = set(v.sample(min(200, len(v)), random_state=GLOBAL_SEED).unique().tolist())\n",
    "            if u.issubset({0,1}):\n",
    "                bin_candidates.append(c)\n",
    "        labels = bin_candidates[:12] if len(bin_candidates) else []\n",
    "    # Extra numeric features (descriptors)\n",
    "    exclude = set([_ for _ in [smiles_col, id_col] if _] + labels)\n",
    "    extras = [c for c in cols if c not in exclude and pd.api.types.is_numeric_dtype(df[c])]\n",
    "    return smiles_col, id_col, labels, extras\n",
    "\n",
    "def rdkit_probe(smiles: List[str], n:int=200) -> Tuple[int, List[str]]:\n",
    "    from rdkit import Chem\n",
    "    rng = np.random.default_rng(GLOBAL_SEED)\n",
    "    if len(smiles) == 0:\n",
    "        return 0, []\n",
    "    idx = rng.choice(len(smiles), size=min(n, len(smiles)), replace=False)\n",
    "    ok = 0\n",
    "    fails = []\n",
    "    for i in idx:\n",
    "        s = str(smiles[i])\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        if m is not None: ok += 1\n",
    "        elif len(fails) < 5: fails.append(s)\n",
    "    return ok, fails\n",
    "\n",
    "def label_stats(df: pd.DataFrame, labels: List[str]) -> Dict[str, Dict[str, float]]:\n",
    "    stats = {}\n",
    "    for c in labels:\n",
    "        s = df[c]\n",
    "        v = s.dropna()\n",
    "        pos = (v==1).mean() if len(v) else np.nan\n",
    "        stats[c] = {\n",
    "            \"count\": int(s.notna().sum()),\n",
    "            \"pos_rate\": None if np.isnan(pos) else float(round(pos,4)),\n",
    "            \"nan_frac\": float(round(1 - s.notna().mean(), 4))\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "def split_leak_check(d_train: pd.DataFrame, d_val: pd.DataFrame, d_test: pd.DataFrame, id_col: Optional[str]) -> Dict[str, int]:\n",
    "    \"\"\"Simple overlap checks by id if available, otherwise by SMILES.\"\"\"\n",
    "    key = id_col if id_col and id_col in d_train.columns else None\n",
    "    if key is None:  # fallback to SMILES\n",
    "        # Re-detect smiles column quickly\n",
    "        sm_col, _, _, _ = detect_cols(d_train)\n",
    "        key = sm_col\n",
    "    if key is None or key not in d_train.columns:\n",
    "        return {\"overlap_train_val\": -1, \"overlap_train_test\": -1, \"overlap_val_test\": -1}\n",
    "    s_train = set(d_train[key].astype(str).tolist())\n",
    "    s_val   = set(d_val[key].astype(str).tolist())\n",
    "    s_test  = set(d_test[key].astype(str).tolist())\n",
    "    return {\n",
    "        \"overlap_train_val\": len(s_train & s_val),\n",
    "        \"overlap_train_test\": len(s_train & s_test),\n",
    "        \"overlap_val_test\": len(s_val & s_test)\n",
    "    }\n",
    "\n",
    "# 1) Discover files\n",
    "found = find_csvs(RAW_DIR)\n",
    "logger.info(f\"Discovered in {RAW_DIR.as_posix()}: { {k: v.name for k,v in found.items()} }\")\n",
    "\n",
    "df_main = None\n",
    "df_train = df_val = df_test = None\n",
    "using_presplits = False\n",
    "which_main = None\n",
    "\n",
    "# 2) Load enriched dataset if available; else raw\n",
    "if \"dataset_selected\" in found:\n",
    "    which_main = \"dataset_selected\"\n",
    "    df_main = pd.read_csv(found[\"dataset_selected\"])\n",
    "elif \"tox21\" in found:\n",
    "    which_main = \"tox21\"\n",
    "    df_main = pd.read_csv(found[\"tox21\"])\n",
    "else:\n",
    "    which_main = None\n",
    "\n",
    "# 3) If pre-splits exist, load them too (we’ll still do scaffold-CV later; this is for sanity)\n",
    "for k in [\"dataset_selected_train\",\"dataset_selected_val\",\"dataset_selected_test\"]:\n",
    "    if k in found:\n",
    "        using_presplits = True\n",
    "if using_presplits:\n",
    "    df_train = pd.read_csv(found.get(\"dataset_selected_train\"))\n",
    "    df_val   = pd.read_csv(found.get(\"dataset_selected_val\"))\n",
    "    df_test  = pd.read_csv(found.get(\"dataset_selected_test\"))\n",
    "\n",
    "# 4) Detect schema(s)\n",
    "smiles_col = id_col = None\n",
    "label_cols = []\n",
    "extra_feats = []\n",
    "\n",
    "if df_main is not None:\n",
    "    smiles_col, id_col, label_cols, extra_feats = detect_cols(df_main)\n",
    "\n",
    "# 5) RDKit sanity on main\n",
    "rdkit_ok, rdkit_fail = (0,[])\n",
    "if df_main is not None and smiles_col is not None:\n",
    "    rdkit_ok, rdkit_fail = rdkit_probe(df_main[smiles_col].astype(str).tolist(), n=200)\n",
    "\n",
    "# 6) Basic stats\n",
    "shape_main = list(df_main.shape) if df_main is not None else [0,0]\n",
    "stats_labels = label_stats(df_main, label_cols) if df_main is not None and label_cols else {}\n",
    "\n",
    "# 7) Pre-split checks (if provided)\n",
    "leak = {}\n",
    "shapes_splits = {}\n",
    "cols_match = True\n",
    "if using_presplits:\n",
    "    shapes_splits = {\n",
    "        \"train\": list(df_train.shape),\n",
    "        \"val\":   list(df_val.shape),\n",
    "        \"test\":  list(df_test.shape)\n",
    "    }\n",
    "    # Check columns match\n",
    "    base_cols = list(df_train.columns)\n",
    "    cols_match = (base_cols == list(df_val.columns) == list(df_test.columns))\n",
    "    # Leakage check\n",
    "    leak = split_leak_check(df_train, df_val, df_test, id_col=id_col)\n",
    "\n",
    "# 8) PyG probe (quick)\n",
    "pyg_probe_ok = False\n",
    "gin_forward_error = None\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch_geometric.nn import GINConv, global_add_pool\n",
    "    x = torch.randn(3, 8, device=\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    edge_index = torch.tensor([[0,1,2,0,1,2],[1,2,0,2,0,1]], dtype=torch.long, device=x.device)\n",
    "    nn_lin = nn.Sequential(nn.Linear(8, 16), nn.ReLU(), nn.Linear(16, 16))\n",
    "    gin = GINConv(nn_lin).to(x.device)\n",
    "    out = gin(x, edge_index)\n",
    "    pooled = global_add_pool(out, torch.tensor([0,0,0], device=x.device))\n",
    "    _ = pooled.sum().item()\n",
    "    pyg_probe_ok = True\n",
    "except Exception as e:\n",
    "    gin_forward_error = str(e)\n",
    "\n",
    "# 9) Persist report\n",
    "report = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"directory\": str(RAW_DIR.resolve()),\n",
    "    \"which_main\": which_main,\n",
    "    \"using_presplits\": bool(using_presplits),\n",
    "    \"paths\": {k: str(v.resolve()) for k,v in found.items()},\n",
    "    \"shape_main\": shape_main,\n",
    "    \"smiles_col\": smiles_col,\n",
    "    \"id_col\": id_col,\n",
    "    \"label_cols\": label_cols,\n",
    "    \"n_extra_numeric_features\": len(extra_feats),\n",
    "    \"label_stats\": stats_labels,\n",
    "    \"rdkit_probe\": {\n",
    "        \"sample\": min(200, shape_main[0]) if shape_main[0] else 0,\n",
    "        \"ok\": rdkit_ok,\n",
    "        \"fail_examples\": rdkit_fail\n",
    "    },\n",
    "    \"presplit_shapes\": shapes_splits,\n",
    "    \"presplit_cols_match\": cols_match,\n",
    "    \"presplit_leak_check\": leak,\n",
    "    \"pyg_gin_forward_ok\": bool(pyg_probe_ok),\n",
    "    \"pyg_error\": gin_forward_error\n",
    "}\n",
    "RESULT_JSON.write_text(json.dumps(report, indent=2))\n",
    "\n",
    "# 10) Update CONFIG\n",
    "CONFIG.setdefault(\"data\", {})\n",
    "CONFIG[\"data\"].update({\n",
    "    \"base_dir\": str(RAW_DIR.resolve()),\n",
    "    \"which_main\": which_main,\n",
    "    \"csv_main\": str(found[which_main].resolve()) if which_main else None,\n",
    "    \"csv_train\": str(found.get(\"dataset_selected_train\", \"\")),\n",
    "    \"csv_val\":   str(found.get(\"dataset_selected_val\", \"\")),\n",
    "    \"csv_test\":  str(found.get(\"dataset_selected_test\", \"\")),\n",
    "    \"using_presplits\": bool(using_presplits),\n",
    "    \"columns\": {\n",
    "        \"smiles\": smiles_col,\n",
    "        \"id\": id_col,\n",
    "        \"labels\": label_cols,\n",
    "        \"extra_numeric_features_count\": len(extra_feats)\n",
    "    }\n",
    "})\n",
    "(RESULTS_DIR / f\"{RUN_ID}_config.json\").write_text(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "# 11) Human summary + a ready-to-copy debug block\n",
    "summary = []\n",
    "summary += [\n",
    "    \"\",\n",
    "    \"----- SANITY SUMMARY -----\",\n",
    "    f\"Base dir:               {report['directory']}\",\n",
    "    f\"Discovered:             { {k: Path(v).name for k,v in report['paths'].items()} }\",\n",
    "    f\"Primary dataset:        {which_main or 'NOT FOUND'}\",\n",
    "    f\"Shape (main):           {shape_main}\",\n",
    "    f\"SMILES column:          {smiles_col}\",\n",
    "    f\"ID column:              {id_col}\",\n",
    "    f\"Label cols (n={len(label_cols)}): {label_cols}\",\n",
    "    f\"Extra numeric features: {len(extra_feats)}\",\n",
    "    f\"RDKit parse (n={report['rdkit_probe']['sample']}): {rdkit_ok} OK, {report['rdkit_probe']['sample']-rdkit_ok} FAIL\",\n",
    "]\n",
    "if rdkit_fail:\n",
    "    summary.append(f\"  Fail examples (≤5): {rdkit_fail}\")\n",
    "if using_presplits:\n",
    "    summary += [\n",
    "        f\"Pre-splits present:     True\",\n",
    "        f\"  Shapes (train/val/test): {shapes_splits}\",\n",
    "        f\"  Columns match:        {cols_match}\",\n",
    "        f\"  Leakage (overlaps):   {leak}\",\n",
    "    ]\n",
    "else:\n",
    "    summary.append(\"Pre-splits present:     False\")\n",
    "summary += [\n",
    "    f\"PyG GIN forward probe:  {pyg_probe_ok} {'(OK)' if pyg_probe_ok else f'(ERROR: {gin_forward_error})'}\",\n",
    "    \"--------------------------\",\n",
    "    \"\",\n",
    "]\n",
    "print(\"\\n\".join(summary))\n",
    "\n",
    "# Copy-paste debug block we will reuse later (stick this into messages if I ask for it)\n",
    "debug_block = {\n",
    "    \"which_main\": which_main,\n",
    "    \"shape_main\": shape_main,\n",
    "    \"smiles_col\": smiles_col,\n",
    "    \"id_col\": id_col,\n",
    "    \"labels\": label_cols,\n",
    "    \"n_extra_numeric_features\": len(extra_feats),\n",
    "    \"using_presplits\": bool(using_presplits),\n",
    "    \"presplit_shapes\": shapes_splits,\n",
    "    \"presplit_leak\": leak\n",
    "}\n",
    "print(\"=== COPY-PASTE DEBUG BLOCK ===\")\n",
    "print(json.dumps(debug_block, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4689e8",
   "metadata": {},
   "source": [
    "## 3: Dataset loader, descriptor scaler, RDKit→PyG graph cache, pos_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e232d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- SUMMARY -----\n",
      "Train/Val/Test sizes: 6265 / 783 / 783\n",
      "Descriptor dims: 257  |  NaN (train) median-imputed, then standardized\n",
      "Saved scaler: tox21_dualenc_v1\\results\\v4\\v4_20250902_165736_desc_scaler.json\n",
      "Saved pos_weight: tox21_dualenc_v1\\results\\v4\\v4_20250902_165736_pos_weight.json\n",
      "Graph cache dir: tox21_dualenc_v1\\data\\cache_v4  (files: ~7048 after first epoch)\n",
      "GIN dry-run: {'n_graphs': '8', 'in_dim': '20', 'pooled_dim': '64', 'n_nodes_total': '135', 'n_edges': '290'}\n",
      "-------------------\n",
      "\n",
      "=== COPY-PASTE DEBUG BLOCK #2 ===\n",
      "{\n",
      "  \"desc_cols\": 257,\n",
      "  \"desc_nan_mean_frac\": 0.0,\n",
      "  \"pos_weight\": {\n",
      "    \"NR-AR\": 23.00383186340332,\n",
      "    \"NR-AR-LBD\": 28.83333396911621,\n",
      "    \"NR-AhR\": 8.897314071655273,\n",
      "    \"NR-Aromatase\": 25.659574508666992,\n",
      "    \"NR-ER\": 8.378742218017578,\n",
      "    \"NR-ER-LBD\": 19.745033264160156,\n",
      "    \"NR-PPAR-gamma\": 43.43262481689453,\n",
      "    \"SR-ARE\": 7.4547905921936035,\n",
      "    \"SR-ATAD5\": 29.12019157409668,\n",
      "    \"SR-HSE\": 20.382251739501953,\n",
      "    \"SR-MMP\": 7.265171527862549,\n",
      "    \"SR-p53\": 19.144695281982422\n",
      "  },\n",
      "  \"gin_probe\": {\n",
      "    \"n_graphs\": \"8\",\n",
      "    \"in_dim\": \"20\",\n",
      "    \"pooled_dim\": \"64\",\n",
      "    \"n_nodes_total\": \"135\",\n",
      "    \"n_edges\": \"290\"\n",
      "  }\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8261"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, math, pickle, hashlib, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdchem\n",
    "\n",
    "from torch_geometric.data import Data as GeoData\n",
    "from torch_geometric.loader import DataLoader as GeoLoader\n",
    "from torch_geometric.nn import GINConv, global_add_pool\n",
    "\n",
    "logger = logging.getLogger(\"v4\")\n",
    "\n",
    "# -----------------------\n",
    "# Config pulled from Cell 2\n",
    "# -----------------------\n",
    "assert CONFIG[\"data\"][\"using_presplits\"] is True, \"Expected pre-splits present.\"\n",
    "DATA_DIR   = Path(CONFIG[\"data\"][\"base_dir\"])\n",
    "CSV_TRAIN  = Path(CONFIG[\"data\"][\"csv_train\"])\n",
    "CSV_VAL    = Path(CONFIG[\"data\"][\"csv_val\"])\n",
    "CSV_TEST   = Path(CONFIG[\"data\"][\"csv_test\"])\n",
    "SMILES_COL = CONFIG[\"data\"][\"columns\"][\"smiles\"]\n",
    "ID_COL     = CONFIG[\"data\"][\"columns\"][\"id\"]  # may be None\n",
    "LABELS     = CONFIG[\"data\"][\"columns\"][\"labels\"]\n",
    "assert SMILES_COL and len(LABELS) == 12, \"SMILES column or 12 Tox21 labels not detected.\"\n",
    "\n",
    "# Where we persist artifacts\n",
    "CACHE_DIR    = Path(\"tox21_dualenc_v1/data/cache_v4\")\n",
    "CACHE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SCALER_FILE  = RESULTS_DIR / f\"{RUN_ID}_desc_scaler.json\"\n",
    "POSW_FILE    = RESULTS_DIR / f\"{RUN_ID}_pos_weight.json\"\n",
    "SCHEMA_FILE  = RESULTS_DIR / f\"{RUN_ID}_schema.json\"\n",
    "CACHE_README = CACHE_DIR / \"README.txt\"\n",
    "if not CACHE_README.exists():\n",
    "    CACHE_README.write_text(\"V4 graph cache (PyG Data pickles) keyed by sha1 of SMILES.\\n\")\n",
    "\n",
    "# -----------------------\n",
    "# 1) Load splits\n",
    "# -----------------------\n",
    "df_train = pd.read_csv(CSV_TRAIN)\n",
    "df_val   = pd.read_csv(CSV_VAL)\n",
    "df_test  = pd.read_csv(CSV_TEST)\n",
    "\n",
    "# Hard-assert schema\n",
    "def _assert_schema(df: pd.DataFrame, name: str):\n",
    "    miss = [c for c in [SMILES_COL] + LABELS if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"{name}: missing required columns: {miss}\")\n",
    "_assert_schema(df_train, \"train\"); _assert_schema(df_val, \"val\"); _assert_schema(df_test, \"test\")\n",
    "\n",
    "# -----------------------\n",
    "# 2) Descriptor columns and scaler (train-only fit)\n",
    "# -----------------------\n",
    "# Numeric, excluding labels + smiles + id\n",
    "exclude = set([SMILES_COL] + LABELS + ([ID_COL] if ID_COL else []))\n",
    "desc_cols = [c for c in df_train.columns if c not in exclude and pd.api.types.is_numeric_dtype(df_train[c])]\n",
    "# Keep same order across splits\n",
    "def _ensure_cols(df: pd.DataFrame, cols: List[str], name: str):\n",
    "    miss = [c for c in cols if c not in df.columns]\n",
    "    if miss:\n",
    "        raise ValueError(f\"{name}: descriptor columns missing vs train: {miss}\")\n",
    "_ensure_cols(df_val, desc_cols, \"val\"); _ensure_cols(df_test, desc_cols, \"test\")\n",
    "\n",
    "# Compute NaN stats on train\n",
    "nan_frac = {c: float(1 - df_train[c].notna().mean()) for c in desc_cols}\n",
    "# Impute with TRAIN median\n",
    "train_medians = {c: float(df_train[c].median()) if df_train[c].notna().any() else 0.0 for c in desc_cols}\n",
    "\n",
    "def _impute_and_stack(df: pd.DataFrame, cols: List[str], med: Dict[str,float]) -> np.ndarray:\n",
    "    X = df[cols].copy()\n",
    "    for c in cols:\n",
    "        X[c] = X[c].fillna(med[c])\n",
    "    return X.values.astype(np.float32)\n",
    "\n",
    "X_train_desc = _impute_and_stack(df_train, desc_cols, train_medians)\n",
    "X_val_desc   = _impute_and_stack(df_val,   desc_cols, train_medians)\n",
    "X_test_desc  = _impute_and_stack(df_test,  desc_cols, train_medians)\n",
    "\n",
    "# Standardize with TRAIN stats\n",
    "mu = X_train_desc.mean(axis=0)\n",
    "sd = X_train_desc.std(axis=0)\n",
    "sd[sd == 0] = 1.0\n",
    "\n",
    "X_train_desc = (X_train_desc - mu) / sd\n",
    "X_val_desc   = (X_val_desc   - mu) / sd\n",
    "X_test_desc  = (X_test_desc  - mu) / sd\n",
    "\n",
    "scaler_dict = {\n",
    "    \"columns\": desc_cols,\n",
    "    \"median\": {c: train_medians[c] for c in desc_cols},\n",
    "    \"mean\": mu.tolist(),\n",
    "    \"std\": sd.tolist(),\n",
    "    \"nan_frac_train\": nan_frac,\n",
    "}\n",
    "SCALER_FILE.write_text(json.dumps(scaler_dict, indent=2))\n",
    "\n",
    "# -----------------------\n",
    "# 3) Labels tensor & pos_weight on TRAIN\n",
    "# -----------------------\n",
    "Y_train = df_train[LABELS].values.astype(np.float32)\n",
    "Y_val   = df_val[LABELS].values.astype(np.float32)\n",
    "Y_test  = df_test[LABELS].values.astype(np.float32)\n",
    "\n",
    "# pos_weight = (N - P) / P computed on non-NaN labels (treat NaN as missing)\n",
    "def _pos_weight(y: np.ndarray) -> np.ndarray:\n",
    "    # y: [N, C] with {0,1} or possibly NaN; treat NaN as ignore\n",
    "    w = []\n",
    "    for j in range(y.shape[1]):\n",
    "        col = y[:, j]\n",
    "        m = ~np.isnan(col)\n",
    "        if m.sum() == 0:\n",
    "            w.append(1.0)\n",
    "            continue\n",
    "        P = (col[m] == 1).sum()\n",
    "        N = m.sum()\n",
    "        P = max(P, 1)  # avoid div by zero\n",
    "        w.append(float((N - P) / P))\n",
    "    return np.array(w, dtype=np.float32)\n",
    "\n",
    "pos_w = _pos_weight(Y_train.copy())\n",
    "POSW_FILE.write_text(json.dumps({\"labels\": LABELS, \"pos_weight\": pos_w.tolist()}, indent=2))\n",
    "\n",
    "# -----------------------\n",
    "# 4) RDKit → PyG graph featurizer with caching\n",
    "# -----------------------\n",
    "def _sha1(s: str) -> str:\n",
    "    return hashlib.sha1(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# Atom feature spec (simple but effective for GIN)\n",
    "_ATOM_FSET = {\n",
    "    \"atomic_num\": list(range(1, 119)),  # 1..118\n",
    "    \"chirality\": [ch for ch in rdchem.ChiralType.values],  # enum\n",
    "    \"hybridization\": [hy for hy in rdchem.HybridizationType.values],\n",
    "    \"formal_charge\": list(range(-3, 4)),\n",
    "    \"implicit_valence\": list(range(0, 7)),\n",
    "    \"degree\": list(range(0, 7)),\n",
    "}\n",
    "\n",
    "def atom_to_feat(atom: rdchem.Atom) -> np.ndarray:\n",
    "    Z = atom.GetAtomicNum()\n",
    "    feats = [\n",
    "        Z / 118.0,\n",
    "        float(atom.GetIsAromatic()),\n",
    "        float(atom.IsInRing()),\n",
    "        float(atom.GetTotalNumHs(includeNeighbors=True)),\n",
    "        float(atom.GetMass() / 250.0),  # scaled approx\n",
    "        float(atom.GetFormalCharge()),\n",
    "        float(atom.GetImplicitValence()),\n",
    "        float(atom.GetTotalValence()),\n",
    "        float(atom.GetDegree()),\n",
    "        float(atom.GetNoImplicit()),\n",
    "    ]\n",
    "    # embed-like one-hots kept small: only for common Z<=10; else 0-vector (we'll let MLP learn)\n",
    "    common_Z = min(Z, 10)\n",
    "    z_oh = np.zeros(10, dtype=np.float32)\n",
    "    if 1 <= common_Z <= 10:\n",
    "        z_oh[common_Z - 1] = 1.0\n",
    "    feats.extend(z_oh.tolist())\n",
    "    return np.asarray(feats, dtype=np.float32)  # dim ~ 10(one-hot)+10(scalars)=20\n",
    "\n",
    "def mol_to_graph(smiles: str) -> Optional[GeoData]:\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    if m is None:\n",
    "        return None\n",
    "    Chem.Kekulize(m, clearAromaticFlags=False)\n",
    "    n = m.GetNumAtoms()\n",
    "    # Node features\n",
    "    x = np.vstack([atom_to_feat(m.GetAtomWithIdx(i)) for i in range(n)]) if n > 0 else np.zeros((0, 20), dtype=np.float32)\n",
    "    # Edges (undirected)\n",
    "    rows, cols = [], []\n",
    "    for b in m.GetBonds():\n",
    "        u = b.GetBeginAtomIdx()\n",
    "        v = b.GetEndAtomIdx()\n",
    "        rows += [u, v]; cols += [v, u]\n",
    "    edge_index = torch.tensor([rows, cols], dtype=torch.long)\n",
    "    data = GeoData(\n",
    "        x=torch.tensor(x, dtype=torch.float32),\n",
    "        edge_index=edge_index,\n",
    "        num_nodes=n,\n",
    "    )\n",
    "    return data\n",
    "\n",
    "def load_or_build_graph(smiles: str) -> Optional[GeoData]:\n",
    "    key = _sha1(smiles)\n",
    "    f = CACHE_DIR / f\"{key}.pkl\"\n",
    "    if f.exists():\n",
    "        try:\n",
    "            with open(f, \"rb\") as h:\n",
    "                return pickle.load(h)\n",
    "        except Exception:\n",
    "            pass  # rebuild\n",
    "    g = mol_to_graph(smiles)\n",
    "    if g is not None:\n",
    "        try:\n",
    "            with open(f, \"wb\") as h:\n",
    "                pickle.dump(g, h, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Failed to cache graph {f.name}: {e}\")\n",
    "    return g\n",
    "\n",
    "# -----------------------\n",
    "# 5) Torch Dataset\n",
    "# -----------------------\n",
    "class Tox21V4Dataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, smiles_col: str, labels: List[str], X_desc: np.ndarray):\n",
    "        assert len(df) == len(X_desc)\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.smiles_col = smiles_col\n",
    "        self.labels = labels\n",
    "        self.X_desc = X_desc.astype(np.float32)\n",
    "        self.has_nan_label = np.isnan(self.df[self.labels].values).any()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        smi = str(row[self.smiles_col])\n",
    "        y = row[self.labels].values.astype(np.float32)\n",
    "        # Graph\n",
    "        g = load_or_build_graph(smi)\n",
    "        if g is None or g.num_nodes == 0 or g.edge_index.numel() == 0:\n",
    "            # Fallback: single isolated node to avoid crashing the batch; mark mask to ignore later if needed\n",
    "            g = GeoData(\n",
    "                x=torch.zeros((1, 20), dtype=torch.float32),\n",
    "                edge_index=torch.zeros((2,0), dtype=torch.long),\n",
    "                num_nodes=1,\n",
    "            )\n",
    "        g.smiles = smi\n",
    "        # Descriptors\n",
    "        x_desc = self.X_desc[idx]\n",
    "        return g, torch.from_numpy(x_desc), torch.from_numpy(y)\n",
    "\n",
    "# -----------------------\n",
    "# 6) Build dataset objects\n",
    "# -----------------------\n",
    "ds_train = Tox21V4Dataset(df_train, SMILES_COL, LABELS, X_train_desc)\n",
    "ds_val   = Tox21V4Dataset(df_val,   SMILES_COL, LABELS, X_val_desc)\n",
    "ds_test  = Tox21V4Dataset(df_test,  SMILES_COL, LABELS, X_test_desc)\n",
    "\n",
    "# -----------------------\n",
    "# 7) Tiny dry-run: graph batch + one GIN layer\n",
    "# -----------------------\n",
    "def tiny_gin_probe(dataset: Tox21V4Dataset, k: int = 8) -> Dict[str, str]:\n",
    "    k = min(k, len(dataset))\n",
    "    idxs = np.random.default_rng(GLOBAL_SEED).choice(len(dataset), size=k, replace=False)\n",
    "    graphs, descs, labels = [], [], []\n",
    "    for i in idxs:\n",
    "        g, d, y = dataset[i]\n",
    "        graphs.append(g); descs.append(d); labels.append(y)\n",
    "    # PyG loader will build batch indices for us\n",
    "    loader = GeoLoader(graphs, batch_size=k, shuffle=False)\n",
    "    batch = next(iter(loader))\n",
    "    # Quick GIN\n",
    "    in_dim = batch.x.size(-1)\n",
    "    mlp = nn.Sequential(nn.Linear(in_dim, 64), nn.ReLU(), nn.Linear(64, 64))\n",
    "    gin = GINConv(mlp).to(batch.x.device)\n",
    "    out = gin(batch.x, batch.edge_index)\n",
    "    pooled = global_add_pool(out, batch.batch)\n",
    "    _ = pooled.mean().item()\n",
    "    return {\n",
    "        \"n_graphs\": str(k),\n",
    "        \"in_dim\": str(in_dim),\n",
    "        \"pooled_dim\": str(pooled.size(-1)),\n",
    "        \"n_nodes_total\": str(batch.x.size(0)),\n",
    "        \"n_edges\": str(batch.edge_index.size(1))\n",
    "    }\n",
    "\n",
    "gin_probe = tiny_gin_probe(ds_train, k=8)\n",
    "\n",
    "# -----------------------\n",
    "# 8) Save schema snapshot\n",
    "# -----------------------\n",
    "schema = {\n",
    "    \"labels\": LABELS,\n",
    "    \"smiles_col\": SMILES_COL,\n",
    "    \"id_col\": ID_COL,\n",
    "    \"desc_cols\": desc_cols,\n",
    "    \"n_train\": len(ds_train),\n",
    "    \"n_val\": len(ds_val),\n",
    "    \"n_test\": len(ds_test),\n",
    "}\n",
    "SCHEMA_FILE.write_text(json.dumps(schema, indent=2))\n",
    "\n",
    "# -----------------------\n",
    "# 9) Surface concise summary + DEBUG BLOCK #2\n",
    "# -----------------------\n",
    "posw_stats = {LABELS[i]: float(pos_w[i]) for i in range(len(LABELS))}\n",
    "\n",
    "print(\"\\n----- SUMMARY -----\")\n",
    "print(f\"Train/Val/Test sizes: {len(ds_train)} / {len(ds_val)} / {len(ds_test)}\")\n",
    "print(f\"Descriptor dims: {len(desc_cols)}  |  NaN (train) median-imputed, then standardized\")\n",
    "print(f\"Saved scaler: {SCALER_FILE}\")\n",
    "print(f\"Saved pos_weight: {POSW_FILE}\")\n",
    "print(f\"Graph cache dir: {CACHE_DIR}  (files: ~{len(list(CACHE_DIR.glob('*.pkl')))} after first epoch)\")\n",
    "print(\"GIN dry-run:\", gin_probe)\n",
    "print(\"-------------------\\n\")\n",
    "\n",
    "debug2 = {\n",
    "    \"desc_cols\": len(desc_cols),\n",
    "    \"desc_nan_mean_frac\": float(np.mean(list(scaler_dict[\"nan_frac_train\"].values()))) if scaler_dict[\"nan_frac_train\"] else 0.0,\n",
    "    \"pos_weight\": posw_stats,\n",
    "    \"gin_probe\": gin_probe\n",
    "}\n",
    "print(\"=== COPY-PASTE DEBUG BLOCK #2 ===\")\n",
    "print(json.dumps(debug2, indent=2))\n",
    "\n",
    "# Keep artifacts references in CONFIG for later cells\n",
    "CONFIG[\"data\"][\"desc_cols\"] = desc_cols\n",
    "CONFIG[\"training\"] = CONFIG.get(\"training\", {})\n",
    "CONFIG[\"training\"][\"pos_weight\"] = pos_w.tolist()\n",
    "CONFIG[\"artifacts\"] = {\n",
    "    \"scaler_file\": str(SCALER_FILE.resolve()),\n",
    "    \"posw_file\": str(POSW_FILE.resolve()),\n",
    "    \"schema_file\": str(SCHEMA_FILE.resolve()),\n",
    "    \"graph_cache_dir\": str(CACHE_DIR.resolve())\n",
    "}\n",
    "(RESULTS_DIR / f\"{RUN_ID}_config.json\").write_text(json.dumps(CONFIG, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24038c9",
   "metadata": {},
   "source": [
    "## 4: Tokenizer, Collator, and Train/Val/Test DataLoaders (+ batch probe & report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab628d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- DATALOADER SUMMARY (CUDA pos_weight) -----\n",
      "Device:               CUDA\n",
      "Tokenizer:            DeepChem/ChemBERTa-100M-MLM | max_len=256\n",
      "Train/Val/Test BS:    32/64/64 | workers=0 | pin_memory=True\n",
      "Probe: B=32, C=12, D_desc=257, nodes=563, edges=1152\n",
      "Text tokens:          shape=(32, 124) | pad_frac=0.784 | avg_len=26.8\n",
      "Label mask present:   1.000\n",
      "pos_weight device:    cuda:0  |  clipped on-device max=50.0\n",
      "pos_weight (first 4): [23.00383186340332, 28.83333396911621, 8.897314071655273, 25.659574508666992]\n",
      "Saved pos_weight:     v4_20250902_165736_pos_weight.json, v4_20250902_165736_pos_weight.pt, v4_20250902_165736_pos_weight_clipped.pt\n",
      "Saved dataloader rpt: tox21_dualenc_v1/results/v4/v4_20250902_165736_dataloader_probe.json\n",
      "------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader as TorchLoader\n",
    "from transformers import AutoTokenizer\n",
    "from torch_geometric.data import Batch as GeoBatch\n",
    "\n",
    "logger = logging.getLogger(\"v4\")\n",
    "\n",
    "# ---- Environment / device ----\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "PIN_MEMORY = (DEVICE.type == \"cuda\")\n",
    "NUM_WORKERS = 0  # Windows-friendly\n",
    "\n",
    "# ---- Config knobs ----\n",
    "TOKENIZER_CANDIDATES = [\n",
    "    \"DeepChem/ChemBERTa-100M-MLM\",\n",
    "    \"seyonec/ChemBERTa-zinc-base-v1\",\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "]\n",
    "MAX_SMILES_LEN = 256\n",
    "TRAIN_BS = 32\n",
    "EVAL_BS  = 64\n",
    "POS_WEIGHT_CLIP = 50.0  # safety cap\n",
    "\n",
    "# ---- Preconditions from Cell 3 ----\n",
    "assert 'df_train' in globals() and 'df_val' in globals() and 'df_test' in globals(), \"Missing splits from Cell 3.\"\n",
    "assert 'LABELS' in globals() and len(LABELS) == 12, \"Labels not set.\"\n",
    "assert 'SMILES_COL' in globals(), \"SMILES_COL not set.\"\n",
    "assert 'ds_train' in globals() and 'ds_val' in globals() and 'ds_test' in globals(), \"Datasets not built (Cell 3).\"\n",
    "\n",
    "# ---- Recompute pos_weight on-device (CUDA if available) ----\n",
    "def _pos_weight_np(y: np.ndarray) -> np.ndarray:\n",
    "    w = []\n",
    "    for j in range(y.shape[1]):\n",
    "        col = y[:, j]\n",
    "        # treat NaNs as missing\n",
    "        m = ~np.isnan(col)\n",
    "        if m.sum() == 0:\n",
    "            w.append(1.0)\n",
    "            continue\n",
    "        P = int((col[m] == 1).sum())\n",
    "        N = int(m.sum())\n",
    "        P = max(P, 1)\n",
    "        w.append((N - P) / P)\n",
    "    return np.asarray(w, dtype=np.float32)\n",
    "\n",
    "Yt = df_train[LABELS].values.astype(np.float32)\n",
    "pos_w_np = _pos_weight_np(Yt)\n",
    "pos_w = torch.tensor(pos_w_np, dtype=torch.float32, device=DEVICE)\n",
    "pos_w_clipped = torch.clamp(pos_w, max=POS_WEIGHT_CLIP)\n",
    "\n",
    "# Save both human-readable JSON and .pt tensors (portable)\n",
    "posw_json_path = RESULTS_DIR / f\"{RUN_ID}_pos_weight.json\"\n",
    "posw_pt_path   = RESULTS_DIR / f\"{RUN_ID}_pos_weight.pt\"\n",
    "posw_pt_clip   = RESULTS_DIR / f\"{RUN_ID}_pos_weight_clipped.pt\"\n",
    "posw_json_path.write_text(json.dumps({\"labels\": LABELS, \"pos_weight\": pos_w.cpu().tolist()}, indent=2))\n",
    "torch.save(pos_w.cpu(), posw_pt_path)         # save on CPU to be portable\n",
    "torch.save(pos_w_clipped.cpu(), posw_pt_clip)\n",
    "\n",
    "# Update CONFIG\n",
    "CONFIG.setdefault(\"training\", {})\n",
    "CONFIG[\"training\"][\"pos_weight_file\"] = str(posw_pt_path.resolve())\n",
    "CONFIG[\"training\"][\"pos_weight_file_clipped\"] = str(posw_pt_clip.resolve())\n",
    "CONFIG[\"training\"][\"pos_weight\"] = pos_w.cpu().tolist()\n",
    "CONFIG[\"training\"][\"pos_weight_clipped\"] = pos_w_clipped.cpu().tolist()\n",
    "(RESULTS_DIR / f\"{RUN_ID}_config.json\").write_text(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "# ---- Tokenizer (with fallbacks) ----\n",
    "tok_name_used = None\n",
    "tokenizer = None\n",
    "errors = []\n",
    "for name in TOKENIZER_CANDIDATES:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name, use_fast=True, trust_remote_code=False)\n",
    "        tok_name_used = name\n",
    "        break\n",
    "    except Exception as e:\n",
    "        errors.append(f\"{name}: {e}\")\n",
    "if tokenizer is None:\n",
    "    raise RuntimeError(\"Failed to load any ChemBERTa tokenizer:\\n\" + \"\\n\".join(errors))\n",
    "\n",
    "CONFIG.setdefault(\"text\", {})\n",
    "CONFIG[\"text\"][\"tokenizer_name\"] = tok_name_used\n",
    "CONFIG[\"text\"][\"max_len\"] = MAX_SMILES_LEN\n",
    "(RESULTS_DIR / f\"{RUN_ID}_config.json\").write_text(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "# ---- Collator & batch container ----\n",
    "from torch_geometric.data import Batch as GeoBatch\n",
    "@dataclass\n",
    "class MultiModalBatch:\n",
    "    graph: GeoBatch\n",
    "    desc: torch.Tensor\n",
    "    labels: torch.Tensor\n",
    "    label_mask: torch.Tensor\n",
    "    text: Dict[str, torch.Tensor]\n",
    "    smiles: List[str]\n",
    "\n",
    "class MultiModalCollator:\n",
    "    def __init__(self, tokenizer, max_len: int = 256):\n",
    "        self.tok = tokenizer\n",
    "        self.max_len = max_len\n",
    "    def __call__(self, batch):\n",
    "        graphs, descs, ys, smiles = [], [], [], []\n",
    "        for g, d, y in batch:\n",
    "            graphs.append(g)\n",
    "            descs.append(d)\n",
    "            ys.append(y)\n",
    "            s = getattr(g, \"smiles\", \"\")\n",
    "            smiles.append(str(s))\n",
    "        g_batch = GeoBatch.from_data_list(graphs)\n",
    "        desc = torch.stack(descs, dim=0).contiguous()\n",
    "        y    = torch.stack(ys, dim=0).contiguous()\n",
    "        label_mask = ~torch.isnan(y)\n",
    "        y = torch.nan_to_num(y, nan=0.0)\n",
    "        toks = self.tok(\n",
    "            smiles, padding=True, truncation=True, max_length=self.max_len, return_tensors=\"pt\"\n",
    "        )\n",
    "        return MultiModalBatch(\n",
    "            graph=g_batch,\n",
    "            desc=desc,\n",
    "            labels=y,\n",
    "            label_mask=label_mask,\n",
    "            text={k: v for k, v in toks.items()},\n",
    "            smiles=smiles\n",
    "        )\n",
    "\n",
    "collate_fn = MultiModalCollator(tokenizer, max_len=MAX_SMILES_LEN)\n",
    "\n",
    "# ---- DataLoaders ----\n",
    "train_loader = TorchLoader(ds_train, batch_size=TRAIN_BS, shuffle=True,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "val_loader   = TorchLoader(ds_val,   batch_size=EVAL_BS, shuffle=False,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "test_loader  = TorchLoader(ds_test,  batch_size=EVAL_BS, shuffle=False,\n",
    "                           num_workers=NUM_WORKERS, pin_memory=PIN_MEMORY, collate_fn=collate_fn)\n",
    "\n",
    "# ---- Probe one batch ----\n",
    "probe = next(iter(train_loader))\n",
    "ids = probe.text[\"input_ids\"]\n",
    "attn = probe.text[\"attention_mask\"]\n",
    "pad_id = tokenizer.pad_token_id if tokenizer.pad_token_id is not None else 0\n",
    "pad_frac = float((ids == pad_id).sum().item() / ids.numel())\n",
    "avg_len = float(attn.sum(1).float().mean().item())\n",
    "\n",
    "# ---- Human-readable summary ----\n",
    "print(\"\\n----- DATALOADER SUMMARY (CUDA pos_weight) -----\")\n",
    "print(f\"Device:               {DEVICE.type.upper()}\")\n",
    "print(f\"Tokenizer:            {tok_name_used} | max_len={MAX_SMILES_LEN}\")\n",
    "print(f\"Train/Val/Test BS:    {TRAIN_BS}/{EVAL_BS}/{EVAL_BS} | workers={NUM_WORKERS} | pin_memory={PIN_MEMORY}\")\n",
    "print(f\"Probe: B={probe.labels.size(0)}, C={probe.labels.size(1)}, D_desc={probe.desc.size(1)}, \"\n",
    "      f\"nodes={probe.graph.x.size(0)}, edges={probe.graph.edge_index.size(1)}\")\n",
    "print(f\"Text tokens:          shape={tuple(ids.shape)} | pad_frac={pad_frac:.3f} | avg_len={avg_len:.1f}\")\n",
    "print(f\"Label mask present:   {probe.label_mask.float().mean().item():.3f}\")\n",
    "print(f\"pos_weight device:    {pos_w.device}  |  clipped on-device max={POS_WEIGHT_CLIP}\")\n",
    "print(f\"pos_weight (first 4): {pos_w[:4].tolist()}\")\n",
    "print(f\"Saved pos_weight:     {posw_json_path.name}, {posw_pt_path.name}, {posw_pt_clip.name}\")\n",
    "print(f\"Saved dataloader rpt: {(RESULTS_DIR / f'{RUN_ID}_dataloader_probe.json').as_posix()}\")\n",
    "print(\"------------------------------------------------\\n\")\n",
    "\n",
    "# Persist brief loader probe for reproducibility\n",
    "dl_report = {\n",
    "    \"batch_size_train\": TRAIN_BS,\n",
    "    \"batch_size_eval\": EVAL_BS,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "    \"pin_memory\": PIN_MEMORY,\n",
    "    \"tokenizer\": tok_name_used,\n",
    "    \"max_len\": MAX_SMILES_LEN,\n",
    "    \"probe\": {\n",
    "        \"B\": int(probe.labels.size(0)),\n",
    "        \"C\": int(probe.labels.size(1)),\n",
    "        \"D_desc\": int(probe.desc.size(1)),\n",
    "        \"n_nodes_total\": int(probe.graph.x.size(0)),\n",
    "        \"n_edges_total\": int(probe.graph.edge_index.size(1)),\n",
    "        \"input_ids_shape\": list(ids.shape),\n",
    "        \"pad_fraction\": round(pad_frac, 4),\n",
    "        \"avg_seq_len_tokens\": round(avg_len, 2),\n",
    "        \"label_mask_frac_present\": round(float(probe.label_mask.float().mean().item()), 4),\n",
    "    }\n",
    "}\n",
    "(RESULTS_DIR / f\"{RUN_ID}_dataloader_probe.json\").write_text(json.dumps(dl_report, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aacb560c",
   "metadata": {},
   "source": [
    "## 5: Model definition (GIN encoder, ChemBERTa encoder, Descriptor MLP, Descriptor-Gated Bi-Directional Cross-Attention) + forward dry-run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39cee287",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CHECK] criterion.pos_weight device: cuda:0\n",
      "\n",
      "----- MODEL DRY-RUN SUMMARY -----\n",
      "Params (total/text/graph/desc/fusion/head): 1790223 / 196864 / 468481 / 131840 / 593154 / 399884\n",
      "logits: [32, 12] | attn t->g: [32, 207, 69] | g->t: [32, 69, 207]\n",
      "loss preview (BCE/NCE/L1/Total): {'bce': -0.8623024821281433, 'nce': 3.7213995456695557, 'attn_l1': 0.009658092632889748, 'total_preview': -0.49016156792640686}\n",
      "Saved model report: tox21_dualenc_v1/results/v4/v4_20250902_165736_model_report.json\n",
      "---------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from typing import Optional, Tuple, Dict, Any\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoModel\n",
    "from torch_geometric.nn import GINConv, global_mean_pool, global_add_pool\n",
    "from torch_geometric.utils import to_dense_batch\n",
    "\n",
    "logger = logging.getLogger(\"v4\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------\n",
    "# Utility helpers\n",
    "# -----------------------\n",
    "def count_params(module: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mask_f = mask.float().unsqueeze(-1)\n",
    "    s = (x * mask_f).sum(dim=dim)\n",
    "    denom = mask_f.sum(dim=dim).clamp_min(1.0)\n",
    "    return s / denom\n",
    "\n",
    "# -----------------------\n",
    "# Masked BCE with device-safe pos_weight\n",
    "# -----------------------\n",
    "class MaskedBCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, pos_weight: Optional[torch.Tensor] = None, reduction: str = \"mean\"):\n",
    "        super().__init__()\n",
    "        self.reduction = reduction\n",
    "        # Register as buffer so it moves with .to(device)\n",
    "        if pos_weight is not None:\n",
    "            if not isinstance(pos_weight, torch.Tensor):\n",
    "                pos_weight = torch.tensor(pos_weight, dtype=torch.float32)\n",
    "            self.register_buffer(\"pos_weight\", pos_weight.clone().detach())\n",
    "        else:\n",
    "            self.pos_weight = None\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        loss = F.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction=\"none\", pos_weight=self.pos_weight\n",
    "        )\n",
    "        if mask is not None:\n",
    "            loss = torch.where(mask, loss, torch.zeros_like(loss))\n",
    "            denom = mask.float().sum().clamp_min(1.0)\n",
    "            return loss.sum() / denom\n",
    "        return loss.mean()\n",
    "\n",
    "def info_nce(z1: torch.Tensor, z2: torch.Tensor, temp: float = 0.07) -> torch.Tensor:\n",
    "    z1 = F.normalize(z1, dim=-1)\n",
    "    z2 = F.normalize(z2, dim=-1)\n",
    "    logits = z1 @ z2.t() / temp\n",
    "    labels = torch.arange(z1.size(0), device=z1.device)\n",
    "    return F.cross_entropy(logits, labels)\n",
    "\n",
    "# -----------------------\n",
    "# Encoders\n",
    "# -----------------------\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, hidden: int = 256, out_dim: int = 256, p_drop: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class GraphEncoderGIN(nn.Module):\n",
    "    def __init__(self, in_dim: int = 20, hidden: int = 256, layers: int = 4, dropout: float = 0.2, use_virtual_node: bool = True):\n",
    "        super().__init__()\n",
    "        self.hidden = hidden\n",
    "        self.layers = layers\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "        self.use_virtual_node = use_virtual_node\n",
    "\n",
    "        def mlp(din, dout):\n",
    "            return nn.Sequential(nn.Linear(din, hidden), nn.ReLU(), nn.Linear(hidden, dout))\n",
    "\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns   = nn.ModuleList()\n",
    "        self.convs.append(GINConv(mlp(in_dim, hidden))); self.bns.append(nn.BatchNorm1d(hidden))\n",
    "        for _ in range(layers - 1):\n",
    "            self.convs.append(GINConv(mlp(hidden, hidden))); self.bns.append(nn.BatchNorm1d(hidden))\n",
    "\n",
    "        # Simple virtual-node emulation via gated global token (buffer-friendly)\n",
    "        if use_virtual_node:\n",
    "            self.vn_token = nn.Parameter(torch.zeros(1, hidden))\n",
    "            nn.init.normal_(self.vn_token, std=0.02)\n",
    "            self.vn_gate  = nn.Linear(hidden, 1)\n",
    "        else:\n",
    "            self.vn_token = None; self.vn_gate = None\n",
    "\n",
    "    def forward(self, batch):\n",
    "        x, edge_index, bidx = batch.x, batch.edge_index, batch.batch\n",
    "        h = x\n",
    "        for conv, bn in zip(self.convs, self.bns):\n",
    "            h = F.relu(bn(conv(h, edge_index)))\n",
    "            if self.vn_token is not None:\n",
    "                g = global_mean_pool(h, bidx)                    # [B, D]\n",
    "                g = g + self.vn_token.expand(g.size(0), -1)      # [B, D]\n",
    "                gate = torch.sigmoid(self.vn_gate(g))            # [B, 1]\n",
    "                h = h + gate[bidx] * g[bidx]\n",
    "            h = self.drop(h)\n",
    "        node_h, mask = to_dense_batch(h, bidx)                   # [B, N, D], [B, N]\n",
    "        z = global_mean_pool(h, bidx)                            # [B, D]\n",
    "        return node_h, mask, z\n",
    "\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, name: str, proj_dim: int = 256, p_drop: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.backbone = AutoModel.from_pretrained(name)\n",
    "        hidden = self.backbone.config.hidden_size\n",
    "        self.proj = nn.Linear(hidden, proj_dim)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "\n",
    "    def freeze_backbone(self):\n",
    "        for p in self.backbone.parameters(): p.requires_grad = False\n",
    "\n",
    "    def unfreeze_last_n_layers(self, n: int = 2):\n",
    "        for p in self.backbone.parameters(): p.requires_grad = False\n",
    "        # try roberta-like path first\n",
    "        enc = getattr(getattr(self.backbone, \"roberta\", self.backbone), \"encoder\", None)\n",
    "        if enc is None or not hasattr(enc, \"layer\"):\n",
    "            # fallback: try .encoder directly\n",
    "            enc = getattr(self.backbone, \"encoder\", None)\n",
    "        if enc is None:  # fallback: unfreeze all\n",
    "            for p in self.backbone.parameters(): p.requires_grad = True\n",
    "            return\n",
    "        layers = enc.layer\n",
    "        n = max(1, min(n, len(layers)))\n",
    "        unfrozen = list(range(len(layers)-n, len(layers)))\n",
    "        for i, layer in enumerate(layers):\n",
    "            req = i in unfrozen\n",
    "            for p in layer.parameters(): p.requires_grad = req\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        h = self.drop(out.last_hidden_state)        # [B, T, H]\n",
    "        h = self.proj(h)                            # [B, T, D]\n",
    "        mask = attention_mask.bool()                # [B, T]\n",
    "        z_text = masked_mean(h, mask, dim=1)        # [B, D]\n",
    "        return h, mask, z_text\n",
    "\n",
    "# -----------------------\n",
    "# Descriptor-Gated Bi-Directional Cross-Attention\n",
    "# -----------------------\n",
    "class DualStreamCoAttn(nn.Module):\n",
    "    def __init__(self, d_model: int = 256, n_heads: int = 4, p_drop: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mha_t_q = nn.MultiheadAttention(d_model, n_heads, dropout=p_drop, batch_first=True)\n",
    "        self.mha_g_q = nn.MultiheadAttention(d_model, n_heads, dropout=p_drop, batch_first=True)\n",
    "        self.norm_t1 = nn.LayerNorm(d_model)\n",
    "        self.norm_g1 = nn.LayerNorm(d_model)\n",
    "        self.drop = nn.Dropout(p_drop)\n",
    "        self.gate_t = nn.Sequential(nn.Linear(512, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "        self.gate_g = nn.Sequential(nn.Linear(512, 64), nn.ReLU(), nn.Linear(64, 1))\n",
    "\n",
    "    def forward(self, h_text, m_text, h_graph, m_graph, z_desc):\n",
    "        # pooled pre-fusion\n",
    "        z_text = masked_mean(h_text, m_text, dim=1)      # [B, D]\n",
    "        z_graph = masked_mean(h_graph, m_graph, dim=1)   # [B, D]\n",
    "        # masks -> key_padding_mask (True = pad)\n",
    "        kp_graph = ~m_graph\n",
    "        kp_text  = ~m_text\n",
    "\n",
    "        # Text <- Graph\n",
    "        upd_t, attn_t = self.mha_t_q(h_text, h_graph, h_graph, key_padding_mask=kp_graph, need_weights=True, average_attn_weights=True)\n",
    "        gate_t = torch.sigmoid(self.gate_t(torch.cat([z_desc, z_graph], dim=-1)))  # [B,1]\n",
    "        h_text_f = self.norm_t1(h_text + self.drop(gate_t.unsqueeze(1) * upd_t))\n",
    "\n",
    "        # Graph <- Text\n",
    "        upd_g, attn_g = self.mha_g_q(h_graph, h_text, h_text, key_padding_mask=kp_text, need_weights=True, average_attn_weights=True)\n",
    "        gate_g = torch.sigmoid(self.gate_g(torch.cat([z_desc, z_text], dim=-1)))   # [B,1]\n",
    "        h_graph_f = self.norm_g1(h_graph + self.drop(gate_g.unsqueeze(1) * upd_g))\n",
    "\n",
    "        return h_text_f, h_graph_f, {\"t_to_g\": attn_t, \"g_to_t\": attn_g, \"gate_t\": gate_t, \"gate_g\": gate_g}\n",
    "\n",
    "# -----------------------\n",
    "# Head & Full model\n",
    "# -----------------------\n",
    "class MultiLabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=256*3, hidden=512, out_dim=12, p_drop=0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.ReLU(), nn.Dropout(p_drop),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class DualEncCoAttnModel(nn.Module):\n",
    "    def __init__(self, cfg: Dict[str, Any]):\n",
    "        super().__init__()\n",
    "        d_model = 256\n",
    "        # Text\n",
    "        self.text = TextEncoder(cfg[\"text\"][\"tokenizer_name\"], proj_dim=d_model, p_drop=0.1)\n",
    "        # Graph\n",
    "        in_dim_graph = ds_train[0][0].x.size(-1)\n",
    "        self.graph = GraphEncoderGIN(in_dim=in_dim_graph, hidden=d_model, layers=4, dropout=0.2,\n",
    "                                     use_virtual_node=cfg[\"model_plan\"][\"use_virtual_node\"])\n",
    "        # Descriptors\n",
    "        self.desc = DescriptorMLP(in_dim=len(cfg[\"data\"][\"desc_cols\"]), hidden=256, out_dim=d_model, p_drop=0.2)\n",
    "        # Fusion\n",
    "        self.fusion = DualStreamCoAttn(d_model=d_model, n_heads=4, p_drop=0.1)\n",
    "        # Head\n",
    "        self.head = MultiLabelHead(in_dim=3*d_model, hidden=512, out_dim=len(LABELS), p_drop=0.2)\n",
    "\n",
    "        # Aux loss weights\n",
    "        aux = cfg[\"model_plan\"].get(\"aux_losses\", {\"nce\": 0.1, \"attn_l1\": 1e-4})\n",
    "        self.w_nce = float(aux.get(\"nce\", 0.0))\n",
    "        self.w_l1  = float(aux.get(\"attn_l1\", 0.0))\n",
    "\n",
    "        # Load pos_weight (clipped) and register in criterion as buffer\n",
    "        pw_path = cfg[\"training\"].get(\"pos_weight_file_clipped\") or cfg[\"training\"].get(\"pos_weight_file\")\n",
    "        if pw_path and Path(pw_path).exists():\n",
    "            pw = torch.load(pw_path, map_location=\"cpu\")\n",
    "        else:\n",
    "            # fallback to list\n",
    "            pw = torch.tensor(cfg[\"training\"][\"pos_weight_clipped\"], dtype=torch.float32)\n",
    "        self.crit = MaskedBCEWithLogitsLoss(pos_weight=pw)\n",
    "\n",
    "    def forward(self, batch, compute_aux: bool = True):\n",
    "        # Text\n",
    "        ids = batch.text[\"input_ids\"].to(DEVICE)\n",
    "        attn = batch.text[\"attention_mask\"].to(DEVICE)\n",
    "        h_text, m_text, z_text = self.text(ids, attn)\n",
    "\n",
    "        # Graph\n",
    "        g = batch.graph.to(DEVICE)\n",
    "        node_h, node_mask, z_graph = self.graph(g)\n",
    "\n",
    "        # Desc\n",
    "        z_desc = self.desc(batch.desc.to(DEVICE))\n",
    "\n",
    "        # Fusion\n",
    "        h_text_f, h_graph_f, attn_maps = self.fusion(h_text, m_text, node_h, node_mask, z_desc)\n",
    "\n",
    "        # Pools\n",
    "        z_text_f  = masked_mean(h_text_f, m_text, dim=1)\n",
    "        z_graph_f = masked_mean(h_graph_f, node_mask, dim=1)\n",
    "\n",
    "        # Head\n",
    "        z = torch.cat([z_text_f, z_graph_f, z_desc], dim=-1)\n",
    "        logits = self.head(z)\n",
    "\n",
    "        out = {\"logits\": logits, \"z_text\": z_text, \"z_graph\": z_graph, \"attn\": attn_maps}\n",
    "\n",
    "        if compute_aux:\n",
    "            loss_nce = info_nce(z_text, z_graph) if self.w_nce > 0 else torch.tensor(0.0, device=logits.device)\n",
    "            if self.w_l1 > 0:\n",
    "                l1_t = attn_maps[\"t_to_g\"].abs().mean()\n",
    "                l1_g = attn_maps[\"g_to_t\"].abs().mean()\n",
    "                loss_l1 = 0.5*(l1_t + l1_g)\n",
    "            else:\n",
    "                loss_l1 = torch.tensor(0.0, device=logits.device)\n",
    "            out[\"loss_nce\"] = loss_nce\n",
    "            out[\"loss_l1\"]  = loss_l1\n",
    "        return out\n",
    "\n",
    "# -----------------------\n",
    "# Instantiate & Dry-run\n",
    "# -----------------------\n",
    "torch.cuda.empty_cache()\n",
    "model = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "model.text.freeze_backbone()  # Stage A\n",
    "\n",
    "# Sanity: ensure loss buffer moved with model\n",
    "if getattr(model.crit, \"pos_weight\", None) is not None:\n",
    "    print(f\"[CHECK] criterion.pos_weight device: {model.crit.pos_weight.device}\")\n",
    "\n",
    "probe = next(iter(train_loader))\n",
    "labels = probe.labels.to(DEVICE)\n",
    "label_mask = probe.label_mask.to(DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = model(probe, compute_aux=True)\n",
    "    logits = out[\"logits\"]\n",
    "    loss_bce = model.crit(logits, labels, label_mask)\n",
    "    loss_nce = out.get(\"loss_nce\", torch.tensor(0.0, device=DEVICE))\n",
    "    loss_l1  = out.get(\"loss_l1\", torch.tensor(0.0, device=DEVICE))\n",
    "    total    = loss_bce + CONFIG[\"model_plan\"][\"aux_losses\"][\"nce\"] * loss_nce + CONFIG[\"model_plan\"][\"aux_losses\"][\"attn_l1\"] * loss_l1\n",
    "\n",
    "rep = {\n",
    "    \"params_total\": count_params(model),\n",
    "    \"params_text\": count_params(model.text),\n",
    "    \"params_graph\": count_params(model.graph),\n",
    "    \"params_desc\": count_params(model.desc),\n",
    "    \"params_fusion\": count_params(model.fusion),\n",
    "    \"params_head\": count_params(model.head),\n",
    "    \"shapes\": {\n",
    "        \"logits\": list(logits.shape),\n",
    "        \"attn_t_to_g\": list(out[\"attn\"][\"t_to_g\"].shape),\n",
    "        \"attn_g_to_t\": list(out[\"attn\"][\"g_to_t\"].shape),\n",
    "    },\n",
    "    \"losses_preview\": {\n",
    "        \"bce\": float(loss_bce.item()),\n",
    "        \"nce\": float(loss_nce.item()),\n",
    "        \"attn_l1\": float(loss_l1.item()),\n",
    "        \"total_preview\": float(total.item())\n",
    "    }\n",
    "}\n",
    "(RESULTS_DIR / f\"{RUN_ID}_model_report.json\").write_text(json.dumps(rep, indent=2))\n",
    "\n",
    "print(\"\\n----- MODEL DRY-RUN SUMMARY -----\")\n",
    "print(f\"Params (total/text/graph/desc/fusion/head): \"\n",
    "      f\"{rep['params_total']} / {rep['params_text']} / {rep['params_graph']} / {rep['params_desc']} / {rep['params_fusion']} / {rep['params_head']}\")\n",
    "print(f\"logits: {rep['shapes']['logits']} | attn t->g: {rep['shapes']['attn_t_to_g']} | g->t: {rep['shapes']['attn_g_to_t']}\")\n",
    "print(f\"loss preview (BCE/NCE/L1/Total): {rep['losses_preview']}\")\n",
    "print(f\"Saved model report: {(RESULTS_DIR / f'{RUN_ID}_model_report.json').as_posix()}\")\n",
    "print(\"---------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af528b",
   "metadata": {},
   "source": [
    "## 6: **Debug before training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b37d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Updated criterion.pos_weight on cuda:0.\n",
      "\n",
      "----- LABEL AUDIT SUMMARY -----\n",
      "Out-of-range counts (before): train=12359, val=1857, test=1810\n",
      "Applied sanitize: train=True, val=True, test=True\n",
      "Out-of-range counts (after):  train=0, val=0, test=0\n",
      "pos_weight (first 5, clipped): [23.00383186340332, 28.83333396911621, 8.897314071655273, 25.659574508666992, 8.378742218017578]\n",
      "FP32 BCE probe (should be >= 0): 1.202815\n",
      "Saved audit: tox21_dualenc_v1/results/v4/v4_20250902_165736_label_audit.json\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "assert 'df_train' in globals() and 'df_val' in globals() and 'df_test' in globals(), \"Missing dfs from Cell 3.\"\n",
    "assert 'LABELS' in globals(), \"LABELS not defined.\"\n",
    "assert 'ds_train' in globals() and 'ds_val' in globals() and 'ds_test' in globals(), \"Datasets not built.\"\n",
    "assert 'train_loader' in globals(), \"Loaders not built.\"\n",
    "assert 'model' in globals(), \"Model not built (run Cell 5 first).\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def _label_stats(df: pd.DataFrame, name: str):\n",
    "    stats = {}\n",
    "    for c in LABELS:\n",
    "        s = df[c]\n",
    "        vals = s.dropna().values\n",
    "        uniq = pd.Series(vals).value_counts().to_dict()\n",
    "        stats[c] = {\n",
    "            \"min\": float(np.nanmin(vals)) if len(vals)>0 else None,\n",
    "            \"max\": float(np.nanmax(vals)) if len(vals)>0 else None,\n",
    "            \"unique\": {str(k): int(v) for k,v in list(uniq.items())[:6]},\n",
    "            \"n_out_of_range\": int(((vals < 0) | (vals > 1)).sum())\n",
    "        }\n",
    "    return stats\n",
    "\n",
    "def _maybe_sanitize(df: pd.DataFrame) -> bool:\n",
    "    \"\"\"Return True if any change was applied.\"\"\"\n",
    "    changed = False\n",
    "    for c in LABELS:\n",
    "        s = df[c]\n",
    "        vals = s.values\n",
    "        # Map {-1,1} -> {0,1}\n",
    "        if set(np.unique(vals[~np.isnan(vals)])).issubset({-1.0, 0.0, 1.0}) and (-1.0 in vals):\n",
    "            df[c] = s.replace({-1.0: 0.0})\n",
    "            changed = True\n",
    "        # Clamp to [0,1] (handles any strays)\n",
    "        if ((vals < 0) | (vals > 1)).any():\n",
    "            df[c] = df[c].clip(lower=0.0, upper=1.0)\n",
    "            changed = True\n",
    "    return changed\n",
    "\n",
    "def _pos_weight_from_df(df: pd.DataFrame) -> np.ndarray:\n",
    "    y = df[LABELS].values.astype(np.float32)\n",
    "    w = []\n",
    "    for j in range(y.shape[1]):\n",
    "        col = y[:, j]\n",
    "        m = ~np.isnan(col)\n",
    "        if m.sum() == 0:\n",
    "            w.append(1.0); continue\n",
    "        P = int((col[m] == 1).sum()); N = int(m.sum()); P = max(P, 1)\n",
    "        w.append((N - P) / P)\n",
    "    return np.asarray(w, dtype=np.float32)\n",
    "\n",
    "# 1) Before stats\n",
    "before = {\n",
    "    \"train\": _label_stats(df_train, \"train\"),\n",
    "    \"val\":   _label_stats(df_val, \"val\"),\n",
    "    \"test\":  _label_stats(df_test, \"test\"),\n",
    "}\n",
    "\n",
    "# 2) Sanitize (train/val/test)\n",
    "changed_train = _maybe_sanitize(df_train)\n",
    "changed_val   = _maybe_sanitize(df_val)\n",
    "changed_test  = _maybe_sanitize(df_test)\n",
    "\n",
    "# 3) Push sanitized labels into existing Dataset copies (they hold their own df)\n",
    "ds_train.df[LABELS] = df_train[LABELS].values\n",
    "ds_val.df[LABELS]   = df_val[LABELS].values\n",
    "ds_test.df[LABELS]  = df_test[LABELS].values\n",
    "\n",
    "# 4) After stats\n",
    "after = {\n",
    "    \"train\": _label_stats(df_train, \"train\"),\n",
    "    \"val\":   _label_stats(df_val, \"val\"),\n",
    "    \"test\":  _label_stats(df_test, \"test\"),\n",
    "}\n",
    "\n",
    "# 5) Recompute pos_weight from sanitized TRAIN and update files + loss buffer\n",
    "pos_w = torch.tensor(_pos_weight_from_df(df_train), dtype=torch.float32)\n",
    "POS_WEIGHT_CLIP = 50.0\n",
    "pos_w_clipped = torch.clamp(pos_w, max=POS_WEIGHT_CLIP)\n",
    "\n",
    "posw_json_path = RESULTS_DIR / f\"{RUN_ID}_pos_weight_SANITIZED.json\"\n",
    "posw_pt_path   = RESULTS_DIR / f\"{RUN_ID}_pos_weight_SANITIZED.pt\"\n",
    "posw_pt_clip   = RESULTS_DIR / f\"{RUN_ID}_pos_weight_SANITIZED_clipped.pt\"\n",
    "posw_json_path.write_text(json.dumps({\"labels\": LABELS, \"pos_weight\": pos_w.tolist()}, indent=2))\n",
    "torch.save(pos_w, posw_pt_path)\n",
    "torch.save(pos_w_clipped, posw_pt_clip)\n",
    "\n",
    "# Update CONFIG references (optional)\n",
    "CONFIG[\"training\"][\"pos_weight_file\"] = str(posw_pt_path.resolve())\n",
    "CONFIG[\"training\"][\"pos_weight_file_clipped\"] = str(posw_pt_clip.resolve())\n",
    "CONFIG[\"training\"][\"pos_weight\"] = pos_w.tolist()\n",
    "CONFIG[\"training\"][\"pos_weight_clipped\"] = pos_w_clipped.tolist()\n",
    "(RESULTS_DIR / f\"{RUN_ID}_config.json\").write_text(json.dumps(CONFIG, indent=2))\n",
    "\n",
    "# Update model criterion buffer if present\n",
    "if hasattr(model, \"crit\") and getattr(model.crit, \"pos_weight\", None) is not None:\n",
    "    with torch.no_grad():\n",
    "        if model.crit.pos_weight.shape != pos_w_clipped.shape:\n",
    "            print(f\"[WARN] crit.pos_weight shape {tuple(model.crit.pos_weight.shape)} != {tuple(pos_w_clipped.shape)}; replacing.\")\n",
    "            delattr(model.crit, \"pos_weight\")\n",
    "            model.crit.register_buffer(\"pos_weight\", pos_w_clipped.to(DEVICE))\n",
    "        else:\n",
    "            model.crit.pos_weight.copy_(pos_w_clipped.to(DEVICE))\n",
    "    print(f\"[OK] Updated criterion.pos_weight on {model.crit.pos_weight.device}.\")\n",
    "\n",
    "# 6) Quick FP32-BCE probe on one train batch (no AMP)\n",
    "probe = next(iter(train_loader))\n",
    "labels = probe.labels.to(DEVICE).float()\n",
    "mask   = probe.label_mask.to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    out = model(probe, compute_aux=False)\n",
    "    logits = out[\"logits\"].float()\n",
    "    bce_probe = model.crit(logits, labels, mask).item()\n",
    "\n",
    "# 7) Save audit report\n",
    "audit = {\n",
    "    \"changed\": {\"train\": changed_train, \"val\": changed_val, \"test\": changed_test},\n",
    "    \"before\": before,\n",
    "    \"after\": after,\n",
    "    \"pos_weight_sanitized\": pos_w.tolist(),\n",
    "    \"bce_probe_fp32\": bce_probe\n",
    "}\n",
    "(RESULTS_DIR / f\"{RUN_ID}_label_audit.json\").write_text(json.dumps(audit, indent=2))\n",
    "\n",
    "# 8) Print concise summary\n",
    "def _count_out_of_range(st):\n",
    "    return sum(st[c][\"n_out_of_range\"] for c in st)\n",
    "\n",
    "print(\"\\n----- LABEL AUDIT SUMMARY -----\")\n",
    "print(f\"Out-of-range counts (before): train={_count_out_of_range(before['train'])}, \"\n",
    "      f\"val={_count_out_of_range(before['val'])}, test={_count_out_of_range(before['test'])}\")\n",
    "print(f\"Applied sanitize: train={changed_train}, val={changed_val}, test={changed_test}\")\n",
    "print(f\"Out-of-range counts (after):  train={_count_out_of_range(after['train'])}, \"\n",
    "      f\"val={_count_out_of_range(after['val'])}, test={_count_out_of_range(after['test'])}\")\n",
    "print(f\"pos_weight (first 5, clipped): {pos_w_clipped[:5].tolist()}\")\n",
    "print(f\"FP32 BCE probe (should be >= 0): {bce_probe:.6f}\")\n",
    "print(f\"Saved audit: {(RESULTS_DIR / f'{RUN_ID}_label_audit.json').as_posix()}\")\n",
    "print(\"--------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8c3b57",
   "metadata": {},
   "source": [
    "## 7: Stage A): Dynamic training with Warmup+Cosine + ReduceLROnPlateau, AMP, early-stop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85dd4df",
   "metadata": {},
   "source": [
    "### 7a) training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57ed760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A (robust): up to 60 epochs | base_lr=0.0003 | AMP fwd=True (torch.bfloat16) | FP32 BCE on clamped logits ===\n",
      "[VAL prevalence] NR-AR:0.0281, NR-AR-LBD:0.0179, NR-AhR:0.0868, NR-Aromatase:0.046, NR-ER:0.083, NR-ER-LBD:0.0307 ...\n",
      "[DEBUG e0 i0] raw logits min/max: -4.84e-01/4.92e-01 | clamp_frac≈0.000\n",
      "[E00] train_loss=1.5492 (bce=1.2498, nce=0.2993, l1=0.00000) | val_roc=0.6978 (valid=12) val_pr=0.1557 | val_bceprev=1.1676 | ma_roc=0.6978 | lr=0.000050 | 10.7s\n",
      "  ↳ Saved BEST checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt (ma_roc=0.6978)\n",
      "[E01] train_loss=1.2814 (bce=1.1079, nce=0.1735, l1=0.00000) | val_roc=0.7359 (valid=12) val_pr=0.1996 | val_bceprev=1.0968 | ma_roc=0.7169 | lr=0.000100 | 10.3s\n",
      "  ↳ Saved BEST checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt (ma_roc=0.7169)\n",
      "[E02] train_loss=1.1406 (bce=1.0222, nce=0.1184, l1=0.00000) | val_roc=0.7513 (valid=12) val_pr=0.2158 | val_bceprev=1.0483 | ma_roc=0.7283 | lr=0.000150 | 10.4s\n",
      "  ↳ Saved BEST checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt (ma_roc=0.7283)\n",
      "[E03] train_loss=1.0577 (bce=0.9612, nce=0.0965, l1=0.00000) | val_roc=0.7736 (valid=12) val_pr=0.2622 | val_bceprev=0.9646 | ma_roc=0.7536 | lr=0.000200 | 10.1s\n",
      "  ↳ Saved BEST checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt (ma_roc=0.7536)\n",
      "[E04] train_loss=0.9605 (bce=0.8762, nce=0.0843, l1=0.00000) | val_roc=0.7694 (valid=12) val_pr=0.2603 | val_bceprev=1.0299 | ma_roc=0.7648 | lr=0.000250 | 10.1s\n",
      "  ↳ Saved BEST checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt (ma_roc=0.7648)\n",
      "[E05] train_loss=0.8841 (bce=0.8100, nce=0.0740, l1=0.00000) | val_roc=0.7638 (valid=12) val_pr=0.2463 | val_bceprev=1.2074 | ma_roc=0.7689 | lr=0.000300 | 10.1s\n",
      "  ↳ Saved BEST checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt (ma_roc=0.7689)\n",
      "[E06] train_loss=0.7995 (bce=0.7334, nce=0.0662, l1=0.00000) | val_roc=0.7714 (valid=12) val_pr=0.2724 | val_bceprev=1.1053 | ma_roc=0.7682 | lr=0.000300 | 10.2s\n",
      "[E07] train_loss=0.7054 (bce=0.6471, nce=0.0583, l1=0.00000) | val_roc=0.7567 (valid=12) val_pr=0.2646 | val_bceprev=1.2855 | ma_roc=0.7640 | lr=0.000299 | 10.1s\n",
      "[E08] train_loss=0.6273 (bce=0.5733, nce=0.0539, l1=0.00000) | val_roc=0.7438 (valid=12) val_pr=0.2732 | val_bceprev=1.3907 | ma_roc=0.7573 | lr=0.000298 | 10.1s\n",
      "[E09] train_loss=0.5801 (bce=0.5299, nce=0.0502, l1=0.00000) | val_roc=0.7420 (valid=12) val_pr=0.2789 | val_bceprev=1.7181 | ma_roc=0.7475 | lr=0.000296 | 10.3s\n",
      "[E10] train_loss=0.5264 (bce=0.4796, nce=0.0468, l1=0.00000) | val_roc=0.7335 (valid=12) val_pr=0.2709 | val_bceprev=2.1328 | ma_roc=0.7398 | lr=0.000294 | 10.2s\n",
      "[E11] train_loss=0.4926 (bce=0.4484, nce=0.0443, l1=0.00000) | val_roc=0.7423 (valid=12) val_pr=0.2581 | val_bceprev=1.8969 | ma_roc=0.7393 | lr=0.000291 | 10.1s\n",
      "[E12] train_loss=0.4685 (bce=0.4255, nce=0.0430, l1=0.00000) | val_roc=0.7275 (valid=12) val_pr=0.2677 | val_bceprev=2.1925 | ma_roc=0.7344 | lr=0.000288 | 10.0s\n",
      "[E13] train_loss=0.4402 (bce=0.3988, nce=0.0414, l1=0.00000) | val_roc=0.7260 (valid=12) val_pr=0.2431 | val_bceprev=2.6001 | ma_roc=0.7320 | lr=0.000284 | 10.1s\n",
      "[E14] train_loss=0.4208 (bce=0.3809, nce=0.0399, l1=0.00000) | val_roc=0.7172 (valid=12) val_pr=0.2562 | val_bceprev=2.8769 | ma_roc=0.7236 | lr=0.000280 | 10.2s\n",
      "[E15] train_loss=0.4001 (bce=0.3614, nce=0.0387, l1=0.00000) | val_roc=0.7259 (valid=12) val_pr=0.2687 | val_bceprev=2.6487 | ma_roc=0.7230 | lr=0.000275 | 10.1s\n",
      "[EARLY STOP] No MA-ROC improvement for 10 epochs. Stopping at epoch 15.\n",
      "\n",
      "----- TRAINING SUMMARY (Stage A) -----\n",
      "Best epoch: 5 | best moving-avg ROC-AUC: 0.7689\n",
      "Best ckpt:  tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt\n",
      "Last ckpt:  tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_last.pt\n",
      "Epoch log:  tox21_dualenc_v1/results/v4/v4_20250902_165736_stageA_log.jsonl\n",
      "Summaries:  tox21_dualenc_v1/results/v4/v4_20250902_165736_stageA_best.json | tox21_dualenc_v1/results/v4/v4_20250902_165736_stageA_last.json\n",
      "--------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, math, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast  # modern API\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    SKLEARN_OK = True\n",
    "except Exception as e:\n",
    "    SKLEARN_OK = False\n",
    "    print(f\"[WARN] sklearn not available ({e}). Metrics limited.\")\n",
    "\n",
    "logger = logging.getLogger(\"v4\")\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Hyperparams (Stage A: text frozen) ---\n",
    "EPOCHS_STAGE_A   = 60\n",
    "BASE_LR_OTHER    = 3e-4       # safer start; we’ll cosine up/down\n",
    "WEIGHT_DECAY     = 1e-4\n",
    "WARMUP_FRAC      = 0.10\n",
    "MAX_NORM         = 1.0\n",
    "USE_AMP          = (DEVICE.type == \"cuda\")\n",
    "AMP_DTYPE        = (torch.bfloat16 if (USE_AMP and torch.cuda.is_bf16_supported())\n",
    "                    else (torch.float16 if USE_AMP else torch.float32))\n",
    "EARLYSTOP_PAT    = 10\n",
    "PLATEAU_PAT      = 5\n",
    "PLATEAU_FACTOR   = 0.5\n",
    "PLATEAU_MIN_LR   = 1e-6\n",
    "PLATEAU_COOLDOWN = 1\n",
    "\n",
    "# --- Files ---\n",
    "CKPT_DIR = CHECKPOINTS_DIR\n",
    "RUN_BASENAME = RUN_ID\n",
    "EPOCH_LOG = RESULTS_DIR / f\"{RUN_BASENAME}_stageA_log.jsonl\"\n",
    "BEST_SUMMARY = RESULTS_DIR / f\"{RUN_BASENAME}_stageA_best.json\"\n",
    "LAST_SUMMARY = RESULTS_DIR / f\"{RUN_BASENAME}_stageA_last.json\"\n",
    "\n",
    "# --- Helpers ---\n",
    "def compute_metrics(logits: np.ndarray, targets: np.ndarray, mask: np.ndarray) -> Dict[str, float]:\n",
    "    if not SKLEARN_OK:\n",
    "        return {\"macro_roc_auc\": float(\"nan\"), \"macro_pr_auc\": float(\"nan\")}\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "    roc_list, pr_list = [], []\n",
    "    for j in range(targets.shape[1]):\n",
    "        m = mask[:, j].astype(bool)\n",
    "        yj = targets[m, j]\n",
    "        pj = probs[m, j]\n",
    "        # skip degenerate labels\n",
    "        if yj.size < 3 or (np.all(yj == 0) or np.all(yj == 1)):\n",
    "            continue\n",
    "        try: roc_list.append(roc_auc_score(yj, pj))\n",
    "        except: pass\n",
    "        try: pr_list.append(average_precision_score(yj, pj))\n",
    "        except: pass\n",
    "    macro_roc = float(np.mean(roc_list)) if roc_list else float(\"nan\")\n",
    "    macro_pr  = float(np.mean(pr_list))  if pr_list else float(\"nan\")\n",
    "    return {\"macro_roc_auc\": macro_roc, \"macro_pr_auc\": macro_pr}\n",
    "\n",
    "class MovingStat:\n",
    "    def __init__(self, k:int=3): self.k=k; self.buf=[]\n",
    "    def update(self, v: float)->float:\n",
    "        self.buf.append(v); \n",
    "        if len(self.buf)>self.k: self.buf.pop(0)\n",
    "        return sum(self.buf)/len(self.buf)\n",
    "    def value(self)->float: return (sum(self.buf)/len(self.buf)) if self.buf else float(\"nan\")\n",
    "\n",
    "def current_lr(optim: torch.optim.Optimizer)->float:\n",
    "    return float(optim.param_groups[0][\"lr\"])\n",
    "\n",
    "# --- Optimizer + schedulers ---\n",
    "def build_optimizer_stage_a(model: nn.Module):\n",
    "    params = [{\"params\":[p], \"lr\": BASE_LR_OTHER, \"weight_decay\": WEIGHT_DECAY}\n",
    "              for n,p in model.named_parameters() if p.requires_grad]\n",
    "    opt = torch.optim.AdamW(params, lr=BASE_LR_OTHER, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "    steps_per_epoch = max(1, len(train_loader))\n",
    "    total_steps = EPOCHS_STAGE_A * steps_per_epoch\n",
    "    warmup_steps = max(1, int(WARMUP_FRAC * total_steps))\n",
    "\n",
    "    def lr_lambda(step):\n",
    "        if step < warmup_steps:\n",
    "            return float(step) / float(max(1, warmup_steps))\n",
    "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "    cosine = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "    plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"max\", factor=PLATEAU_FACTOR, patience=PLATEAU_PAT,\n",
    "        threshold=1e-4, threshold_mode=\"rel\", cooldown=PLATEAU_COOLDOWN,\n",
    "        min_lr=PLATEAU_MIN_LR, verbose=False\n",
    "    )\n",
    "    return opt, cosine, plateau, steps_per_epoch\n",
    "\n",
    "# --- Loss in FP32 on clamped logits ---\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0  # logits clamp for BCE stability\n",
    "\n",
    "def bce_stable(model, logits_raw, labels, mask):\n",
    "    # 1) guard labels\n",
    "    labels = labels.float().clamp_(0.0, 1.0)\n",
    "    # 2) clamp logits and compute BCE\n",
    "    logits = logits_raw.float().clamp_(CLAMP_MIN, CLAMP_MAX)\n",
    "    return model.crit(logits, labels, mask)\n",
    "\n",
    "@torch.no_grad()\n",
    "def val_label_prevalence(loader) -> Dict[str,float]:\n",
    "    all_y = []\n",
    "    for b in loader: all_y.append(b.labels)\n",
    "    Y = torch.cat(all_y, dim=0).numpy()\n",
    "    prev = (Y.mean(axis=0)).tolist()\n",
    "    return {LABELS[i]: float(round(prev[i], 4)) for i in range(len(LABELS))}\n",
    "\n",
    "# --- Train / Eval ---\n",
    "def train_one_epoch(epoch: int, model: nn.Module, opt, cosine, steps_per_epoch: int)->Dict[str,float]:\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    total_loss = total_bce = total_nce = total_l1 = 0.0\n",
    "    total_count = 0\n",
    "    gstep = epoch * steps_per_epoch\n",
    "\n",
    "    for it, batch in enumerate(train_loader):\n",
    "        labels = batch.labels.to(DEVICE)\n",
    "        label_mask = batch.label_mask.to(DEVICE)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=USE_AMP, dtype=AMP_DTYPE):\n",
    "            out = model(batch, compute_aux=True)\n",
    "            logits_raw = out[\"logits\"]\n",
    "\n",
    "        # Diagnostics on raw logits\n",
    "        with torch.no_grad():\n",
    "            lmin = float(logits_raw.min().item())\n",
    "            lmax = float(logits_raw.max().item())\n",
    "        # Stable BCE\n",
    "        loss_bce = bce_stable(model, logits_raw, labels, label_mask)\n",
    "\n",
    "        loss_nce = model.w_nce * out.get(\"loss_nce\", torch.tensor(0.0, device=logits_raw.device))\n",
    "        loss_l1  = model.w_l1  * out.get(\"loss_l1\",  torch.tensor(0.0, device=logits_raw.device))\n",
    "        loss = loss_bce + loss_nce + loss_l1\n",
    "\n",
    "        if (it == 0) and (epoch == 0):\n",
    "            with torch.no_grad():\n",
    "                # how many elements would clamp?\n",
    "                clamp_frac = float(( (logits_raw < CLAMP_MIN) | (logits_raw > CLAMP_MAX) ).float().mean().item())\n",
    "                print(f\"[DEBUG e{epoch} i{it}] raw logits min/max: {lmin:.2e}/{lmax:.2e} | clamp_frac≈{clamp_frac:.3f}\")\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), MAX_NORM)\n",
    "        opt.step()\n",
    "\n",
    "        gstep += 1\n",
    "        cosine.step()\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        total_bce  += float(loss_bce.item())\n",
    "        total_nce  += float(loss_nce.item())\n",
    "        total_l1   += float(loss_l1.item())\n",
    "        total_count += 1\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, total_count),\n",
    "        \"bce\":  total_bce  / max(1, total_count),\n",
    "        \"nce\":  total_nce  / max(1, total_count),\n",
    "        \"l1\":   total_l1   / max(1, total_count),\n",
    "        \"sec\":  dt,\n",
    "        \"lr\":   current_lr(opt)\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader)->Dict[str,float]:\n",
    "    model.eval()\n",
    "    all_logits, all_labels, all_mask = [], [], []\n",
    "    for batch in loader:\n",
    "        out = model(batch, compute_aux=False)\n",
    "        all_logits.append(out[\"logits\"].float().cpu())\n",
    "        all_labels.append(batch.labels.cpu())\n",
    "        all_mask.append(batch.label_mask.cpu())\n",
    "    logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    mask   = torch.cat(all_mask,   dim=0).numpy().astype(np.bool_)\n",
    "\n",
    "    # BCE preview on clamped logits (fp32)\n",
    "    b_idx = np.random.default_rng(GLOBAL_SEED).choice(len(labels), size=min(256, len(labels)), replace=False)\n",
    "    logits_b = torch.tensor(logits[b_idx], dtype=torch.float32, device=DEVICE).clamp_(CLAMP_MIN, CLAMP_MAX)\n",
    "    labels_b = torch.tensor(labels[b_idx], dtype=torch.float32, device=DEVICE).clamp_(0.0, 1.0)\n",
    "    mask_b   = torch.tensor(mask[b_idx],   dtype=torch.bool,     device=DEVICE)\n",
    "    bce_preview = float(model.crit(logits_b, labels_b, mask_b).item())\n",
    "\n",
    "    mets = compute_metrics(np.clip(logits, CLAMP_MIN, CLAMP_MAX), labels, mask)\n",
    "    # Count valid labels for AUC\n",
    "    valid_labels = 0\n",
    "    if SKLEARN_OK:\n",
    "        for j in range(labels.shape[1]):\n",
    "            yj = labels[mask[:, j], j]\n",
    "            if yj.size >= 3 and not (np.all(yj==0) or np.all(yj==1)):\n",
    "                valid_labels += 1\n",
    "    mets[\"valid_labels_for_auc\"] = int(valid_labels)\n",
    "    mets[\"bce_preview\"] = bce_preview\n",
    "    return mets\n",
    "\n",
    "# --- Orchestrate Stage A ---\n",
    "opt, cosine, plateau, steps_per_epoch = build_optimizer_stage_a(model)\n",
    "best_val = -1.0\n",
    "best_epoch = -1\n",
    "mov = MovingStat(k=3)\n",
    "no_improve_epochs = 0\n",
    "start_time = time.time()\n",
    "\n",
    "if EPOCH_LOG.exists(): EPOCH_LOG.unlink()\n",
    "epoch_f = open(EPOCH_LOG, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "print(f\"\\n=== Stage A (robust): up to {EPOCHS_STAGE_A} epochs | base_lr={BASE_LR_OTHER} | AMP fwd={USE_AMP} ({AMP_DTYPE}) | FP32 BCE on clamped logits ===\")\n",
    "try:\n",
    "    prev = val_label_prevalence(val_loader)\n",
    "    prev_str = \", \".join([f\"{k}:{v}\" for k,v in list(prev.items())[:6]]) + \" ...\"\n",
    "    print(f\"[VAL prevalence] {prev_str}\")\n",
    "except Exception as e:\n",
    "    print(f\"[VAL prevalence] skipped ({e})\")\n",
    "\n",
    "for epoch in range(EPOCHS_STAGE_A):\n",
    "    tr = train_one_epoch(epoch, model, opt, cosine, steps_per_epoch)\n",
    "    va = evaluate(model, val_loader)\n",
    "    ma = mov.update(va[\"macro_roc_auc\"])\n",
    "    plateau.step(0.0 if math.isnan(ma) else ma)\n",
    "\n",
    "    rec = {\"epoch\": epoch, \"train\": tr, \"val\": va, \"val_ma_roc\": ma, \"lr\": [g[\"lr\"] for g in opt.param_groups]}\n",
    "    epoch_f.write(json.dumps(rec) + \"\\n\"); epoch_f.flush()\n",
    "\n",
    "    print(f\"[E{epoch:02d}] \"\n",
    "          f\"train_loss={tr['loss']:.4f} (bce={tr['bce']:.4f}, nce={tr['nce']:.4f}, l1={tr['l1']:.5f}) \"\n",
    "          f\"| val_roc={va['macro_roc_auc']:.4f} (valid={va['valid_labels_for_auc']}) \"\n",
    "          f\"val_pr={va['macro_pr_auc']:.4f} | val_bceprev={va['bce_preview']:.4f} \"\n",
    "          f\"| ma_roc={ma:.4f} | lr={tr['lr']:.6f} | {tr['sec']:.1f}s\")\n",
    "\n",
    "    score = ma if not math.isnan(ma) else va[\"macro_roc_auc\"]\n",
    "    if score > best_val:\n",
    "        best_val = score; best_epoch = epoch; no_improve_epochs = 0\n",
    "        ckpt_path = CKPT_DIR / f\"{RUN_BASENAME}_stageA_best.pt\"\n",
    "        torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": epoch}, ckpt_path)\n",
    "        print(f\"  ↳ Saved BEST checkpoint: {ckpt_path.as_posix()} (ma_roc={best_val:.4f})\")\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "\n",
    "    if no_improve_epochs >= EARLYSTOP_PAT:\n",
    "        print(f\"[EARLY STOP] No MA-ROC improvement for {EARLYSTOP_PAT} epochs. Stopping at epoch {epoch}.\")\n",
    "        break\n",
    "\n",
    "# Save last & summaries\n",
    "last_ckpt = CKPT_DIR / f\"{RUN_BASENAME}_stageA_last.pt\"\n",
    "torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": epoch}, last_ckpt)\n",
    "epoch_f.close()\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "BEST_SUMMARY.write_text(json.dumps({\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_val_ma_roc\": best_val,\n",
    "    \"best_ckpt\": str((CKPT_DIR / f\"{RUN_BASENAME}_stageA_best.pt\").resolve()),\n",
    "    \"epochs_planned\": EPOCHS_STAGE_A,\n",
    "    \"epochs_ran\": epoch + 1,\n",
    "    \"amp_forward\": USE_AMP,\n",
    "    \"bce_logits_clamp\": [CLAMP_MIN, CLAMP_MAX],\n",
    "    \"base_lr\": BASE_LR_OTHER,\n",
    "    \"wd\": WEIGHT_DECAY,\n",
    "    \"elapsed_sec\": round(elapsed, 1)\n",
    "}, indent=2))\n",
    "\n",
    "LAST_SUMMARY.write_text(json.dumps({\n",
    "    \"epoch_last\": epoch,\n",
    "    \"last_ckpt\": str(last_ckpt.resolve()),\n",
    "    \"lr_last\": [g[\"lr\"] for g in opt.param_groups],\n",
    "    \"elapsed_sec\": round(elapsed, 1)\n",
    "}, indent=2))\n",
    "\n",
    "print(\"\\n----- TRAINING SUMMARY (Stage A) -----\")\n",
    "print(f\"Best epoch: {best_epoch} | best moving-avg ROC-AUC: {best_val:.4f}\")\n",
    "print(f\"Best ckpt:  {(CKPT_DIR / f'{RUN_BASENAME}_stageA_best.pt').as_posix()}\")\n",
    "print(f\"Last ckpt:  {last_ckpt.as_posix()}\")\n",
    "print(f\"Epoch log:  {EPOCH_LOG.as_posix()}\")\n",
    "print(f\"Summaries:  {BEST_SUMMARY.as_posix()} | {LAST_SUMMARY.as_posix()}\")\n",
    "print(\"--------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b40262",
   "metadata": {},
   "source": [
    "### 7b) eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf86b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using checkpoint: D:/Coding Projects/Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI/tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt\n",
      "\n",
      "----- STAGE A EVALUATION -----\n",
      "Val  macro ROC / PR : 0.7638 / 0.2463\n",
      "Test macro ROC / PR : 0.7620 / 0.2524\n",
      "Test macro F1       : 0.2968\n",
      "Saved JSONs: v4_20250902_165736_eval_val.json, v4_20250902_165736_eval_test.json, thresholds → v4_20250902_165736_thresholds.json\n",
      "Per-label CSVs: v4_20250902_165736_val_perlabel.csv, v4_20250902_165736_test_perlabel.csv, v4_20250902_165736_test_cls_perlabel.csv\n",
      "Summary JSON: v4_20250902_165736_summary.json in tox21_dualenc_v1/results/v4/eval/stage_A\n",
      "------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Optional metrics\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve\n",
    "    SKLEARN_OK = True\n",
    "except Exception as e:\n",
    "    SKLEARN_OK = False\n",
    "    print(f\"[WARN] sklearn not available ({e}). Metrics will be limited.\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Save location ---\n",
    "EVAL_DIR = Path(\"tox21_dualenc_v1/results/v4/eval/stage_A\")\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Find best checkpoint ---\n",
    "best_summary_path = RESULTS_DIR / f\"{RUN_ID}_stageA_best.json\"\n",
    "ckpt_path = None\n",
    "if best_summary_path.exists():\n",
    "    try:\n",
    "        ckpt_path = Path(json.loads(best_summary_path.read_text())[\"best_ckpt\"])\n",
    "    except Exception:\n",
    "        ckpt_path = None\n",
    "if not ckpt_path or not ckpt_path.exists():\n",
    "    cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_ID}_stageA_best.pt\"))\n",
    "    if cands:\n",
    "        ckpt_path = cands[-1]\n",
    "    else:\n",
    "        cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_ID}_stageA_last.pt\"))\n",
    "        assert cands, \"No Stage-A checkpoints found.\"\n",
    "        ckpt_path = cands[-1]\n",
    "\n",
    "print(f\"[INFO] Using checkpoint: {ckpt_path.as_posix()}\")\n",
    "\n",
    "# --- Load state dict robustly for torch 2.6 ---\n",
    "def load_state_dict_robust(path: Path) -> dict:\n",
    "    \"\"\"\n",
    "    Try weights_only safe load first (torch 2.6 default). If it fails due to new\n",
    "    unpickler restrictions, allowlist TorchVersion and fallback to weights_only=False.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\")  # default weights_only=True in torch 2.6\n",
    "    except pickle.UnpicklingError as e:\n",
    "        # allowlist TorchVersion then retry with weights_only=False (ONLY for trusted files)\n",
    "        try:\n",
    "            from torch.serialization import add_safe_globals\n",
    "            from torch.torch_version import TorchVersion\n",
    "            add_safe_globals([TorchVersion])\n",
    "        except Exception:\n",
    "            pass\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except Exception:\n",
    "        # generic fallback\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "    # common keys: \"model\" or \"state_dict\"; else assume whole object is the state dict\n",
    "    if isinstance(obj, dict):\n",
    "        if \"model\" in obj and isinstance(obj[\"model\"], dict):\n",
    "            return obj[\"model\"]\n",
    "        if \"state_dict\" in obj and isinstance(obj[\"state_dict\"], dict):\n",
    "            return obj[\"state_dict\"]\n",
    "    if isinstance(obj, dict):\n",
    "        return obj\n",
    "    raise RuntimeError(\"Could not extract a state_dict from checkpoint.\")\n",
    "    \n",
    "# --- Rebuild model & load weights ---\n",
    "assert 'DualEncCoAttnModel' in globals(), \"Model class not in scope. Please re-run Cell 5.\"\n",
    "model_eval = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "state = load_state_dict_robust(ckpt_path)\n",
    "missing, unexpected = model_eval.load_state_dict(state, strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] State dict load: missing={missing}, unexpected={unexpected}\")\n",
    "model_eval.eval()\n",
    "\n",
    "# --- Helpers ---\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logits(loader):\n",
    "    all_logits, all_labels, all_mask = [], [], []\n",
    "    for b in loader:\n",
    "        out = model_eval(b, compute_aux=False)\n",
    "        all_logits.append(out[\"logits\"].float().cpu())\n",
    "        all_labels.append(b.labels.cpu())\n",
    "        all_mask.append(b.label_mask.cpu())\n",
    "    logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    mask   = torch.cat(all_mask,   dim=0).numpy().astype(bool)\n",
    "    return logits, labels, mask\n",
    "\n",
    "def per_label_metrics(logits, labels, mask, label_names):\n",
    "    probs = 1.0 / (1.0 + np.exp(-np.clip(logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "    rows = []\n",
    "    for j, name in enumerate(label_names):\n",
    "        m = mask[:, j]\n",
    "        y = labels[m, j]\n",
    "        p = probs[m, j]\n",
    "        roc = pr = float(\"nan\")\n",
    "        if SKLEARN_OK and y.size >= 3 and not (np.all(y==0) or np.all(y==1)):\n",
    "            try: roc = roc_auc_score(y, p)\n",
    "            except: pass\n",
    "            try: pr  = average_precision_score(y, p)\n",
    "            except: pass\n",
    "        rows.append({\n",
    "            \"label\": name,\n",
    "            \"roc_auc\": None if np.isnan(roc) else float(roc),\n",
    "            \"pr_auc\":  None if np.isnan(pr)  else float(pr),\n",
    "            \"pos_rate\": float(y.mean()) if y.size>0 else None,\n",
    "            \"n\": int(y.size)\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def choose_thresholds_val(logits, labels, mask):\n",
    "    probs = 1.0 / (1.0 + np.exp(-np.clip(logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "    th = []\n",
    "    for j in range(labels.shape[1]):\n",
    "        m = mask[:, j]\n",
    "        y = labels[m, j]\n",
    "        p = probs[m, j]\n",
    "        if (not SKLEARN_OK) or y.size < 3 or np.all(y==0) or np.all(y==1):\n",
    "            th.append(0.5); continue\n",
    "        pr, rc, tt = precision_recall_curve(y, p)\n",
    "        if tt.size == 0: th.append(0.5)\n",
    "        else:\n",
    "            f1 = 2*pr*rc/(pr+rc+1e-12)\n",
    "            th.append(float(tt[np.argmax(f1)]))\n",
    "    return np.asarray(th, dtype=np.float32)\n",
    "\n",
    "def apply_thresholds(logits, thresholds):\n",
    "    probs = 1.0 / (1.0 + np.exp(-np.clip(logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "    return (probs >= thresholds[None, :]).astype(np.float32)\n",
    "\n",
    "def per_label_cls_metrics(y_true, y_pred, mask, label_names):\n",
    "    rows = []\n",
    "    for j, name in enumerate(label_names):\n",
    "        m = mask[:, j]\n",
    "        y = y_true[m, j]\n",
    "        yhat = y_pred[m, j]\n",
    "        if y.size == 0:\n",
    "            rows.append({\"label\": name, \"f1\": None, \"precision\": None, \"recall\": None, \"n\": 0})\n",
    "            continue\n",
    "        if SKLEARN_OK:\n",
    "            f1 = f1_score(y, yhat, zero_division=0)\n",
    "        else:\n",
    "            tp = ((y==1)&(yhat==1)).sum(); fp = ((y==0)&(yhat==1)).sum(); fn = ((y==1)&(yhat==0)).sum()\n",
    "            prec = tp/(tp+fp+1e-12); rec = tp/(tp+fn+1e-12)\n",
    "            f1 = 2*prec*rec/(prec+rec+1e-12)\n",
    "        tp = ((y==1)&(yhat==1)).sum(); fp = ((y==0)&(yhat==1)).sum(); fn = ((y==1)&(yhat==0)).sum()\n",
    "        prec = float(tp/(tp+fp+1e-12)); rec = float(tp/(tp+fn+1e-12))\n",
    "        rows.append({\"label\": name, \"f1\": float(f1), \"precision\": prec, \"recall\": rec, \"n\": int(y.size)})\n",
    "    return rows\n",
    "\n",
    "def macro_avg(rows, key):\n",
    "    vals = [r[key] for r in rows if (r[key] is not None) and not (isinstance(r[key], float) and (math.isnan(r[key]) or math.isinf(r[key])))]\n",
    "    return float(np.mean(vals)) if vals else float(\"nan\")\n",
    "\n",
    "# --- Collect logits ---\n",
    "assert 'val_loader' in globals() and 'test_loader' in globals(), \"Missing dataloaders. Please re-run Cell 4.\"\n",
    "val_logits,  val_labels,  val_mask  = collect_logits(val_loader)\n",
    "test_logits, test_labels, test_mask = collect_logits(test_loader)\n",
    "\n",
    "# --- Per-label metrics ---\n",
    "val_rows  = per_label_metrics(val_logits,  val_labels,  val_mask,  LABELS)\n",
    "test_rows = per_label_metrics(test_logits, test_labels, test_mask, LABELS)\n",
    "\n",
    "# --- Thresholds on val (max-F1), apply to test ---\n",
    "thresholds = choose_thresholds_val(val_logits, val_labels, val_mask)\n",
    "test_pred  = apply_thresholds(test_logits, thresholds)\n",
    "test_cls_rows = per_label_cls_metrics(test_labels, test_pred, test_mask, LABELS)\n",
    "\n",
    "# --- Macro summaries ---\n",
    "summary = {\n",
    "    \"val_macro_roc\":  macro_avg(val_rows,  \"roc_auc\"),\n",
    "    \"val_macro_pr\":   macro_avg(val_rows,  \"pr_auc\"),\n",
    "    \"test_macro_roc\": macro_avg(test_rows, \"roc_auc\"),\n",
    "    \"test_macro_pr\":  macro_avg(test_rows, \"pr_auc\"),\n",
    "    \"test_macro_f1\":  macro_avg(test_cls_rows, \"f1\"),\n",
    "}\n",
    "\n",
    "# --- Save artifacts to eval/stage_A ---\n",
    "import pandas as pd\n",
    "val_csv      = EVAL_DIR / f\"{RUN_ID}_val_perlabel.csv\"\n",
    "test_csv     = EVAL_DIR / f\"{RUN_ID}_test_perlabel.csv\"\n",
    "test_cls_csv = EVAL_DIR / f\"{RUN_ID}_test_cls_perlabel.csv\"\n",
    "pd.DataFrame(val_rows).to_csv(val_csv, index=False)\n",
    "pd.DataFrame(test_rows).to_csv(test_csv, index=False)\n",
    "pd.DataFrame(test_cls_rows).to_csv(test_cls_csv, index=False)\n",
    "\n",
    "val_json  = EVAL_DIR / f\"{RUN_ID}_eval_val.json\"\n",
    "test_json = EVAL_DIR / f\"{RUN_ID}_eval_test.json\"\n",
    "th_json   = EVAL_DIR / f\"{RUN_ID}_thresholds.json\"\n",
    "(sum_json := EVAL_DIR / f\"{RUN_ID}_summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "val_json.write_text(json.dumps({\"per_label\": val_rows, \"summary\": summary}, indent=2))\n",
    "test_json.write_text(json.dumps({\"per_label\": test_rows, \"summary\": summary}, indent=2))\n",
    "th_json.write_text(json.dumps({\"thresholds\": thresholds.tolist(), \"labels\": LABELS}, indent=2))\n",
    "\n",
    "print(\"\\n----- STAGE A EVALUATION -----\")\n",
    "print(f\"Val  macro ROC / PR : {summary['val_macro_roc']:.4f} / {summary['val_macro_pr']:.4f}\")\n",
    "print(f\"Test macro ROC / PR : {summary['test_macro_roc']:.4f} / {summary['test_macro_pr']:.4f}\")\n",
    "print(f\"Test macro F1       : {summary['test_macro_f1']:.4f}\")\n",
    "print(f\"Saved JSONs: {val_json.name}, {test_json.name}, thresholds → {th_json.name}\")\n",
    "print(f\"Per-label CSVs: {val_csv.name}, {test_csv.name}, {test_cls_csv.name}\")\n",
    "print(f\"Summary JSON: {sum_json.name} in {EVAL_DIR.as_posix()}\")\n",
    "print(\"------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bec7418",
   "metadata": {},
   "source": [
    "## 8: Stage B): Stage B (LLRD unfreeze + EMA) — robust trainer with FP32 BCE on clamped "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433feb15",
   "metadata": {},
   "source": [
    "### 8a) trining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1501b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Stage B init from: D:/Coding Projects/Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI/tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageA_best.pt\n",
      "\n",
      "=== Stage B: LLRD unfreeze last-2 layers + EMA ===\n",
      "Text LRs last-2: 1e-05 (L-1), 2e-05 (L) | Proj: 0.0001 | Others: 0.0002\n",
      "AMP=True (torch.bfloat16)  |  EMA=True (decay=0.999)\n",
      "[E00 B] train_loss=0.7515 (bce=0.6892, nce=0.0623, l1=0.00000) | val_roc=0.7662 val_pr=0.2508 | val_bceprev=1.1831 | ma_roc=0.7662 | lr0=0.000003 | 12.6s | EMA=True\n",
      "  ↳ Saved BEST Stage-B checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageB_best.pt (ma_roc=0.7662)\n",
      "[E01 B] train_loss=0.7029 (bce=0.6503, nce=0.0526, l1=0.00000) | val_roc=0.7688 val_pr=0.2545 | val_bceprev=1.1650 | ma_roc=0.7675 | lr0=0.000007 | 12.3s | EMA=True\n",
      "  ↳ Saved BEST Stage-B checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageB_best.pt (ma_roc=0.7675)\n",
      "[E02 B] train_loss=0.6845 (bce=0.6351, nce=0.0494, l1=0.00000) | val_roc=0.7702 val_pr=0.2613 | val_bceprev=1.1642 | ma_roc=0.7684 | lr0=0.000010 | 12.4s | EMA=True\n",
      "  ↳ Saved BEST Stage-B checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageB_best.pt (ma_roc=0.7684)\n",
      "[E03 B] train_loss=0.6547 (bce=0.6074, nce=0.0474, l1=0.00000) | val_roc=0.7714 val_pr=0.2693 | val_bceprev=1.1729 | ma_roc=0.7701 | lr0=0.000010 | 12.3s | EMA=True\n",
      "  ↳ Saved BEST Stage-B checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageB_best.pt (ma_roc=0.7701)\n",
      "[E04 B] train_loss=0.6033 (bce=0.5607, nce=0.0426, l1=0.00000) | val_roc=0.7701 val_pr=0.2724 | val_bceprev=1.1998 | ma_roc=0.7706 | lr0=0.000010 | 12.3s | EMA=True\n",
      "  ↳ Saved BEST Stage-B checkpoint: tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageB_best.pt (ma_roc=0.7706)\n",
      "[E05 B] train_loss=0.5489 (bce=0.5095, nce=0.0394, l1=0.00000) | val_roc=0.7689 val_pr=0.2792 | val_bceprev=1.2456 | ma_roc=0.7701 | lr0=0.000010 | 12.9s | EMA=True\n",
      "[E06 B] train_loss=0.5176 (bce=0.4809, nce=0.0368, l1=0.00000) | val_roc=0.7667 val_pr=0.2784 | val_bceprev=1.3063 | ma_roc=0.7686 | lr0=0.000009 | 12.6s | EMA=True\n",
      "[E07 B] train_loss=0.4822 (bce=0.4475, nce=0.0347, l1=0.00000) | val_roc=0.7637 val_pr=0.2792 | val_bceprev=1.3881 | ma_roc=0.7664 | lr0=0.000009 | 12.6s | EMA=True\n",
      "[E08 B] train_loss=0.4471 (bce=0.4136, nce=0.0334, l1=0.00000) | val_roc=0.7611 val_pr=0.2807 | val_bceprev=1.4776 | ma_roc=0.7638 | lr0=0.000009 | 12.5s | EMA=True\n",
      "[E09 B] train_loss=0.4170 (bce=0.3848, nce=0.0322, l1=0.00000) | val_roc=0.7593 val_pr=0.2831 | val_bceprev=1.5774 | ma_roc=0.7613 | lr0=0.000008 | 12.7s | EMA=True\n",
      "[E10 B] train_loss=0.4021 (bce=0.3712, nce=0.0309, l1=0.00000) | val_roc=0.7571 val_pr=0.2836 | val_bceprev=1.6820 | ma_roc=0.7591 | lr0=0.000008 | 12.8s | EMA=True\n",
      "[E11 B] train_loss=0.3691 (bce=0.3396, nce=0.0295, l1=0.00000) | val_roc=0.7545 val_pr=0.2845 | val_bceprev=1.8091 | ma_roc=0.7570 | lr0=0.000008 | 12.9s | EMA=True\n",
      "[E12 B] train_loss=0.3494 (bce=0.3207, nce=0.0287, l1=0.00000) | val_roc=0.7524 val_pr=0.2837 | val_bceprev=1.9397 | ma_roc=0.7547 | lr0=0.000007 | 12.3s | EMA=True\n",
      "[EARLY STOP] No MA-ROC improvement for 8 epochs. Stopping at epoch 12.\n",
      "\n",
      "----- TRAINING SUMMARY (Stage B) -----\n",
      "Best epoch: 4 | best moving-avg ROC-AUC: 0.7706\n",
      "Best ckpt:  tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageB_best.pt\n",
      "Last ckpt:  tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageB_last.pt\n",
      "Epoch log:  tox21_dualenc_v1/results/v4/v4_20250902_165736_stageB_log.jsonl\n",
      "Summaries:  tox21_dualenc_v1/results/v4/v4_20250902_165736_stageB_best.json | tox21_dualenc_v1/results/v4/v4_20250902_165736_stageB_last.json\n",
      "--------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, math, time, pickle\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast  # modern API\n",
    "\n",
    "# Optional metrics\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    SKLEARN_OK = True\n",
    "except Exception as e:\n",
    "    SKLEARN_OK = False\n",
    "    print(f\"[WARN] sklearn not available ({e}). Metrics will be limited.\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------\n",
    "# Stage B hyperparams\n",
    "# -----------------------\n",
    "EPOCHS_STAGE_B    = 30\n",
    "# LRs: small for text layers, moderate for projection/other modules\n",
    "LR_TEXT_LN1       = 1.0e-5   # 2nd-to-last transformer layer\n",
    "LR_TEXT_LN2       = 2.0e-5   # last transformer layer (highest)\n",
    "LR_TEXT_PROJ      = 1.0e-4   # our linear proj on top of backbone\n",
    "LR_OTHERS         = 2.0e-4   # graph/desc/fusion/head\n",
    "WARMUP_FRAC_B     = 0.10\n",
    "WEIGHT_DECAY_TXT  = 0.01\n",
    "WEIGHT_DECAY_OTH  = 1e-4\n",
    "MAX_NORM          = 1.0\n",
    "USE_AMP           = (DEVICE.type == \"cuda\")\n",
    "AMP_DTYPE         = (torch.bfloat16 if (USE_AMP and torch.cuda.is_bf16_supported())\n",
    "                     else (torch.float16 if USE_AMP else torch.float32))\n",
    "EARLYSTOP_PAT     = 8\n",
    "PLATEAU_PAT       = 4\n",
    "PLATEAU_FACTOR    = 0.5\n",
    "PLATEAU_MIN_LR    = 1e-6\n",
    "PLATEAU_COOLDOWN  = 1\n",
    "USE_EMA           = True\n",
    "EMA_DECAY         = 0.999\n",
    "\n",
    "# --- Files ---\n",
    "CKPT_DIR = CHECKPOINTS_DIR\n",
    "RUN_BASENAME = RUN_ID\n",
    "EPOCH_LOG_B  = RESULTS_DIR / f\"{RUN_BASENAME}_stageB_log.jsonl\"\n",
    "BEST_SUMMARY_B = RESULTS_DIR / f\"{RUN_BASENAME}_stageB_best.json\"\n",
    "LAST_SUMMARY_B = RESULTS_DIR / f\"{RUN_BASENAME}_stageB_last.json\"\n",
    "\n",
    "# --- Stability guards (match Stage A)\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "\n",
    "def bce_stable(model, logits_raw, labels, mask):\n",
    "    labels = labels.float().clamp_(0.0, 1.0)\n",
    "    logits = logits_raw.float().clamp_(CLAMP_MIN, CLAMP_MAX)\n",
    "    return model.crit(logits, labels, mask)\n",
    "\n",
    "def compute_metrics(logits: np.ndarray, targets: np.ndarray, mask: np.ndarray) -> Dict[str, float]:\n",
    "    if not SKLEARN_OK:\n",
    "        return {\"macro_roc_auc\": float(\"nan\"), \"macro_pr_auc\": float(\"nan\")}\n",
    "    probs = 1.0 / (1.0 + np.exp(-np.clip(logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "    roc_list, pr_list = [], []\n",
    "    for j in range(targets.shape[1]):\n",
    "        m = mask[:, j].astype(bool)\n",
    "        yj = targets[m, j]; pj = probs[m, j]\n",
    "        if yj.size < 3 or (np.all(yj==0) or np.all(yj==1)): continue\n",
    "        try: roc_list.append(roc_auc_score(yj, pj))\n",
    "        except: pass\n",
    "        try: pr_list.append(average_precision_score(yj, pj))\n",
    "        except: pass\n",
    "    macro_roc = float(np.mean(roc_list)) if roc_list else float(\"nan\")\n",
    "    macro_pr  = float(np.mean(pr_list))  if pr_list else float(\"nan\")\n",
    "    return {\"macro_roc_auc\": macro_roc, \"macro_pr_auc\": macro_pr}\n",
    "\n",
    "class MovingStat:\n",
    "    def __init__(self, k:int=3): self.k=k; self.buf=[]\n",
    "    def update(self, v: float)->float:\n",
    "        self.buf.append(v); \n",
    "        if len(self.buf)>self.k: self.buf.pop(0)\n",
    "        return sum(self.buf)/len(self.buf)\n",
    "    def value(self)->float: return (sum(self.buf)/len(self.buf)) if self.buf else float(\"nan\")\n",
    "\n",
    "def current_lr(optim: torch.optim.Optimizer)->float:\n",
    "    return float(optim.param_groups[0][\"lr\"])\n",
    "\n",
    "# -----------------------\n",
    "# Load best Stage-A weights\n",
    "# -----------------------\n",
    "def load_state_dict_robust(path: Path) -> dict:\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\")  # torch 2.6 default weights_only=True\n",
    "    except pickle.UnpicklingError:\n",
    "        try:\n",
    "            from torch.serialization import add_safe_globals\n",
    "            from torch.torch_version import TorchVersion\n",
    "            add_safe_globals([TorchVersion])\n",
    "        except Exception:\n",
    "            pass\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except Exception:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    if isinstance(obj, dict):\n",
    "        if \"model\" in obj and isinstance(obj[\"model\"], dict): return obj[\"model\"]\n",
    "        if \"state_dict\" in obj and isinstance(obj[\"state_dict\"], dict): return obj[\"state_dict\"]\n",
    "        return obj\n",
    "    raise RuntimeError(\"Could not extract a state_dict from checkpoint.\")\n",
    "\n",
    "bestA = RESULTS_DIR / f\"{RUN_BASENAME}_stageA_best.json\"\n",
    "if bestA.exists():\n",
    "    bestA_path = Path(json.loads(bestA.read_text())[\"best_ckpt\"])\n",
    "else:\n",
    "    cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_BASENAME}_stageA_best.pt\"))\n",
    "    assert len(cands), \"No Stage-A checkpoint found.\"\n",
    "    bestA_path = cands[-1]\n",
    "print(f\"[INFO] Stage B init from: {bestA_path.as_posix()}\")\n",
    "\n",
    "# If a model already exists, reuse and load; else instantiate\n",
    "if 'model' not in globals():\n",
    "    model = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "sdA = load_state_dict_robust(bestA_path)\n",
    "missing, unexpected = model.load_state_dict(sdA, strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] load_state_dict (Stage A → B): missing={missing}, unexpected={unexpected}\")\n",
    "\n",
    "# -----------------------\n",
    "# Unfreeze last 2 transformer layers (LLRD)\n",
    "# -----------------------\n",
    "# Freeze everything first, then unfreeze selective\n",
    "model.text.freeze_backbone()\n",
    "model.text.unfreeze_last_n_layers(n=2)  # sets requires_grad on last-2 layers\n",
    "\n",
    "# Build param groups with LLRD + no weight decay on bias/LayerNorm\n",
    "def is_bias_or_ln(n: str, p: torch.nn.Parameter) -> bool:\n",
    "    return (n.endswith(\".bias\")) or (\"LayerNorm.weight\" in n) or (\".ln\" in n) or (\".layer_norm\" in n)\n",
    "\n",
    "def name_in_module(module: nn.Module, name: str) -> bool:\n",
    "    try:\n",
    "        module.get_submodule(name)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Identify encoder layers for LLRD\n",
    "enc = getattr(getattr(model.text.backbone, \"roberta\", model.text.backbone), \"encoder\", None)\n",
    "assert enc is not None and hasattr(enc, \"layer\"), \"Could not locate transformer encoder layers.\"\n",
    "num_layers = len(enc.layer)\n",
    "last_idx = num_layers - 1\n",
    "prev_idx = num_layers - 2\n",
    "\n",
    "pgs = []\n",
    "decay_txt = WEIGHT_DECAY_TXT\n",
    "decay_oth = WEIGHT_DECAY_OTH\n",
    "\n",
    "# Text last-2 layers (LLRD)\n",
    "for idx, lr in [(prev_idx, LR_TEXT_LN1), (last_idx, LR_TEXT_LN2)]:\n",
    "    layer = enc.layer[idx]\n",
    "    for n, p in layer.named_parameters():\n",
    "        if not p.requires_grad: \n",
    "            p.requires_grad = True\n",
    "        full_name = f\"text.backbone.encoder.layer.{idx}.{n}\" if hasattr(model.text.backbone, \"encoder\") else f\"text.backbone.roberta.encoder.layer.{idx}.{n}\"\n",
    "        wd = 0.0 if is_bias_or_ln(n, p) else decay_txt\n",
    "        pgs.append({\"params\": [p], \"lr\": lr, \"weight_decay\": wd})\n",
    "\n",
    "# Text projection head (our Linear)\n",
    "for n, p in model.text.proj.named_parameters():\n",
    "    if p.requires_grad:\n",
    "        wd = 0.0 if is_bias_or_ln(n, p) else decay_txt\n",
    "        pgs.append({\"params\": [p], \"lr\": LR_TEXT_PROJ, \"weight_decay\": wd})\n",
    "\n",
    "# Graph / Descriptor / Fusion / Head\n",
    "def add_module_params(mod: nn.Module, base_lr: float, wd: float):\n",
    "    for n, p in mod.named_parameters():\n",
    "        if not p.requires_grad: p.requires_grad = True\n",
    "        wdec = 0.0 if is_bias_or_ln(n, p) else wd\n",
    "        pgs.append({\"params\": [p], \"lr\": base_lr, \"weight_decay\": wdec})\n",
    "\n",
    "add_module_params(model.graph, LR_OTHERS, decay_oth)\n",
    "add_module_params(model.desc,  LR_OTHERS, decay_oth)\n",
    "add_module_params(model.fusion,LR_OTHERS, decay_oth)\n",
    "add_module_params(model.head,  LR_OTHERS, decay_oth)\n",
    "\n",
    "# Optimizer & schedules\n",
    "opt = torch.optim.AdamW(pgs)\n",
    "\n",
    "steps_per_epoch = max(1, len(train_loader))\n",
    "total_steps = EPOCHS_STAGE_B * steps_per_epoch\n",
    "warmup_steps = max(1, int(WARMUP_FRAC_B * total_steps))\n",
    "\n",
    "def lr_lambda(step):\n",
    "    if step < warmup_steps:\n",
    "        return float(step) / float(max(1, warmup_steps))\n",
    "    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "cosine = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    opt, mode=\"max\", factor=PLATEAU_FACTOR, patience=PLATEAU_PAT,\n",
    "    threshold=1e-4, threshold_mode=\"rel\", cooldown=PLATEAU_COOLDOWN,\n",
    "    min_lr=PLATEAU_MIN_LR, verbose=False\n",
    ")\n",
    "\n",
    "# EMA\n",
    "class EMA:\n",
    "    def __init__(self, module: nn.Module, decay: float = 0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k, v in module.state_dict().items() if v.dtype.is_floating_point}\n",
    "    @torch.no_grad()\n",
    "    def update(self, module: nn.Module):\n",
    "        for k, v in module.state_dict().items():\n",
    "            if k in self.shadow and v.dtype.is_floating_point:\n",
    "                self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0 - self.decay)\n",
    "    @torch.no_grad()\n",
    "    def apply_to(self, module: nn.Module):\n",
    "        self.backup = {}\n",
    "        for k, v in module.state_dict().items():\n",
    "            if k in self.shadow and v.dtype.is_floating_point:\n",
    "                self.backup[k] = v.detach().clone()\n",
    "                v.copy_(self.shadow[k])\n",
    "    @torch.no_grad()\n",
    "    def restore(self, module: nn.Module):\n",
    "        for k, buf in self.backup.items():\n",
    "            module.state_dict()[k].copy_(buf)\n",
    "        self.backup = {}\n",
    "\n",
    "ema = EMA(model, EMA_DECAY) if USE_EMA else None\n",
    "\n",
    "# --------- Train / Eval ----------\n",
    "def train_one_epoch_b(epoch: int) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    t0 = time.time()\n",
    "    total_loss = total_bce = total_nce = total_l1 = 0.0\n",
    "    total_count = 0\n",
    "    gstep = epoch * steps_per_epoch\n",
    "\n",
    "    for it, batch in enumerate(train_loader):\n",
    "        labels = batch.labels.to(DEVICE)\n",
    "        label_mask = batch.label_mask.to(DEVICE)\n",
    "\n",
    "        with autocast(\"cuda\", enabled=USE_AMP, dtype=AMP_DTYPE):\n",
    "            out = model(batch, compute_aux=True)\n",
    "            logits_raw = out[\"logits\"]\n",
    "\n",
    "        loss_bce = bce_stable(model, logits_raw, labels, label_mask)\n",
    "        loss_nce = model.w_nce * out.get(\"loss_nce\", torch.tensor(0.0, device=logits_raw.device))\n",
    "        loss_l1  = model.w_l1  * out.get(\"loss_l1\",  torch.tensor(0.0, device=logits_raw.device))\n",
    "        loss = loss_bce + loss_nce + loss_l1\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), MAX_NORM)\n",
    "        opt.step()\n",
    "        cosine.step()\n",
    "\n",
    "        if ema is not None:\n",
    "            ema.update(model)\n",
    "\n",
    "        total_loss += float(loss.item())\n",
    "        total_bce  += float(loss_bce.item())\n",
    "        total_nce  += float(loss_nce.item())\n",
    "        total_l1   += float(loss_l1.item())\n",
    "        total_count += 1\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    return {\n",
    "        \"loss\": total_loss / max(1, total_count),\n",
    "        \"bce\":  total_bce  / max(1, total_count),\n",
    "        \"nce\":  total_nce  / max(1, total_count),\n",
    "        \"l1\":   total_l1   / max(1, total_count),\n",
    "        \"sec\":  dt,\n",
    "        \"lr0\":  float(opt.param_groups[0][\"lr\"])\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_b(use_ema: bool = True) -> Dict[str, float]:\n",
    "    # Optionally swap EMA weights in for eval\n",
    "    if ema is not None and use_ema:\n",
    "        ema.apply_to(model)\n",
    "    model.eval()\n",
    "    all_logits, all_labels, all_mask = [], [], []\n",
    "    for batch in val_loader:\n",
    "        out = model(batch, compute_aux=False)\n",
    "        all_logits.append(out[\"logits\"].float().cpu())\n",
    "        all_labels.append(batch.labels.cpu())\n",
    "        all_mask.append(batch.label_mask.cpu())\n",
    "    logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    mask   = torch.cat(all_mask,   dim=0).numpy().astype(np.bool_)\n",
    "    # BCE preview\n",
    "    idx = np.random.default_rng(GLOBAL_SEED).choice(len(labels), size=min(256, len(labels)), replace=False)\n",
    "    logits_b = torch.tensor(np.clip(logits[idx], CLAMP_MIN, CLAMP_MAX), dtype=torch.float32, device=DEVICE)\n",
    "    labels_b = torch.tensor(labels[idx], dtype=torch.float32, device=DEVICE)\n",
    "    mask_b   = torch.tensor(mask[idx], dtype=torch.bool, device=DEVICE)\n",
    "    bce_prev = float(model.crit(logits_b, labels_b.clamp_(0,1), mask_b).item())\n",
    "    mets = compute_metrics(logits, labels, mask)\n",
    "    mets[\"bce_preview\"] = bce_prev\n",
    "    mets[\"using_ema\"] = bool(ema is not None and use_ema)\n",
    "    if ema is not None and use_ema:\n",
    "        ema.restore(model)\n",
    "    return mets\n",
    "\n",
    "# --------- Orchestrate Stage B ----------\n",
    "if EPOCH_LOG_B.exists(): EPOCH_LOG_B.unlink()\n",
    "epoch_f = open(EPOCH_LOG_B, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "best_val = -1.0\n",
    "best_epoch = -1\n",
    "mov = MovingStat(k=3)\n",
    "no_improve = 0\n",
    "start = time.time()\n",
    "\n",
    "# Print a brief of param groups\n",
    "print(\"\\n=== Stage B: LLRD unfreeze last-2 layers + EMA ===\")\n",
    "print(f\"Text LRs last-2: {LR_TEXT_LN1} (L-1), {LR_TEXT_LN2} (L) | Proj: {LR_TEXT_PROJ} | Others: {LR_OTHERS}\")\n",
    "print(f\"AMP={USE_AMP} ({AMP_DTYPE})  |  EMA={USE_EMA} (decay={EMA_DECAY})\")\n",
    "\n",
    "for epoch in range(EPOCHS_STAGE_B):\n",
    "    tr = train_one_epoch_b(epoch)\n",
    "    va = evaluate_b(use_ema=True)  # evaluate with EMA weights if enabled\n",
    "    ma = mov.update(va[\"macro_roc_auc\"])\n",
    "    plateau.step(0.0 if math.isnan(ma) else ma)\n",
    "\n",
    "    rec = {\"epoch\": epoch, \"train\": tr, \"val\": va, \"val_ma_roc\": ma,\n",
    "           \"lrs\": [g[\"lr\"] for g in opt.param_groups]}\n",
    "    epoch_f.write(json.dumps(rec) + \"\\n\"); epoch_f.flush()\n",
    "\n",
    "    print(f\"[E{epoch:02d} B] \"\n",
    "          f\"train_loss={tr['loss']:.4f} (bce={tr['bce']:.4f}, nce={tr['nce']:.4f}, l1={tr['l1']:.5f}) \"\n",
    "          f\"| val_roc={va['macro_roc_auc']:.4f} val_pr={va['macro_pr_auc']:.4f} \"\n",
    "          f\"| val_bceprev={va['bce_preview']:.4f} | ma_roc={ma:.4f} | lr0={tr['lr0']:.6f} | {tr['sec']:.1f}s \"\n",
    "          f\"| EMA={va['using_ema']}\")\n",
    "\n",
    "    score = ma if not math.isnan(ma) else va[\"macro_roc_auc\"]\n",
    "    if score > best_val:\n",
    "        best_val = score; best_epoch = epoch; no_improve = 0\n",
    "        ckpt_path = CKPT_DIR / f\"{RUN_BASENAME}_stageB_best.pt\"\n",
    "        # Save EMA weights if enabled, else current model weights\n",
    "        if ema is not None:\n",
    "            ema.apply_to(model)\n",
    "            torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": epoch, \"ema\": True}, ckpt_path)\n",
    "            ema.restore(model)\n",
    "        else:\n",
    "            torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": epoch, \"ema\": False}, ckpt_path)\n",
    "        print(f\"  ↳ Saved BEST Stage-B checkpoint: {ckpt_path.as_posix()} (ma_roc={best_val:.4f})\")\n",
    "    else:\n",
    "        no_improve += 1\n",
    "\n",
    "    if no_improve >= EARLYSTOP_PAT:\n",
    "        print(f\"[EARLY STOP] No MA-ROC improvement for {EARLYSTOP_PAT} epochs. Stopping at epoch {epoch}.\")\n",
    "        break\n",
    "\n",
    "# Save last\n",
    "last_ckpt = CKPT_DIR / f\"{RUN_BASENAME}_stageB_last.pt\"\n",
    "if ema is not None:\n",
    "    ema.apply_to(model)\n",
    "    torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": epoch, \"ema\": True}, last_ckpt)\n",
    "    ema.restore(model)\n",
    "else:\n",
    "    torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": epoch, \"ema\": False}, last_ckpt)\n",
    "epoch_f.close()\n",
    "\n",
    "elapsed = time.time() - start\n",
    "BEST_SUMMARY_B.write_text(json.dumps({\n",
    "    \"best_epoch\": best_epoch,\n",
    "    \"best_val_ma_roc\": best_val,\n",
    "    \"best_ckpt\": str((CKPT_DIR / f\"{RUN_BASENAME}_stageB_best.pt\").resolve()),\n",
    "    \"epochs_planned\": EPOCHS_STAGE_B,\n",
    "    \"epochs_ran\": epoch + 1,\n",
    "    \"ema_used\": USE_EMA,\n",
    "    \"amp_forward\": USE_AMP,\n",
    "    \"elapsed_sec\": round(elapsed, 1)\n",
    "}, indent=2))\n",
    "\n",
    "LAST_SUMMARY_B.write_text(json.dumps({\n",
    "    \"epoch_last\": epoch,\n",
    "    \"last_ckpt\": str(last_ckpt.resolve()),\n",
    "    \"lr_last\": [g[\"lr\"] for g in opt.param_groups],\n",
    "    \"elapsed_sec\": round(elapsed, 1)\n",
    "}, indent=2))\n",
    "\n",
    "print(\"\\n----- TRAINING SUMMARY (Stage B) -----\")\n",
    "print(f\"Best epoch: {best_epoch} | best moving-avg ROC-AUC: {best_val:.4f}\")\n",
    "print(f\"Best ckpt:  {(CKPT_DIR / f'{RUN_BASENAME}_stageB_best.pt').as_posix()}\")\n",
    "print(f\"Last ckpt:  {last_ckpt.as_posix()}\")\n",
    "print(f\"Epoch log:  {EPOCH_LOG_B.as_posix()}\")\n",
    "print(f\"Summaries:  {BEST_SUMMARY_B.as_posix()} | {LAST_SUMMARY_B.as_posix()}\")\n",
    "print(\"--------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362eb714",
   "metadata": {},
   "source": [
    "### 8b) eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45af0a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Stage-B checkpoint: D:/Coding Projects/Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI/tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_stageB_best.pt\n",
      "[INFO] EMA flag in checkpoint: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- STAGE B EVALUATION -----\n",
      "EMA in ckpt          : True\n",
      "Val  macro ROC / PR  : 0.7701 / 0.2724\n",
      "Test macro ROC / PR  : 0.7745 / 0.2671\n",
      "Test macro F1        : 0.2848\n",
      "Saved JSONs: v4_20250902_165736_eval_val.json, v4_20250902_165736_eval_test.json, thresholds → v4_20250902_165736_thresholds.json\n",
      "Per-label CSVs: v4_20250902_165736_val_perlabel.csv, v4_20250902_165736_test_perlabel.csv, v4_20250902_165736_test_cls_perlabel.csv\n",
      "Summary JSON: v4_20250902_165736_summary.json in tox21_dualenc_v1/results/v4/eval/stage_B\n",
      "--------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, math, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Optional metrics\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score, f1_score, precision_recall_curve\n",
    "    SKLEARN_OK = True\n",
    "except Exception as e:\n",
    "    SKLEARN_OK = False\n",
    "    print(f\"[WARN] sklearn not available ({e}). Metrics will be limited.\")\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Save location ---\n",
    "EVAL_DIR = Path(\"tox21_dualenc_v1/results/v4/eval/stage_B\")\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Locate best Stage-B checkpoint (prefer summary, fallback to glob) ---\n",
    "best_summary_b = RESULTS_DIR / f\"{RUN_ID}_stageB_best.json\"\n",
    "ckpt_path = None\n",
    "if best_summary_b.exists():\n",
    "    try:\n",
    "        ckpt_path = Path(json.loads(best_summary_b.read_text())[\"best_ckpt\"])\n",
    "    except Exception:\n",
    "        ckpt_path = None\n",
    "if ckpt_path is None or (not ckpt_path.exists()):\n",
    "    cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_ID}_stageB_best.pt\"))\n",
    "    if cands:\n",
    "        ckpt_path = cands[-1]\n",
    "    else:\n",
    "        cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_ID}_stageB_last.pt\"))\n",
    "        assert cands, \"No Stage-B checkpoint found.\"\n",
    "        ckpt_path = cands[-1]\n",
    "\n",
    "print(f\"[INFO] Using Stage-B checkpoint: {ckpt_path.as_posix()}\")\n",
    "\n",
    "# --- Robust loader for torch 2.6 ---\n",
    "def load_checkpoint_robust(path: Path) -> dict:\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\")  # torch 2.6 default weights_only=True\n",
    "    except pickle.UnpicklingError:\n",
    "        try:\n",
    "            from torch.serialization import add_safe_globals\n",
    "            from torch.torch_version import TorchVersion\n",
    "            add_safe_globals([TorchVersion])\n",
    "        except Exception:\n",
    "            pass\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except Exception:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    assert isinstance(obj, dict), \"Unexpected checkpoint format.\"\n",
    "    return obj\n",
    "\n",
    "ckpt = load_checkpoint_robust(ckpt_path)\n",
    "sd = ckpt.get(\"model\", ckpt.get(\"state_dict\", ckpt))\n",
    "ema_flag = bool(ckpt.get(\"ema\", False))\n",
    "print(f\"[INFO] EMA flag in checkpoint: {ema_flag}\")\n",
    "\n",
    "# --- Rebuild model & load weights ---\n",
    "assert 'DualEncCoAttnModel' in globals(), \"Model class not in scope. Please re-run Cell 5.\"\n",
    "model_eval = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "missing, unexpected = model_eval.load_state_dict(sd, strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] State dict load: missing={missing}, unexpected={unexpected}\")\n",
    "model_eval.eval()\n",
    "\n",
    "# --- Helpers ---\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_logits(loader):\n",
    "    all_logits, all_labels, all_mask = [], [], []\n",
    "    for b in loader:\n",
    "        out = model_eval(b, compute_aux=False)\n",
    "        all_logits.append(out[\"logits\"].float().cpu())\n",
    "        all_labels.append(b.labels.cpu())\n",
    "        all_mask.append(b.label_mask.cpu())\n",
    "    logits = torch.cat(all_logits, dim=0).numpy()\n",
    "    labels = torch.cat(all_labels, dim=0).numpy()\n",
    "    mask   = torch.cat(all_mask,   dim=0).numpy().astype(bool)\n",
    "    return logits, labels, mask\n",
    "\n",
    "def per_label_metrics(logits, labels, mask, label_names):\n",
    "    probs = 1.0 / (1.0 + np.exp(-np.clip(logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "    rows = []\n",
    "    for j, name in enumerate(label_names):\n",
    "        m = mask[:, j]\n",
    "        y = labels[m, j]\n",
    "        p = probs[m, j]\n",
    "        roc = pr = float(\"nan\")\n",
    "        if SKLEARN_OK and y.size >= 3 and not (np.all(y==0) or np.all(y==1)):\n",
    "            try: roc = roc_auc_score(y, p)\n",
    "            except: pass\n",
    "            try: pr  = average_precision_score(y, p)\n",
    "            except: pass\n",
    "        rows.append({\n",
    "            \"label\": name,\n",
    "            \"roc_auc\": None if np.isnan(roc) else float(roc),\n",
    "            \"pr_auc\":  None if np.isnan(pr)  else float(pr),\n",
    "            \"pos_rate\": float(y.mean()) if y.size>0 else None,\n",
    "            \"n\": int(y.size)\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "def choose_thresholds_val(logits, labels, mask):\n",
    "    probs = 1.0 / (1.0 + np.exp(-np.clip(logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "    th = []\n",
    "    for j in range(labels.shape[1]):\n",
    "        m = mask[:, j]\n",
    "        y = labels[m, j]\n",
    "        p = probs[m, j]\n",
    "        if (not SKLEARN_OK) or y.size < 3 or np.all(y==0) or np.all(y==1):\n",
    "            th.append(0.5); continue\n",
    "        pr, rc, tt = precision_recall_curve(y, p)\n",
    "        if tt.size == 0: th.append(0.5)\n",
    "        else:\n",
    "            f1 = 2*pr*rc/(pr+rc+1e-12)\n",
    "            th.append(float(tt[np.argmax(f1)]))\n",
    "    return np.asarray(th, dtype=np.float32)\n",
    "\n",
    "def apply_thresholds(logits, thresholds):\n",
    "    probs = 1.0 / (1.0 + np.exp(-np.clip(logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "    return (probs >= thresholds[None, :]).astype(np.float32)\n",
    "\n",
    "def per_label_cls_metrics(y_true, y_pred, mask, label_names):\n",
    "    rows = []\n",
    "    for j, name in enumerate(label_names):\n",
    "        m = mask[:, j]\n",
    "        y = y_true[m, j]\n",
    "        yhat = y_pred[m, j]\n",
    "        if y.size == 0:\n",
    "            rows.append({\"label\": name, \"f1\": None, \"precision\": None, \"recall\": None, \"n\": 0})\n",
    "            continue\n",
    "        if SKLEARN_OK:\n",
    "            f1 = f1_score(y, yhat, zero_division=0)\n",
    "        else:\n",
    "            tp = ((y==1)&(yhat==1)).sum(); fp = ((y==0)&(yhat==1)).sum(); fn = ((y==1)&(yhat==0)).sum()\n",
    "            prec = tp/(tp+fp+1e-12); rec  = tp/(tp+fn+1e-12)\n",
    "            f1 = 2*prec*rec/(prec+rec+1e-12)\n",
    "        tp = ((y==1)&(yhat==1)).sum(); fp = ((y==0)&(yhat==1)).sum(); fn = ((y==1)&(yhat==0)).sum()\n",
    "        prec = float(tp/(tp+fp+1e-12)); rec  = float(tp/(tp+fn+1e-12))\n",
    "        rows.append({\"label\": name, \"f1\": float(f1), \"precision\": prec, \"recall\": rec, \"n\": int(y.size)})\n",
    "    return rows\n",
    "\n",
    "def macro_avg(rows, key):\n",
    "    vals = [r[key] for r in rows if (r[key] is not None) and not (isinstance(r[key], float) and (math.isnan(r[key]) or math.isinf(r[key])))]\n",
    "    return float(np.mean(vals)) if vals else float(\"nan\")\n",
    "\n",
    "# --- Collect logits (val & test) ---\n",
    "assert 'val_loader' in globals() and 'test_loader' in globals(), \"Missing dataloaders. Please re-run Cell 4.\"\n",
    "val_logits,  val_labels,  val_mask  = collect_logits(val_loader)\n",
    "test_logits, test_labels, test_mask = collect_logits(test_loader)\n",
    "\n",
    "# --- Metrics (per-label) ---\n",
    "val_rows  = per_label_metrics(val_logits,  val_labels,  val_mask,  LABELS)\n",
    "test_rows = per_label_metrics(test_logits, test_labels, test_mask, LABELS)\n",
    "\n",
    "# --- Thresholds on val (max-F1), apply to test ---\n",
    "thresholds = choose_thresholds_val(val_logits, val_labels, val_mask)\n",
    "test_pred  = apply_thresholds(test_logits, thresholds)\n",
    "test_cls_rows = per_label_cls_metrics(test_labels, test_pred, test_mask, LABELS)\n",
    "\n",
    "# --- Macro summaries ---\n",
    "summary = {\n",
    "    \"ema_in_ckpt\": ema_flag,\n",
    "    \"val_macro_roc\":  macro_avg(val_rows,  \"roc_auc\"),\n",
    "    \"val_macro_pr\":   macro_avg(val_rows,  \"pr_auc\"),\n",
    "    \"test_macro_roc\": macro_avg(test_rows, \"roc_auc\"),\n",
    "    \"test_macro_pr\":  macro_avg(test_rows, \"pr_auc\"),\n",
    "    \"test_macro_f1\":  macro_avg(test_cls_rows, \"f1\"),\n",
    "}\n",
    "\n",
    "# --- Save artifacts to eval/stage_B ---\n",
    "import pandas as pd\n",
    "val_csv      = EVAL_DIR / f\"{RUN_ID}_val_perlabel.csv\"\n",
    "test_csv     = EVAL_DIR / f\"{RUN_ID}_test_perlabel.csv\"\n",
    "test_cls_csv = EVAL_DIR / f\"{RUN_ID}_test_cls_perlabel.csv\"\n",
    "pd.DataFrame(val_rows).to_csv(val_csv, index=False)\n",
    "pd.DataFrame(test_rows).to_csv(test_csv, index=False)\n",
    "pd.DataFrame(test_cls_rows).to_csv(test_cls_csv, index=False)\n",
    "\n",
    "val_json  = EVAL_DIR / f\"{RUN_ID}_eval_val.json\"\n",
    "test_json = EVAL_DIR / f\"{RUN_ID}_eval_test.json\"\n",
    "th_json   = EVAL_DIR / f\"{RUN_ID}_thresholds.json\"\n",
    "sum_json  = EVAL_DIR / f\"{RUN_ID}_summary.json\"\n",
    "\n",
    "val_json.write_text(json.dumps({\"per_label\": val_rows, \"summary\": summary}, indent=2))\n",
    "test_json.write_text(json.dumps({\"per_label\": test_rows, \"summary\": summary}, indent=2))\n",
    "th_json.write_text(json.dumps({\"thresholds\": thresholds.tolist(), \"labels\": LABELS}, indent=2))\n",
    "sum_json.write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"\\n----- STAGE B EVALUATION -----\")\n",
    "print(f\"EMA in ckpt          : {summary['ema_in_ckpt']}\")\n",
    "print(f\"Val  macro ROC / PR  : {summary['val_macro_roc']:.4f} / {summary['val_macro_pr']:.4f}\")\n",
    "print(f\"Test macro ROC / PR  : {summary['test_macro_roc']:.4f} / {summary['test_macro_pr']:.4f}\")\n",
    "print(f\"Test macro F1        : {summary['test_macro_f1']:.4f}\")\n",
    "print(f\"Saved JSONs: {val_json.name}, {test_json.name}, thresholds → {th_json.name}\")\n",
    "print(f\"Per-label CSVs: {val_csv.name}, {test_csv.name}, {test_cls_csv.name}\")\n",
    "print(f\"Summary JSON: {sum_json.name} in {EVAL_DIR.as_posix()}\")\n",
    "print(\"--------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8d5ea3",
   "metadata": {},
   "source": [
    "## 9: Stage-B fine-tuning knobs (resume + short run + eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188d9d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage-B tuner (b_tune1) ===\n",
      "Start: stageB_best | LRs text (L-1/L): 1.5e-05/3e-05 | proj: 0.0001 | others: 0.0002 | EMA=True(0.9995)\n",
      "[DEBUG b_tune1 e0 i0] raw logits min/max: -8.500/4.062 | clamp≈0.000\n",
      "[b_tune1 E00] train_loss=1.0066 (bce=0.5965, nce=0.4102, l1=0.00000) | val_roc=0.7698 val_pr=0.2734 | val_bceprev=1.2004\n",
      "  ↳ Saved BEST (b_tune1) → tox21_dualenc_v1/models/checkpoints_v4/v4_20250902_165736_b_tune1_best.pt (val_ma_roc=0.7698)\n",
      "[b_tune1 E01] train_loss=0.9038 (bce=0.5887, nce=0.3151, l1=0.00000) | val_roc=0.7691 val_pr=0.2750 | val_bceprev=1.2077\n",
      "[b_tune1 E02] train_loss=0.7782 (bce=0.5389, nce=0.2393, l1=0.00000) | val_roc=0.7683 val_pr=0.2768 | val_bceprev=1.2264\n",
      "[b_tune1 E03] train_loss=0.7068 (bce=0.4995, nce=0.2073, l1=0.00000) | val_roc=0.7671 val_pr=0.2771 | val_bceprev=1.2531\n",
      "[b_tune1 E04] train_loss=0.6286 (bce=0.4510, nce=0.1776, l1=0.00000) | val_roc=0.7663 val_pr=0.2765 | val_bceprev=1.2860\n",
      "[b_tune1 E05] train_loss=0.5732 (bce=0.4185, nce=0.1547, l1=0.00000) | val_roc=0.7651 val_pr=0.2768 | val_bceprev=1.3240\n",
      "[b_tune1 E06] train_loss=0.5477 (bce=0.3957, nce=0.1519, l1=0.00000) | val_roc=0.7641 val_pr=0.2768 | val_bceprev=1.3650\n",
      "[EARLY STOP b_tune1] no improvement for 6 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- b_tune1 SUMMARY ---\n",
      "Val best ROC: 0.7698 | Test ROC/PR/F1: 0.7745/0.2663/0.2899\n",
      "Saved: v4_20250902_165736_b_tune1_best.pt, v4_20250902_165736_b_tune1_last.pt, and summary JSON in tox21_dualenc_v1/results/v4/eval/stage_B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, math, pickle, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast\n",
    "\n",
    "# -------- Knobs you can tweak --------\n",
    "KNOBS = {\n",
    "    \"tag\": \"b_tune1\",          # used in filenames\n",
    "    \"epochs\": 8,               # short extension\n",
    "    \"warmup_frac\": 0.10,\n",
    "    # LLRD learning rates\n",
    "    \"lr_text_prev\": 1.5e-5,    # last-2 (L-1)\n",
    "    \"lr_text_last\": 3.0e-5,    # last (L)\n",
    "    \"lr_text_proj\": 1.0e-4,    # text projection\n",
    "    \"lr_others\":    2.0e-4,    # graph/desc/fusion/head\n",
    "    # Regularization / stability\n",
    "    \"weight_decay_txt\": 0.01,\n",
    "    \"weight_decay_oth\": 1e-4,\n",
    "    \"clamp_min\": -30.0,\n",
    "    \"clamp_max\":  30.0,\n",
    "    # Loss weights (if your model uses them)\n",
    "    \"w_nce\": 1.0,              # keep as in Stage B unless you want to ablate\n",
    "    \"w_l1\":  1e-4,             # mild sparsity on attention (set 0.0 to disable)\n",
    "    # EMA\n",
    "    \"use_ema\": True,\n",
    "    \"ema_decay\": 0.9995,\n",
    "    # Early stopping / schedule\n",
    "    \"earlystop_pat\": 6,\n",
    "    \"plateau_pat\":  3,\n",
    "    \"plateau_factor\": 0.5,\n",
    "    \"plateau_min_lr\": 1e-6,\n",
    "    \"plateau_cooldown\": 1,\n",
    "    # Start point\n",
    "    \"start_from\": \"stageB_best\",  # \"stageB_best\" | \"stageA_best\"\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP = DEVICE.type == \"cuda\"\n",
    "AMP_DTYPE = (torch.bfloat16 if (AMP and torch.cuda.is_bf16_supported())\n",
    "             else (torch.float16 if AMP else torch.float32))\n",
    "\n",
    "# ---- Paths & ckpt selection ----\n",
    "def _pick_ckpt(kind: str) -> Path:\n",
    "    if kind == \"stageB_best\":\n",
    "        summ = RESULTS_DIR / f\"{RUN_ID}_stageB_best.json\"\n",
    "        if summ.exists():\n",
    "            return Path(json.loads(summ.read_text())[\"best_ckpt\"])\n",
    "        cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_ID}_stageB_best.pt\"))\n",
    "        if cands: return cands[-1]\n",
    "    elif kind == \"stageA_best\":\n",
    "        summ = RESULTS_DIR / f\"{RUN_ID}_stageA_best.json\"\n",
    "        if summ.exists():\n",
    "            return Path(json.loads(summ.read_text())[\"best_ckpt\"])\n",
    "        cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_ID}_stageA_best.pt\"))\n",
    "        if cands: return cands[-1]\n",
    "    raise FileNotFoundError(f\"No checkpoint found for {kind}\")\n",
    "\n",
    "def _torch_load_robust(path: Path) -> dict:\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\")\n",
    "    except pickle.UnpicklingError:\n",
    "        from torch.serialization import add_safe_globals\n",
    "        from torch.torch_version import TorchVersion\n",
    "        try: add_safe_globals([TorchVersion])\n",
    "        except Exception: pass\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except Exception:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    assert isinstance(obj, dict), \"Unexpected checkpoint format.\"\n",
    "    return obj\n",
    "\n",
    "# ---- Build model & param groups (LLRD) ----\n",
    "def build_model_and_opt(knobs: Dict[str, Any]):\n",
    "    # fresh model\n",
    "    mdl = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "    # load seed weights\n",
    "    seed_ckpt = _pick_ckpt(knobs[\"start_from\"])\n",
    "    ck = _torch_load_robust(seed_ckpt)\n",
    "    sd = ck.get(\"model\", ck.get(\"state_dict\", ck))\n",
    "    mdl.load_state_dict(sd, strict=False)\n",
    "\n",
    "    # make sure loss weights inside model match knobs (if exposed)\n",
    "    if hasattr(mdl, \"w_nce\"): mdl.w_nce = float(knobs[\"w_nce\"])\n",
    "    if hasattr(mdl, \"w_l1\"):  mdl.w_l1  = float(knobs[\"w_l1\"])\n",
    "\n",
    "    # (re)freeze all text, then unfreeze last-2\n",
    "    mdl.text.freeze_backbone()\n",
    "    mdl.text.unfreeze_last_n_layers(n=2)\n",
    "\n",
    "    # gather layers\n",
    "    enc = getattr(getattr(mdl.text.backbone, \"roberta\", mdl.text.backbone), \"encoder\", None)\n",
    "    assert enc is not None and hasattr(enc, \"layer\")\n",
    "    L = len(enc.layer)\n",
    "    last_idx, prev_idx = L-1, L-2\n",
    "\n",
    "    def is_bias_or_ln(n: str) -> bool:\n",
    "        return n.endswith(\".bias\") or (\"LayerNorm.weight\" in n) or (\".ln\" in n) or (\".layer_norm\" in n)\n",
    "\n",
    "    pgs = []\n",
    "    # last-2 layers with decay rules\n",
    "    for idx, lr in [(prev_idx, knobs[\"lr_text_prev\"]), (last_idx, knobs[\"lr_text_last\"])]:\n",
    "        for n, p in enc.layer[idx].named_parameters():\n",
    "            if not p.requires_grad: p.requires_grad = True\n",
    "            wd = 0.0 if is_bias_or_ln(n) else knobs[\"weight_decay_txt\"]\n",
    "            pgs.append({\"params\":[p], \"lr\": lr, \"weight_decay\": wd})\n",
    "    # text projection\n",
    "    for n, p in mdl.text.proj.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            wd = 0.0 if is_bias_or_ln(n) else knobs[\"weight_decay_txt\"]\n",
    "            pgs.append({\"params\":[p], \"lr\": knobs[\"lr_text_proj\"], \"weight_decay\": wd})\n",
    "\n",
    "    # others (graph/desc/fusion/head)\n",
    "    def add_mod(mod, base_lr, wd):\n",
    "        for n, p in mod.named_parameters():\n",
    "            if not p.requires_grad: p.requires_grad = True\n",
    "            wdec = 0.0 if is_bias_or_ln(n) else wd\n",
    "            pgs.append({\"params\":[p], \"lr\": base_lr, \"weight_decay\": wdec})\n",
    "    add_mod(mdl.graph,  knobs[\"lr_others\"], knobs[\"weight_decay_oth\"])\n",
    "    add_mod(mdl.desc,   knobs[\"lr_others\"], knobs[\"weight_decay_oth\"])\n",
    "    add_mod(mdl.fusion, knobs[\"lr_others\"], knobs[\"weight_decay_oth\"])\n",
    "    add_mod(mdl.head,   knobs[\"lr_others\"], knobs[\"weight_decay_oth\"])\n",
    "\n",
    "    opt = torch.optim.AdamW(pgs)\n",
    "    return mdl, opt\n",
    "\n",
    "# ---- Scheduler, EMA, metrics, loss ----\n",
    "def build_schedulers(opt, steps_per_epoch, knobs):\n",
    "    total = knobs[\"epochs\"] * steps_per_epoch\n",
    "    warm = max(1, int(knobs[\"warmup_frac\"] * total))\n",
    "    def lr_lambda(step):\n",
    "        if step < warm: return float(step)/float(max(1, warm))\n",
    "        prog = float(step - warm)/float(max(1, total - warm))\n",
    "        return 0.5*(1.0 + math.cos(math.pi * prog))\n",
    "    cosine = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "    plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"max\", factor=knobs[\"plateau_factor\"], patience=knobs[\"plateau_pat\"],\n",
    "        threshold=1e-4, threshold_mode=\"rel\", cooldown=knobs[\"plateau_cooldown\"],\n",
    "        min_lr=knobs[\"plateau_min_lr\"], verbose=False\n",
    "    )\n",
    "    return cosine, plateau\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, module: nn.Module, decay: float):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k,v in module.state_dict().items() if v.dtype.is_floating_point}\n",
    "        self.backup = {}\n",
    "    @torch.no_grad()\n",
    "    def update(self, module):\n",
    "        for k, v in module.state_dict().items():\n",
    "            if k in self.shadow and v.dtype.is_floating_point:\n",
    "                self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0-self.decay)\n",
    "    @torch.no_grad()\n",
    "    def apply_to(self, module):\n",
    "        self.backup = {}\n",
    "        for k, v in module.state_dict().items():\n",
    "            if k in self.shadow and v.dtype.is_floating_point:\n",
    "                self.backup[k] = v.detach().clone()\n",
    "                v.copy_(self.shadow[k])\n",
    "    @torch.no_grad()\n",
    "    def restore(self, module):\n",
    "        for k, buf in self.backup.items():\n",
    "            module.state_dict()[k].copy_(buf)\n",
    "        self.backup = {}\n",
    "\n",
    "CL_MIN, CL_MAX = KNOBS[\"clamp_min\"], KNOBS[\"clamp_max\"]\n",
    "\n",
    "def bce_stable(model, logits_raw, labels, mask):\n",
    "    return model.crit(logits_raw.float().clamp_(CL_MIN, CL_MAX),\n",
    "                      labels.float().clamp_(0,1),\n",
    "                      mask)\n",
    "\n",
    "def compute_metrics_np(logits, labels, mask):\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    probs = 1.0/(1.0+np.exp(-np.clip(logits, CL_MIN, CL_MAX)))\n",
    "    roc_list, pr_list = [], []\n",
    "    for j in range(labels.shape[1]):\n",
    "        m = mask[:, j].astype(bool)\n",
    "        yj, pj = labels[m, j], probs[m, j]\n",
    "        if yj.size < 3 or (np.all(yj==0) or np.all(yj==1)): continue\n",
    "        try: roc_list.append(roc_auc_score(yj, pj))\n",
    "        except: pass\n",
    "        try: pr_list.append(average_precision_score(yj, pj))\n",
    "        except: pass\n",
    "    return float(np.mean(roc_list)) if roc_list else float(\"nan\"), \\\n",
    "           float(np.mean(pr_list))  if pr_list  else float(\"nan\")\n",
    "\n",
    "# ---- Train / Eval loops ----\n",
    "def evaluate(model, loader, use_ema=False, ema=None):\n",
    "    if use_ema and ema is not None: ema.apply_to(model)\n",
    "    model.eval()\n",
    "    all_logits, all_labels, all_mask = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            out = model(b, compute_aux=False)\n",
    "            all_logits.append(out[\"logits\"].float().cpu())\n",
    "            all_labels.append(b.labels.cpu())\n",
    "            all_mask.append(b.label_mask.cpu())\n",
    "    if use_ema and ema is not None: ema.restore(model)\n",
    "    logits = torch.cat(all_logits, 0).numpy()\n",
    "    labels = torch.cat(all_labels, 0).numpy()\n",
    "    mask   = torch.cat(all_mask,   0).numpy().astype(bool)\n",
    "    roc, pr = compute_metrics_np(logits, labels, mask)\n",
    "    # quick BCE preview on subset\n",
    "    idx = np.random.default_rng(GLOBAL_SEED).choice(len(labels), size=min(256, len(labels)), replace=False)\n",
    "    bce_prev = float(\n",
    "        model.crit(torch.tensor(np.clip(logits[idx], CL_MIN, CL_MAX), dtype=torch.float32, device=DEVICE),\n",
    "                   torch.tensor(labels[idx], dtype=torch.float32, device=DEVICE),\n",
    "                   torch.tensor(mask[idx], dtype=torch.bool, device=DEVICE)).item()\n",
    "    )\n",
    "    return {\"macro_roc_auc\": roc, \"macro_pr_auc\": pr, \"bce_preview\": bce_prev}\n",
    "\n",
    "def run_tune(knobs: Dict[str,Any]):\n",
    "    model, opt = build_model_and_opt(knobs)\n",
    "    steps_per_epoch = max(1, len(train_loader))\n",
    "    cosine, plateau = build_schedulers(opt, steps_per_epoch, knobs)\n",
    "    ema = EMA(model, knobs[\"ema_decay\"]) if knobs[\"use_ema\"] else None\n",
    "\n",
    "    tag = knobs[\"tag\"]\n",
    "    log_path = RESULTS_DIR / f\"{RUN_ID}_{tag}_log.jsonl\"\n",
    "    if log_path.exists(): log_path.unlink()\n",
    "    f = open(log_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    best = -1.0; best_epoch = -1\n",
    "    ckpt_best = CHECKPOINTS_DIR / f\"{RUN_ID}_{tag}_best.pt\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"\\n=== Stage-B tuner ({tag}) ===\")\n",
    "    print(f\"Start: {knobs['start_from']} | LRs text (L-1/L): {knobs['lr_text_prev']}/{knobs['lr_text_last']} | \"\n",
    "          f\"proj: {knobs['lr_text_proj']} | others: {knobs['lr_others']} | EMA={knobs['use_ema']}({knobs['ema_decay']})\")\n",
    "\n",
    "    for epoch in range(knobs[\"epochs\"]):\n",
    "        # --- train ---\n",
    "        model.train()\n",
    "        s = time.time()\n",
    "        tot, tb, tn, tl, cnt = 0.0, 0.0, 0.0, 0.0, 0\n",
    "        for it, b in enumerate(train_loader):\n",
    "            with autocast(\"cuda\", enabled=AMP, dtype=AMP_DTYPE):\n",
    "                out = model(b, compute_aux=True)\n",
    "                logits = out[\"logits\"]\n",
    "            loss_bce = bce_stable(model, logits, b.labels.to(DEVICE), b.label_mask.to(DEVICE))\n",
    "            loss_nce = model.w_nce * out.get(\"loss_nce\", torch.tensor(0.0, device=logits.device))\n",
    "            loss_l1  = model.w_l1  * out.get(\"loss_l1\",  torch.tensor(0.0, device=logits.device))\n",
    "            loss = loss_bce + loss_nce + loss_l1\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step(); cosine.step()\n",
    "            if ema is not None: ema.update(model)\n",
    "\n",
    "            tot += float(loss.item()); tb += float(loss_bce.item()); tn += float(loss_nce.item()); tl += float(loss_l1.item()); cnt += 1\n",
    "\n",
    "            if epoch==0 and it==0:\n",
    "                with torch.no_grad():\n",
    "                    clamp_frac = float(((logits < CL_MIN) | (logits > CL_MAX)).float().mean().item())\n",
    "                    print(f\"[DEBUG {tag} e0 i0] raw logits min/max: {logits.min().item():.3f}/{logits.max().item():.3f} | clamp≈{clamp_frac:.3f}\")\n",
    "\n",
    "        tr = {\"loss\": tot/max(1,cnt), \"bce\": tb/max(1,cnt), \"nce\": tn/max(1,cnt), \"l1\": tl/max(1,cnt), \"sec\": time.time()-s}\n",
    "\n",
    "        # --- val ---\n",
    "        va = evaluate(model, val_loader, use_ema=True, ema=ema)\n",
    "        plateau.step(0.0 if math.isnan(va[\"macro_roc_auc\"]) else va[\"macro_roc_auc\"])\n",
    "\n",
    "        rec = {\"epoch\": epoch, \"train\": tr, \"val\": va, \"lrs\": [g[\"lr\"] for g in opt.param_groups]}\n",
    "        f.write(json.dumps(rec)+\"\\n\"); f.flush()\n",
    "\n",
    "        print(f\"[{tag} E{epoch:02d}] train_loss={tr['loss']:.4f} (bce={tr['bce']:.4f}, nce={tr['nce']:.4f}, l1={tr['l1']:.5f}) \"\n",
    "              f\"| val_roc={va['macro_roc_auc']:.4f} val_pr={va['macro_pr_auc']:.4f} | val_bceprev={va['bce_preview']:.4f}\")\n",
    "\n",
    "        score = va[\"macro_roc_auc\"]\n",
    "        if score > best:\n",
    "            best = score; best_epoch = epoch\n",
    "            # save EMA weights if present\n",
    "            if ema is not None:\n",
    "                ema.apply_to(model); torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": epoch, \"ema\": True}, ckpt_best); ema.restore(model)\n",
    "            else:\n",
    "                torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": epoch, \"ema\": False}, ckpt_best)\n",
    "            print(f\"  ↳ Saved BEST ({tag}) → {ckpt_best.as_posix()} (val_ma_roc={best:.4f})\")\n",
    "\n",
    "        # basic early stop\n",
    "        if epoch - best_epoch >= KNOBS[\"earlystop_pat\"]:\n",
    "            print(f\"[EARLY STOP {tag}] no improvement for {KNOBS['earlystop_pat']} epochs.\")\n",
    "            break\n",
    "\n",
    "    f.close()\n",
    "    # final last ckpt\n",
    "    ckpt_last = CHECKPOINTS_DIR / f\"{RUN_ID}_{tag}_last.pt\"\n",
    "    if KNOBS[\"use_ema\"]:\n",
    "        ema.apply_to(model); torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": best_epoch, \"ema\": True}, ckpt_last); ema.restore(model)\n",
    "    else:\n",
    "        torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": best_epoch, \"ema\": False}, ckpt_last)\n",
    "\n",
    "    # quick eval on test for the best checkpoint of this tag (reuse Stage-B eval helpers)\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score, roc_auc_score, average_precision_score\n",
    "    ck = _torch_load_robust(ckpt_best); sd = ck.get(\"model\", ck.get(\"state_dict\", ck))\n",
    "    model_eval = DualEncCoAttnModel(CONFIG).to(DEVICE); model_eval.load_state_dict(sd, strict=False); model_eval.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def collect(loader):\n",
    "        L, Y, M = [], [], []\n",
    "        for b in loader:\n",
    "            o = model_eval(b, compute_aux=False)\n",
    "            L.append(o[\"logits\"].float().cpu())\n",
    "            Y.append(b.labels.cpu())\n",
    "            M.append(b.label_mask.cpu())\n",
    "        return torch.cat(L).numpy(), torch.cat(Y).numpy(), torch.cat(M).numpy().astype(bool)\n",
    "\n",
    "    val_logits, val_labels, val_mask = collect(val_loader)\n",
    "    test_logits, test_labels, test_mask = collect(test_loader)\n",
    "\n",
    "    # thresholds from val (max-F1)\n",
    "    probs_val = 1/(1+np.exp(-np.clip(val_logits, CL_MIN, CL_MAX)))\n",
    "    th = []\n",
    "    for j in range(val_labels.shape[1]):\n",
    "        m = val_mask[:, j]; y = val_labels[m, j]; p = probs_val[m, j]\n",
    "        if y.size < 3 or np.all(y==0) or np.all(y==1):\n",
    "            th.append(0.5); continue\n",
    "        pr, rc, tt = precision_recall_curve(y, p)\n",
    "        f1 = 2*pr*rc/(pr+rc+1e-12); th.append(float(tt[np.argmax(f1)]) if tt.size else 0.5)\n",
    "    th = np.asarray(th, np.float32)\n",
    "\n",
    "    # test metrics\n",
    "    probs_test = 1/(1+np.exp(-np.clip(test_logits, CL_MIN, CL_MAX)))\n",
    "    yhat = (probs_test >= th[None,:]).astype(np.float32)\n",
    "\n",
    "    def macro_auc(logits, labels, mask):\n",
    "        prob = 1/(1+np.exp(-np.clip(logits, CL_MIN, CL_MAX)))\n",
    "        r, p = [], []\n",
    "        for j in range(labels.shape[1]):\n",
    "            m = mask[:, j]; y = labels[m, j]; q = prob[m, j]\n",
    "            if y.size < 3 or np.all(y==0) or np.all(y==1): continue\n",
    "            try: r.append(roc_auc_score(y, q))\n",
    "            except: pass\n",
    "            try: p.append(average_precision_score(y, q))\n",
    "            except: pass\n",
    "        return float(np.mean(r)) if r else float(\"nan\"), float(np.mean(p)) if p else float(\"nan\")\n",
    "\n",
    "    test_roc, test_pr = macro_auc(test_logits, test_labels, test_mask)\n",
    "    f1s=[]\n",
    "    for j in range(test_labels.shape[1]):\n",
    "        m = test_mask[:, j]; y = test_labels[m, j]; yh = yhat[m, j]\n",
    "        if y.size==0: continue\n",
    "        f1s.append(f1_score(y, yh, zero_division=0))\n",
    "    test_f1 = float(np.mean(f1s)) if f1s else float(\"nan\")\n",
    "\n",
    "    # save run summary\n",
    "    tune_dir = RESULTS_DIR / \"eval\" / \"stage_B\"\n",
    "    tune_dir.mkdir(parents=True, exist_ok=True)\n",
    "    summ = {\n",
    "        \"tag\": tag, \"best_val_roc\": best, \"epochs\": knobs[\"epochs\"],\n",
    "        \"test_macro_roc\": test_roc, \"test_macro_pr\": test_pr, \"test_macro_f1\": test_f1,\n",
    "        \"ckpt_best\": ckpt_best.as_posix(), \"ckpt_last\": ckpt_last.as_posix(),\n",
    "        \"knobs\": knobs\n",
    "    }\n",
    "    (tune_dir / f\"{RUN_ID}_{tag}_summary.json\").write_text(json.dumps(summ, indent=2))\n",
    "    print(f\"\\n--- {tag} SUMMARY ---\")\n",
    "    print(f\"Val best ROC: {best:.4f} | Test ROC/PR/F1: {test_roc:.4f}/{test_pr:.4f}/{test_f1:.4f}\")\n",
    "    print(f\"Saved: {ckpt_best.name}, {ckpt_last.name}, and summary JSON in {tune_dir.as_posix()}\\n\")\n",
    "\n",
    "# ---- Run the tuner ----\n",
    "run_tune(KNOBS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bb76c6",
   "metadata": {},
   "source": [
    "## 10: Additional Tune to squeeze better results  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544964c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== pro_tune_ema_long (epochs=24) ===\n",
      "LLRD text LRs (L-1/L): 2e-05/5e-05 | proj=0.0001 | others=0.0002\n",
      "Scheduler=cosine (warmup=0.10) | EMA=True(0.9995) | SWA=False(start@19, lr=0.0001)\n",
      "[DEBUG pro_tune_ema_long e0 i0] logits min/max: -8.688/5.344 | clamp≈0.000\n",
      "[pro_tune_ema_long E00] train_loss=1.0150 (bce=0.5848, nce=0.4303, l1=0.00000) | val_roc=0.7701 val_pr=0.2726 | val_bceprev=1.2007\n",
      "  ↳ Saved BEST (pro_tune_ema_long) → v4_20250902_165736_pro_tune_ema_long_best.pt (val_roc=0.7701)\n",
      "[pro_tune_ema_long E01] train_loss=0.9316 (bce=0.5904, nce=0.3412, l1=0.00000) | val_roc=0.7699 val_pr=0.2740 | val_bceprev=1.2066\n",
      "[pro_tune_ema_long E02] train_loss=0.8371 (bce=0.5646, nce=0.2725, l1=0.00000) | val_roc=0.7689 val_pr=0.2762 | val_bceprev=1.2221\n",
      "[pro_tune_ema_long E03] train_loss=0.7698 (bce=0.5413, nce=0.2285, l1=0.00000) | val_roc=0.7679 val_pr=0.2787 | val_bceprev=1.2404\n",
      "[pro_tune_ema_long E04] train_loss=0.6988 (bce=0.5055, nce=0.1932, l1=0.00000) | val_roc=0.7673 val_pr=0.2790 | val_bceprev=1.2655\n",
      "[pro_tune_ema_long E05] train_loss=0.6403 (bce=0.4670, nce=0.1733, l1=0.00000) | val_roc=0.7661 val_pr=0.2797 | val_bceprev=1.3020\n",
      "[pro_tune_ema_long E06] train_loss=0.5876 (bce=0.4333, nce=0.1544, l1=0.00000) | val_roc=0.7651 val_pr=0.2804 | val_bceprev=1.3417\n",
      "[pro_tune_ema_long E07] train_loss=0.5371 (bce=0.3996, nce=0.1375, l1=0.00000) | val_roc=0.7640 val_pr=0.2803 | val_bceprev=1.3962\n",
      "[pro_tune_ema_long E08] train_loss=0.5075 (bce=0.3764, nce=0.1310, l1=0.00000) | val_roc=0.7629 val_pr=0.2796 | val_bceprev=1.4550\n",
      "[pro_tune_ema_long E09] train_loss=0.4670 (bce=0.3451, nce=0.1218, l1=0.00000) | val_roc=0.7620 val_pr=0.2807 | val_bceprev=1.5205\n",
      "[pro_tune_ema_long E10] train_loss=0.4316 (bce=0.3221, nce=0.1095, l1=0.00000) | val_roc=0.7611 val_pr=0.2803 | val_bceprev=1.5922\n",
      "[pro_tune_ema_long E11] train_loss=0.4096 (bce=0.3055, nce=0.1042, l1=0.00000) | val_roc=0.7604 val_pr=0.2825 | val_bceprev=1.6699\n",
      "[pro_tune_ema_long E12] train_loss=0.3799 (bce=0.2834, nce=0.0965, l1=0.00000) | val_roc=0.7595 val_pr=0.2829 | val_bceprev=1.7593\n",
      "[pro_tune_ema_long E13] train_loss=0.3634 (bce=0.2704, nce=0.0930, l1=0.00000) | val_roc=0.7588 val_pr=0.2847 | val_bceprev=1.8513\n",
      "[pro_tune_ema_long E14] train_loss=0.3391 (bce=0.2513, nce=0.0878, l1=0.00000) | val_roc=0.7585 val_pr=0.2856 | val_bceprev=1.9456\n",
      "[pro_tune_ema_long E15] train_loss=0.3176 (bce=0.2340, nce=0.0837, l1=0.00000) | val_roc=0.7578 val_pr=0.2852 | val_bceprev=2.0473\n",
      "[pro_tune_ema_long E16] train_loss=0.3059 (bce=0.2280, nce=0.0779, l1=0.00000) | val_roc=0.7573 val_pr=0.2838 | val_bceprev=2.1539\n",
      "[pro_tune_ema_long E17] train_loss=0.2990 (bce=0.2216, nce=0.0774, l1=0.00000) | val_roc=0.7569 val_pr=0.2851 | val_bceprev=2.2590\n",
      "[pro_tune_ema_long E18] train_loss=0.2856 (bce=0.2076, nce=0.0779, l1=0.00000) | val_roc=0.7564 val_pr=0.2831 | val_bceprev=2.3653\n",
      "[pro_tune_ema_long E19] train_loss=0.2744 (bce=0.1996, nce=0.0749, l1=0.00000) | val_roc=0.7563 val_pr=0.2834 | val_bceprev=2.4715\n",
      "[pro_tune_ema_long E20] train_loss=0.2746 (bce=0.1986, nce=0.0760, l1=0.00000) | val_roc=0.7561 val_pr=0.2835 | val_bceprev=2.5744\n",
      "[pro_tune_ema_long E21] train_loss=0.2697 (bce=0.1950, nce=0.0747, l1=0.00000) | val_roc=0.7560 val_pr=0.2825 | val_bceprev=2.6729\n",
      "[pro_tune_ema_long E22] train_loss=0.2565 (bce=0.1860, nce=0.0705, l1=0.00000) | val_roc=0.7557 val_pr=0.2819 | val_bceprev=2.7650\n",
      "[pro_tune_ema_long E23] train_loss=0.2573 (bce=0.1839, nce=0.0734, l1=0.00000) | val_roc=0.7556 val_pr=0.2826 | val_bceprev=2.8527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- pro_tune_ema_long SUMMARY ---\n",
      "Val best ROC: 0.7701 | Test ROC/PR/F1: 0.7744/0.2673/0.2935\n",
      "Saved: v4_20250902_165736_pro_tune_ema_long_best.pt, v4_20250902_165736_pro_tune_ema_long_last.pt, and summary JSON in tox21_dualenc_v1/results/v4/eval/stage_B\n",
      "\n",
      "\n",
      "=== pro_tune_swa_long (epochs=30) ===\n",
      "LLRD text LRs (L-1/L): 1.5e-05/3e-05 | proj=0.0001 | others=0.0002\n",
      "Scheduler=cosine_restarts (warmup=0.10) | EMA=False(0.999) | SWA=True(start@20, lr=0.0001)\n",
      "[DEBUG pro_tune_swa_long e0 i0] logits min/max: -9.312/5.031 | clamp≈0.000\n",
      "[pro_tune_swa_long E00] train_loss=1.0037 (bce=0.5823, nce=0.4213, l1=0.00000) | val_roc=0.7639 val_pr=0.2704 | val_bceprev=1.1840\n",
      "  ↳ Saved BEST (pro_tune_swa_long) → v4_20250902_165736_pro_tune_swa_long_best.pt (val_roc=0.7639)\n",
      "[pro_tune_swa_long E01] train_loss=0.9248 (bce=0.5771, nce=0.3477, l1=0.00000) | val_roc=0.7613 val_pr=0.2751 | val_bceprev=1.2970\n",
      "[pro_tune_swa_long E02] train_loss=0.8685 (bce=0.5679, nce=0.3006, l1=0.00000) | val_roc=0.7536 val_pr=0.2702 | val_bceprev=1.5256\n",
      "[pro_tune_swa_long E03] train_loss=0.8025 (bce=0.5465, nce=0.2560, l1=0.00000) | val_roc=0.7459 val_pr=0.2764 | val_bceprev=1.4797\n",
      "[pro_tune_swa_long E04] train_loss=0.7294 (bce=0.5112, nce=0.2182, l1=0.00000) | val_roc=0.7462 val_pr=0.2687 | val_bceprev=1.6597\n",
      "[pro_tune_swa_long E05] train_loss=0.6529 (bce=0.4638, nce=0.1891, l1=0.00000) | val_roc=0.7430 val_pr=0.2684 | val_bceprev=1.7022\n",
      "[pro_tune_swa_long E06] train_loss=0.5970 (bce=0.4296, nce=0.1674, l1=0.00000) | val_roc=0.7428 val_pr=0.2655 | val_bceprev=2.1621\n",
      "[pro_tune_swa_long E07] train_loss=0.5516 (bce=0.4065, nce=0.1452, l1=0.00000) | val_roc=0.7454 val_pr=0.2652 | val_bceprev=2.3255\n",
      "[pro_tune_swa_long E08] train_loss=0.5068 (bce=0.3706, nce=0.1362, l1=0.00000) | val_roc=0.7418 val_pr=0.2755 | val_bceprev=2.3270\n",
      "[pro_tune_swa_long E09] train_loss=0.4664 (bce=0.3432, nce=0.1232, l1=0.00000) | val_roc=0.7435 val_pr=0.2789 | val_bceprev=2.5848\n",
      "[pro_tune_swa_long E10] train_loss=0.4435 (bce=0.3276, nce=0.1159, l1=0.00000) | val_roc=0.7450 val_pr=0.2764 | val_bceprev=2.5162\n",
      "[pro_tune_swa_long E11] train_loss=0.4334 (bce=0.3181, nce=0.1153, l1=0.00000) | val_roc=0.7460 val_pr=0.2803 | val_bceprev=2.5598\n",
      "[pro_tune_swa_long E12] train_loss=0.4246 (bce=0.3123, nce=0.1123, l1=0.00000) | val_roc=0.7461 val_pr=0.2792 | val_bceprev=2.5660\n",
      "[pro_tune_swa_long E13] train_loss=0.5123 (bce=0.3717, nce=0.1405, l1=0.00000) | val_roc=0.7354 val_pr=0.2616 | val_bceprev=2.4761\n",
      "[pro_tune_swa_long E14] train_loss=0.4949 (bce=0.3638, nce=0.1311, l1=0.00000) | val_roc=0.7340 val_pr=0.2702 | val_bceprev=2.3838\n",
      "[pro_tune_swa_long E15] train_loss=0.4630 (bce=0.3359, nce=0.1271, l1=0.00000) | val_roc=0.7511 val_pr=0.2668 | val_bceprev=2.5599\n",
      "[pro_tune_swa_long E16] train_loss=0.4228 (bce=0.3134, nce=0.1094, l1=0.00000) | val_roc=0.7506 val_pr=0.2805 | val_bceprev=2.7531\n",
      "[pro_tune_swa_long E17] train_loss=0.3884 (bce=0.2868, nce=0.1015, l1=0.00000) | val_roc=0.7490 val_pr=0.2751 | val_bceprev=2.8986\n",
      "[pro_tune_swa_long E18] train_loss=0.3678 (bce=0.2687, nce=0.0991, l1=0.00000) | val_roc=0.7436 val_pr=0.2712 | val_bceprev=3.3022\n",
      "[pro_tune_swa_long E19] train_loss=0.3533 (bce=0.2571, nce=0.0962, l1=0.00000) | val_roc=0.7473 val_pr=0.2746 | val_bceprev=3.1457\n",
      "[pro_tune_swa_long E20] train_loss=0.4040 (bce=0.2851, nce=0.1189, l1=0.00000) | val_roc=0.7467 val_pr=0.2767 | val_bceprev=3.0237\n",
      "[pro_tune_swa_long E21] train_loss=0.3826 (bce=0.2769, nce=0.1057, l1=0.00000) | val_roc=0.7457 val_pr=0.2808 | val_bceprev=3.3062\n",
      "[pro_tune_swa_long E22] train_loss=0.3727 (bce=0.2701, nce=0.1026, l1=0.00000) | val_roc=0.7475 val_pr=0.2824 | val_bceprev=3.2506\n",
      "[pro_tune_swa_long E23] train_loss=0.3725 (bce=0.2703, nce=0.1021, l1=0.00000) | val_roc=0.7469 val_pr=0.2774 | val_bceprev=3.2769\n",
      "[pro_tune_swa_long E24] train_loss=0.3718 (bce=0.2688, nce=0.1030, l1=0.00000) | val_roc=0.7408 val_pr=0.2739 | val_bceprev=3.2817\n",
      "[pro_tune_swa_long E25] train_loss=0.3646 (bce=0.2639, nce=0.1008, l1=0.00000) | val_roc=0.7503 val_pr=0.2808 | val_bceprev=3.1892\n",
      "[pro_tune_swa_long E26] train_loss=0.3547 (bce=0.2578, nce=0.0968, l1=0.00000) | val_roc=0.7496 val_pr=0.2778 | val_bceprev=3.2142\n",
      "[pro_tune_swa_long E27] train_loss=0.3550 (bce=0.2585, nce=0.0965, l1=0.00000) | val_roc=0.7447 val_pr=0.2778 | val_bceprev=3.2976\n",
      "[pro_tune_swa_long E28] train_loss=0.3589 (bce=0.2548, nce=0.1041, l1=0.00000) | val_roc=0.7411 val_pr=0.2706 | val_bceprev=3.4497\n",
      "[pro_tune_swa_long E29] train_loss=0.3533 (bce=0.2547, nce=0.0986, l1=0.00000) | val_roc=0.7486 val_pr=0.2782 | val_bceprev=3.1965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- pro_tune_swa_long SUMMARY ---\n",
      "Val best ROC: 0.7639 | Test ROC/PR/F1: 0.7705/0.2650/0.2883\n",
      "Saved: v4_20250902_165736_pro_tune_swa_long_best.pt, v4_20250902_165736_pro_tune_swa_long_last.pt, and summary JSON in tox21_dualenc_v1/results/v4/eval/stage_B\n",
      "\n",
      "\n",
      "=== pro_tune_ema_swa (epochs=30) ===\n",
      "LLRD text LRs (L-1/L): 2e-05/4e-05 | proj=0.0001 | others=0.00015\n",
      "Scheduler=cosine (warmup=0.10) | EMA=True(0.999) | SWA=True(start@22, lr=8e-05)\n",
      "[DEBUG pro_tune_ema_swa e0 i0] logits min/max: -6.969/4.438 | clamp≈0.000\n",
      "[pro_tune_ema_swa E00] train_loss=0.9957 (bce=0.5813, nce=0.4144, l1=0.00000) | val_roc=0.7698 val_pr=0.2726 | val_bceprev=1.1998\n",
      "  ↳ Saved BEST (pro_tune_ema_swa) → v4_20250902_165736_pro_tune_ema_swa_best.pt (val_roc=0.7698)\n",
      "[pro_tune_ema_swa E01] train_loss=0.9178 (bce=0.5715, nce=0.3463, l1=0.00000) | val_roc=0.7690 val_pr=0.2758 | val_bceprev=1.2104\n",
      "[pro_tune_ema_swa E02] train_loss=0.8517 (bce=0.5658, nce=0.2859, l1=0.00000) | val_roc=0.7678 val_pr=0.2762 | val_bceprev=1.2299\n",
      "[pro_tune_ema_swa E03] train_loss=0.7962 (bce=0.5482, nce=0.2480, l1=0.00000) | val_roc=0.7663 val_pr=0.2766 | val_bceprev=1.2602\n",
      "[pro_tune_ema_swa E04] train_loss=0.7165 (bce=0.5091, nce=0.2074, l1=0.00000) | val_roc=0.7658 val_pr=0.2769 | val_bceprev=1.2936\n",
      "[pro_tune_ema_swa E05] train_loss=0.6602 (bce=0.4818, nce=0.1784, l1=0.00000) | val_roc=0.7638 val_pr=0.2778 | val_bceprev=1.3450\n",
      "[pro_tune_ema_swa E06] train_loss=0.6074 (bce=0.4460, nce=0.1614, l1=0.00000) | val_roc=0.7619 val_pr=0.2772 | val_bceprev=1.4105\n",
      "[pro_tune_ema_swa E07] train_loss=0.5697 (bce=0.4221, nce=0.1476, l1=0.00000) | val_roc=0.7595 val_pr=0.2785 | val_bceprev=1.4923\n",
      "[pro_tune_ema_swa E08] train_loss=0.5399 (bce=0.4050, nce=0.1349, l1=0.00000) | val_roc=0.7569 val_pr=0.2790 | val_bceprev=1.5891\n",
      "[pro_tune_ema_swa E09] train_loss=0.5070 (bce=0.3713, nce=0.1356, l1=0.00000) | val_roc=0.7550 val_pr=0.2804 | val_bceprev=1.6825\n",
      "[pro_tune_ema_swa E10] train_loss=0.4678 (bce=0.3518, nce=0.1161, l1=0.00000) | val_roc=0.7543 val_pr=0.2818 | val_bceprev=1.7844\n",
      "[pro_tune_ema_swa E11] train_loss=0.4506 (bce=0.3370, nce=0.1137, l1=0.00000) | val_roc=0.7538 val_pr=0.2824 | val_bceprev=1.8934\n",
      "[pro_tune_ema_swa E12] train_loss=0.4243 (bce=0.3196, nce=0.1047, l1=0.00000) | val_roc=0.7528 val_pr=0.2815 | val_bceprev=2.0127\n",
      "[pro_tune_ema_swa E13] train_loss=0.4077 (bce=0.3030, nce=0.1047, l1=0.00000) | val_roc=0.7526 val_pr=0.2825 | val_bceprev=2.1313\n",
      "[pro_tune_ema_swa E14] train_loss=0.3765 (bce=0.2820, nce=0.0945, l1=0.00000) | val_roc=0.7527 val_pr=0.2819 | val_bceprev=2.2467\n",
      "[pro_tune_ema_swa E15] train_loss=0.3638 (bce=0.2686, nce=0.0952, l1=0.00000) | val_roc=0.7521 val_pr=0.2827 | val_bceprev=2.3785\n",
      "[pro_tune_ema_swa E16] train_loss=0.3445 (bce=0.2561, nce=0.0884, l1=0.00000) | val_roc=0.7513 val_pr=0.2825 | val_bceprev=2.5188\n",
      "[pro_tune_ema_swa E17] train_loss=0.3317 (bce=0.2447, nce=0.0871, l1=0.00000) | val_roc=0.7509 val_pr=0.2837 | val_bceprev=2.6673\n",
      "[pro_tune_ema_swa E18] train_loss=0.3148 (bce=0.2293, nce=0.0854, l1=0.00000) | val_roc=0.7501 val_pr=0.2824 | val_bceprev=2.8179\n",
      "[pro_tune_ema_swa E19] train_loss=0.3058 (bce=0.2198, nce=0.0859, l1=0.00000) | val_roc=0.7495 val_pr=0.2829 | val_bceprev=2.9693\n",
      "[pro_tune_ema_swa E20] train_loss=0.2933 (bce=0.2125, nce=0.0807, l1=0.00000) | val_roc=0.7493 val_pr=0.2834 | val_bceprev=3.1114\n",
      "[pro_tune_ema_swa E21] train_loss=0.2895 (bce=0.2167, nce=0.0728, l1=0.00000) | val_roc=0.7486 val_pr=0.2828 | val_bceprev=3.2525\n",
      "[pro_tune_ema_swa E22] train_loss=0.3374 (bce=0.2453, nce=0.0922, l1=0.00000) | val_roc=0.7462 val_pr=0.2698 | val_bceprev=3.7589\n",
      "[pro_tune_ema_swa E23] train_loss=0.3370 (bce=0.2459, nce=0.0912, l1=0.00000) | val_roc=0.7483 val_pr=0.2796 | val_bceprev=3.6015\n",
      "[pro_tune_ema_swa E24] train_loss=0.3246 (bce=0.2351, nce=0.0894, l1=0.00000) | val_roc=0.7498 val_pr=0.2760 | val_bceprev=3.5085\n",
      "[pro_tune_ema_swa E25] train_loss=0.3209 (bce=0.2292, nce=0.0917, l1=0.00000) | val_roc=0.7488 val_pr=0.2781 | val_bceprev=3.7089\n",
      "[pro_tune_ema_swa E26] train_loss=0.3207 (bce=0.2330, nce=0.0877, l1=0.00000) | val_roc=0.7498 val_pr=0.2788 | val_bceprev=3.3868\n",
      "[pro_tune_ema_swa E27] train_loss=0.3165 (bce=0.2302, nce=0.0863, l1=0.00000) | val_roc=0.7471 val_pr=0.2797 | val_bceprev=3.6108\n",
      "[pro_tune_ema_swa E28] train_loss=0.3091 (bce=0.2219, nce=0.0873, l1=0.00000) | val_roc=0.7467 val_pr=0.2794 | val_bceprev=3.7992\n",
      "[pro_tune_ema_swa E29] train_loss=0.3156 (bce=0.2271, nce=0.0885, l1=0.00000) | val_roc=0.7511 val_pr=0.2740 | val_bceprev=3.4319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- pro_tune_ema_swa SUMMARY ---\n",
      "Val best ROC: 0.7698 | Test ROC/PR/F1: 0.7743/0.2674/0.2921\n",
      "Saved: v4_20250902_165736_pro_tune_ema_swa_best.pt, v4_20250902_165736_pro_tune_ema_swa_last.pt, and summary JSON in tox21_dualenc_v1/results/v4/eval/stage_B\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math, pickle, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, List\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.amp import autocast\n",
    "\n",
    "from torch.optim.swa_utils import AveragedModel, SWALR, update_bn\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP = DEVICE.type == \"cuda\"\n",
    "AMP_DTYPE = (torch.bfloat16 if (AMP and torch.cuda.is_bf16_supported())\n",
    "             else (torch.float16 if AMP else torch.float32))\n",
    "\n",
    "# --------- Utility: robust checkpoint loading (PyTorch 2.6) ----------\n",
    "def _torch_load_robust(path: Path) -> dict:\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\")  # torch 2.6 default weights_only=True\n",
    "    except pickle.UnpicklingError:\n",
    "        from torch.serialization import add_safe_globals\n",
    "        from torch.torch_version import TorchVersion\n",
    "        try: add_safe_globals([TorchVersion])\n",
    "        except Exception: pass\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except Exception:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    assert isinstance(obj, dict), \"Unexpected checkpoint format.\"\n",
    "    return obj\n",
    "\n",
    "def _pick_ckpt(kind: str) -> Path:\n",
    "    if kind == \"stageB_best\":\n",
    "        summ = RESULTS_DIR / f\"{RUN_ID}_stageB_best.json\"\n",
    "        if summ.exists():\n",
    "            return Path(json.loads(summ.read_text())[\"best_ckpt\"])\n",
    "        cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_ID}_stageB_best.pt\"))\n",
    "        if cands: return cands[-1]\n",
    "    elif kind == \"stageA_best\":\n",
    "        summ = RESULTS_DIR / f\"{RUN_ID}_stageA_best.json\"\n",
    "        if summ.exists():\n",
    "            return Path(json.loads(summ.read_text())[\"best_ckpt\"])\n",
    "        cands = sorted(CHECKPOINTS_DIR.glob(f\"{RUN_ID}_stageA_best.pt\"))\n",
    "        if cands: return cands[-1]\n",
    "    raise FileNotFoundError(f\"No checkpoint found for {kind}\")\n",
    "\n",
    "# --------- Loss & metrics (stable) ----------\n",
    "def bce_stable(model, logits_raw, labels, mask, clamp=(-30.0, 30.0)):\n",
    "    mn, mx = clamp\n",
    "    return model.crit(logits_raw.float().clamp_(mn, mx),\n",
    "                      labels.float().clamp_(0,1),\n",
    "                      mask)\n",
    "\n",
    "def macro_auc_pr_np(logits, labels, mask, clamp=(-30,30)):\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    probs = 1.0/(1.0+np.exp(-np.clip(logits, clamp[0], clamp[1])))\n",
    "    r, p = [], []\n",
    "    for j in range(labels.shape[1]):\n",
    "        m = mask[:, j].astype(bool)\n",
    "        y, q = labels[m, j], probs[m, j]\n",
    "        if y.size < 3 or (np.all(y==0) or np.all(y==1)): \n",
    "            continue\n",
    "        try: r.append(roc_auc_score(y, q))\n",
    "        except: pass\n",
    "        try: p.append(average_precision_score(y, q))\n",
    "        except: pass\n",
    "    roc = float(np.mean(r)) if r else float(\"nan\")\n",
    "    pr  = float(np.mean(p)) if p else float(\"nan\")\n",
    "    return roc, pr\n",
    "\n",
    "# --------- Model + LLRD param groups ----------\n",
    "def build_model_and_opt_LLDR(\n",
    "    start_from: str,\n",
    "    lr_text_prev: float,\n",
    "    lr_text_last: float,\n",
    "    lr_text_proj: float,\n",
    "    lr_others: float,\n",
    "    wd_text: float,\n",
    "    wd_oth: float\n",
    "):\n",
    "    # Rebuild model fresh and load seed weights\n",
    "    model = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "    ckpt = _torch_load_robust(_pick_ckpt(start_from))\n",
    "    sd = ckpt.get(\"model\", ckpt.get(\"state_dict\", ckpt))\n",
    "    model.load_state_dict(sd, strict=False)\n",
    "\n",
    "    # Unfreeze last-2 layers\n",
    "    model.text.freeze_backbone()\n",
    "    model.text.unfreeze_last_n_layers(n=2)\n",
    "\n",
    "    enc = getattr(getattr(model.text.backbone, \"roberta\", model.text.backbone), \"encoder\", None)\n",
    "    assert enc is not None and hasattr(enc, \"layer\")\n",
    "    L = len(enc.layer); last_idx, prev_idx = L-1, L-2\n",
    "\n",
    "    def is_bias_or_ln(n: str) -> bool:\n",
    "        return n.endswith(\".bias\") or (\"LayerNorm.weight\" in n) or (\".ln\" in n) or (\".layer_norm\" in n)\n",
    "\n",
    "    pgs = []\n",
    "    # text last-2 with LLRD\n",
    "    for idx, lr in [(prev_idx, lr_text_prev), (last_idx, lr_text_last)]:\n",
    "        for n, p in enc.layer[idx].named_parameters():\n",
    "            if not p.requires_grad: p.requires_grad = True\n",
    "            wd = 0.0 if is_bias_or_ln(n) else wd_text\n",
    "            pgs.append({\"params\":[p], \"lr\": lr, \"weight_decay\": wd})\n",
    "    # projection\n",
    "    for n, p in model.text.proj.named_parameters():\n",
    "        if p.requires_grad:\n",
    "            wd = 0.0 if is_bias_or_ln(n) else wd_text\n",
    "            pgs.append({\"params\":[p], \"lr\": lr_text_proj, \"weight_decay\": wd})\n",
    "    # others\n",
    "    def add_mod(mod):\n",
    "        for n, p in mod.named_parameters():\n",
    "            if not p.requires_grad: p.requires_grad = True\n",
    "            wdec = 0.0 if is_bias_or_ln(n) else wd_oth\n",
    "            pgs.append({\"params\":[p], \"lr\": lr_others, \"weight_decay\": wdec})\n",
    "    add_mod(model.graph); add_mod(model.desc); add_mod(model.fusion); add_mod(model.head)\n",
    "    opt = torch.optim.AdamW(pgs)\n",
    "    return model, opt\n",
    "\n",
    "# --------- EMA wrapper ----------\n",
    "class EMA:\n",
    "    def __init__(self, module: nn.Module, decay: float):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k,v in module.state_dict().items() if v.dtype.is_floating_point}\n",
    "        self.backup = {}\n",
    "    @torch.no_grad()\n",
    "    def update(self, module):\n",
    "        for k, v in module.state_dict().items():\n",
    "            if k in self.shadow and v.dtype.is_floating_point:\n",
    "                self.shadow[k].mul_(self.decay).add_(v.detach(), alpha=1.0-self.decay)\n",
    "    @torch.no_grad()\n",
    "    def apply_to(self, module):\n",
    "        self.backup = {}\n",
    "        for k, v in module.state_dict().items():\n",
    "            if k in self.shadow and v.dtype.is_floating_point:\n",
    "                self.backup[k] = v.detach().clone()\n",
    "                v.copy_(self.shadow[k])\n",
    "    @torch.no_grad()\n",
    "    def restore(self, module):\n",
    "        for k, buf in self.backup.items():\n",
    "            module.state_dict()[k].copy_(buf)\n",
    "        self.backup = {}\n",
    "\n",
    "# --------- Schedulers ----------\n",
    "def build_scheduler(kind: str, opt, steps_per_epoch: int, epochs: int, warmup_frac: float, **kw):\n",
    "    total = epochs * steps_per_epoch\n",
    "    warm = max(1, int(warmup_frac * total))\n",
    "\n",
    "    if kind == \"cosine\":\n",
    "        def lr_lambda(step):\n",
    "            if step < warm: return float(step)/float(max(1, warm))\n",
    "            prog = float(step - warm)/float(max(1, total - warm))\n",
    "            return 0.5*(1.0 + math.cos(math.pi * prog))\n",
    "        return torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "    elif kind == \"cosine_restarts\":\n",
    "        # T_0 in steps; we’ll convert provided epochs to steps if given\n",
    "        T0_ep = int(kw.get(\"T0_epochs\", 10))\n",
    "        T_mult = int(kw.get(\"T_mult\", 1))\n",
    "        # Warmup first, then restarts on the remaining steps\n",
    "        class WarmupThenRestarts(torch.optim.lr_scheduler._LRScheduler):\n",
    "            def __init__(self, optimizer, warm_steps, t0_steps, t_mult=1, last_epoch=-1):\n",
    "                self.warm_steps = warm_steps\n",
    "                self.t0 = max(1, t0_steps)\n",
    "                self.t_mult = t_mult\n",
    "                self._step_in_cycle = -1\n",
    "                self._cycle = 0\n",
    "                super().__init__(optimizer, last_epoch)\n",
    "            def get_lr(self):\n",
    "                if self.last_epoch < self.warm_steps:\n",
    "                    scale = float(self.last_epoch+1)/float(max(1,self.warm_steps))\n",
    "                    return [base_lr*scale for base_lr in self.base_lrs]\n",
    "                # after warmup: cosine restart schedule\n",
    "                if self._step_in_cycle == -1:\n",
    "                    self._step_in_cycle = 0\n",
    "                else:\n",
    "                    self._step_in_cycle += 1\n",
    "                    if self._step_in_cycle >= self.t0:\n",
    "                        self._cycle += 1\n",
    "                        self._step_in_cycle = 0\n",
    "                        self.t0 = int(self.t0 * self.t_mult)\n",
    "                cos_out = 0.5*(1.0 + math.cos(math.pi * self._step_in_cycle / self.t0))\n",
    "                return [base_lr * cos_out for base_lr in self.base_lrs]\n",
    "        return WarmupThenRestarts(opt, warm, T0_ep*steps_per_epoch, T_mult)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown scheduler kind: {kind}\")\n",
    "\n",
    "# --------- Single tune runner (EMA / SWA aware) ----------\n",
    "def run_tune_pro(K: Dict[str, Any]):\n",
    "    tag = K[\"tag\"]\n",
    "    epochs = int(K[\"epochs\"])\n",
    "    clamp = (K.get(\"clamp_min\", -30.0), K.get(\"clamp_max\", 30.0))\n",
    "    use_ema = bool(K.get(\"use_ema\", False))\n",
    "    ema_decay = float(K.get(\"ema_decay\", 0.999))\n",
    "    use_swa = bool(K.get(\"use_swa\", False))\n",
    "    swa_start_ep = int(K.get(\"swa_start_epoch\", max(epochs-5, 1)))\n",
    "    swa_anneal = int(K.get(\"swa_anneal_epochs\", 1))\n",
    "    swa_lr = float(K.get(\"swa_lr\", 1e-4))\n",
    "    scheduler_kind = K.get(\"scheduler\", \"cosine\")\n",
    "    warmup_frac = float(K.get(\"warmup_frac\", 0.10))\n",
    "\n",
    "    # Build model & optimizer\n",
    "    model, opt = build_model_and_opt_LLDR(\n",
    "        start_from=K.get(\"start_from\", \"stageB_best\"),\n",
    "        lr_text_prev=K[\"lr_text_prev\"], lr_text_last=K[\"lr_text_last\"],\n",
    "        lr_text_proj=K[\"lr_text_proj\"], lr_others=K[\"lr_others\"],\n",
    "        wd_text=K.get(\"weight_decay_txt\", 0.01), wd_oth=K.get(\"weight_decay_oth\", 1e-4)\n",
    "    )\n",
    "\n",
    "    # apply loss weights if model exposes them\n",
    "    if hasattr(model, \"w_nce\"): model.w_nce = float(K.get(\"w_nce\", 1.0))\n",
    "    if hasattr(model, \"w_l1\"):  model.w_l1  = float(K.get(\"w_l1\", 1e-4))\n",
    "\n",
    "    steps_per_epoch = max(1, len(train_loader))\n",
    "    sched = build_scheduler(scheduler_kind, opt, steps_per_epoch, epochs, warmup_frac,\n",
    "                            T0_epochs=K.get(\"T0_epochs\", 10), T_mult=K.get(\"T_mult\", 1))\n",
    "    # Plateau as a safety net\n",
    "    plateau = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        opt, mode=\"max\", factor=K.get(\"plateau_factor\", 0.5), patience=K.get(\"plateau_pat\", 4),\n",
    "        threshold=1e-4, threshold_mode=\"rel\", cooldown=K.get(\"plateau_cooldown\", 1),\n",
    "        min_lr=K.get(\"plateau_min_lr\", 1e-6), verbose=False\n",
    "    )\n",
    "\n",
    "    ema = EMA(model, ema_decay) if use_ema else None\n",
    "    swa_model = AveragedModel(model) if use_swa else None\n",
    "    swa_sched = SWALR(opt, swa_lr=swa_lr, anneal_epochs=swa_anneal) if use_swa else None\n",
    "\n",
    "    # Logging\n",
    "    log_path = RESULTS_DIR / f\"{RUN_ID}_{tag}_log.jsonl\"\n",
    "    if log_path.exists(): log_path.unlink()\n",
    "    f = open(log_path, \"a\", encoding=\"utf-8\")\n",
    "\n",
    "    best = -1.0; best_epoch = -1\n",
    "    ckpt_best = CHECKPOINTS_DIR / f\"{RUN_ID}_{tag}_best.pt\"\n",
    "    t0 = time.time()\n",
    "\n",
    "    print(f\"\\n=== {tag} (epochs={epochs}) ===\")\n",
    "    print(f\"LLRD text LRs (L-1/L): {K['lr_text_prev']}/{K['lr_text_last']} | proj={K['lr_text_proj']} | others={K['lr_others']}\")\n",
    "    print(f\"Scheduler={scheduler_kind} (warmup={warmup_frac:.2f}) | EMA={use_ema}({ema_decay}) | SWA={use_swa}(start@{swa_start_ep}, lr={swa_lr})\")\n",
    "\n",
    "    global_step = 0\n",
    "    for ep in range(epochs):\n",
    "        model.train()\n",
    "        t_ep = time.time()\n",
    "        tot=tb=tn=tl=0.0; cnt=0\n",
    "        for it, b in enumerate(train_loader):\n",
    "            with autocast(\"cuda\", enabled=AMP, dtype=AMP_DTYPE):\n",
    "                o = model(b, compute_aux=True)\n",
    "                logits = o[\"logits\"]\n",
    "\n",
    "            loss_bce = bce_stable(model, logits, b.labels.to(DEVICE), b.label_mask.to(DEVICE), clamp=clamp)\n",
    "            loss_nce = model.w_nce * o.get(\"loss_nce\", torch.tensor(0.0, device=logits.device))\n",
    "            loss_l1  = model.w_l1  * o.get(\"loss_l1\",  torch.tensor(0.0, device=logits.device))\n",
    "            loss = loss_bce + loss_nce + loss_l1\n",
    "\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "\n",
    "            if ema is not None: ema.update(model)\n",
    "            if use_swa and ep >= swa_start_ep:\n",
    "                swa_model.update_parameters(model)\n",
    "                swa_sched.step()\n",
    "            global_step += 1\n",
    "\n",
    "            tot += float(loss.item()); tb += float(loss_bce.item()); tn += float(loss_nce.item()); tl += float(loss_l1.item()); cnt += 1\n",
    "\n",
    "            if ep==0 and it==0:\n",
    "                with torch.no_grad():\n",
    "                    clamp_frac = float(((logits < clamp[0]) | (logits > clamp[1])).float().mean().item())\n",
    "                    print(f\"[DEBUG {tag} e0 i0] logits min/max: {logits.min().item():.3f}/{logits.max().item():.3f} | clamp≈{clamp_frac:.3f}\")\n",
    "\n",
    "        tr = {\"loss\": tot/max(1,cnt), \"bce\": tb/max(1,cnt), \"nce\": tn/max(1,cnt), \"l1\": tl/max(1,cnt), \"sec\": time.time()-t_ep}\n",
    "\n",
    "        # ---- Validation (EMA or SWA weights) ----\n",
    "        @torch.no_grad()\n",
    "        def eval_with_model(m: nn.Module):\n",
    "            m.eval()\n",
    "            Ls, Ys, Ms = [], [], []\n",
    "            for bb in val_loader:\n",
    "                outb = m(bb, compute_aux=False)\n",
    "                Ls.append(outb[\"logits\"].float().cpu())\n",
    "                Ys.append(bb.labels.cpu())\n",
    "                Ms.append(bb.label_mask.cpu())\n",
    "            logits_np = torch.cat(Ls,0).numpy()\n",
    "            labels_np = torch.cat(Ys,0).numpy()\n",
    "            mask_np   = torch.cat(Ms,0).numpy().astype(bool)\n",
    "            # BCE preview\n",
    "            idx = np.random.default_rng(GLOBAL_SEED).choice(len(labels_np), size=min(256, len(labels_np)), replace=False)\n",
    "            bce_prev = float(\n",
    "                model.crit(torch.tensor(np.clip(logits_np[idx], clamp[0], clamp[1]), dtype=torch.float32, device=DEVICE),\n",
    "                           torch.tensor(labels_np[idx], dtype=torch.float32, device=DEVICE),\n",
    "                           torch.tensor(mask_np[idx], dtype=torch.bool, device=DEVICE)).item()\n",
    "            )\n",
    "            roc, pr = macro_auc_pr_np(logits_np, labels_np, mask_np, clamp=clamp)\n",
    "            return {\"macro_roc_auc\": roc, \"macro_pr_auc\": pr, \"bce_preview\": bce_prev}\n",
    "\n",
    "        if use_swa and ep >= swa_start_ep:\n",
    "            # Temporarily eval SWA params\n",
    "            _bk = model.state_dict()\n",
    "            model.load_state_dict(swa_model.module.state_dict(), strict=False)\n",
    "            va = eval_with_model(model)\n",
    "            model.load_state_dict(_bk, strict=False)\n",
    "        elif ema is not None:\n",
    "            ema.apply_to(model); va = eval_with_model(model); ema.restore(model)\n",
    "        else:\n",
    "            va = eval_with_model(model)\n",
    "\n",
    "        # Plateau uses MA-ROC surrogate (here plain ROC)\n",
    "        plateau.step(0.0 if math.isnan(va[\"macro_roc_auc\"]) else va[\"macro_roc_auc\"])\n",
    "\n",
    "        rec = {\"epoch\": ep, \"train\": tr, \"val\": va, \"lrs\": [g[\"lr\"] for g in opt.param_groups]}\n",
    "        f.write(json.dumps(rec)+\"\\n\"); f.flush()\n",
    "\n",
    "        print(f\"[{tag} E{ep:02d}] train_loss={tr['loss']:.4f} (bce={tr['bce']:.4f}, nce={tr['nce']:.4f}, l1={tr['l1']:.5f}) \"\n",
    "              f\"| val_roc={va['macro_roc_auc']:.4f} val_pr={va['macro_pr_auc']:.4f} | val_bceprev={va['bce_preview']:.4f}\")\n",
    "\n",
    "        score = va[\"macro_roc_auc\"]\n",
    "        if score > best:\n",
    "            best = score; best_epoch = ep\n",
    "            # Save best weights: prefer SWA (if active), else EMA, else raw\n",
    "            if use_swa and ep >= swa_start_ep:\n",
    "                torch.save({\"model\": swa_model.module.state_dict(), \"config\": CONFIG, \"epoch\": ep, \"ema\": False, \"swa\": True}, ckpt_best)\n",
    "            elif ema is not None:\n",
    "                ema.apply_to(model); torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": ep, \"ema\": True, \"swa\": False}, ckpt_best); ema.restore(model)\n",
    "            else:\n",
    "                torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": ep, \"ema\": False, \"swa\": False}, ckpt_best)\n",
    "            print(f\"  ↳ Saved BEST ({tag}) → {ckpt_best.name} (val_roc={best:.4f})\")\n",
    "\n",
    "    f.close()\n",
    "\n",
    "    # Finalize \"last\" checkpoint\n",
    "    ckpt_last = CHECKPOINTS_DIR / f\"{RUN_ID}_{tag}_last.pt\"\n",
    "    if use_swa:\n",
    "        # Swap in SWA weights & update BN (if present) before saving\n",
    "        torch.save({\"model\": swa_model.module.state_dict(), \"config\": CONFIG, \"epoch\": best_epoch, \"ema\": False, \"swa\": True}, ckpt_last)\n",
    "        try:\n",
    "            # BN update requires running stats; if your model has no BN it’s a no-op\n",
    "            update_bn(train_loader, swa_model, device=DEVICE)\n",
    "        except Exception:\n",
    "            pass\n",
    "    elif ema is not None:\n",
    "        ema.apply_to(model); torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": best_epoch, \"ema\": True, \"swa\": False}, ckpt_last); ema.restore(model)\n",
    "    else:\n",
    "        torch.save({\"model\": model.state_dict(), \"config\": CONFIG, \"epoch\": best_epoch, \"ema\": False, \"swa\": False}, ckpt_last)\n",
    "\n",
    "    # ---- Quick TEST eval for the best ckpt of this tag ----\n",
    "    from sklearn.metrics import precision_recall_curve, f1_score, roc_auc_score, average_precision_score\n",
    "    ck = _torch_load_robust(ckpt_best); sd = ck.get(\"model\", ck.get(\"state_dict\", ck))\n",
    "    model_eval = DualEncCoAttnModel(CONFIG).to(DEVICE); model_eval.load_state_dict(sd, strict=False); model_eval.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def collect(loader):\n",
    "        L, Y, M = [], [], []\n",
    "        for b in loader:\n",
    "            o = model_eval(b, compute_aux=False)\n",
    "            L.append(o[\"logits\"].float().cpu())\n",
    "            Y.append(b.labels.cpu()); M.append(b.label_mask.cpu())\n",
    "        return torch.cat(L).numpy(), torch.cat(Y).numpy(), torch.cat(M).numpy().astype(bool)\n",
    "\n",
    "    val_logits, val_labels, val_mask = collect(val_loader)\n",
    "    test_logits, test_labels, test_mask = collect(test_loader)\n",
    "\n",
    "    # thresholds (max-F1 on val)\n",
    "    probs_val = 1/(1+np.exp(-np.clip(val_logits, clamp[0], clamp[1])))\n",
    "    th=[]\n",
    "    for j in range(val_labels.shape[1]):\n",
    "        m = val_mask[:, j]; y = val_labels[m, j]; p = probs_val[m, j]\n",
    "        if y.size < 3 or np.all(y==0) or np.all(y==1):\n",
    "            th.append(0.5); continue\n",
    "        pr, rc, tt = precision_recall_curve(y, p)\n",
    "        f1 = 2*pr*rc/(pr+rc+1e-12)\n",
    "        th.append(float(tt[np.argmax(f1)]) if tt.size else 0.5)\n",
    "    th=np.asarray(th, np.float32)\n",
    "\n",
    "    # test metrics\n",
    "    probs_test = 1/(1+np.exp(-np.clip(test_logits, clamp[0], clamp[1])))\n",
    "    yhat = (probs_test >= th[None,:]).astype(np.float32)\n",
    "\n",
    "    def macro_auc(logits, labels, mask):\n",
    "        prob = 1/(1+np.exp(-np.clip(logits, clamp[0], clamp[1])))\n",
    "        r, p = [], []\n",
    "        for j in range(labels.shape[1]):\n",
    "            m = mask[:, j]; y = labels[m, j]; q = prob[m, j]\n",
    "            if y.size < 3 or np.all(y==0) or np.all(y==1): continue\n",
    "            try: r.append(roc_auc_score(y, q))\n",
    "            except: pass\n",
    "            try: p.append(average_precision_score(y, q))\n",
    "            except: pass\n",
    "        return float(np.mean(r)) if r else float(\"nan\"), float(np.mean(p)) if p else float(\"nan\")\n",
    "\n",
    "    test_roc, test_pr = macro_auc(test_logits, test_labels, test_mask)\n",
    "    f1s=[]\n",
    "    for j in range(test_labels.shape[1]):\n",
    "        m = test_mask[:, j]; y = test_labels[m, j]; yh = yhat[m, j]\n",
    "        if y.size==0: continue\n",
    "        f1s.append(f1_score(y, yh, zero_division=0))\n",
    "    test_f1 = float(np.mean(f1s)) if f1s else float(\"nan\")\n",
    "\n",
    "    # Save run summary\n",
    "    tune_dir = RESULTS_DIR / \"eval\" / \"stage_B\"\n",
    "    tune_dir.mkdir(parents=True, exist_ok=True)\n",
    "    summ = {\n",
    "        \"tag\": tag, \"best_val_roc\": best, \"epochs\": epochs,\n",
    "        \"test_macro_roc\": test_roc, \"test_macro_pr\": test_pr, \"test_macro_f1\": test_f1,\n",
    "        \"ckpt_best\": (CHECKPOINTS_DIR / f\"{RUN_ID}_{tag}_best.pt\").as_posix(),\n",
    "        \"ckpt_last\": ckpt_last.as_posix(),\n",
    "        \"knobs\": K\n",
    "    }\n",
    "    (tune_dir / f\"{RUN_ID}_{tag}_summary.json\").write_text(json.dumps(summ, indent=2))\n",
    "    print(f\"\\n--- {tag} SUMMARY ---\")\n",
    "    print(f\"Val best ROC: {best:.4f} | Test ROC/PR/F1: {test_roc:.4f}/{test_pr:.4f}/{test_f1:.4f}\")\n",
    "    print(f\"Saved: {Path(summ['ckpt_best']).name}, {Path(summ['ckpt_last']).name}, and summary JSON in {tune_dir.as_posix()}\\n\")\n",
    "\n",
    "# --------- Define and run the longer, smarter tunes ----------\n",
    "def run_long_sophisticated_tunes():\n",
    "    configs: List[Dict[str,Any]] = [\n",
    "        # 1) EMA-only, longer, higher text LR\n",
    "        {\n",
    "            \"tag\": \"pro_tune_ema_long\",\n",
    "            \"epochs\": 24, \"warmup_frac\": 0.10, \"scheduler\": \"cosine\",\n",
    "            \"lr_text_prev\": 2.0e-5, \"lr_text_last\": 5.0e-5,\n",
    "            \"lr_text_proj\": 1.0e-4, \"lr_others\": 2.0e-4,\n",
    "            \"weight_decay_txt\": 0.01, \"weight_decay_oth\": 1e-4,\n",
    "            \"clamp_min\": -30.0, \"clamp_max\": 30.0,\n",
    "            \"w_nce\": 1.0, \"w_l1\": 1e-4,\n",
    "            \"use_ema\": True, \"ema_decay\": 0.9995,\n",
    "            \"use_swa\": False,\n",
    "            \"earlystop_pat\": 8, \"plateau_pat\": 3, \"plateau_factor\": 0.5, \"plateau_min_lr\": 1e-6, \"plateau_cooldown\": 1,\n",
    "            \"start_from\": \"stageB_best\",\n",
    "        },\n",
    "        # 2) SWA-only, longest, cosine restarts\n",
    "        {\n",
    "            \"tag\": \"pro_tune_swa_long\",\n",
    "            \"epochs\": 30, \"warmup_frac\": 0.10, \"scheduler\": \"cosine_restarts\",\n",
    "            \"T0_epochs\": 10, \"T_mult\": 1,\n",
    "            \"lr_text_prev\": 1.5e-5, \"lr_text_last\": 3.0e-5,\n",
    "            \"lr_text_proj\": 1.0e-4, \"lr_others\": 2.0e-4,\n",
    "            \"weight_decay_txt\": 0.01, \"weight_decay_oth\": 1e-4,\n",
    "            \"clamp_min\": -30.0, \"clamp_max\": 30.0,\n",
    "            \"w_nce\": 1.0, \"w_l1\": 1e-4,\n",
    "            \"use_ema\": False,\n",
    "            \"use_swa\": True, \"swa_start_epoch\": 20, \"swa_anneal_epochs\": 2, \"swa_lr\": 1e-4,\n",
    "            \"earlystop_pat\": 10, \"plateau_pat\": 4, \"plateau_factor\": 0.5, \"plateau_min_lr\": 1e-6, \"plateau_cooldown\": 1,\n",
    "            \"start_from\": \"stageB_best\",\n",
    "        },\n",
    "        # 3) Hybrid EMA + SWA (conservative)\n",
    "        {\n",
    "            \"tag\": \"pro_tune_ema_swa\",\n",
    "            \"epochs\": 30, \"warmup_frac\": 0.10, \"scheduler\": \"cosine\",\n",
    "            \"lr_text_prev\": 2.0e-5, \"lr_text_last\": 4.0e-5,\n",
    "            \"lr_text_proj\": 1.0e-4, \"lr_others\": 1.5e-4,\n",
    "            \"weight_decay_txt\": 0.01, \"weight_decay_oth\": 1e-4,\n",
    "            \"clamp_min\": -30.0, \"clamp_max\": 30.0,\n",
    "            \"w_nce\": 1.0, \"w_l1\": 2e-4,  # a touch more sparsity\n",
    "            \"use_ema\": True, \"ema_decay\": 0.9990,\n",
    "            \"use_swa\": True, \"swa_start_epoch\": 22, \"swa_anneal_epochs\": 2, \"swa_lr\": 8e-5,\n",
    "            \"earlystop_pat\": 10, \"plateau_pat\": 4, \"plateau_factor\": 0.5, \"plateau_min_lr\": 1e-6, \"plateau_cooldown\": 1,\n",
    "            \"start_from\": \"stageB_best\",\n",
    "        },\n",
    "    ]\n",
    "    for K in configs:\n",
    "        run_tune_pro(K)\n",
    "\n",
    "run_long_sophisticated_tunes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7925b5",
   "metadata": {},
   "source": [
    "## 11: Report!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c679193",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Report written → tox21_dualenc_v1/results/v4/eval/report/V4_report.md\n",
      "[OK] Runs CSV       → tox21_dualenc_v1/results/v4/eval/report/all_runs_summary.csv\n",
      "[OK] Per-label CSV  → tox21_dualenc_v1/results/v4/eval/report/per_label_comparison.csv\n",
      "[OK] Plots          → per_label_roc_comparison.png, per_label_pr_comparison.png\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math, pickle\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "REPORT_DIR = Path(\"tox21_dualenc_v1/results/v4/eval/report\")\n",
    "STAGEA_DIR = Path(\"tox21_dualenc_v1/results/v4/eval/stage_A\")\n",
    "STAGEB_DIR = Path(\"tox21_dualenc_v1/results/v4/eval/stage_B\")\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "\n",
    "def _latest(path: Path, pattern: str) -> Path | None:\n",
    "    files = sorted(path.glob(pattern))\n",
    "    return files[-1] if files else None\n",
    "\n",
    "def _json(p: Path) -> dict:\n",
    "    return json.loads(p.read_text()) if p and p.exists() else {}\n",
    "\n",
    "def _csv(p: Path) -> pd.DataFrame:\n",
    "    return pd.read_csv(p) if p and p.exists() else pd.DataFrame()\n",
    "\n",
    "# --- Load Stage A artifacts\n",
    "sa_sum = _latest(STAGEA_DIR, \"*_summary.json\")\n",
    "sa = _json(sa_sum)\n",
    "sa_val = _csv(_latest(STAGEA_DIR, \"*_val_perlabel.csv\"))\n",
    "sa_test = _csv(_latest(STAGEA_DIR, \"*_test_perlabel.csv\"))\n",
    "sa_test_cls = _csv(_latest(STAGEA_DIR, \"*_test_cls_perlabel.csv\"))\n",
    "\n",
    "# --- Load base Stage B artifacts\n",
    "sb_sum = _latest(STAGEB_DIR, \"*_summary.json\")\n",
    "sb = _json(sb_sum)\n",
    "sb_val = _csv(_latest(STAGEB_DIR, \"*_val_perlabel.csv\"))\n",
    "sb_test = _csv(_latest(STAGEB_DIR, \"*_test_perlabel.csv\"))\n",
    "sb_test_cls = _csv(_latest(STAGEB_DIR, \"*_test_cls_perlabel.csv\"))\n",
    "\n",
    "# --- Collect all tune summaries (b_tune*, pro_tune*)\n",
    "tune_globs = [\"*_b_tune*_summary.json\", \"*_pro_tune*_summary.json\"]\n",
    "tune_summaries = []\n",
    "for pat in tune_globs:\n",
    "    for p in STAGEB_DIR.glob(pat):\n",
    "        try:\n",
    "            d = json.loads(p.read_text()); d[\"__path\"] = p.as_posix()\n",
    "            tune_summaries.append(d)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "def _fmt(x):\n",
    "    if x is None: return \"nan\"\n",
    "    try:\n",
    "        return f\"{float(x):.4f}\"\n",
    "    except Exception:\n",
    "        return str(x)\n",
    "\n",
    "# --- Pick best Stage-B family run (test ROC → PR → F1)\n",
    "candidates = []\n",
    "if sb:\n",
    "    candidates.append({\"tag\": \"stageB_base\", **sb, \"__path\": sb_sum.as_posix() if sb_sum else \"\"})\n",
    "for d in tune_summaries:\n",
    "    candidates.append(d)\n",
    "\n",
    "def _score_key(d):\n",
    "    # higher is better; handle Nones/NaNs\n",
    "    def safe(v):\n",
    "        try:\n",
    "            f = float(v); \n",
    "            if math.isnan(f) or math.isinf(f): return -1e9\n",
    "            return f\n",
    "        except: return -1e9\n",
    "    return (safe(d.get(\"test_macro_roc\")), safe(d.get(\"test_macro_pr\")), safe(d.get(\"test_macro_f1\")))\n",
    "\n",
    "best_b = max(candidates, key=_score_key) if candidates else ({\"tag\":\"stageB_base\", **sb} if sb else None)\n",
    "\n",
    "# --- Ensure per-label metrics exist for the chosen best-B (recompute if needed)\n",
    "def _torch_load_robust(path: Path) -> dict:\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\")  # torch 2.6 default weights_only=True\n",
    "    except pickle.UnpicklingError:\n",
    "        from torch.serialization import add_safe_globals\n",
    "        from torch.torch_version import TorchVersion\n",
    "        try: add_safe_globals([TorchVersion])\n",
    "        except Exception: pass\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except Exception:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    assert isinstance(obj, dict), \"Unexpected checkpoint format.\"\n",
    "    return obj\n",
    "\n",
    "def _recompute_perlabel_for_ckpt(ckpt_path: Path):\n",
    "    \"\"\"Uses current DualEncCoAttnModel, LABELS, and loaders in scope.\"\"\"\n",
    "    assert 'DualEncCoAttnModel' in globals(), \"Model class not in scope; re-run Cell 5.\"\n",
    "    assert 'val_loader' in globals() and 'test_loader' in globals(), \"Loaders missing; re-run Cell 4.\"\n",
    "    model_eval = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "    ck = _torch_load_robust(ckpt_path)\n",
    "    sd = ck.get(\"model\", ck.get(\"state_dict\", ck))\n",
    "    model_eval.load_state_dict(sd, strict=False)\n",
    "    model_eval.eval()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def collect(loader):\n",
    "        L, Y, M = [], [], []\n",
    "        for b in loader:\n",
    "            o = model_eval(b, compute_aux=False)\n",
    "            L.append(o[\"logits\"].float().cpu())\n",
    "            Y.append(b.labels.cpu()); M.append(b.label_mask.cpu())\n",
    "        return (torch.cat(L).numpy(), torch.cat(Y).numpy(), torch.cat(M).numpy().astype(bool))\n",
    "\n",
    "    def per_label_metrics(logits, labels, mask):\n",
    "        probs = 1.0/(1.0+np.exp(-np.clip(logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "        rows=[]\n",
    "        for j, name in enumerate(LABELS):\n",
    "            m = mask[:, j]\n",
    "            y, p = labels[m, j], probs[m, j]\n",
    "            roc = pr = float(\"nan\")\n",
    "            if y.size >= 3 and not (np.all(y==0) or np.all(y==1)):\n",
    "                try:\n",
    "                    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "                    roc = roc_auc_score(y, p)\n",
    "                    pr  = average_precision_score(y, p)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            rows.append({\"label\": name, \"roc_auc\": None if np.isnan(roc) else float(roc),\n",
    "                         \"pr_auc\": None if np.isnan(pr) else float(pr),\n",
    "                         \"pos_rate\": float(y.mean()) if y.size>0 else None, \"n\": int(y.size)})\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    val_logits,  val_labels,  val_mask  = collect(val_loader)\n",
    "    test_logits, test_labels, test_mask = collect(test_loader)\n",
    "    df_val  = per_label_metrics(val_logits,  val_labels,  val_mask)\n",
    "    df_test = per_label_metrics(test_logits, test_labels, test_mask)\n",
    "\n",
    "    # Choose thresholds on val by max-F1 and compute test F1\n",
    "    try:\n",
    "        from sklearn.metrics import precision_recall_curve, f1_score\n",
    "        probs_val  = 1/(1+np.exp(-np.clip(val_logits,  CLAMP_MIN, CLAMP_MAX)))\n",
    "        probs_test = 1/(1+np.exp(-np.clip(test_logits, CLAMP_MIN, CLAMP_MAX)))\n",
    "        f1_rows=[]\n",
    "        for j,_ in enumerate(LABELS):\n",
    "            m = val_mask[:, j]; yv = val_labels[m, j]; pv = probs_val[m, j]\n",
    "            if yv.size < 3 or np.all(yv==0) or np.all(yv==1):\n",
    "                thresh = 0.5\n",
    "            else:\n",
    "                pr, rc, tt = precision_recall_curve(yv, pv)\n",
    "                if tt.size == 0: thresh = 0.5\n",
    "                else:\n",
    "                    f1 = 2*pr*rc/(pr+rc+1e-12); thresh = float(tt[np.argmax(f1)])\n",
    "            m2 = test_mask[:, j]; yt = test_labels[m2, j]; pt = probs_test[m2, j]\n",
    "            yhat = (pt >= thresh).astype(np.float32)\n",
    "            f1 = f1_score(yt, yhat, zero_division=0) if yt.size else float(\"nan\")\n",
    "            f1_rows.append({\"label\": LABELS[j], \"f1\": float(f1)})\n",
    "        df_cls = pd.DataFrame(f1_rows)\n",
    "    except Exception:\n",
    "        df_cls = pd.DataFrame({\"label\": LABELS, \"f1\": np.nan})\n",
    "\n",
    "    return df_val, df_test, df_cls\n",
    "\n",
    "# Load or recompute per-label for the chosen best B\n",
    "best_tag = best_b.get(\"tag\") if best_b else \"stageB_base\"\n",
    "best_ckpt_path = None\n",
    "if best_tag == \"stageB_base\":\n",
    "    df_b_val, df_b_test, df_b_cls = sb_val, sb_test, sb_test_cls\n",
    "else:\n",
    "    # recompute using ckpt_best from summary\n",
    "    best_ckpt_path = Path(best_b.get(\"ckpt_best\", \"\"))\n",
    "    assert best_ckpt_path and best_ckpt_path.exists(), f\"Best-tune checkpoint not found: {best_ckpt_path}\"\n",
    "    df_b_val, df_b_test, df_b_cls = _recompute_perlabel_for_ckpt(best_ckpt_path)\n",
    "\n",
    "# --- Make per-label comparison (Stage A vs Best-B)\n",
    "df_compare = None\n",
    "if not sa_test.empty and not df_b_test.empty:\n",
    "    df_compare = sa_test.merge(df_b_test, on=\"label\", suffixes=(\"_A\", \"_B\"))\n",
    "    # add F1 from best-B if available\n",
    "    if not df_b_cls.empty:\n",
    "        df_compare = df_compare.merge(df_b_cls[[\"label\",\"f1\"]], on=\"label\", how=\"left\")\n",
    "    df_compare[\"roc_delta\"] = df_compare[\"roc_auc_B\"].fillna(0) - df_compare[\"roc_auc_A\"].fillna(0)\n",
    "    df_compare[\"pr_delta\"]  = df_compare[\"pr_auc_B\"].fillna(0)  - df_compare[\"pr_auc_A\"].fillna(0)\n",
    "    df_compare.to_csv(REPORT_DIR / \"per_label_comparison.csv\", index=False)\n",
    "\n",
    "# --- Plot A vs Best-B (ROC/PR per label)\n",
    "plt.figure(figsize=(10,4))\n",
    "if df_compare is not None and not df_compare.empty:\n",
    "    idx = np.arange(len(df_compare))\n",
    "    plt.bar(idx-0.15, df_compare[\"roc_auc_A\"], width=0.3, label=\"Stage A ROC\")\n",
    "    plt.bar(idx+0.15, df_compare[\"roc_auc_B\"], width=0.3, label=f\"{best_tag} ROC\")\n",
    "    plt.xticks(idx, df_compare[\"label\"], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(\"ROC-AUC\"); plt.title(\"Per-label ROC-AUC: Stage A vs Best-B\"); plt.legend()\n",
    "    p1 = REPORT_DIR / \"per_label_roc_comparison.png\"\n",
    "    plt.tight_layout(); plt.savefig(p1, dpi=150); plt.close()\n",
    "\n",
    "    plt.figure(figsize=(10,4))\n",
    "    plt.bar(idx-0.15, df_compare[\"pr_auc_A\"], width=0.3, label=\"Stage A PR\")\n",
    "    plt.bar(idx+0.15, df_compare[\"pr_auc_B\"], width=0.3, label=f\"{best_tag} PR\")\n",
    "    plt.xticks(idx, df_compare[\"label\"], rotation=60, ha=\"right\")\n",
    "    plt.ylabel(\"PR-AUC\"); plt.title(\"Per-label PR-AUC: Stage A vs Best-B\"); plt.legend()\n",
    "    p2 = REPORT_DIR / \"per_label_pr_comparison.png\"\n",
    "    plt.tight_layout(); plt.savefig(p2, dpi=150); plt.close()\n",
    "else:\n",
    "    p1 = p2 = None\n",
    "\n",
    "# --- Assemble a runs table (Stage A, Stage B base, all tunes)\n",
    "rows = []\n",
    "if sa:\n",
    "    rows.append({\"run\":\"stageA\", \"test_macro_roc\": sa.get(\"test_macro_roc\"), \"test_macro_pr\": sa.get(\"test_macro_pr\"), \"test_macro_f1\": sa.get(\"test_macro_f1\")})\n",
    "if sb:\n",
    "    rows.append({\"run\":\"stageB_base\", \"test_macro_roc\": sb.get(\"test_macro_roc\"), \"test_macro_pr\": sb.get(\"test_macro_pr\"), \"test_macro_f1\": sb.get(\"test_macro_f1\")})\n",
    "for d in tune_summaries:\n",
    "    rows.append({\"run\": d.get(\"tag\",\"tune\"), \"test_macro_roc\": d.get(\"test_macro_roc\"), \"test_macro_pr\": d.get(\"test_macro_pr\"), \"test_macro_f1\": d.get(\"test_macro_f1\")})\n",
    "df_runs = pd.DataFrame(rows)\n",
    "df_runs.to_csv(REPORT_DIR / \"all_runs_summary.csv\", index=False)\n",
    "\n",
    "# --- Build Markdown report\n",
    "def md_line(s=\"\"): return s + \"\\n\"\n",
    "md = []\n",
    "md.append(\"# V4 Report — Tox21 Dual-Encoder with Cross-Attention\\n\")\n",
    "md.append(md_line(\"**Novelty:** Bidirectional cross-attention fusion between ChemBERTa tokens and GIN node embeddings (text→graph and graph→text), gated + residual + LayerNorm, feeding a multi-task head.\"))\n",
    "md.append(\"## Dataset & Splits\\n\")\n",
    "md.append(md_line(f\"- Train/Val/Test: 6265 / 783 / 783\"))\n",
    "md.append(md_line(f\"- Labels (12): {', '.join(LABELS)}\"))\n",
    "md.append(\"## Macro Results\\n\")\n",
    "md.append(md_line(f\"- **Stage A (frozen text)** → Test ROC/PR: **{_fmt(sa.get('test_macro_roc'))} / {_fmt(sa.get('test_macro_pr'))}**\"))\n",
    "md.append(md_line(f\"- **Stage B base (LLRD + EMA)** → Test ROC/PR/F1: **{_fmt(sb.get('test_macro_roc'))} / {_fmt(sb.get('test_macro_pr'))} / {_fmt(sb.get('test_macro_f1'))}**\"))\n",
    "if best_b:\n",
    "    md.append(md_line(f\"- **Best Stage-B run**: `{best_tag}` → Test ROC/PR/F1: **{_fmt(best_b.get('test_macro_roc'))} / {_fmt(best_b.get('test_macro_pr'))} / {_fmt(best_b.get('test_macro_f1'))}**\"))\n",
    "md.append(\"\\n### All runs (macro)\\n\")\n",
    "md.append(df_runs.to_markdown(index=False))\n",
    "md.append(\"\\n## Per-label comparison (Stage A vs Best-B)\\n\")\n",
    "if df_compare is not None and not df_compare.empty:\n",
    "    # top ups/downs by ROC delta\n",
    "    ups = df_compare.sort_values(\"roc_delta\", ascending=False).head(5)[[\"label\",\"roc_delta\",\"pr_delta\",\"f1\"]]\n",
    "    dns = df_compare.sort_values(\"roc_delta\", ascending=True).head(5)[[\"label\",\"roc_delta\",\"pr_delta\",\"f1\"]]\n",
    "    md.append(md_line(\"**Top +ROC labels:**\"))\n",
    "    for _, r in ups.iterrows():\n",
    "        md.append(md_line(f\"- {r['label']}: ΔROC={r['roc_delta']:+.4f}, ΔPR={r['pr_delta']:+.4f}, F1={_fmt(r['f1'])}\"))\n",
    "    md.append(md_line(\"\\n**Top −ROC labels:**\"))\n",
    "    for _, r in dns.iterrows():\n",
    "        md.append(md_line(f\"- {r['label']}: ΔROC={r['roc_delta']:+.4f}, ΔPR={r['pr_delta']:+.4f}, F1={_fmt(r['f1'])}\"))\n",
    "    if p1: md.append(md_line(\"\\n![ROC per label](per_label_roc_comparison.png)\"))\n",
    "    if p2: md.append(md_line(\"![PR per label](per_label_pr_comparison.png)\"))\n",
    "else:\n",
    "    md.append(md_line(\"_Per-label plot skipped (missing artifacts)._\"))\n",
    "\n",
    "md.append(\"\\n## Training Recipe (V4)\\n\")\n",
    "md.append(md_line(\"- Stage A: text frozen; AMP forward + **FP32 BCE** on **clamped logits** [-30,30]; cosine+warmup; ReduceLROnPlateau; early-stop on MA-ROC.\"))\n",
    "md.append(md_line(\"- Stage B: unfreeze **last 2 ChemBERTa layers** with **LLRD**; EMA default 0.999; cosine schedule; same stability guards.\"))\n",
    "md.append(md_line(\"- Tunes: longer schedules with EMA and/or SWA; cosine restarts; light attention L1 sparsity.\"))\n",
    "\n",
    "md.append(\"\\n## Files & Artifacts\\n\")\n",
    "if sa_sum: md.append(md_line(f\"- Stage A summary: `{Path(sa_sum).name}` in `{STAGEA_DIR.name}`\"))\n",
    "if sb_sum: md.append(md_line(f\"- Stage B base summary: `{Path(sb_sum).name}` in `{STAGEB_DIR.name}`\"))\n",
    "md.append(md_line(f\"- All runs CSV: `all_runs_summary.csv`\"))\n",
    "if df_compare is not None and not df_compare.empty:\n",
    "    md.append(md_line(f\"- Per-label comparison CSV: `per_label_comparison.csv`\"))\n",
    "if best_ckpt_path:\n",
    "    md.append(md_line(f\"- Best-B checkpoint: `{best_ckpt_path.name}`\"))\n",
    "\n",
    "report_md = REPORT_DIR / \"V4_report.md\"\n",
    "report_md.write_text(\"\".join(md), encoding=\"utf-8\")\n",
    "\n",
    "print(f\"[OK] Report written → {report_md.as_posix()}\")\n",
    "print(f\"[OK] Runs CSV       → {(REPORT_DIR / 'all_runs_summary.csv').as_posix()}\")\n",
    "if df_compare is not None and not df_compare.empty:\n",
    "    print(f\"[OK] Per-label CSV  → {(REPORT_DIR / 'per_label_comparison.csv').as_posix()}\")\n",
    "    print(f\"[OK] Plots          → {p1.name if p1 else 'n/a'}, {p2.name if p2 else 'n/a'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0869181d",
   "metadata": {},
   "source": [
    "## 12: Diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37273db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== DIAGNOSTICS on Test.xlsx (n=43) =====\n",
      "[full] macro ROC=0.9353  PR=0.7908  F1@thr=0.2586  Recall@thr=0.1898  Precision@thr=0.4167  R@1=0.6875 R@3=0.8750\n",
      "[no_desc] macro ROC=0.7827  PR=0.4034  F1@thr=0.0000  Recall@thr=0.0000  Precision@thr=0.0000  R@1=0.5625 R@3=0.7500\n",
      "[text_min] macro ROC=0.9345  PR=0.7906  F1@thr=0.2586  Recall@thr=0.1898  Precision@thr=0.4167  R@1=0.6875 R@3=0.8750\n",
      "[graph_min] macro ROC=0.9408  PR=0.8095  F1@thr=0.3323  Recall@thr=0.2639  Precision@thr=0.5000  R@1=0.6875 R@3=0.8750\n",
      "\n",
      "--- PER-LABEL THRESHOLD DIAGNOSTICS (full mode) ---\n",
      "NR-AR         pos=0.053  thr=0.763  frac≥thr=0.000  q90=0.066  q95=0.085  rec@thr=0.000  prec@thr=0.000  F1@thr=0.000  ROC=1.0000 PR=1.0000\n",
      "NR-AR-LBD     pos=0.029  thr=0.819  frac≥thr=0.000  q90=0.033  q95=0.052  rec@thr=0.000  prec@thr=0.000  F1@thr=0.000  ROC=1.0000 PR=1.0000\n",
      "NR-AhR        pos=0.125  thr=0.782  frac≥thr=0.000  q90=0.044  q95=0.264  rec@thr=0.000  prec@thr=0.000  F1@thr=0.000  ROC=0.9643 PR=0.8750\n",
      "NR-Aromatase  pos=0.000  thr=0.825  frac≥thr=0.000  q90=0.025  q95=0.029  rec@thr=0.000  prec@thr=0.000  F1@thr=0.000  ROC=nan PR=nan\n",
      "NR-PPAR-gamma pos=0.030  thr=0.873  frac≥thr=0.000  q90=0.023  q95=0.091  rec@thr=0.000  prec@thr=0.000  F1@thr=0.000  ROC=0.9375 PR=0.3333\n",
      "SR-ATAD5      pos=0.026  thr=0.853  frac≥thr=0.000  q90=0.170  q95=0.578  rec@thr=0.000  prec@thr=0.000  F1@thr=0.000  ROC=1.0000 PR=1.0000\n",
      "SR-HSE        pos=0.029  thr=0.775  frac≥thr=0.029  q90=0.296  q95=0.483  rec@thr=0.000  prec@thr=0.000  F1@thr=0.000  ROC=0.9412 PR=0.3333\n",
      "NR-ER-LBD     pos=0.135  thr=0.894  frac≥thr=0.054  q90=0.430  q95=0.840  rec@thr=0.400  prec@thr=1.000  F1@thr=0.571  ROC=0.9062 PR=0.8126\n",
      "NR-ER         pos=0.167  thr=0.825  frac≥thr=0.056  q90=0.653  q95=0.785  rec@thr=0.333  prec@thr=1.000  F1@thr=0.500  ROC=0.8889 PR=0.8456\n",
      "SR-p53        pos=0.121  thr=0.684  frac≥thr=0.061  q90=0.332  q95=0.590  rec@thr=0.500  prec@thr=1.000  F1@thr=0.667  ROC=0.9828 PR=0.9167\n",
      "SR-MMP        pos=0.185  thr=0.697  frac≥thr=0.111  q90=0.668  q95=0.839  rec@thr=0.600  prec@thr=1.000  F1@thr=0.750  ROC=0.8182 PR=0.8017\n",
      "SR-ARE        pos=0.265  thr=0.707  frac≥thr=0.118  q90=0.726  q95=0.814  rec@thr=0.444  prec@thr=1.000  F1@thr=0.615  ROC=0.8489 PR=0.7806\n",
      "\n",
      "=== COPY-PASTE DEBUG BLOCK ===\n",
      "{\n",
      "  \"macro\": {\n",
      "    \"full\": {\n",
      "      \"macro_roc\": 0.9352664001384727,\n",
      "      \"macro_pr\": 0.7907946926367978,\n",
      "      \"macro_f1\": 0.25862332112332115,\n",
      "      \"macro_recall\": 0.1898148148148148,\n",
      "      \"macro_precision\": 0.4166666666666667\n",
      "    },\n",
      "    \"no_desc\": {\n",
      "      \"macro_roc\": 0.7827272862924256,\n",
      "      \"macro_pr\": 0.40335523657232114,\n",
      "      \"macro_f1\": 0.0,\n",
      "      \"macro_recall\": 0.0,\n",
      "      \"macro_precision\": 0.0\n",
      "    },\n",
      "    \"text_min\": {\n",
      "      \"macro_roc\": 0.9344571714883351,\n",
      "      \"macro_pr\": 0.7905649334139035,\n",
      "      \"macro_f1\": 0.25862332112332115,\n",
      "      \"macro_recall\": 0.1898148148148148,\n",
      "      \"macro_precision\": 0.4166666666666667\n",
      "    },\n",
      "    \"graph_min\": {\n",
      "      \"macro_roc\": 0.9407735201518626,\n",
      "      \"macro_pr\": 0.8094712480219726,\n",
      "      \"macro_f1\": 0.3323412698412698,\n",
      "      \"macro_recall\": 0.2638888888888889,\n",
      "      \"macro_precision\": 0.5\n",
      "    }\n",
      "  },\n",
      "  \"topk\": {\n",
      "    \"full\": {\n",
      "      \"1\": 0.6875,\n",
      "      \"3\": 0.875\n",
      "    },\n",
      "    \"no_desc\": {\n",
      "      \"1\": 0.5625,\n",
      "      \"3\": 0.75\n",
      "    },\n",
      "    \"text_min\": {\n",
      "      \"1\": 0.6875,\n",
      "      \"3\": 0.875\n",
      "    },\n",
      "    \"graph_min\": {\n",
      "      \"1\": 0.6875,\n",
      "      \"3\": 0.875\n",
      "    }\n",
      "  },\n",
      "  \"per_label_full\": [\n",
      "    {\n",
      "      \"label\": \"NR-AR\",\n",
      "      \"n\": 38,\n",
      "      \"pos_rate\": 0.05263157933950424,\n",
      "      \"thr\": 0.7629274129867554,\n",
      "      \"frac_above\": 0.0,\n",
      "      \"q50\": 0.03570426069200039,\n",
      "      \"q90\": 0.0656491979956627,\n",
      "      \"q95\": 0.08513461686670772,\n",
      "      \"recall\": 0.0,\n",
      "      \"precision\": 0.0,\n",
      "      \"f1\": 0.0,\n",
      "      \"roc\": 1.0,\n",
      "      \"pr\": 1.0\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"NR-AR-LBD\",\n",
      "      \"n\": 35,\n",
      "      \"pos_rate\": 0.02857142873108387,\n",
      "      \"thr\": 0.8190221786499023,\n",
      "      \"frac_above\": 0.0,\n",
      "      \"q50\": 0.006929417606443167,\n",
      "      \"q90\": 0.03292481005191804,\n",
      "      \"q95\": 0.05229223817586891,\n",
      "      \"recall\": 0.0,\n",
      "      \"precision\": 0.0,\n",
      "      \"f1\": 0.0,\n",
      "      \"roc\": 1.0,\n",
      "      \"pr\": 1.0\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"NR-AhR\",\n",
      "      \"n\": 32,\n",
      "      \"pos_rate\": 0.125,\n",
      "      \"thr\": 0.7818915843963623,\n",
      "      \"frac_above\": 0.0,\n",
      "      \"q50\": 0.020428738556802273,\n",
      "      \"q90\": 0.044460529834032064,\n",
      "      \"q95\": 0.2641112796962258,\n",
      "      \"recall\": 0.0,\n",
      "      \"precision\": 0.0,\n",
      "      \"f1\": 0.0,\n",
      "      \"roc\": 0.9642857142857143,\n",
      "      \"pr\": 0.875\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"NR-Aromatase\",\n",
      "      \"n\": 27,\n",
      "      \"pos_rate\": 0.0,\n",
      "      \"thr\": 0.8253428936004639,\n",
      "      \"frac_above\": 0.0,\n",
      "      \"q50\": 0.01448653545230627,\n",
      "      \"q90\": 0.024995713680982603,\n",
      "      \"q95\": 0.028506378084421156,\n",
      "      \"recall\": 0.0,\n",
      "      \"precision\": 0.0,\n",
      "      \"f1\": 0.0,\n",
      "      \"roc\": NaN,\n",
      "      \"pr\": NaN\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"NR-ER\",\n",
      "      \"n\": 36,\n",
      "      \"pos_rate\": 0.1666666716337204,\n",
      "      \"thr\": 0.8251241445541382,\n",
      "      \"frac_above\": 0.05555555555555555,\n",
      "      \"q50\": 0.4102940112352371,\n",
      "      \"q90\": 0.6534601747989655,\n",
      "      \"q95\": 0.7846175879240036,\n",
      "      \"recall\": 0.3333333333333333,\n",
      "      \"precision\": 1.0,\n",
      "      \"f1\": 0.5,\n",
      "      \"roc\": 0.888888888888889,\n",
      "      \"pr\": 0.8455555555555555\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"NR-ER-LBD\",\n",
      "      \"n\": 37,\n",
      "      \"pos_rate\": 0.13513512909412384,\n",
      "      \"thr\": 0.8941881656646729,\n",
      "      \"frac_above\": 0.05405405405405406,\n",
      "      \"q50\": 0.0857422798871994,\n",
      "      \"q90\": 0.4301204562187194,\n",
      "      \"q95\": 0.8396100282669062,\n",
      "      \"recall\": 0.4,\n",
      "      \"precision\": 1.0,\n",
      "      \"f1\": 0.5714285714285714,\n",
      "      \"roc\": 0.90625,\n",
      "      \"pr\": 0.8126315789473684\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"NR-PPAR-gamma\",\n",
      "      \"n\": 33,\n",
      "      \"pos_rate\": 0.03030303120613098,\n",
      "      \"thr\": 0.8727028369903564,\n",
      "      \"frac_above\": 0.0,\n",
      "      \"q50\": 0.003987912088632584,\n",
      "      \"q90\": 0.02334195859730244,\n",
      "      \"q95\": 0.09110959172248838,\n",
      "      \"recall\": 0.0,\n",
      "      \"precision\": 0.0,\n",
      "      \"f1\": 0.0,\n",
      "      \"roc\": 0.9375,\n",
      "      \"pr\": 0.3333333333333333\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"SR-ARE\",\n",
      "      \"n\": 34,\n",
      "      \"pos_rate\": 0.2647058963775635,\n",
      "      \"thr\": 0.7072502970695496,\n",
      "      \"frac_above\": 0.11764705882352941,\n",
      "      \"q50\": 0.4222787171602249,\n",
      "      \"q90\": 0.7259517490863799,\n",
      "      \"q95\": 0.8135533928871155,\n",
      "      \"recall\": 0.4444444444444444,\n",
      "      \"precision\": 1.0,\n",
      "      \"f1\": 0.6153846153846154,\n",
      "      \"roc\": 0.8488888888888888,\n",
      "      \"pr\": 0.780554484501853\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"SR-ATAD5\",\n",
      "      \"n\": 38,\n",
      "      \"pos_rate\": 0.02631578966975212,\n",
      "      \"thr\": 0.8526983261108398,\n",
      "      \"frac_above\": 0.0,\n",
      "      \"q50\": 0.01590711483731866,\n",
      "      \"q90\": 0.16993887871503865,\n",
      "      \"q95\": 0.5776035606861114,\n",
      "      \"recall\": 0.0,\n",
      "      \"precision\": 0.0,\n",
      "      \"f1\": 0.0,\n",
      "      \"roc\": 1.0,\n",
      "      \"pr\": 1.0\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"SR-HSE\",\n",
      "      \"n\": 35,\n",
      "      \"pos_rate\": 0.02857142873108387,\n",
      "      \"thr\": 0.7752707004547119,\n",
      "      \"frac_above\": 0.02857142857142857,\n",
      "      \"q50\": 0.09997601062059402,\n",
      "      \"q90\": 0.2955489158630371,\n",
      "      \"q95\": 0.48347100913524615,\n",
      "      \"recall\": 0.0,\n",
      "      \"precision\": 0.0,\n",
      "      \"f1\": 0.0,\n",
      "      \"roc\": 0.9411764705882353,\n",
      "      \"pr\": 0.3333333333333333\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"SR-MMP\",\n",
      "      \"n\": 27,\n",
      "      \"pos_rate\": 0.18518517911434174,\n",
      "      \"thr\": 0.6973726749420166,\n",
      "      \"frac_above\": 0.1111111111111111,\n",
      "      \"q50\": 0.0496363490819931,\n",
      "      \"q90\": 0.6677203774452215,\n",
      "      \"q95\": 0.8389404296875,\n",
      "      \"recall\": 0.6,\n",
      "      \"precision\": 1.0,\n",
      "      \"f1\": 0.75,\n",
      "      \"roc\": 0.8181818181818182,\n",
      "      \"pr\": 0.8016666666666667\n",
      "    },\n",
      "    {\n",
      "      \"label\": \"SR-p53\",\n",
      "      \"n\": 33,\n",
      "      \"pos_rate\": 0.12121212482452393,\n",
      "      \"thr\": 0.683661162853241,\n",
      "      \"frac_above\": 0.06060606060606061,\n",
      "      \"q50\": 0.08159676194190979,\n",
      "      \"q90\": 0.3318705379962921,\n",
      "      \"q95\": 0.5902242362499228,\n",
      "      \"recall\": 0.5,\n",
      "      \"precision\": 1.0,\n",
      "      \"f1\": 0.6666666666666666,\n",
      "      \"roc\": 0.9827586206896551,\n",
      "      \"pr\": 0.9166666666666666\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json, math, re\n",
    "from types import SimpleNamespace\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "for name in [\"LABELS\",\"TOKENIZER\",\"model\",\"descriptors_for_smiles_list\",\"_mol_to_pyg_20d\",\n",
    "             \"RESULTS_DIR\",\"RUN_ID\"]:\n",
    "    assert name in globals(), f\"{name} not found — please run the Option-B evaluation cell first.\"\n",
    "\n",
    "DEVICE = next(model.parameters()).device\n",
    "MAX_LEN = 256\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "EVAL_FILE = Path(\"tox21_dualenc_v1/data/raw/Test.xlsx\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# --- thresholds loader (saved Stage-B) ---\n",
    "def _load_thresholds():\n",
    "    p = (RESULTS_DIR / \"eval\" / \"stage_B\" / f\"{RUN_ID}_thresholds.json\")\n",
    "    if p.exists():\n",
    "        try:\n",
    "            d = json.loads(p.read_text())\n",
    "            th = d.get(\"thresholds\")\n",
    "            if th is not None and len(th) == len(LABELS):\n",
    "                return np.asarray(th, dtype=np.float32)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.full((len(LABELS),), 0.5, dtype=np.float32)\n",
    "\n",
    "TH_SAVED = _load_thresholds()\n",
    "\n",
    "# --- helpers: batching with overrides for ablations ---\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Batch as PyGBatch, Data as PyGData\n",
    "\n",
    "def _dummy_graph():\n",
    "    # single node with self-edge\n",
    "    x = torch.zeros((1, 20), dtype=torch.float32)  # 20-d atom feat placeholder\n",
    "    edge_index = torch.tensor([[0],[0]], dtype=torch.long)\n",
    "    return PyGData(x=x, edge_index=edge_index)\n",
    "\n",
    "def _build_batch(smiles_list, desc_np, mode=\"full\", start_idx=0):\n",
    "    toks = TOKENIZER(smiles_list, padding=True, truncation=True,\n",
    "                     max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    input_ids = toks[\"input_ids\"]\n",
    "    attn_mask = toks[\"attention_mask\"]\n",
    "\n",
    "    if mode == \"text_min\":\n",
    "        # Replace text with minimal BOS/EOS + pads so attention_mask==2\n",
    "        pad = TOKENIZER.pad_token_id\n",
    "        bos = TOKENIZER.bos_token_id if TOKENIZER.bos_token_id is not None else pad\n",
    "        eos = TOKENIZER.eos_token_id if TOKENIZER.eos_token_id is not None else pad\n",
    "        input_ids = torch.full_like(input_ids, pad)\n",
    "        attn_mask = torch.zeros_like(attn_mask)\n",
    "        input_ids[:, 0] = bos\n",
    "        if input_ids.shape[1] > 1:\n",
    "            input_ids[:, 1] = eos\n",
    "            attn_mask[:, :2] = 1\n",
    "        else:\n",
    "            attn_mask[:, 0] = 1\n",
    "\n",
    "    # Graphs\n",
    "    mols = []\n",
    "    for s in smiles_list:\n",
    "        m = Chem.MolFromSmiles(s)\n",
    "        mols.append(m if m is not None else Chem.MolFromSmiles(\"C\"))\n",
    "\n",
    "    if mode == \"graph_min\":\n",
    "        graphs = [_dummy_graph() for _ in mols]\n",
    "    else:\n",
    "        graphs = []\n",
    "        for m in mols:\n",
    "            graphs.append(_mol_to_pyg_20d(m))\n",
    "    pyg_batch = PyGBatch.from_data_list(graphs)\n",
    "\n",
    "    # Descriptors\n",
    "    if mode == \"no_desc\":\n",
    "        desc = torch.zeros((len(smiles_list), desc_np.shape[1]), dtype=torch.float32)\n",
    "    else:\n",
    "        sl = slice(start_idx, start_idx + len(smiles_list))\n",
    "        desc = torch.tensor(desc_np[sl, :], dtype=torch.float32)\n",
    "\n",
    "    # Labels/mask placeholders\n",
    "    C = len(LABELS)\n",
    "    labels = torch.zeros((len(smiles_list), C), dtype=torch.float32)\n",
    "    label_mask = torch.ones((len(smiles_list), C), dtype=torch.bool)\n",
    "\n",
    "    # To device\n",
    "    batch = SimpleNamespace()\n",
    "    batch.text = {\"input_ids\": input_ids.to(DEVICE), \"attention_mask\": attn_mask.to(DEVICE)}\n",
    "    batch.graph = pyg_batch.to(DEVICE)\n",
    "    batch.desc = desc.to(DEVICE)\n",
    "    batch.labels = labels.to(DEVICE)\n",
    "    batch.label_mask = label_mask.to(DEVICE)\n",
    "    return batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_mode(smiles, Y_true_df, mask_np, desc_matrix, mode=\"full\", compute_aux=False):\n",
    "    model.eval()\n",
    "    probs = np.zeros((len(smiles), len(LABELS)), dtype=np.float32)\n",
    "    attn_stats = {}\n",
    "    for s in range(0, len(smiles), BATCH_SIZE):\n",
    "        chunk = smiles[s:s+BATCH_SIZE]\n",
    "        batch = _build_batch(chunk, desc_matrix, mode=mode, start_idx=s)\n",
    "        out = model(batch, compute_aux=compute_aux)\n",
    "        logits = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX)\n",
    "        probs[s:s+len(chunk), :] = torch.sigmoid(logits).cpu().numpy()\n",
    "\n",
    "        if compute_aux and (\"attn_t2g\" in out or \"attn_g2t\" in out):\n",
    "            # summarize attention peaky-ness (entropy & max)\n",
    "            def _summ(attn):\n",
    "                # attn shape ~ [B, T, G] or [B, G, T]\n",
    "                a = attn.detach().float().softmax(dim=-1).cpu().numpy()\n",
    "                # entropy over last dim\n",
    "                eps = 1e-12\n",
    "                ent = -(a * np.log(a + eps)).sum(axis=-1)  # [B,T] or [B,G]\n",
    "                mx = a.max(axis=-1)\n",
    "                return dict(ent_mean=float(ent.mean()), ent_std=float(ent.std()),\n",
    "                            max_mean=float(mx.mean()), max_std=float(mx.std()))\n",
    "            if \"attn_t2g\" in out:\n",
    "                attn_stats.setdefault(\"t2g\", []).append(_summ(out[\"attn_t2g\"]))\n",
    "            if \"attn_g2t\" in out:\n",
    "                attn_stats.setdefault(\"g2t\", []).append(_summ(out[\"attn_g2t\"]))\n",
    "    # aggregate attn stats\n",
    "    if attn_stats:\n",
    "        for k,v in attn_stats.items():\n",
    "            attn_stats[k] = {\n",
    "                \"ent_mean\": float(np.mean([d[\"ent_mean\"] for d in v])),\n",
    "                \"ent_std\":  float(np.mean([d[\"ent_std\"] for d in v])),\n",
    "                \"max_mean\": float(np.mean([d[\"max_mean\"] for d in v])),\n",
    "                \"max_std\":  float(np.mean([d[\"max_std\"] for d in v])),\n",
    "            }\n",
    "    return probs, attn_stats\n",
    "\n",
    "def safe_auc(y, p, kind=\"roc\"):\n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    if len(y) < 3 or np.all(y==0) or np.all(y==1):\n",
    "        return float(\"nan\")\n",
    "    try:\n",
    "        return float(roc_auc_score(y,p)) if kind==\"roc\" else float(average_precision_score(y,p))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "def eval_at_thresholds(Y, P, mask, thresholds):\n",
    "    from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "    C = P.shape[1]\n",
    "    out_rows = []\n",
    "    for j in range(C):\n",
    "        m = mask[:, j]\n",
    "        y = Y[m, j].astype(np.float32)\n",
    "        p = P[m, j].astype(np.float32)\n",
    "        thr = float(thresholds[j])\n",
    "        h = (p >= thr).astype(np.int32)\n",
    "        n = int(m.sum())\n",
    "        pos_rate = float(y.mean()) if n>0 else float(\"nan\")\n",
    "        prec = float(precision_score(y, h, zero_division=0))\n",
    "        rec  = float(recall_score(y, h, zero_division=0))\n",
    "        f1   = float(f1_score(y, h, zero_division=0))\n",
    "        # distribution vs threshold\n",
    "        q50 = float(np.nanpercentile(p, 50)) if n>0 else float(\"nan\")\n",
    "        q90 = float(np.nanpercentile(p, 90)) if n>0 else float(\"nan\")\n",
    "        q95 = float(np.nanpercentile(p, 95)) if n>0 else float(\"nan\")\n",
    "        frac_above = float((p >= thr).mean()) if n>0 else 0.0\n",
    "        roc = safe_auc(y,p,\"roc\")\n",
    "        pr  = safe_auc(y,p,\"pr\")\n",
    "        out_rows.append(dict(label=LABELS[j], n=n, pos_rate=pos_rate, thr=thr,\n",
    "                             frac_above=frac_above, q50=q50, q90=q90, q95=q95,\n",
    "                             recall=rec, precision=prec, f1=f1, roc=roc, pr=pr))\n",
    "    # macros\n",
    "    macro = dict(\n",
    "        macro_roc = float(np.nanmean([r[\"roc\"] for r in out_rows])),\n",
    "        macro_pr  = float(np.nanmean([r[\"pr\"]  for r in out_rows])),\n",
    "        macro_f1  = float(np.nanmean([r[\"f1\"]  for r in out_rows])),\n",
    "        macro_recall = float(np.nanmean([r[\"recall\"] for r in out_rows])),\n",
    "        macro_precision = float(np.nanmean([r[\"precision\"] for r in out_rows]))\n",
    "    )\n",
    "    return out_rows, macro\n",
    "\n",
    "def topk_recall(Y, P, mask, ks=(1,3)):\n",
    "    \"\"\"Recall@k over labels per-sample: whether any true positive label is within the top-k probs.\"\"\"\n",
    "    ks = tuple(sorted(set(ks)))\n",
    "    hit = {k: [] for k in ks}\n",
    "    for i in range(Y.shape[0]):\n",
    "        mrow = mask[i]\n",
    "        # indices of labels we evaluate for this row (valid labels)\n",
    "        cols = np.where(mrow)[0]\n",
    "        if cols.size == 0:\n",
    "            continue\n",
    "        y = Y[i, cols]\n",
    "        if not (y==1).any():\n",
    "            continue\n",
    "        p = P[i, cols]\n",
    "        order = np.argsort(-p)\n",
    "        for k in ks:\n",
    "            topk = set(cols[order[:min(k, len(cols))]])\n",
    "            true_pos_labels = set(cols[np.where(y==1)[0]])\n",
    "            hit[k].append(1 if len(topk & true_pos_labels)>0 else 0)\n",
    "    return {k: (float(np.mean(hit[k])) if hit[k] else float(\"nan\")) for k in ks}\n",
    "\n",
    "# --- load external file & truths ---\n",
    "assert EVAL_FILE.exists(), f\"Missing file: {EVAL_FILE}\"\n",
    "df = pd.read_excel(EVAL_FILE) if EVAL_FILE.suffix.lower() in (\".xlsx\",\".xls\") else pd.read_csv(EVAL_FILE)\n",
    "\n",
    "smiles_col = None\n",
    "for c in df.columns:\n",
    "    if str(c).strip().lower() in (\"smiles\",\"smile\",\"smiles_string\"):\n",
    "        smiles_col = c; break\n",
    "assert smiles_col is not None, \"Could not find a SMILES column.\"\n",
    "\n",
    "label_cols = [lbl for lbl in LABELS if lbl in df.columns]\n",
    "assert len(label_cols)>0, f\"No known Tox21 labels found. Expected a subset of: {LABELS}\"\n",
    "\n",
    "Y_true = df[label_cols].copy()\n",
    "mask = pd.DataFrame(True, index=Y_true.index, columns=label_cols)\n",
    "for c in label_cols:\n",
    "    v = Y_true[c].values\n",
    "    ok = np.isin(v, [0,1,0.0,1.0,True,False])\n",
    "    mask.loc[~ok, c] = False\n",
    "    Y_true.loc[~ok, c] = 0\n",
    "Y_true = Y_true.astype(float).values\n",
    "mask_np = mask.values.astype(bool)\n",
    "\n",
    "SMI = df[smiles_col].astype(str).tolist()\n",
    "desc_full = descriptors_for_smiles_list(SMI)  # aligned & standardized\n",
    "\n",
    "# --- run four modes ---\n",
    "modes = [\"full\",\"no_desc\",\"text_min\",\"graph_min\"]\n",
    "results = {}\n",
    "for md in modes:\n",
    "    probs, attn_stats = run_mode(SMI, Y_true, mask_np, desc_full, mode=md, compute_aux=(md==\"full\"))\n",
    "    # align thresholds: use saved thresholds for ALL 12; we evaluate only present labels\n",
    "    th = TH_SAVED.copy()\n",
    "    # evaluate on the subset of labels present\n",
    "    idxs = [LABELS.index(c) for c in label_cols]\n",
    "    rows, macro = eval_at_thresholds(Y_true[:, idxs], probs[:, idxs], mask_np[:, idxs], th[idxs])\n",
    "    tk = topk_recall(Y_true[:, idxs], probs[:, idxs], mask_np[:, idxs], ks=(1,3))\n",
    "    results[md] = dict(per_label=rows, macro=macro, topk=tk, attn=attn_stats, probs=probs[:, idxs])\n",
    "\n",
    "# --- pretty print summary ---\n",
    "def _fmt(x):\n",
    "    return \"nan\" if (x is None or (isinstance(x,float) and (math.isnan(x) or math.isinf(x)))) else f\"{x:.4f}\"\n",
    "\n",
    "print(f\"\\n===== DIAGNOSTICS on {EVAL_FILE.name} (n={len(SMI)}) =====\")\n",
    "for md in modes:\n",
    "    m = results[md][\"macro\"]; tk = results[md][\"topk\"]\n",
    "    print(f\"[{md}] macro ROC={_fmt(m['macro_roc'])}  PR={_fmt(m['macro_pr'])}  F1@thr={_fmt(m['macro_f1'])}  \"\n",
    "          f\"Recall@thr={_fmt(m['macro_recall'])}  Precision@thr={_fmt(m['macro_precision'])}  \"\n",
    "          f\"R@1={_fmt(tk.get(1))} R@3={_fmt(tk.get(3))}\")\n",
    "    if md==\"full\" and results[md][\"attn\"]:\n",
    "        a = results[md][\"attn\"]\n",
    "        print(f\"   attn t2g: max_mean={_fmt(a.get('t2g',{}).get('max_mean'))}  ent_mean={_fmt(a.get('t2g',{}).get('ent_mean'))}  \"\n",
    "              f\"| g2t: max_mean={_fmt(a.get('g2t',{}).get('max_mean'))}  ent_mean={_fmt(a.get('g2t',{}).get('ent_mean'))}\")\n",
    "\n",
    "print(\"\\n--- PER-LABEL THRESHOLD DIAGNOSTICS (full mode) ---\")\n",
    "rows = results[\"full\"][\"per_label\"]\n",
    "# sort labels where very few scores exceed thr (potential over-tight thresholding)\n",
    "rows_sorted = sorted(rows, key=lambda r: r[\"frac_above\"])\n",
    "for r in rows_sorted:\n",
    "    print(f\"{r['label']:<13} pos={r['pos_rate']:.3f}  thr={r['thr']:.3f}  \"\n",
    "          f\"frac≥thr={r['frac_above']:.3f}  q90={r['q90']:.3f}  q95={r['q95']:.3f}  \"\n",
    "          f\"rec@thr={r['recall']:.3f}  prec@thr={r['precision']:.3f}  F1@thr={r['f1']:.3f}  \"\n",
    "          f\"ROC={_fmt(r['roc'])} PR={_fmt(r['pr'])}\")\n",
    "\n",
    "# --- minimal copy-paste debug block for me ---\n",
    "dbg = {\n",
    "    \"macro\": {md: results[md][\"macro\"] for md in modes},\n",
    "    \"topk\":  {md: results[md][\"topk\"]  for md in modes},\n",
    "    \"per_label_full\": results[\"full\"][\"per_label\"][:],  # subset already aligned\n",
    "}\n",
    "print(\"\\n=== COPY-PASTE DEBUG BLOCK ===\")\n",
    "print(json.dumps(dbg, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52dfe563",
   "metadata": {},
   "source": [
    "## 13: post-hoc calibration (DIDNT WORK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0b70a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Collecting validation logits/labels...\n",
      "[INFO] val shapes: logits=(783, 12), y=(783, 12), mask=(783, 12)\n",
      "[INFO] Fitting Platt scalers per label & re-deriving thresholds on VAL...\n",
      "[SAVED] Platt params & thresholds → tox21_dualenc_v1/results/v4/eval/calibration\n",
      "\n",
      "[BEFORE] Using saved thresholds only:\n",
      "{'macro_roc': 0.9352664001384727, 'macro_pr': 0.7907946926367978, 'macro_f1': 0.25862332112332115}\n",
      "\n",
      "[AFTER] Using Platt-calibrated probs + val-tuned thresholds:\n",
      "{'macro_roc': 0.9352664001384727, 'macro_pr': 0.7907946926367978, 'macro_f1': 0.2463684191625368}\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "# --- prerequisites we expect from previous cells ---\n",
    "assert 'model' in globals() and 'val_loader' in globals(), \"Run training cells to define model and val_loader.\"\n",
    "assert 'LABELS' in globals() and 'RESULTS_DIR' in globals() and 'RUN_ID' in globals() and 'TOKENIZER' in globals()\n",
    "DEVICE = next(model.parameters()).device\n",
    "model.eval()\n",
    "\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "\n",
    "def _collect_logits_labels(loader):\n",
    "    all_logits, all_y, all_m = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            out = model(batch, compute_aux=False)\n",
    "            logits = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX)\n",
    "            y = batch.labels.float()\n",
    "            m = batch.label_mask.bool()\n",
    "            all_logits.append(logits.detach().cpu())\n",
    "            all_y.append(y.detach().cpu())\n",
    "            all_m.append(m.detach().cpu())\n",
    "    L = torch.cat(all_logits, dim=0).numpy()\n",
    "    Y = torch.cat(all_y, dim=0).numpy()\n",
    "    M = torch.cat(all_m, dim=0).numpy()\n",
    "    return L, Y, M\n",
    "\n",
    "print(\"[INFO] Collecting validation logits/labels...\")\n",
    "val_logits, val_y, val_m = _collect_logits_labels(val_loader)  # [N, C]\n",
    "C = len(LABELS)\n",
    "print(f\"[INFO] val shapes: logits={val_logits.shape}, y={val_y.shape}, mask={val_m.shape}\")\n",
    "\n",
    "# --- Per-label Platt scaling: optimize A,B in sigmoid(A*logit + B) to minimize BCE on valid labels\n",
    "def fit_platt(z, y):\n",
    "    # z: logits [n], y: {0,1} [n]\n",
    "    z_t = torch.tensor(z, dtype=torch.float32, device='cpu')\n",
    "    y_t = torch.tensor(y, dtype=torch.float32, device='cpu')\n",
    "    A = torch.tensor(1.0, requires_grad=True)\n",
    "    B = torch.tensor(0.0, requires_grad=True)\n",
    "    opt = torch.optim.LBFGS([A, B], lr=1.0, max_iter=200, line_search_fn=\"strong_wolfe\")\n",
    "\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def closure():\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        s = A * z_t + B\n",
    "        loss = bce(s, y_t)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "\n",
    "    try:\n",
    "        opt.step(closure)\n",
    "    except Exception:\n",
    "        # Fallback small Adam if LBFGS fails\n",
    "        opt2 = torch.optim.Adam([A, B], lr=0.05)\n",
    "        for _ in range(400):\n",
    "            opt2.zero_grad(set_to_none=True)\n",
    "            s = A * z_t + B\n",
    "            loss = bce(s, y_t)\n",
    "            loss.backward()\n",
    "            opt2.step()\n",
    "    A_f = float(A.detach().cpu())\n",
    "    B_f = float(B.detach().cpu())\n",
    "    return A_f, B_f\n",
    "\n",
    "platt = {}\n",
    "thr_cal = {}\n",
    "pre_thr = {}\n",
    "\n",
    "# Load saved Stage-B thresholds as baseline\n",
    "def _load_saved_thresholds():\n",
    "    p = RESULTS_DIR / \"eval\" / \"stage_B\" / f\"{RUN_ID}_thresholds.json\"\n",
    "    if p.exists():\n",
    "        try:\n",
    "            d = json.loads(p.read_text())\n",
    "            if \"thresholds\" in d and len(d[\"thresholds\"]) == C:\n",
    "                return np.array(d[\"thresholds\"], dtype=np.float32)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return np.full((C,), 0.5, dtype=np.float32)\n",
    "\n",
    "thr_saved = _load_saved_thresholds()\n",
    "\n",
    "def best_thr_max_f1(y, p):\n",
    "    # y in {0,1}, p in [0,1]\n",
    "    xs = np.unique(np.concatenate([p, [0.0, 0.5, 1.0]]))\n",
    "    best_f1, best_t = -1.0, 0.5\n",
    "    for t in xs:\n",
    "        h = (p >= t).astype(np.int32)\n",
    "        f1 = f1_score(y, h, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, float(t)\n",
    "    return best_t, best_f1\n",
    "\n",
    "print(\"[INFO] Fitting Platt scalers per label & re-deriving thresholds on VAL...\")\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    m = val_m[:, j].astype(bool)\n",
    "    z = val_logits[m, j]\n",
    "    y = val_y[m, j]\n",
    "    # Guard: if all y are same class, skip calibration & thresholding\n",
    "    if (len(y) < 3) or (np.all(y == 0) or np.all(y == 1)):\n",
    "        platt[lbl] = {\"A\": 1.0, \"B\": 0.0}\n",
    "        pre_thr[lbl] = float(thr_saved[j])\n",
    "        thr_cal[lbl] = float(thr_saved[j])\n",
    "        continue\n",
    "    # Fit A,B\n",
    "    A, B = fit_platt(z, y)\n",
    "    platt[lbl] = {\"A\": A, \"B\": B}\n",
    "    # Before calibration thresholds (baseline)\n",
    "    p_pre = 1 / (1 + np.exp(-z))\n",
    "    t_pre, _ = best_thr_max_f1(y, p_pre)\n",
    "    pre_thr[lbl] = float(t_pre)\n",
    "    # After calibration\n",
    "    p_cal = 1 / (1 + np.exp(-(A * z + B)))\n",
    "    t_cal, _ = best_thr_max_f1(y, p_cal)\n",
    "    thr_cal[lbl] = float(t_cal)\n",
    "\n",
    "# Save calibration artifacts\n",
    "calib_dir = RESULTS_DIR / \"eval\" / \"calibration\"\n",
    "calib_dir.mkdir(parents=True, exist_ok=True)\n",
    "(calib_dir / f\"{RUN_ID}_platt.json\").write_text(json.dumps(platt, indent=2))\n",
    "(calib_dir / f\"{RUN_ID}_thr_calibrated.json\").write_text(json.dumps({\"thresholds\": [thr_cal[l] for l in LABELS]}, indent=2))\n",
    "(calib_dir / f\"{RUN_ID}_thr_preval.json\").write_text(json.dumps({\"thresholds\": [pre_thr[l] for l in LABELS]}, indent=2))\n",
    "print(f\"[SAVED] Platt params & thresholds → {calib_dir.as_posix()}\")\n",
    "\n",
    "# --- helper to evaluate external file quickly with optional calibration/thresholds ---\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Batch as PyGBatch\n",
    "def eval_file_with(calib=None, thresholds=None, file_path=Path(\"tox21_dualenc_v1/data/raw/Test.xlsx\")):\n",
    "    # Reuse Option-B helpers in your session\n",
    "    assert 'descriptors_for_smiles_list' in globals() and '_mol_to_pyg_20d' in globals()\n",
    "    import pandas as pd\n",
    "    if file_path.suffix.lower() in (\".xlsx\", \".xls\"):\n",
    "        df = pd.read_excel(file_path)\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "    # Find smiles + labels\n",
    "    smiles_col = next((c for c in df.columns if str(c).strip().lower() in (\"smiles\", \"smile\", \"smiles_string\")), None)\n",
    "    assert smiles_col, \"No SMILES column.\"\n",
    "    label_cols = [lbl for lbl in LABELS if lbl in df.columns]\n",
    "    Y = df[label_cols].copy()\n",
    "    M = np.ones_like(Y.values, dtype=bool)\n",
    "    for c in label_cols:\n",
    "        v = Y[c].values\n",
    "        ok = np.isin(v, [0, 1, 0.0, 1.0, True, False])\n",
    "        M[:, Y.columns.get_loc(c)] = ok\n",
    "        Y.loc[~ok, c] = 0\n",
    "    Y = Y.values.astype(float)\n",
    "\n",
    "    SMI = df[smiles_col].astype(str).tolist()\n",
    "    desc_mat = descriptors_for_smiles_list(SMI)\n",
    "\n",
    "    probs = np.zeros((len(SMI), len(LABELS)), dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for s in range(0, len(SMI), 64):\n",
    "            chunk = SMI[s:s + 64]\n",
    "            # batch build (same as Option-B)\n",
    "            toks = TOKENIZER(chunk, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "            input_ids = toks[\"input_ids\"].to(DEVICE)\n",
    "            attn_mask = toks[\"attention_mask\"].to(DEVICE)\n",
    "            mols = [Chem.MolFromSmiles(x) or Chem.MolFromSmiles(\"C\") for x in chunk]\n",
    "            graphs = [_mol_to_pyg_20d(m) for m in mols]\n",
    "            pyg_batch = PyGBatch.from_data_list(graphs).to(DEVICE)\n",
    "            desc = torch.tensor(desc_mat[s:s + len(chunk), :], dtype=torch.float32, device=DEVICE)\n",
    "            Cn = len(LABELS)\n",
    "            batch = type(\"B\", (), {})()\n",
    "            batch.text = {\"input_ids\": input_ids, \"attention_mask\": attn_mask}\n",
    "            batch.graph = pyg_batch\n",
    "            batch.desc = desc\n",
    "            batch.labels = torch.zeros((len(chunk), Cn), dtype=torch.float32, device=DEVICE)\n",
    "            batch.label_mask = torch.ones((len(chunk), Cn), dtype=torch.bool, device=DEVICE)\n",
    "            out = model(batch, compute_aux=False)\n",
    "            z = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX).cpu().numpy()\n",
    "            if calib is not None:\n",
    "                # apply per-label Platt\n",
    "                for j, lbl in enumerate(LABELS):\n",
    "                    A = calib.get(lbl, {}).get(\"A\", 1.0)\n",
    "                    B = calib.get(lbl, {}).get(\"B\", 0.0)\n",
    "                    z[:, j] = A * z[:, j] + B\n",
    "            p = 1 / (1 + np.exp(-z))\n",
    "            probs[s:s + len(chunk), :] = p\n",
    "\n",
    "    idxs = [LABELS.index(c) for c in label_cols]\n",
    "    P = probs[:, idxs]\n",
    "    if thresholds:\n",
    "        T = np.array([thresholds.get(lbl, 0.5) for lbl in label_cols], dtype=np.float32)\n",
    "    else:\n",
    "        T = np.full((len(idxs),), 0.5, dtype=np.float32)\n",
    "    H = (P >= T[None, :]).astype(int)\n",
    "\n",
    "    # metrics\n",
    "    roc_list, pr_list, f1_list = [], [], []\n",
    "    for j in range(len(idxs)):\n",
    "        m = M[:, j]\n",
    "        y = Y[m, j].astype(np.float32)\n",
    "        p = P[m, j].astype(np.float32)\n",
    "        h = H[m, j].astype(np.int32)\n",
    "        if m.sum() > 0:\n",
    "            f1_list.append(float(f1_score(y, h, zero_division=0)))\n",
    "            # ROC\n",
    "            if (len(y) >= 3) and not (np.all(y == 0) or np.all(y == 1)):\n",
    "                try:\n",
    "                    r = float(roc_auc_score(y, p))\n",
    "                    roc_list.append(r)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                try:\n",
    "                    q = float(average_precision_score(y, p))\n",
    "                    pr_list.append(q)\n",
    "                except Exception:\n",
    "                    pass\n",
    "    return dict(\n",
    "        macro_roc=float(np.mean(roc_list)) if roc_list else float(\"nan\"),\n",
    "        macro_pr=float(np.mean(pr_list)) if pr_list else float(\"nan\"),\n",
    "        macro_f1=float(np.mean(f1_list)) if f1_list else float(\"nan\"),\n",
    "    )\n",
    "\n",
    "# Load saved Stage-B thresholds and eval Test.xlsx BEFORE\n",
    "thr_saved_map = {lbl: float(x) for lbl, x in zip(LABELS, _load_saved_thresholds())}\n",
    "print(\"\\n[BEFORE] Using saved thresholds only:\")\n",
    "res_before = eval_file_with(calib=None, thresholds=thr_saved_map)\n",
    "print(res_before)\n",
    "\n",
    "# AFTER: use Platt + calibrated thresholds\n",
    "thr_cal_map = {lbl: thr_cal[lbl] for lbl in LABELS}\n",
    "print(\"\\n[AFTER] Using Platt-calibrated probs + val-tuned thresholds:\")\n",
    "res_after = eval_file_with(calib=platt, thresholds=thr_cal_map)\n",
    "print(res_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc996eb",
   "metadata": {},
   "source": [
    "## 14: Threshold policy sweep (val-tuned) + external evaluation scoreboard "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af160c87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] VAL calibrated shapes: P=(783, 12), Y=(783, 12), M=(783, 12)\n",
      "[INFO] Derived threshold sets: ['saved', 'maxF1_val', 'prec>=0.70', 'rec>=0.60', 'global_val', 'saved*x0.30']\n",
      "\n",
      "===== THRESHOLD POLICY SCOREBOARD (calibrated probs) =====\n",
      "rec>=0.60    | VAL F1=0.269 | EXT F1=0.330 | EXT ROC=0.922 | EXT PR=0.623\n",
      "global_val   | VAL F1=0.326 | EXT F1=0.289 | EXT ROC=0.922 | EXT PR=0.623\n",
      "maxF1_val    | VAL F1=0.380 | EXT F1=0.220 | EXT ROC=0.922 | EXT PR=0.623\n",
      "saved*x0.30  | VAL F1=0.280 | EXT F1=0.199 | EXT ROC=0.922 | EXT PR=0.623\n",
      "prec>=0.70   | VAL F1=0.238 | EXT F1=0.021 | EXT ROC=0.922 | EXT PR=0.623\n",
      "saved        | VAL F1=0.014 | EXT F1=0.000 | EXT ROC=0.922 | EXT PR=0.623\n",
      "\n",
      "[SAVED] Best policy 'rec>=0.60' thresholds → tox21_dualenc_v1/results/v4/eval/calibration\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score, average_precision_score\n",
    "\n",
    "assert 'model' in globals() and 'val_loader' in globals(), \"Need model & val_loader from earlier cells.\"\n",
    "assert 'LABELS' in globals() and 'RESULTS_DIR' in globals() and 'RUN_ID' in globals(), \"Missing globals.\"\n",
    "DEVICE = next(model.parameters()).device\n",
    "model.eval()\n",
    "\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "EVAL_FILE = Path(\"tox21_dualenc_v1/data/raw/Test.xlsx\")\n",
    "\n",
    "def _json_load(p: Path):\n",
    "    try: return json.loads(p.read_text())\n",
    "    except Exception: return None\n",
    "\n",
    "def _load_saved_thresholds():\n",
    "    p = RESULTS_DIR / \"eval\" / \"stage_B\" / f\"{RUN_ID}_thresholds.json\"\n",
    "    if p.exists():\n",
    "        d = _json_load(p)\n",
    "        if d and \"thresholds\" in d and len(d[\"thresholds\"]) == len(LABELS):\n",
    "            return np.asarray(d[\"thresholds\"], dtype=np.float32)\n",
    "    return np.full((len(LABELS),), 0.5, dtype=np.float32)\n",
    "\n",
    "def _load_platt():\n",
    "    p = RESULTS_DIR / \"eval\" / \"calibration\" / f\"{RUN_ID}_platt.json\"\n",
    "    d = _json_load(p) or {}\n",
    "    # ensure A,B for every label\n",
    "    out = {}\n",
    "    for i,lbl in enumerate(LABELS):\n",
    "        x = d.get(lbl, {})\n",
    "        out[lbl] = {\"A\": float(x.get(\"A\", 1.0)), \"B\": float(x.get(\"B\", 0.0))}\n",
    "    return out\n",
    "\n",
    "thr_saved = _load_saved_thresholds()\n",
    "platt = _load_platt()\n",
    "\n",
    "# --- collect VAL logits/labels/mask and make calibrated probabilities ---\n",
    "@torch.no_grad()\n",
    "def _collect_val():\n",
    "    logits, Y, M = [], [], []\n",
    "    for batch in val_loader:\n",
    "        out = model(batch, compute_aux=False)\n",
    "        z = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX).cpu().numpy()\n",
    "        # apply Platt per label\n",
    "        for j,lbl in enumerate(LABELS):\n",
    "            A, B = platt[lbl][\"A\"], platt[lbl][\"B\"]\n",
    "            z[:, j] = A * z[:, j] + B\n",
    "        p = 1 / (1 + np.exp(-z))\n",
    "        logits.append(p)\n",
    "        Y.append(batch.labels.detach().cpu().numpy())\n",
    "        M.append(batch.label_mask.detach().cpu().numpy())\n",
    "    P = np.concatenate(logits, axis=0)  # [N, C] calibrated probs\n",
    "    Y = np.concatenate(Y, axis=0).astype(np.float32)\n",
    "    M = np.concatenate(M, axis=0).astype(bool)\n",
    "    return P, Y, M\n",
    "\n",
    "val_P, val_Y, val_M = _collect_val()\n",
    "print(f\"[INFO] VAL calibrated shapes: P={val_P.shape}, Y={val_Y.shape}, M={val_M.shape}\")\n",
    "\n",
    "def _safe_pr_curve(y, p):\n",
    "    try:\n",
    "        return precision_recall_curve(y, p)\n",
    "    except Exception:\n",
    "        return np.array([0.0]), np.array([0.0]), np.array([])\n",
    "\n",
    "# --- threshold policies ---\n",
    "def thr_maxF1_perlabel(P, Y, M):\n",
    "    C = P.shape[1]\n",
    "    thr = np.zeros(C, dtype=np.float32)\n",
    "    for j in range(C):\n",
    "        m = M[:, j]; y = Y[m, j].astype(np.float32); p = P[m, j].astype(np.float32)\n",
    "        if len(y) < 3 or np.all(y==0) or np.all(y==1):\n",
    "            thr[j] = 0.5; continue\n",
    "        t_candidates = np.unique(np.concatenate([p, [0.0, 0.5, 1.0]]))\n",
    "        best_f1, best_t = -1.0, 0.5\n",
    "        for t in t_candidates:\n",
    "            h = (p >= t).astype(np.int32)\n",
    "            f1 = f1_score(y, h, zero_division=0)\n",
    "            if f1 > best_f1: best_f1, best_t = f1, float(t)\n",
    "        thr[j] = best_t\n",
    "    return thr\n",
    "\n",
    "def thr_prec_at_least(P, Y, M, target_prec=0.70):\n",
    "    C = P.shape[1]\n",
    "    thr = np.zeros(C, dtype=np.float32)\n",
    "    for j in range(C):\n",
    "        m = M[:, j]; y = Y[m, j].astype(np.float32); p = P[m, j].astype(np.float32)\n",
    "        if len(y) < 3 or np.all(y==0) or np.all(y==1):\n",
    "            thr[j] = 0.5; continue\n",
    "        prec, rec, t = _safe_pr_curve(y, p)\n",
    "        # precision_recall_curve returns thresholds aligned to rec[:-1]\n",
    "        # find any with precision >= target, choose the one with max recall\n",
    "        mask_ok = prec[:-1] >= target_prec\n",
    "        if mask_ok.any():\n",
    "            idx = np.argmax(rec[:-1][mask_ok])\n",
    "            thr[j] = float(t[mask_ok][idx])\n",
    "        else:\n",
    "            # fallback to max-F1\n",
    "            thr[j] = thr_maxF1_perlabel(P, Y, M)[j]\n",
    "    return thr\n",
    "\n",
    "def thr_recall_at_least(P, Y, M, target_rec=0.60):\n",
    "    C = P.shape[1]\n",
    "    thr = np.zeros(C, dtype=np.float32)\n",
    "    for j in range(C):\n",
    "        m = M[:, j]; y = Y[m, j].astype(np.float32); p = P[m, j].astype(np.float32)\n",
    "        if len(y) < 3 or np.all(y==0) or np.all(y==1):\n",
    "            thr[j] = 0.5; continue\n",
    "        prec, rec, t = _safe_pr_curve(y, p)\n",
    "        mask_ok = rec[:-1] >= target_rec\n",
    "        if mask_ok.any():\n",
    "            # among those, pick the one with highest precision\n",
    "            idx = np.argmax(prec[:-1][mask_ok])\n",
    "            thr[j] = float(t[mask_ok][idx])\n",
    "        else:\n",
    "            # fallback to max-F1\n",
    "            thr[j] = thr_maxF1_perlabel(P, Y, M)[j]\n",
    "    return thr\n",
    "\n",
    "def thr_global_by_val(P, Y, M, grid=np.linspace(0.05, 0.9, 36)):\n",
    "    best_t, best_f1 = 0.5, -1.0\n",
    "    for t in grid:\n",
    "        f = []\n",
    "        for j in range(P.shape[1]):\n",
    "            m = M[:, j]; y = Y[m, j].astype(np.float32); p = P[m, j].astype(np.float32)\n",
    "            h = (p >= t).astype(np.int32)\n",
    "            f.append(f1_score(y, h, zero_division=0))\n",
    "        mf = float(np.mean(f))\n",
    "        if mf > best_f1: best_f1, best_t = mf, float(t)\n",
    "    return np.full((P.shape[1],), best_t, dtype=np.float32)\n",
    "\n",
    "def thr_scaled_saved(saved_thr, scales=(0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0), P=None, Y=None, M=None):\n",
    "    # evaluate macro-F1 on val to pick best scale\n",
    "    best_s, best_f = 1.0, -1.0\n",
    "    for s in scales:\n",
    "        tvec = saved_thr * s\n",
    "        f = []\n",
    "        for j in range(P.shape[1]):\n",
    "            m = M[:, j]; y = Y[m, j].astype(np.float32); p = P[m, j].astype(np.float32)\n",
    "            h = (p >= tvec[j]).astype(np.int32)\n",
    "            f.append(f1_score(y, h, zero_division=0))\n",
    "        mf = float(np.mean(f))\n",
    "        if mf > best_f: best_f, best_s = mf, s\n",
    "    return saved_thr * best_s, best_s, best_f\n",
    "\n",
    "thr_sets = {}\n",
    "thr_sets[\"saved\"] = thr_saved.copy()\n",
    "thr_sets[\"maxF1_val\"] = thr_maxF1_perlabel(val_P, val_Y, val_M)\n",
    "thr_sets[\"prec>=0.70\"] = thr_prec_at_least(val_P, val_Y, val_M, target_prec=0.70)\n",
    "thr_sets[\"rec>=0.60\"] = thr_recall_at_least(val_P, val_Y, val_M, target_rec=0.60)\n",
    "thr_sets[\"global_val\"] = thr_global_by_val(val_P, val_Y, val_M)\n",
    "thr_scaled, best_s, best_s_f1 = thr_scaled_saved(thr_saved, P=val_P, Y=val_Y, M=val_M)\n",
    "thr_sets[f\"saved*x{best_s:.2f}\"] = thr_scaled\n",
    "\n",
    "print(\"[INFO] Derived threshold sets:\", list(thr_sets.keys()))\n",
    "\n",
    "# --- helper: evaluate external file using calibrated probs + chosen thresholds ---\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Batch as PyGBatch\n",
    "def _eval_external(th_map, file_path=EVAL_FILE):\n",
    "    # Load file & build descriptors (reuse Option-B function)\n",
    "    assert 'descriptors_for_smiles_list' in globals() and '_mol_to_pyg_20d' in globals()\n",
    "    import pandas as pd\n",
    "    if file_path.suffix.lower() in (\".xlsx\",\".xls\"):\n",
    "        df = pd.read_excel(file_path)\n",
    "    else:\n",
    "        df = pd.read_csv(file_path)\n",
    "    smiles_col = next((c for c in df.columns if str(c).strip().lower() in (\"smiles\",\"smile\",\"smiles_string\")), None)\n",
    "    assert smiles_col, \"No SMILES column.\"\n",
    "    label_cols = [lbl for lbl in LABELS if lbl in df.columns]\n",
    "    Y = df[label_cols].copy()\n",
    "    M = np.ones_like(Y.values, dtype=bool)\n",
    "    for c in label_cols:\n",
    "        v = Y[c].values\n",
    "        ok = np.isin(v, [0,1,0.0,1.0,True,False])\n",
    "        M[:, Y.columns.get_loc(c)] = ok\n",
    "        Y.loc[~ok, c] = 0\n",
    "    Y = Y.values.astype(np.float32)\n",
    "    SMI = df[smiles_col].astype(str).tolist()\n",
    "\n",
    "    # descriptors & forward\n",
    "    desc_mat = descriptors_for_smiles_list(SMI)\n",
    "    probs = np.zeros((len(SMI), len(LABELS)), dtype=np.float32)\n",
    "    with torch.no_grad():\n",
    "        for s in range(0, len(SMI), 64):\n",
    "            chunk = SMI[s:s+64]\n",
    "            toks = TOKENIZER(chunk, padding=True, truncation=True, max_length=256, return_tensors=\"pt\")\n",
    "            ids = toks[\"input_ids\"].to(DEVICE); attn = toks[\"attention_mask\"].to(DEVICE)\n",
    "            mols = [Chem.MolFromSmiles(x) or Chem.MolFromSmiles(\"C\") for x in chunk]\n",
    "            graphs = [_mol_to_pyg_20d(m) for m in mols]\n",
    "            pyg_batch = PyGBatch.from_data_list(graphs).to(DEVICE)\n",
    "            desc = torch.tensor(desc_mat[s:s+len(chunk), :], dtype=torch.float32, device=DEVICE)\n",
    "            Cn = len(LABELS)\n",
    "            batch = type(\"B\", (), {})()\n",
    "            batch.text = {\"input_ids\": ids, \"attention_mask\": attn}\n",
    "            batch.graph = pyg_batch; batch.desc = desc\n",
    "            batch.labels = torch.zeros((len(chunk), Cn), dtype=torch.float32, device=DEVICE)\n",
    "            batch.label_mask = torch.ones((len(chunk), Cn), dtype=torch.bool, device=DEVICE)\n",
    "            out = model(batch, compute_aux=False)\n",
    "            z = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX).cpu().numpy()\n",
    "            # apply Platt\n",
    "            for j,lbl in enumerate(LABELS):\n",
    "                A, B = platt[lbl][\"A\"], platt[lbl][\"B\"]\n",
    "                z[:, j] = A*z[:, j] + B\n",
    "            p = 1 / (1 + np.exp(-z))\n",
    "            probs[s:s+len(chunk), :] = p\n",
    "\n",
    "    # metric on labels present\n",
    "    idxs = [LABELS.index(c) for c in label_cols]\n",
    "    P = probs[:, idxs]\n",
    "    T = np.array([th_map[j] for j in idxs], dtype=np.float32)  # th_map indexed by label index\n",
    "    H = (P >= T[None,:]).astype(np.int32)\n",
    "\n",
    "    # macro F1 / ROC / PR\n",
    "    roc_list, pr_list, f1_list = [], [], []\n",
    "    for j in range(len(idxs)):\n",
    "        y = Y[:, j]; p = P[:, j]; h = H[:, j]\n",
    "        f1_list.append(float(f1_score(y, h, zero_division=0)))\n",
    "        if (len(y) >= 3) and not (np.all(y==0) or np.all(y==1)):\n",
    "            try: roc_list.append(float(roc_auc_score(y, p)))\n",
    "            except: pass\n",
    "            try: pr_list.append(float(average_precision_score(y, p)))\n",
    "            except: pass\n",
    "    return dict(\n",
    "        macro_roc=float(np.mean(roc_list)) if roc_list else float(\"nan\"),\n",
    "        macro_pr=float(np.mean(pr_list)) if pr_list else float(\"nan\"),\n",
    "        macro_f1=float(np.mean(f1_list)) if f1_list else float(\"nan\"),\n",
    "    )\n",
    "\n",
    "# --- Scoreboard ---\n",
    "rows = []\n",
    "for name, thr_vec in thr_sets.items():\n",
    "    # build indexable map by label index\n",
    "    th_map = {i: float(thr_vec[i]) for i in range(len(LABELS))}\n",
    "    res_ext = _eval_external(th_map, EVAL_FILE)\n",
    "    # also compute macro-F1 on VAL for reference\n",
    "    f_val = []\n",
    "    for j in range(len(LABELS)):\n",
    "        m = val_M[:, j]\n",
    "        y = val_Y[m, j]; p = val_P[m, j]\n",
    "        h = (p >= thr_vec[j]).astype(np.int32)\n",
    "        f_val.append(float(f1_score(y, h, zero_division=0)))\n",
    "    rows.append({\n",
    "        \"policy\": name,\n",
    "        \"val_macro_f1\": float(np.mean(f_val)),\n",
    "        \"ext_macro_f1\": res_ext[\"macro_f1\"],\n",
    "        \"ext_macro_roc\": res_ext[\"macro_roc\"],\n",
    "        \"ext_macro_pr\": res_ext[\"macro_pr\"],\n",
    "    })\n",
    "\n",
    "# Pretty print\n",
    "rows_sorted = sorted(rows, key=lambda r: r[\"ext_macro_f1\"], reverse=True)\n",
    "print(\"\\n===== THRESHOLD POLICY SCOREBOARD (calibrated probs) =====\")\n",
    "for r in rows_sorted:\n",
    "    print(f\"{r['policy']:<12} | VAL F1={r['val_macro_f1']:.3f} | EXT F1={r['ext_macro_f1']:.3f} \"\n",
    "          f\"| EXT ROC={r['ext_macro_roc']:.3f} | EXT PR={r['ext_macro_pr']:.3f}\")\n",
    "\n",
    "# Optional: save the best policy thresholds\n",
    "best = rows_sorted[0][\"policy\"]\n",
    "best_thr_vec = thr_sets[best]\n",
    "out_dir = RESULTS_DIR / \"eval\" / \"calibration\"\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "(out_dir / f\"{RUN_ID}_thresholds_{best.replace('>=','ge').replace(' ','_')}.json\").write_text(\n",
    "    json.dumps({\"thresholds\": [float(x) for x in best_thr_vec.tolist()]}, indent=2)\n",
    ")\n",
    "print(f\"\\n[SAVED] Best policy '{best}' thresholds → {out_dir.as_posix()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ee7923",
   "metadata": {},
   "source": [
    "## 15: Calibrated interactive test (rec>=0.60 thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca39ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Thresholds file: v4_20250902_165736_thresholds_recge0.60.json\n",
      "----- CALIBRATED PREDICTIONS (policy: rec≥0.60) -----\n",
      "[0] CCOc1ccc2nc(S(N)(=O)=O)sc2c1\n",
      "  + NR-ER         p=0.094  thr=0.075\n",
      "  top-5: SR-ARE:0.139, NR-ER:0.094, SR-p53:0.048, SR-MMP:0.029, SR-HSE:0.023\n",
      "[1] CCN1C(=O)NC(c2ccccc2)C1=O\n",
      "  + NR-ER         p=0.095  thr=0.075\n",
      "  top-5: SR-ARE:0.104, NR-ER:0.095, SR-p53:0.028, NR-AR:0.014, SR-HSE:0.013\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.0088159 , 0.00455074, 0.01129054, 0.01334403, 0.09356246,\n",
       "         0.01752037, 0.0073212 , 0.13889676, 0.02086532, 0.02336168,\n",
       "         0.02903289, 0.04786992],\n",
       "        [0.01404578, 0.00446969, 0.00320449, 0.00934877, 0.09450033,\n",
       "         0.01205815, 0.00452459, 0.1036837 , 0.01003969, 0.01343455,\n",
       "         0.01214845, 0.02768941]], dtype=float32),\n",
       " array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "\n",
    "need = [\"model\",\"LABELS\",\"TOKENIZER\",\"descriptors_for_smiles_list\",\"_mol_to_pyg_20d\",\"RESULTS_DIR\",\"RUN_ID\"]\n",
    "for n in need: assert n in globals(), f\"{n} missing — run earlier cells first.\"\n",
    "DEVICE = next(model.parameters()).device\n",
    "model.eval()\n",
    "MAX_LEN = 256\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "\n",
    "def _json_load(p: Path):\n",
    "    try: return json.loads(p.read_text())\n",
    "    except Exception: return None\n",
    "\n",
    "def _load_platt():\n",
    "    p = RESULTS_DIR / \"eval\" / \"calibration\" / f\"{RUN_ID}_platt.json\"\n",
    "    d = _json_load(p) or {}\n",
    "    out = {}\n",
    "    for lbl in LABELS:\n",
    "        x = d.get(lbl, {})\n",
    "        out[lbl] = {\"A\": float(x.get(\"A\", 1.0)), \"B\": float(x.get(\"B\", 0.0))}\n",
    "    return out\n",
    "\n",
    "def _load_rec60_thresholds():\n",
    "    # try the exact saved filename pattern from Cell 16\n",
    "    caldir = RESULTS_DIR / \"eval\" / \"calibration\"\n",
    "    cands = list(caldir.glob(f\"{RUN_ID}_thresholds_recge*.json\"))\n",
    "    if not cands:\n",
    "        # fallback: any thresholds_* file (pick most recent)\n",
    "        cands = sorted(list(caldir.glob(f\"{RUN_ID}_thresholds_*.json\")), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "    if cands:\n",
    "        d = _json_load(cands[0]) or {}\n",
    "        arr = d.get(\"thresholds\")\n",
    "        if isinstance(arr, list) and len(arr) == len(LABELS):\n",
    "            print(f\"[INFO] Thresholds file: {cands[0].name}\")\n",
    "            return np.asarray(arr, dtype=np.float32)\n",
    "    # final fallback: stage-B saved thresholds\n",
    "    sb = RESULTS_DIR / \"eval\" / \"stage_B\" / f\"{RUN_ID}_thresholds.json\"\n",
    "    d2 = _json_load(sb) or {}\n",
    "    arr2 = d2.get(\"thresholds\", [0.5]*len(LABELS))\n",
    "    print(\"[WARN] rec>=0.60 thresholds not found — falling back to stage_B thresholds.\")\n",
    "    return np.asarray(arr2, dtype=np.float32)\n",
    "\n",
    "PLATT = _load_platt()\n",
    "THRESH = _load_rec60_thresholds()\n",
    "\n",
    "# --- batch builder (Option-B style) ---\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Batch as PyGBatch\n",
    "\n",
    "def _build_batch(smiles_list, desc_mat, start=0):\n",
    "    toks = TOKENIZER(smiles_list, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    ids = toks[\"input_ids\"].to(DEVICE); attn = toks[\"attention_mask\"].to(DEVICE)\n",
    "    mols = [Chem.MolFromSmiles(s) or Chem.MolFromSmiles(\"C\") for s in smiles_list]\n",
    "    graphs = [_mol_to_pyg_20d(m) for m in mols]\n",
    "    pyg = PyGBatch.from_data_list(graphs).to(DEVICE)\n",
    "    desc = torch.tensor(desc_mat[start:start+len(smiles_list), :], dtype=torch.float32, device=DEVICE)\n",
    "\n",
    "    B, C = len(smiles_list), len(LABELS)\n",
    "    batch = SimpleNamespace()\n",
    "    batch.text = {\"input_ids\": ids, \"attention_mask\": attn}\n",
    "    batch.graph = pyg; batch.desc = desc\n",
    "    batch.labels = torch.zeros((B, C), dtype=torch.float32, device=DEVICE)\n",
    "    batch.label_mask = torch.ones((B, C), dtype=torch.bool, device=DEVICE)\n",
    "    return batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_calibrated(smiles_list, topk=5, batch_size=64):\n",
    "    smiles_list = [s.strip() for s in smiles_list if s and s.strip()]\n",
    "    if not smiles_list:\n",
    "        print(\"[HINT] Provide at least one SMILES string.\")\n",
    "        return\n",
    "    # descriptors via Option-B (lookup+compute → impute → standardize)\n",
    "    desc_mat = descriptors_for_smiles_list(smiles_list)\n",
    "\n",
    "    probs = np.zeros((len(smiles_list), len(LABELS)), dtype=np.float32)\n",
    "    for s in range(0, len(smiles_list), batch_size):\n",
    "        chunk = smiles_list[s:s+batch_size]\n",
    "        batch = _build_batch(chunk, desc_mat, start=s)\n",
    "        out = model(batch, compute_aux=False)\n",
    "        z = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX).cpu().numpy()\n",
    "        # Apply Platt per-label: z' = A*z + B\n",
    "        for j, lbl in enumerate(LABELS):\n",
    "            A = PLATT[lbl][\"A\"]; B = PLATT[lbl][\"B\"]\n",
    "            z[:, j] = A * z[:, j] + B\n",
    "        p = 1 / (1 + np.exp(-z))\n",
    "        probs[s:s+len(chunk), :] = p\n",
    "\n",
    "    # thresholded predictions\n",
    "    yhat = (probs >= THRESH[None, :]).astype(np.int32)\n",
    "\n",
    "    # Print concise results\n",
    "    print(\"----- CALIBRATED PREDICTIONS (policy: rec≥0.60) -----\")\n",
    "    for i, smi in enumerate(smiles_list):\n",
    "        pos = [(LABELS[j], float(probs[i, j]), float(THRESH[j])) for j in range(len(LABELS)) if yhat[i, j] == 1]\n",
    "        pos.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"[{i}] {smi}\")\n",
    "        if pos:\n",
    "            for name, p, t in pos:\n",
    "                print(f\"  + {name:<12}  p={p:.3f}  thr={t:.3f}\")\n",
    "        else:\n",
    "            print(\"  —\")\n",
    "        # top-k glance\n",
    "        pairs = [(LABELS[j], float(probs[i, j])) for j in range(len(LABELS))]\n",
    "        pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        tops = \", \".join([f\"{n}:{v:.3f}\" for n, v in pairs[:topk]])\n",
    "        print(f\"  top-{topk}: {tops}\")\n",
    "    return probs, yhat\n",
    "\n",
    "# === Try it: paste SMILES below (one per element); examples included ===\n",
    "SMILES_TEST = [\n",
    "    # your earlier examples:\n",
    "    \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",      # known NR-AhR=1, SR-ARE=1 in your sheet\n",
    "    \"CCN1C(=O)NC(c2ccccc2)C1=O\",\n",
    "    # add more if you like...\n",
    "]\n",
    "predict_calibrated(SMILES_TEST, topk=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64270fb",
   "metadata": {},
   "source": [
    "## 16: Stage C — Fusion-Boosted Fine-Tune (modality dropout + focal BCE + deeper LLRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37acfed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LLRD] Found HF encoder with 12 layers via layout='encoder.layer'.\n",
      "[LLRD] Param groups: 7 | sizes=[7087872, 7087872, 7087872, 7087872, 593154, 399884, 64572673] | lrs=[1e-05, 1.5e-05, 2e-05, 3e-05, 0.0001, 0.0001, 0.0002]\n",
      "[INFO 19:25:06] Stage C init | epochs=40 | warmup=0.10 | drop(text/graph/desc)=0.2/0.2/0.3 | AMP=True(torch.bfloat16)\n",
      "[DEBUG e0 i0] logits min/max: -7.469/1.219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_21456\\3242689129.py:315: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=AMP, growth_interval=200)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E00 C] train_loss=0.5103 (bce≈0.3256, nce=0.482, l1=0.000) | val_roc=0.7678 val_pr=0.2708 | 15.6s | drops(text:29/196 graph:42/196 desc:60/196)\n",
      "  ↳ Saved BEST Stage-C checkpoint: v4_20250902_165736_stageC_best.pt (val_macro_pr=0.2708)\n",
      "[DEBUG e1 i0] logits min/max: -9.188/3.078\n",
      "[E01 C] train_loss=0.4513 (bce≈0.3723, nce=0.428, l1=0.000) | val_roc=0.7641 val_pr=0.2672 | 15.5s | drops(text:30/196 graph:38/196 desc:57/196)\n",
      "[DEBUG e2 i0] logits min/max: -8.625/3.141\n",
      "[E02 C] train_loss=0.4673 (bce≈0.4040, nce=0.446, l1=0.000) | val_roc=0.7606 val_pr=0.2638 | 15.2s | drops(text:36/196 graph:36/196 desc:60/196)\n",
      "[DEBUG e3 i0] logits min/max: -7.844/1.672\n",
      "[E03 C] train_loss=0.4688 (bce≈0.4131, nce=0.449, l1=0.000) | val_roc=0.7563 val_pr=0.2588 | 15.3s | drops(text:30/196 graph:45/196 desc:48/196)\n",
      "[DEBUG e4 i0] logits min/max: -7.062/4.844\n",
      "[E04 C] train_loss=0.4450 (bce≈0.4380, nce=0.425, l1=0.000) | val_roc=0.7544 val_pr=0.2580 | 15.2s | drops(text:44/196 graph:30/196 desc:75/196)\n",
      "[DEBUG e5 i0] logits min/max: -7.562/1.594\n",
      "[E05 C] train_loss=0.4747 (bce≈0.4320, nce=0.455, l1=0.000) | val_roc=0.7523 val_pr=0.2557 | 15.2s | drops(text:37/196 graph:42/196 desc:64/196)\n",
      "[DEBUG e6 i0] logits min/max: -7.125/4.219\n",
      "[E06 C] train_loss=0.4214 (bce≈0.4277, nce=0.402, l1=0.000) | val_roc=0.7503 val_pr=0.2532 | 15.9s | drops(text:38/196 graph:38/196 desc:59/196)\n",
      "[EARLY STOP] No PR improvement for 6 epochs. Stopping at epoch 6.\n",
      "[DONE] Stage C finished. Best val_macro_pr=0.2708\n",
      "Checkpoints → tox21_dualenc_v1/models/checkpoints_v4 | Logs → tox21_dualenc_v1/results/v4/v4_20250902_165736_stageC_log.jsonl\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math, random, time\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Data as PyGData, Batch as PyGBatch\n",
    "\n",
    "# --- prerequisites created in earlier cells ---\n",
    "need = [\"DualEncCoAttnModel\",\"CONFIG\",\"RUN_ID\",\"CKPT_DIR\",\"RESULTS_DIR\",\"train_loader\",\"val_loader\",\"LABELS\",\"TOKENIZER\"]\n",
    "for n in need:\n",
    "    assert n in globals(), f\"Missing global: {n}. Please run earlier cells.\"\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP = True\n",
    "AMP_DTYPE = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "\n",
    "# ---------- Hyperparams ----------\n",
    "EPOCHS = 40\n",
    "WARMUP = 0.10\n",
    "P_TEXT_DROP = 0.20\n",
    "P_GRAPH_DROP = 0.20\n",
    "P_DESC_DROP  = 0.30\n",
    "FOCAL_GAMMA = 1.5\n",
    "L_NCE   = 0.25    # upweight alignment\n",
    "L_ATTL1 = 0.01\n",
    "AGREE_PROB = 0.0  # set to 0.25 to add a consistency KL with no-desc auxiliary pass\n",
    "\n",
    "# LLRD for last 4 transformer layers\n",
    "LR_TEXT_L3  = 1e-5     # layer L-4\n",
    "LR_TEXT_L2  = 1.5e-5   # layer L-3\n",
    "LR_TEXT_L1  = 2e-5     # layer L-2\n",
    "LR_TEXT_L0  = 3e-5     # layer L-1 (top)\n",
    "LR_PROJ     = 1e-4     # text/graph/desc projection heads, fusion, cross-attn, head\n",
    "LR_OTHERS   = 2e-4     # everything else\n",
    "WD          = 0.01\n",
    "EMA_DECAY   = 0.999\n",
    "\n",
    "def _now(): return time.strftime(\"%H:%M:%S\")\n",
    "\n",
    "# ---------- Losses ----------\n",
    "class FocalBCEWithLogitsLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-label focal BCE with:\n",
    "      - per-label alpha in [0,1] (shape [C])\n",
    "      - gamma (scalar, e.g., 1.5)\n",
    "      - pos_weight like BCEWithLogits (tensor [C], on same device)\n",
    "    Reduction = mean over valid entries (mask) across batch & classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, alpha: torch.Tensor, gamma: float, pos_weight: torch.Tensor):\n",
    "        super().__init__()\n",
    "        self.register_buffer(\"alpha\", alpha)           # [C]\n",
    "        self.gamma = float(gamma)\n",
    "        self.register_buffer(\"pos_weight\", pos_weight) # [C]\n",
    "\n",
    "    def forward(self, logits, targets, mask: torch.Tensor):\n",
    "        logits = logits.clamp_(CLAMP_MIN, CLAMP_MAX)\n",
    "        probs  = torch.sigmoid(logits)\n",
    "        y = targets\n",
    "        pw = self.pos_weight  # [C]\n",
    "\n",
    "        # base BCE (manual pos_weight)\n",
    "        loss_pos = -y * torch.log(probs + 1e-8) * pw\n",
    "        loss_neg = -(1 - y) * torch.log(1 - probs + 1e-8)\n",
    "        bce = loss_pos + loss_neg  # [B,C]\n",
    "\n",
    "        # focal modulation\n",
    "        pt = torch.where(y > 0.5, probs, 1.0 - probs)\n",
    "        alpha_t = torch.where(y > 0.5, self.alpha, 1.0 - self.alpha)\n",
    "        focal_w = alpha_t * torch.pow(1.0 - pt, self.gamma)\n",
    "        loss = focal_w * bce\n",
    "\n",
    "        loss = loss[mask].mean() if mask is not None else loss.mean()\n",
    "        return loss\n",
    "\n",
    "def bce_preview(logits, targets, mask):\n",
    "    # FP32 BCE preview for logging (no focal/pos_weight) — safe & informative\n",
    "    with torch.no_grad():\n",
    "        l = logits.detach().float().clamp_(CLAMP_MIN, CLAMP_MAX)\n",
    "        p = torch.sigmoid(l)\n",
    "        y = targets.detach().float()\n",
    "        b = F.binary_cross_entropy(p, y, reduction=\"none\")\n",
    "        if mask is not None: b = b[mask]\n",
    "        return float(b.mean().cpu())\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_p, all_y, all_m = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            out = model(batch, compute_aux=False)\n",
    "            z = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX)\n",
    "            p = torch.sigmoid(z).cpu().numpy()\n",
    "            y = batch.labels.detach().cpu().numpy()\n",
    "            m = batch.label_mask.detach().cpu().numpy().astype(bool)\n",
    "            all_p.append(p); all_y.append(y); all_m.append(m)\n",
    "    P = np.concatenate(all_p, 0); Y = np.concatenate(all_y, 0); M = np.concatenate(all_m, 0)\n",
    "    rocs, prs = [], []\n",
    "    for j in range(P.shape[1]):\n",
    "        m = M[:, j]\n",
    "        y = Y[m, j]; p = P[m, j]\n",
    "        if len(y) >= 3 and not (np.all(y==0) or np.all(y==1)):\n",
    "            try: rocs.append(roc_auc_score(y, p))\n",
    "            except: pass\n",
    "            try: prs.append(average_precision_score(y, p))\n",
    "            except: pass\n",
    "    return dict(macro_roc=float(np.mean(rocs)) if rocs else float(\"nan\"),\n",
    "                macro_pr=float(np.mean(prs)) if prs else float(\"nan\"),\n",
    "                bce_preview=float(F.binary_cross_entropy(torch.tensor(P), torch.tensor(Y)).item()))\n",
    "\n",
    "# ---------- Prevalence & pos_weight ----------\n",
    "def _estimate_prevalence(loader):\n",
    "    tot = None; n = 0\n",
    "    for i, b in enumerate(loader):\n",
    "        y = b.labels.detach().float().sum(dim=0).cpu().numpy()\n",
    "        if tot is None: tot = y\n",
    "        else: tot += y\n",
    "        n += b.labels.shape[0]\n",
    "        if i >= 64:  # quick estimate is fine\n",
    "            break\n",
    "    prev = (tot / max(1, n)).astype(np.float32)  # [C]\n",
    "    prev = np.clip(prev, 1e-4, 1-1e-4)\n",
    "    return prev\n",
    "\n",
    "PREV = _estimate_prevalence(train_loader)\n",
    "alpha_vec = torch.tensor(1.0 - PREV, dtype=torch.float32, device=DEVICE)  # emphasize positives in rare labels\n",
    "\n",
    "# pull pos_weight tensor from earlier cells (CUDA)\n",
    "posw = None\n",
    "for name in [\"POS_WEIGHT_CUDA\", \"POS_WEIGHT_CLIPPED_CUDA\", \"POS_WEIGHT\", \"POS_WEIGHT_CLIPPED\"]:\n",
    "    if name in globals():\n",
    "        w = globals()[name]\n",
    "        if isinstance(w, torch.Tensor):\n",
    "            posw = w.to(DEVICE).float()\n",
    "            break\n",
    "if posw is None:\n",
    "    posw = torch.ones((len(LABELS),), device=DEVICE)\n",
    "\n",
    "crit_focal = FocalBCEWithLogitsLoss(alpha=alpha_vec, gamma=FOCAL_GAMMA, pos_weight=posw)\n",
    "\n",
    "# ---------- Modality dropout helpers ----------\n",
    "def _text_minimize(batch):\n",
    "    pad = TOKENIZER.pad_token_id\n",
    "    bos = TOKENIZER.bos_token_id if TOKENIZER.bos_token_id is not None else pad\n",
    "    eos = TOKENIZER.eos_token_id if TOKENIZER.eos_token_id is not None else pad\n",
    "    ids = batch.text[\"input_ids\"]\n",
    "    att = batch.text[\"attention_mask\"]\n",
    "    ids.fill_(pad)\n",
    "    att.zero_()\n",
    "    ids[:, 0] = bos\n",
    "    if ids.shape[1] > 1:\n",
    "        ids[:, 1] = eos\n",
    "        att[:, :2] = 1\n",
    "    else:\n",
    "        att[:, 0] = 1\n",
    "\n",
    "def _graph_minimize_like(batch):\n",
    "    B = batch.desc.shape[0]\n",
    "    x = torch.zeros((1, 20), dtype=torch.float32, device=DEVICE)\n",
    "    ei = torch.tensor([[0],[0]], dtype=torch.long, device=DEVICE)\n",
    "    dummy = PyGData(x=x, edge_index=ei)\n",
    "    new_batch = PyGBatch.from_data_list([dummy.clone() for _ in range(B)]).to(DEVICE)\n",
    "    batch.graph = new_batch\n",
    "\n",
    "def _desc_zero(batch):\n",
    "    batch.desc.zero_()\n",
    "\n",
    "# ---------- Robust LLRD optimizer over wrapped HF encoders ----------\n",
    "def _locate_hf_text_module(txt_module: nn.Module):\n",
    "    \"\"\"\n",
    "    Return (module_with_layers, layout_string) where layout is \"encoder.layer\" or \"transformer.layer\"\n",
    "    or a dotted path to a .layer ModuleList inside the wrapper.\n",
    "    \"\"\"\n",
    "    # 1) Direct\n",
    "    if hasattr(txt_module, \"encoder\") and hasattr(txt_module.encoder, \"layer\"):\n",
    "        return txt_module, \"encoder.layer\"\n",
    "    if hasattr(txt_module, \"transformer\") and hasattr(txt_module.transformer, \"layer\"):\n",
    "        return txt_module, \"transformer.layer\"\n",
    "    # 2) Common inner attrs\n",
    "    for attr in (\"roberta\", \"bert\", \"deberta\", \"xlm_roberta\", \"model\",\n",
    "                 \"backbone\", \"base_model\", \"hf_model\", \"transformer\"):\n",
    "        if hasattr(txt_module, attr):\n",
    "            cand = getattr(txt_module, attr)\n",
    "            if hasattr(cand, \"encoder\") and hasattr(cand.encoder, \"layer\"):\n",
    "                return cand, \"encoder.layer\"\n",
    "            if hasattr(cand, \"transformer\") and hasattr(cand.transformer, \"layer\"):\n",
    "                return cand, \"transformer.layer\"\n",
    "    # 3) Fallback: scan any child exposing \".layer\"\n",
    "    for name, child in txt_module.named_modules():\n",
    "        if hasattr(child, \"layer\") and isinstance(getattr(child, \"layer\"), (list, nn.ModuleList)):\n",
    "            return txt_module, f\"{name}.layer\"\n",
    "    return None, None\n",
    "\n",
    "def build_optimizer(model):\n",
    "    txt = model.text  # wrapper\n",
    "    hf, layout = _locate_hf_text_module(txt)\n",
    "\n",
    "    groups = []\n",
    "    assigned = set()\n",
    "\n",
    "    if hf is not None:\n",
    "        # Freeze all text params first\n",
    "        for p in hf.parameters():\n",
    "            p.requires_grad_(False)\n",
    "\n",
    "        # Resolve the list of encoder layers\n",
    "        def _get_layers_from_layout(hf, layout: str):\n",
    "            if layout == \"encoder.layer\":   return list(hf.encoder.layer)\n",
    "            if layout == \"transformer.layer\": return list(hf.transformer.layer)\n",
    "            # dotted path, e.g., \"roberta.encoder.layer\"\n",
    "            parts = layout.split(\".\")\n",
    "            mod = hf\n",
    "            for k in parts[:-1]:\n",
    "                mod = getattr(mod, k)\n",
    "            return list(getattr(mod, parts[-1]))\n",
    "        try:\n",
    "            layers = _get_layers_from_layout(hf, layout)\n",
    "        except Exception:\n",
    "            layers = []\n",
    "\n",
    "        L = len(layers)\n",
    "        print(f\"[LLRD] Found HF encoder with {L} layers via layout='{layout}'.\")\n",
    "        # Map last-4 layers to LRs\n",
    "        layer_lrs = [LR_TEXT_L3, LR_TEXT_L2, LR_TEXT_L1, LR_TEXT_L0]\n",
    "        k = min(4, L)\n",
    "        for i in range(k):\n",
    "            layer = layers[L - k + i]\n",
    "            lr = layer_lrs[4 - k + i]\n",
    "            params = list(layer.parameters())\n",
    "            for p in params:\n",
    "                p.requires_grad_(True)\n",
    "                assigned.add(id(p))\n",
    "            if params:\n",
    "                groups.append({\"params\": params, \"lr\": lr, \"weight_decay\": WD})\n",
    "    else:\n",
    "        print(\"[LLRD][WARN] Could not locate HF text encoder; training all text params at LR_TEXT_L0.\")\n",
    "        txt_params = [p for p in txt.parameters()]\n",
    "        for p in txt_params:\n",
    "            p.requires_grad_(True)\n",
    "            assigned.add(id(p))\n",
    "        if txt_params:\n",
    "            groups.append({\"params\": txt_params, \"lr\": LR_TEXT_L0, \"weight_decay\": WD})\n",
    "\n",
    "    # Helper to add module params with LR, avoiding duplicates\n",
    "    def _add_module(mod: nn.Module, lr: float, wd: float):\n",
    "        params = [p for p in mod.parameters() if id(p) not in assigned]\n",
    "        for p in params: assigned.add(id(p))\n",
    "        if params:\n",
    "            groups.append({\"params\": params, \"lr\": lr, \"weight_decay\": wd})\n",
    "\n",
    "    # Projections / fusion / co-attn / head if present\n",
    "    for name in (\"proj_text\", \"proj_graph\", \"proj_desc\", \"fusion\", \"co_attn\", \"head\"):\n",
    "        if hasattr(model, name):\n",
    "            _add_module(getattr(model, name), LR_PROJ, WD)\n",
    "\n",
    "    # Others (GIN etc.)\n",
    "    others = [p for p in model.parameters() if (id(p) not in assigned)]\n",
    "    if others:\n",
    "        groups.append({\"params\": others, \"lr\": LR_OTHERS, \"weight_decay\": WD})\n",
    "        for p in others: assigned.add(id(p))\n",
    "\n",
    "    sizes = [sum(p.numel() for p in g[\"params\"]) for g in groups]\n",
    "    lrs   = [g[\"lr\"] for g in groups]\n",
    "    print(f\"[LLRD] Param groups: {len(groups)} | sizes={sizes} | lrs={lrs}\")\n",
    "\n",
    "    opt = torch.optim.AdamW(groups, betas=(0.9, 0.999), eps=1e-8)\n",
    "    return opt\n",
    "\n",
    "# ---------- EMA ----------\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=EMA_DECAY):\n",
    "        self.decay = decay\n",
    "        self.shadow = {k: v.detach().clone() for k,v in model.state_dict().items()}\n",
    "    def update(self, model):\n",
    "        with torch.no_grad():\n",
    "            for k, v in model.state_dict().items():\n",
    "                if v.dtype.is_floating_point:\n",
    "                    self.shadow[k].mul_((self.decay)).add_(v.detach(), alpha=1.0 - self.decay)\n",
    "    def apply_to(self, model):\n",
    "        model.load_state_dict(self.shadow, strict=False)\n",
    "\n",
    "# ---------- Scheduler ----------\n",
    "def build_scheduler(opt, total_steps, warmup_ratio=WARMUP):\n",
    "    warm = int(total_steps * warmup_ratio)\n",
    "    def lr_lambda(step):\n",
    "        if step < warm:\n",
    "            return float(step) / max(1, warm)\n",
    "        prog = (step - warm) / max(1, total_steps - warm)\n",
    "        return 0.5 * (1.0 + math.cos(math.pi * prog))\n",
    "    return torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda)\n",
    "\n",
    "# ---------- Train setup ----------\n",
    "ckpt_stageB = CKPT_DIR / f\"{RUN_ID}_stageB_best.pt\"\n",
    "assert ckpt_stageB.exists(), f\"Need Stage B ckpt at {ckpt_stageB}\"\n",
    "\n",
    "model_c = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "sd = torch.load(ckpt_stageB, map_location=\"cpu\", weights_only=False)\n",
    "sd_model = sd.get(\"model\", sd)\n",
    "missing, unexpected = model_c.load_state_dict(sd_model, strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] load_state_dict: missing={missing}, unexpected={unexpected}\")\n",
    "\n",
    "opt = build_optimizer(model_c)\n",
    "steps_per_epoch = len(train_loader)\n",
    "total_steps = steps_per_epoch * EPOCHS\n",
    "sched = build_scheduler(opt, total_steps)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=AMP, growth_interval=200)\n",
    "ema = EMA(model_c, decay=EMA_DECAY)\n",
    "\n",
    "print(f\"[INFO { _now() }] Stage C init | epochs={EPOCHS} | warmup={WARMUP:.2f} | \"\n",
    "      f\"drop(text/graph/desc)={P_TEXT_DROP}/{P_GRAPH_DROP}/{P_DESC_DROP} | AMP={AMP}({AMP_DTYPE})\")\n",
    "\n",
    "# ---------- Train loop ----------\n",
    "best_metric = -1.0\n",
    "best_path = CKPT_DIR / f\"{RUN_ID}_stageC_best.pt\"\n",
    "last_path = CKPT_DIR / f\"{RUN_ID}_stageC_last.pt\"\n",
    "log_file = RESULTS_DIR / f\"{RUN_ID}_stageC_log.jsonl\"\n",
    "patience = 6\n",
    "stale = 0\n",
    "gstep = 0\n",
    "\n",
    "def batch_clone_to_device(b):\n",
    "    B = SimpleNamespace()\n",
    "    B.text = {\"input_ids\": b.text[\"input_ids\"].clone().to(DEVICE),\n",
    "              \"attention_mask\": b.text[\"attention_mask\"].clone().to(DEVICE)}\n",
    "    B.graph = b.graph.to(DEVICE)\n",
    "    B.desc = b.desc.clone().to(DEVICE)\n",
    "    B.labels = b.labels.clone().to(DEVICE)\n",
    "    B.label_mask = b.label_mask.clone().to(DEVICE)\n",
    "    return B\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model_c.train()\n",
    "    t0 = time.time()\n",
    "    loss_meter = []; bce_prev_meter = []; nce_meter = []; l1_meter = []\n",
    "    drop_stats = [0,0,0]  # text, graph, desc\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        gstep += 1\n",
    "        B = batch_clone_to_device(batch)\n",
    "\n",
    "        # Random modality dropout (in-place on cloned batch)\n",
    "        if random.random() < P_TEXT_DROP:\n",
    "            _text_minimize(B); drop_stats[0]+=1\n",
    "        if random.random() < P_GRAPH_DROP:\n",
    "            _graph_minimize_like(B); drop_stats[1]+=1\n",
    "        if random.random() < P_DESC_DROP:\n",
    "            _desc_zero(B); drop_stats[2]+=1\n",
    "\n",
    "        with torch.amp.autocast(device_type=\"cuda\", enabled=AMP, dtype=AMP_DTYPE):\n",
    "            out = model_c(B, compute_aux=True)\n",
    "            logits = out[\"logits\"]\n",
    "            loss_bce = crit_focal(logits, B.labels, B.label_mask)\n",
    "            loss_nce = out.get(\"loss_nce\", torch.tensor(0.0, device=DEVICE)) * L_NCE\n",
    "            loss_l1  = out.get(\"attn_l1\", torch.tensor(0.0, device=DEVICE)) * L_ATTL1\n",
    "            loss = loss_bce + loss_nce + loss_l1\n",
    "\n",
    "            if AGREE_PROB > 0.0 and (random.random() < AGREE_PROB):\n",
    "                B_aux = batch_clone_to_device(batch)\n",
    "                _desc_zero(B_aux)\n",
    "                out_aux = model_c(B_aux, compute_aux=False)\n",
    "                p_main = torch.sigmoid(logits.detach()).clamp_(1e-6, 1-1e-6)\n",
    "                p_aux  = torch.sigmoid(out_aux[\"logits\"]).clamp_(1e-6, 1-1e-6)\n",
    "                conf = ((p_main <= 0.2) | (p_main >= 0.8)).float()\n",
    "                kl = (p_main * (torch.log(p_main) - torch.log(p_aux)) +\n",
    "                      (1-p_main) * (torch.log(1-p_main) - torch.log(1-p_aux)))\n",
    "                loss = loss + 0.02 * (kl * conf).mean()\n",
    "\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt)\n",
    "        scaler.update()\n",
    "        sched.step()\n",
    "        ema.update(model_c)\n",
    "\n",
    "        loss_meter.append(float(loss.item()))\n",
    "        bce_prev_meter.append(bce_preview(logits, B.labels, B.label_mask))\n",
    "        nce_meter.append(float(loss_nce.item()))\n",
    "        l1_meter.append(float(loss_l1.item()))\n",
    "\n",
    "        if i == 0:\n",
    "            z = logits.detach().float()\n",
    "            print(f\"[DEBUG e{epoch} i0] logits min/max: {z.min().item():.3f}/{z.max().item():.3f}\")\n",
    "\n",
    "    # ---- validation with EMA weights applied ----\n",
    "    ema.apply_to(model_c)\n",
    "    val = evaluate(model_c, val_loader)\n",
    "    secs = time.time() - t0\n",
    "    dtxt = f\"text:{drop_stats[0]}/{len(train_loader)} graph:{drop_stats[1]}/{len(train_loader)} desc:{drop_stats[2]}/{len(train_loader)}\"\n",
    "    print(f\"[E{epoch:02d} C] train_loss={np.mean(loss_meter):.4f} (bce≈{np.mean(bce_prev_meter):.4f}, nce={np.mean(nce_meter):.3f}, l1={np.mean(l1_meter):.3f}) | \"\n",
    "          f\"val_roc={val['macro_roc']:.4f} val_pr={val['macro_pr']:.4f} | {secs:.1f}s | drops({dtxt})\")\n",
    "\n",
    "    metric = val[\"macro_pr\"]  # early stop on PR-AUC (imbalanced)\n",
    "    with open(log_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps({\n",
    "            \"epoch\": epoch,\n",
    "            \"train_loss\": float(np.mean(loss_meter)),\n",
    "            \"bce_preview\": float(np.mean(bce_prev_meter)),\n",
    "            \"val_macro_roc\": val[\"macro_roc\"],\n",
    "            \"val_macro_pr\": val[\"macro_pr\"],\n",
    "            \"drops\": {\"text\": drop_stats[0], \"graph\": drop_stats[1], \"desc\": drop_stats[2]},\n",
    "            \"lr\": [g[\"lr\"] for g in opt.param_groups],\n",
    "        }) + \"\\n\")\n",
    "\n",
    "    # save best (by PR)\n",
    "    if metric > best_metric:\n",
    "        best_metric = metric; stale = 0\n",
    "        torch.save({\"model\": model_c.state_dict(),\n",
    "                    \"best_metric\": best_metric,\n",
    "                    \"config\": CONFIG,\n",
    "                    \"epoch\": epoch},\n",
    "                   best_path)\n",
    "        print(f\"  ↳ Saved BEST Stage-C checkpoint: {best_path.name} (val_macro_pr={best_metric:.4f})\")\n",
    "    else:\n",
    "        stale += 1\n",
    "        if stale >= 6:\n",
    "            print(f\"[EARLY STOP] No PR improvement for {6} epochs. Stopping at epoch {epoch}.\")\n",
    "            break\n",
    "\n",
    "# save last\n",
    "torch.save({\"model\": model_c.state_dict(), \"config\": CONFIG, \"epoch\": epoch}, last_path)\n",
    "print(f\"[DONE] Stage C finished. Best val_macro_pr={best_metric:.4f}\")\n",
    "print(f\"Checkpoints → {CKPT_DIR.as_posix()} | Logs → {log_file.as_posix()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0528ae45",
   "metadata": {},
   "source": [
    "## 17: Evaluate Stage C on val/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e484e138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-100M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- STAGE C EVALUATION -----\n",
      "Val  macro ROC / PR : 0.7678 / 0.2708\n",
      "Test macro ROC / PR : 0.7717 / 0.2653\n",
      "Test macro F1       : 0.2929  (val-tuned max-F1 thresholds)\n",
      "Saved: summary + thresholds_maxF1 → tox21_dualenc_v1/results/v4/eval/stage_C\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "assert 'DualEncCoAttnModel' in globals() and 'CONFIG' in globals(), \"Model class/CONFIG missing.\"\n",
    "assert 'val_loader' in globals() and 'test_loader' in globals(), \"Need val_loader & test_loader.\"\n",
    "assert 'RESULTS_DIR' in globals() and 'CKPT_DIR' in globals() and 'RUN_ID' in globals() and 'LABELS' in globals()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EVAL_DIR = RESULTS_DIR / \"eval\" / \"stage_C\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _torch_load_robust(path: Path) -> dict:\n",
    "    import pickle\n",
    "    try:\n",
    "        obj = torch.load(path, map_location=\"cpu\")  # torch 2.6 defaults to weights_only=True\n",
    "    except pickle.UnpicklingError:\n",
    "        from torch.serialization import add_safe_globals\n",
    "        from torch.torch_version import TorchVersion\n",
    "        try: add_safe_globals([TorchVersion])\n",
    "        except Exception: pass\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    except Exception:\n",
    "        obj = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    if isinstance(obj, dict): return obj\n",
    "    return {\"model\": obj}\n",
    "\n",
    "def _collect_probs(loader, model):\n",
    "    model.eval()\n",
    "    P, Y, M = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for b in loader:\n",
    "            out = model(b, compute_aux=False)\n",
    "            z = out[\"logits\"].float().clamp_(-30, 30)\n",
    "            p = torch.sigmoid(z).cpu().numpy()\n",
    "            y = b.labels.detach().cpu().numpy()\n",
    "            m = b.label_mask.detach().cpu().numpy().astype(bool)\n",
    "            P.append(p); Y.append(y); M.append(m)\n",
    "    P = np.concatenate(P, 0); Y = np.concatenate(Y, 0); M = np.concatenate(M, 0)\n",
    "    return P, Y, M\n",
    "\n",
    "def _macro_roc_pr(P, Y, M):\n",
    "    rocs, prs = [], []\n",
    "    for j in range(P.shape[1]):\n",
    "        m = M[:, j]; y = Y[m, j]; p = P[m, j]\n",
    "        if len(y) >= 3 and not (np.all(y==0) or np.all(y==1)):\n",
    "            try: rocs.append(roc_auc_score(y, p))\n",
    "            except: pass\n",
    "            try: prs.append(average_precision_score(y, p))\n",
    "            except: pass\n",
    "    return float(np.mean(rocs)) if rocs else float(\"nan\"), float(np.mean(prs)) if prs else float(\"nan\")\n",
    "\n",
    "def _thr_maxF1_perlabel(P, Y, M):\n",
    "    C = P.shape[1]\n",
    "    thr = np.zeros(C, dtype=np.float32)\n",
    "    for j in range(C):\n",
    "        m = M[:, j]; y = Y[m, j].astype(np.float32); p = P[m, j].astype(np.float32)\n",
    "        if len(y) < 3 or np.all(y==0) or np.all(y==1):\n",
    "            thr[j] = 0.5; continue\n",
    "        ts = np.unique(np.concatenate([p, [0.0, 0.5, 1.0]]))\n",
    "        best_f1, best_t = -1.0, 0.5\n",
    "        for t in ts:\n",
    "            h = (p >= t).astype(np.int32)\n",
    "            f = f1_score(y, h, zero_division=0)\n",
    "            if f > best_f1: best_f1, best_t = f, float(t)\n",
    "        thr[j] = best_t\n",
    "    return thr\n",
    "\n",
    "# 1) Load Stage C best checkpoint and build model\n",
    "ckpt = CKPT_DIR / f\"{RUN_ID}_stageC_best.pt\"\n",
    "assert ckpt.exists(), f\"Missing {ckpt}\"\n",
    "sd = _torch_load_robust(ckpt)\n",
    "modelC = DualEncCoAttnModel(CONFIG).to(DEVICE)\n",
    "missing, unexpected = modelC.load_state_dict(sd.get(\"model\", sd), strict=False)\n",
    "if missing or unexpected:\n",
    "    print(f\"[WARN] load_state_dict: missing={missing}, unexpected={unexpected}\")\n",
    "\n",
    "# 2) Collect calibrated-free probabilities (raw sigmoid) for val/test\n",
    "val_P, val_Y, val_M = _collect_probs(val_loader, modelC)\n",
    "test_P, test_Y, test_M = _collect_probs(test_loader, modelC)\n",
    "\n",
    "# 3) Compute ROC/PR\n",
    "val_roc,  val_pr  = _macro_roc_pr(val_P,  val_Y,  val_M)\n",
    "test_roc, test_pr = _macro_roc_pr(test_P, test_Y, test_M)\n",
    "\n",
    "# 4) Derive per-label thresholds on VAL (max-F1), apply to TEST for macro-F1\n",
    "thr_val = _thr_maxF1_perlabel(val_P, val_Y, val_M)\n",
    "H_test = (test_P >= thr_val[None, :]).astype(np.int32)\n",
    "f1s = []\n",
    "for j in range(test_P.shape[1]):\n",
    "    m = test_M[:, j]; y = test_Y[m, j].astype(np.float32); h = H_test[m, j].astype(np.int32)\n",
    "    f1s.append(float(f1_score(y, h, zero_division=0)))\n",
    "test_f1 = float(np.mean(f1s))\n",
    "\n",
    "# 5) Save artifacts\n",
    "summary = {\n",
    "    \"val_macro_roc\": val_roc, \"val_macro_pr\": val_pr,\n",
    "    \"test_macro_roc\": test_roc, \"test_macro_pr\": test_pr, \"test_macro_f1\": test_f1,\n",
    "}\n",
    "(EVAL_DIR / f\"{RUN_ID}_summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "(EVAL_DIR / f\"{RUN_ID}_thresholds_maxF1.json\").write_text(json.dumps({\"thresholds\": thr_val.tolist()}, indent=2))\n",
    "np.save(EVAL_DIR / f\"{RUN_ID}_val_probs.npy\",  val_P)\n",
    "np.save(EVAL_DIR / f\"{RUN_ID}_test_probs.npy\", test_P)\n",
    "\n",
    "print(\"\\n----- STAGE C EVALUATION -----\")\n",
    "print(f\"Val  macro ROC / PR : {val_roc:.4f} / {val_pr:.4f}\")\n",
    "print(f\"Test macro ROC / PR : {test_roc:.4f} / {test_pr:.4f}\")\n",
    "print(f\"Test macro F1       : {test_f1:.4f}  (val-tuned max-F1 thresholds)\")\n",
    "print(f\"Saved: summary + thresholds_maxF1 → {EVAL_DIR.as_posix()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8860c04f",
   "metadata": {},
   "source": [
    "## 18: Stage C Platt calibration + threshold policy sweep (saves rec>=0.60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db574a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[SAVED] Stage C calibration:\n",
      "  • Platt params  → v4_20250902_165736_stageC_platt.json\n",
      "  • rec>=0.60 thr → v4_20250902_165736_stageC_thresholds_recge0.60.json\n",
      "You can now point your inference cell to the Stage C thresholds file.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import f1_score, precision_recall_curve, roc_auc_score, average_precision_score\n",
    "\n",
    "assert 'modelC' in globals() and 'val_loader' in globals(), \"Run Cell 19 first (builds modelC).\"\n",
    "assert 'RESULTS_DIR' in globals() and 'RUN_ID' in globals() and 'LABELS' in globals()\n",
    "assert 'TOKENIZER' in globals() and 'descriptors_for_smiles_list' in globals() and '_mol_to_pyg_20d' in globals()\n",
    "\n",
    "DEVICE = next(modelC.parameters()).device\n",
    "modelC.eval()\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "CAL_DIR = RESULTS_DIR / \"eval\" / \"stage_C\" / \"calibration\"\n",
    "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def _collect_val_logits(model):\n",
    "    L, Y, M = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for b in val_loader:\n",
    "            out = model(b, compute_aux=False)\n",
    "            z = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX).cpu().numpy()\n",
    "            L.append(z)\n",
    "            Y.append(b.labels.detach().cpu().numpy())\n",
    "            M.append(b.label_mask.detach().cpu().numpy().astype(bool))\n",
    "    return np.concatenate(L, 0), np.concatenate(Y, 0), np.concatenate(M, 0)\n",
    "\n",
    "def fit_platt(z, y):\n",
    "    # z: logits (n,), y: {0,1}\n",
    "    z_t = torch.tensor(z, dtype=torch.float32)\n",
    "    y_t = torch.tensor(y, dtype=torch.float32)\n",
    "    A = torch.tensor(1.0, requires_grad=True)\n",
    "    B = torch.tensor(0.0, requires_grad=True)\n",
    "    opt = torch.optim.LBFGS([A, B], lr=1.0, max_iter=200, line_search_fn=\"strong_wolfe\")\n",
    "    bce = nn.BCEWithLogitsLoss()\n",
    "    def closure():\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        s = A * z_t + B\n",
    "        loss = bce(s, y_t)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    try:\n",
    "        opt.step(closure)\n",
    "    except Exception:\n",
    "        opt2 = torch.optim.Adam([A, B], lr=0.05)\n",
    "        for _ in range(400):\n",
    "            opt2.zero_grad(set_to_none=True)\n",
    "            s = A * z_t + B\n",
    "            loss = bce(s, y_t); loss.backward(); opt2.step()\n",
    "    return float(A.detach()), float(B.detach())\n",
    "\n",
    "def _safe_pr_curve(y, p):\n",
    "    try: return precision_recall_curve(y, p)\n",
    "    except Exception: return np.array([0.0]), np.array([0.0]), np.array([])\n",
    "\n",
    "def thr_prec_at_least(P, Y, M, target_prec=0.70):\n",
    "    C = P.shape[1]\n",
    "    thr = np.zeros(C, dtype=np.float32)\n",
    "    for j in range(C):\n",
    "        m = M[:, j]; y = Y[m, j].astype(np.float32); p = P[m, j].astype(np.float32)\n",
    "        if len(y) < 3 or np.all(y==0) or np.all(y==1):\n",
    "            thr[j] = 0.5; continue\n",
    "        prec, rec, t = _safe_pr_curve(y, p)\n",
    "        mask_ok = prec[:-1] >= target_prec\n",
    "        if mask_ok.any():\n",
    "            idx = np.argmax(rec[:-1][mask_ok])\n",
    "            thr[j] = float(t[mask_ok][idx])\n",
    "        else:\n",
    "            # fallback to per-label max-F1\n",
    "            ts = np.unique(np.concatenate([p, [0.0, 0.5, 1.0]]))\n",
    "            best_f1, best_t = -1.0, 0.5\n",
    "            for tt in ts:\n",
    "                h = (p >= tt).astype(np.int32)\n",
    "                f = f1_score(y, h, zero_division=0)\n",
    "                if f > best_f1: best_f1, best_t = f, float(tt)\n",
    "            thr[j] = best_t\n",
    "    return thr\n",
    "\n",
    "def thr_recall_at_least(P, Y, M, target_rec=0.60):\n",
    "    C = P.shape[1]\n",
    "    thr = np.zeros(C, dtype=np.float32)\n",
    "    for j in range(C):\n",
    "        m = M[:, j]; y = Y[m, j].astype(np.float32); p = P[m, j].astype(np.float32)\n",
    "        if len(y) < 3 or np.all(y==0) or np.all(y==1):\n",
    "            thr[j] = 0.5; continue\n",
    "        prec, rec, t = _safe_pr_curve(y, p)\n",
    "        mask_ok = rec[:-1] >= target_rec\n",
    "        if mask_ok.any():\n",
    "            idx = np.argmax(prec[:-1][mask_ok])\n",
    "            thr[j] = float(t[mask_ok][idx])\n",
    "        else:\n",
    "            ts = np.unique(np.concatenate([p, [0.0, 0.5, 1.0]]))\n",
    "            best_f1, best_t = -1.0, 0.5\n",
    "            for tt in ts:\n",
    "                h = (p >= tt).astype(np.int32)\n",
    "                f = f1_score(y, h, zero_division=0)\n",
    "                if f > best_f1: best_f1, best_t = f, float(tt)\n",
    "            thr[j] = best_t\n",
    "    return thr\n",
    "\n",
    "# 1) Collect VAL logits & fit Platt per label\n",
    "val_L, val_Y, val_M = _collect_val_logits(modelC)\n",
    "platt = {}\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    m = val_M[:, j]; z = val_L[m, j]; y = val_Y[m, j]\n",
    "    if (len(y) < 3) or (np.all(y==0) or np.all(y==1)):\n",
    "        platt[lbl] = {\"A\": 1.0, \"B\": 0.0}\n",
    "    else:\n",
    "        A, B = fit_platt(z, y); platt[lbl] = {\"A\": A, \"B\": B}\n",
    "(CAL_DIR / f\"{RUN_ID}_stageC_platt.json\").write_text(json.dumps(platt, indent=2))\n",
    "\n",
    "# 2) Apply Platt to get calibrated VAL probabilities\n",
    "Z = val_L.copy()\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    A, B = platt[lbl][\"A\"], platt[lbl][\"B\"]\n",
    "    Z[:, j] = A * Z[:, j] + B\n",
    "val_P = 1.0 / (1.0 + np.exp(-Z))\n",
    "\n",
    "# 3) Build threshold sets and save the rec>=0.60 one\n",
    "thr_rec60 = thr_recall_at_least(val_P, val_Y, val_M, target_rec=0.60)\n",
    "(CAL_DIR / f\"{RUN_ID}_stageC_thresholds_recge0.60.json\").write_text(\n",
    "    json.dumps({\"thresholds\": thr_rec60.tolist()}, indent=2)\n",
    ")\n",
    "\n",
    "print(\"\\n[SAVED] Stage C calibration:\")\n",
    "print(\"  • Platt params  →\", (CAL_DIR / f\"{RUN_ID}_stageC_platt.json\").name)\n",
    "print(\"  • rec>=0.60 thr →\", (CAL_DIR / f\"{RUN_ID}_stageC_thresholds_recge0.60.json\").name)\n",
    "print(\"You can now point your inference cell to the Stage C thresholds file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b813caee",
   "metadata": {},
   "source": [
    "## 19: New Inference via stage C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83098a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using Platt: v4_20250902_165736_stageC_platt.json | Thresholds: v4_20250902_165736_stageC_thresholds_recge0.60.json\n",
      "----- STAGE C • CALIBRATED PREDICTIONS (policy: rec≥0.60) -----\n",
      "[0] CCOc1ccc2nc(S(N)(=O)=O)sc2c1\n",
      "  + NR-ER         p=0.098  thr=0.072\n",
      "  top-5: SR-ARE:0.137, NR-ER:0.098, SR-p53:0.045, SR-MMP:0.027, SR-HSE:0.024\n",
      "[1] CCN1C(=O)NC(c2ccccc2)C1=O\n",
      "  + NR-ER         p=0.099  thr=0.072\n",
      "  top-5: SR-ARE:0.103, NR-ER:0.099, SR-p53:0.026, SR-HSE:0.014, NR-AR:0.014\n",
      "[2] O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\n",
      "  + SR-ARE        p=0.285  thr=0.148\n",
      "  + NR-ER         p=0.095  thr=0.072\n",
      "  + SR-HSE        p=0.067  thr=0.066\n",
      "  + NR-ER-LBD     p=0.029  thr=0.028\n",
      "  top-5: SR-ARE:0.285, SR-MMP:0.168, NR-ER:0.095, SR-p53:0.072, SR-HSE:0.067\n",
      "[3] Cc1cc(/C=C/c2ccc3cc(N(C)C)ccc3[n+]2C)c(C)n1-c1ccccc1.Cc1cc(/C=C/c2ccc3cc(N(C)C)ccc3[n+]2C)c(C)n1-c1ccccc1.O=C([O-])c1cc2ccccc2c(Cc2c(O)c(C(=O)[O-])cc3ccccc23)c1O\n",
      "  + SR-p53        p=0.355  thr=0.120\n",
      "  + SR-ARE        p=0.337  thr=0.148\n",
      "  + SR-HSE        p=0.122  thr=0.066\n",
      "  + SR-ATAD5      p=0.082  thr=0.067\n",
      "  top-5: SR-p53:0.355, SR-ARE:0.337, SR-MMP:0.167, SR-HSE:0.122, SR-ATAD5:0.082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[0.00854548, 0.00429171, 0.01009018, 0.01235891, 0.09803253,\n",
       "         0.01706613, 0.00746916, 0.1372679 , 0.02164035, 0.02400891,\n",
       "         0.02679489, 0.04505747],\n",
       "        [0.01358121, 0.00424217, 0.00276546, 0.00864229, 0.09940892,\n",
       "         0.01178594, 0.00471268, 0.1027023 , 0.01058569, 0.01412759,\n",
       "         0.01103347, 0.02614827],\n",
       "        [0.00594077, 0.00154741, 0.00875983, 0.05198474, 0.09461163,\n",
       "         0.02852109, 0.01560882, 0.2854595 , 0.01451341, 0.06662678,\n",
       "         0.16813609, 0.07233238],\n",
       "        [0.0073267 , 0.00832819, 0.01330406, 0.0228979 , 0.05819259,\n",
       "         0.01712286, 0.04857562, 0.3372853 , 0.08208582, 0.12154063,\n",
       "         0.16659144, 0.35459903]], dtype=float32),\n",
       " array([[0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import json, numpy as np, torch\n",
    "from pathlib import Path\n",
    "from types import SimpleNamespace\n",
    "from rdkit import Chem\n",
    "from torch_geometric.data import Batch as PyGBatch\n",
    "\n",
    "# --- prerequisites (from earlier cells) ---\n",
    "need = [\"DualEncCoAttnModel\",\"CONFIG\",\"RUN_ID\",\"CKPT_DIR\",\"RESULTS_DIR\",\"LABELS\",\"TOKENIZER\",\n",
    "        \"descriptors_for_smiles_list\",\"_mol_to_pyg_20d\"]\n",
    "for n in need: \n",
    "    assert n in globals(), f\"Missing global: {n}. Run earlier cells first.\"\n",
    "\n",
    "# --- config ---\n",
    "MAX_LEN = 256\n",
    "CLAMP_MIN, CLAMP_MAX = -30.0, 30.0\n",
    "\n",
    "def _json_load(p: Path):\n",
    "    try: return json.loads(p.read_text())\n",
    "    except Exception: return None\n",
    "\n",
    "def _load_stageC_model():\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if \"modelC\" in globals():\n",
    "        m = globals()[\"modelC\"].to(device)\n",
    "        m.eval()\n",
    "        return m, device\n",
    "    # else load from ckpt\n",
    "    ck = CKPT_DIR / f\"{RUN_ID}_stageC_best.pt\"\n",
    "    assert ck.exists(), f\"Stage C checkpoint not found: {ck}\"\n",
    "    obj = torch.load(ck, map_location=\"cpu\", weights_only=False)\n",
    "    sd = obj.get(\"model\", obj)\n",
    "    m = DualEncCoAttnModel(CONFIG).to(device)\n",
    "    miss, unexp = m.load_state_dict(sd, strict=False)\n",
    "    if miss or unexp:\n",
    "        print(f\"[WARN] load_state_dict: missing={miss}, unexpected={unexp}\")\n",
    "    m.eval()\n",
    "    return m, device\n",
    "\n",
    "def _load_stageC_platt_and_thresholds():\n",
    "    caldir = RESULTS_DIR / \"eval\" / \"stage_C\" / \"calibration\"\n",
    "    # Platt\n",
    "    platt_file = caldir / f\"{RUN_ID}_stageC_platt.json\"\n",
    "    platt = _json_load(platt_file) or {}\n",
    "    # Thresholds: prefer rec>=0.60\n",
    "    thr_file = caldir / f\"{RUN_ID}_stageC_thresholds_recge0.60.json\"\n",
    "    if not thr_file.exists():\n",
    "        # fallback to any stageC thresholds_* (most recent)\n",
    "        cands = sorted(caldir.glob(f\"{RUN_ID}_stageC_thresholds_*.json\"),\n",
    "                       key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if cands: thr_file = cands[0]\n",
    "        else:\n",
    "            # last fallback: Stage B thresholds\n",
    "            bdir = RESULTS_DIR / \"eval\" / \"stage_B\"\n",
    "            cands_b = sorted(bdir.glob(f\"{RUN_ID}_thresholds*.json\"),\n",
    "                             key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "            assert cands_b, \"No thresholds found for Stage C or Stage B.\"\n",
    "            thr_file = cands_b[0]\n",
    "    thr_obj = _json_load(thr_file) or {}\n",
    "    thresholds = np.asarray(thr_obj.get(\"thresholds\", [0.5]*len(LABELS)), dtype=np.float32)\n",
    "    print(f\"[INFO] Using Platt: {platt_file.name if platt else 'identity'} | Thresholds: {thr_file.name}\")\n",
    "    # Normalize platt dict with defaults\n",
    "    pl = {}\n",
    "    for lbl in LABELS:\n",
    "        item = platt.get(lbl, {})\n",
    "        A = float(item.get(\"A\", 1.0)); B = float(item.get(\"B\", 0.0))\n",
    "        pl[lbl] = {\"A\": A, \"B\": B}\n",
    "    return pl, thresholds\n",
    "\n",
    "def _build_infer_batch(smiles_list, device):\n",
    "    toks = TOKENIZER(smiles_list, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    ids  = toks[\"input_ids\"].to(device)\n",
    "    attn = toks[\"attention_mask\"].to(device)\n",
    "    mols = [Chem.MolFromSmiles(s) or Chem.MolFromSmiles(\"C\") for s in smiles_list]\n",
    "    graphs = [_mol_to_pyg_20d(m) for m in mols]\n",
    "    pyg = PyGBatch.from_data_list(graphs).to(device)\n",
    "    # Option-B descriptors (lookup/compute → impute/standardize inside your helper)\n",
    "    desc = torch.tensor(descriptors_for_smiles_list(smiles_list), dtype=torch.float32, device=device)\n",
    "    C = len(LABELS); B = len(smiles_list)\n",
    "    batch = SimpleNamespace()\n",
    "    batch.text = {\"input_ids\": ids, \"attention_mask\": attn}\n",
    "    batch.graph = pyg\n",
    "    batch.desc  = desc\n",
    "    batch.labels     = torch.zeros((B, C), dtype=torch.float32, device=device)\n",
    "    batch.label_mask = torch.ones((B, C),  dtype=torch.bool,   device=device)\n",
    "    return batch\n",
    "\n",
    "@torch.no_grad()\n",
    "def infer_stageC(smiles_list, topk=5):\n",
    "    # clean input\n",
    "    smiles_list = [s.strip() for s in smiles_list if s and s.strip()]\n",
    "    if not smiles_list:\n",
    "        print(\"[HINT] Provide at least one SMILES string in SMILES_TEST.\")\n",
    "        return None, None\n",
    "    model, device = _load_stageC_model()\n",
    "    platt, thresholds = _load_stageC_platt_and_thresholds()\n",
    "\n",
    "    batch = _build_infer_batch(smiles_list, device)\n",
    "    out = model(batch, compute_aux=False)\n",
    "    logits = out[\"logits\"].float().clamp_(CLAMP_MIN, CLAMP_MAX).cpu().numpy()  # [B, C]\n",
    "    # Apply Stage C Platt per label\n",
    "    for j, lbl in enumerate(LABELS):\n",
    "        A = platt[lbl][\"A\"]; B = platt[lbl][\"B\"]\n",
    "        logits[:, j] = A * logits[:, j] + B\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "    yhat = (probs >= thresholds[None, :]).astype(np.int32)\n",
    "\n",
    "    # Print concise results\n",
    "    print(\"----- STAGE C • CALIBRATED PREDICTIONS (policy: rec≥0.60) -----\")\n",
    "    for i, smi in enumerate(smiles_list):\n",
    "        pos = [(LABELS[j], float(probs[i, j]), float(thresholds[j])) for j in range(len(LABELS)) if yhat[i, j] == 1]\n",
    "        pos.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(f\"[{i}] {smi}\")\n",
    "        if pos:\n",
    "            for name, p, t in pos:\n",
    "                print(f\"  + {name:<12}  p={p:.3f}  thr={t:.3f}\")\n",
    "        else:\n",
    "            print(\"  —\")\n",
    "        # top-k glance\n",
    "        pairs = [(LABELS[j], float(probs[i, j])) for j in range(len(LABELS))]\n",
    "        pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "        tops = \", \".join([f\"{n}:{v:.3f}\" for n, v in pairs[:topk]])\n",
    "        print(f\"  top-{topk}: {tops}\")\n",
    "    return probs, yhat\n",
    "\n",
    "# ---- Try it: paste SMILES below ----\n",
    "SMILES_TEST = [\n",
    "    # examples:\n",
    "    \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",\n",
    "    \"CCN1C(=O)NC(c2ccccc2)C1=O\",\n",
    "    \"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\",\n",
    "    \"Cc1cc(/C=C/c2ccc3cc(N(C)C)ccc3[n+]2C)c(C)n1-c1ccccc1.Cc1cc(/C=C/c2ccc3cc(N(C)C)ccc3[n+]2C)c(C)n1-c1ccccc1.O=C([O-])c1cc2ccccc2c(Cc2c(O)c(C(=O)[O-])cc3ccccc23)c1O\"\n",
    "\n",
    "]\n",
    "infer_stageC(SMILES_TEST, topk=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac0b9f9",
   "metadata": {},
   "source": [
    "# v5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b745473",
   "metadata": {},
   "source": [
    "## 1: Environment, paths, seeds, and basic guards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b7d7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V5 bootstrap…\n",
      "Working dir: d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\n",
      "Checkpoints: d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v5\n",
      "Results    : d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v5\n",
      "Torch: 2.6.0+cu124 | CUDA available: True | device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4070 Ti\n",
      "Transformers: 4.43.3\n",
      "RDKit: OK\n",
      "Cell 1 ✅ — Environment ready.\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, random, math, time, gc, pickle\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "\n",
    "# ---- Paths (as requested) ----\n",
    "CKPT_DIR = \"tox21_dualenc_v1/models/checkpoints_v5\"\n",
    "RES_DIR  = \"tox21_dualenc_v1/results/v5\"\n",
    "os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "os.makedirs(RES_DIR, exist_ok=True)\n",
    "\n",
    "# ---- Print basic environment info ----\n",
    "print(\"V5 bootstrap…\")\n",
    "print(\"Working dir:\", os.getcwd())\n",
    "print(\"Checkpoints:\", os.path.abspath(CKPT_DIR))\n",
    "print(\"Results    :\", os.path.abspath(RES_DIR))\n",
    "\n",
    "# ---- Determinism (best-effort) ----\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Torch:\", torch.__version__, \"| CUDA available:\", torch.cuda.is_available(), \"| device:\", device)\n",
    "\n",
    "# ---- Optional: show GPU name if available ----\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        print(\"GPU:\", torch.cuda.get_device_name(0))\n",
    "    except Exception as e:\n",
    "        print(\"GPU name check failed:\", e)\n",
    "\n",
    "# ---- HuggingFace + RDKit checks ----\n",
    "missing = []\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "    import transformers\n",
    "    print(\"Transformers:\", transformers.__version__)\n",
    "except Exception as e:\n",
    "    missing.append(\"transformers (pip install transformers>=4.40)\")\n",
    "    print(\"Transformers not available:\", e)\n",
    "\n",
    "try:\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors\n",
    "    print(\"RDKit: OK\")\n",
    "except Exception as e:\n",
    "    missing.append(\"rdkit (conda install -c conda-forge rdkit)\")\n",
    "    print(\"RDKit not available:\", e)\n",
    "\n",
    "# ---- Fail fast if essentials missing ----\n",
    "if missing:\n",
    "    raise RuntimeError(\n",
    "        \"Missing required packages:\\n - \" + \"\\n - \".join(missing) +\n",
    "        \"\\nPlease install them, restart the kernel, and rerun this cell.\"\n",
    "    )\n",
    "\n",
    "print(\"Cell 1 ✅ — Environment ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddfb2c4",
   "metadata": {},
   "source": [
    "## 2: Config + Saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15206d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V5 Config:\n",
      "{\n",
      "  \"dataset_csv\": \"tox21_dualenc_v1/data/raw/dataset_selected.csv\",\n",
      "  \"label_cols\": [\n",
      "    \"NR-AR\",\n",
      "    \"NR-AR-LBD\",\n",
      "    \"NR-AhR\",\n",
      "    \"NR-Aromatase\",\n",
      "    \"NR-ER\",\n",
      "    \"NR-ER-LBD\",\n",
      "    \"NR-PPAR-gamma\",\n",
      "    \"SR-ARE\",\n",
      "    \"SR-ATAD5\",\n",
      "    \"SR-HSE\",\n",
      "    \"SR-MMP\",\n",
      "    \"SR-p53\"\n",
      "  ],\n",
      "  \"smiles_col\": \"smiles\",\n",
      "  \"id_col\": \"mol_id\",\n",
      "  \"missing_label_value\": -1,\n",
      "  \"text_model_name\": \"DeepChem/ChemBERTa-77M-MLM\",\n",
      "  \"max_smiles_len\": 256,\n",
      "  \"do_random_smiles_train\": true,\n",
      "  \"tta_smiles_at_infer\": 8,\n",
      "  \"use_virtual_node\": true,\n",
      "  \"atom_feature_set\": \"chemprop_plus\",\n",
      "  \"bond_feature_set\": \"chemprop_plus\",\n",
      "  \"use_rdkit_desc\": true,\n",
      "  \"desc_impute_strategy\": \"median\",\n",
      "  \"desc_scaler_path\": \"tox21_dualenc_v1/results/v5\\\\rdkit_desc_scaler_v5.pkl\",\n",
      "  \"use_labelwise_gating\": true,\n",
      "  \"gate_hidden_dim\": 128,\n",
      "  \"label_emb_dim\": 32,\n",
      "  \"use_asl\": true,\n",
      "  \"asl_gamma_pos\": 0.0,\n",
      "  \"asl_gamma_neg\": 3.0,\n",
      "  \"asl_m\": 0.05,\n",
      "  \"use_class_balanced_weighting\": true,\n",
      "  \"use_infonce\": true,\n",
      "  \"infonce_weight\": 0.5,\n",
      "  \"lr\": 0.0002,\n",
      "  \"weight_decay\": 0.0001,\n",
      "  \"use_sam\": true,\n",
      "  \"ema_decay\": 0.999,\n",
      "  \"use_swa\": true,\n",
      "  \"dropout_text\": 0.2,\n",
      "  \"dropout_graph\": 0.2,\n",
      "  \"dropout_desc\": 0.5,\n",
      "  \"epochs_A\": 4,\n",
      "  \"epochs_B\": 6,\n",
      "  \"epochs_C\": 8,\n",
      "  \"epochs_hardneg\": 2,\n",
      "  \"batch_size\": 64,\n",
      "  \"num_workers\": 4,\n",
      "  \"use_platt\": true,\n",
      "  \"use_temp_scaling\": true,\n",
      "  \"base_recall_target\": 0.6,\n",
      "  \"overrides_recall_50\": [\n",
      "    \"NR-AhR\",\n",
      "    \"SR-ARE\"\n",
      "  ],\n",
      "  \"run_tag\": \"v5_20250903_124631\"\n",
      "}\n",
      "Config saved to: tox21_dualenc_v1/results/v5\\v5_20250903_124631_config.json\n",
      "Cell 2 ✅ — Config locked.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "TOX21_LABELS = [\n",
    "    \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\",\n",
    "    \"NR-ER\", \"NR-ER-LBD\", \"NR-PPAR-gamma\",\n",
    "    \"SR-ARE\", \"SR-ATAD5\", \"SR-HSE\", \"SR-MMP\", \"SR-p53\"\n",
    "]\n",
    "\n",
    "@dataclass\n",
    "class V5Config:\n",
    "    # Data\n",
    "    dataset_csv: str = \"tox21_dualenc_v1/data/raw/dataset_selected.csv\"\n",
    "    label_cols: List[str] = tuple(TOX21_LABELS)\n",
    "    smiles_col: str = \"smiles\"\n",
    "    id_col: str = \"mol_id\"\n",
    "    missing_label_value: int = -1  # labels with -1 will be masked\n",
    "    \n",
    "    # Text encoder (ChemBERTa)\n",
    "    text_model_name: str = \"DeepChem/ChemBERTa-77M-MLM\"  # Track A (safe default)\n",
    "    max_smiles_len: int = 256\n",
    "    do_random_smiles_train: bool = True\n",
    "    tta_smiles_at_infer: int = 8  # you approved the budget\n",
    "    \n",
    "    # Graph featurization (GIN-ready, no PyG required)\n",
    "    use_virtual_node: bool = True\n",
    "    atom_feature_set: str = \"chemprop_plus\"  # we'll implement a rich set\n",
    "    bond_feature_set: str = \"chemprop_plus\"\n",
    "\n",
    "    # Descriptors\n",
    "    use_rdkit_desc: bool = True\n",
    "    desc_impute_strategy: str = \"median\"\n",
    "    desc_scaler_path: str = os.path.join(RES_DIR, \"rdkit_desc_scaler_v5.pkl\")\n",
    "    \n",
    "    # Fusion\n",
    "    use_labelwise_gating: bool = True\n",
    "    gate_hidden_dim: int = 128\n",
    "    label_emb_dim: int = 32\n",
    "    \n",
    "    # Losses & sampling\n",
    "    use_asl: bool = True\n",
    "    asl_gamma_pos: float = 0.0\n",
    "    asl_gamma_neg: float = 3.0\n",
    "    asl_m: float = 0.05\n",
    "    use_class_balanced_weighting: bool = True\n",
    "    \n",
    "    # InfoNCE alignment\n",
    "    use_infonce: bool = True\n",
    "    infonce_weight: float = 0.5\n",
    "    \n",
    "    # Optimizer stack\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    use_sam: bool = True\n",
    "    ema_decay: float = 0.999\n",
    "    use_swa: bool = True\n",
    "    \n",
    "    # Dropouts\n",
    "    dropout_text: float = 0.2\n",
    "    dropout_graph: float = 0.2\n",
    "    dropout_desc: float = 0.5  # raised vs V4 to avoid descriptor dominance\n",
    "    \n",
    "    # Training schedule (epochs per stage)\n",
    "    epochs_A: int = 4\n",
    "    epochs_B: int = 6\n",
    "    epochs_C: int = 8\n",
    "    epochs_hardneg: int = 2\n",
    "    \n",
    "    # Batching\n",
    "    batch_size: int = 64\n",
    "    num_workers: int = 4\n",
    "    \n",
    "    # Calibration/thresholds\n",
    "    use_platt: bool = True\n",
    "    use_temp_scaling: bool = True  # we'll pick per-label on VAL\n",
    "    base_recall_target: float = 0.60\n",
    "    overrides_recall_50: Tuple[str, ...] = (\"NR-AhR\", \"SR-ARE\")  # as discussed\n",
    "    \n",
    "    # Logging\n",
    "    run_tag: str = datetime.now().strftime(\"v5_%Y%m%d_%H%M%S\")\n",
    "\n",
    "CFG = V5Config()\n",
    "print(\"V5 Config:\")\n",
    "print(json.dumps(asdict(CFG), indent=2))\n",
    "\n",
    "# Save config snapshot (so results are reproducible)\n",
    "cfg_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_config.json\")\n",
    "with open(cfg_path, \"w\") as f:\n",
    "    json.dump(asdict(CFG), f, indent=2)\n",
    "print(\"Config saved to:\", cfg_path)\n",
    "print(\"Cell 2 ✅ — Config locked.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9669fbbf",
   "metadata": {},
   "source": [
    "## 3: Load dataset + data card (counts, masks, SMILES checks, co-occurrence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34c91e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV shape: (7831, 271)\n",
      "First columns: ['row_id', 'split', 'smiles', 'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53', 'MolWt', 'TPSA', 'SlogP', 'HBD', 'HBA'] ...\n",
      "INFO: 'mol_id' not found. Creating a stable ID from SMILES…\n",
      "Unique mol_id: 7831\n",
      "SMILES length > 256: 14\n",
      "\n",
      "Per-label stats:\n",
      "               n_valid  n_pos   n_neg  n_missing  prevalence_valid\n",
      "NR-AR           7265.0  309.0  6956.0      566.0            0.0425\n",
      "NR-AR-LBD       6758.0  237.0  6521.0     1073.0            0.0351\n",
      "NR-AhR          6549.0  768.0  5781.0     1282.0            0.1173\n",
      "NR-Aromatase    5821.0  300.0  5521.0     2010.0            0.0515\n",
      "NR-ER           6193.0  793.0  5400.0     1638.0            0.1280\n",
      "NR-ER-LBD       6955.0  350.0  6605.0      876.0            0.0503\n",
      "NR-PPAR-gamma   6450.0  186.0  6264.0     1381.0            0.0288\n",
      "SR-ARE          5832.0  942.0  4890.0     1999.0            0.1615\n",
      "SR-ATAD5        7072.0  264.0  6808.0      759.0            0.0373\n",
      "SR-HSE          6467.0  372.0  6095.0     1364.0            0.0575\n",
      "SR-MMP          5810.0  918.0  4892.0     2021.0            0.1580\n",
      "SR-p53          6774.0  423.0  6351.0     1057.0            0.0624\n",
      "\n",
      "Detected split column: 'split' with counts: {'train': 6265, 'test': 783, 'val': 783}\n",
      "\n",
      "Detected 257 feature columns (descriptors/fingerprints).\n",
      "Feature sample: ['row_id', 'MolWt', 'TPSA', 'SlogP', 'HBD', 'HBA', 'NumRotBonds', 'RingCount', 'FractionCSP3', 'AromaticProportion']\n",
      "\n",
      "Saved:\n",
      " - Data card: tox21_dualenc_v1/results/v5\\v5_20250903_124631_data_card.json\n",
      " - Per-label stats CSV: tox21_dualenc_v1/results/v5\\v5_20250903_124631_per_label_stats.csv\n",
      " - Co-occurrence counts CSV: tox21_dualenc_v1/results/v5\\v5_20250903_124631_cooccurrence_counts.csv\n",
      " - Pos-Pos counts CSV: tox21_dualenc_v1/results/v5\\v5_20250903_124631_pospos_counts.csv\n",
      " - Dataset copy with ID: tox21_dualenc_v1/results/v5\\v5_20250903_124631_dataset_with_id.csv\n",
      "\n",
      "Cell 3 ✅ — Dataset validated and summarized.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import json, os, hashlib\n",
    "\n",
    "csv_path = CFG.dataset_csv\n",
    "assert os.path.exists(csv_path), f\"CSV not found: {csv_path}\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "print(\"Loaded CSV shape:\", df.shape)\n",
    "print(\"First columns:\", list(df.columns)[:20], \"...\")\n",
    "\n",
    "# ---- Ensure ID column exists (fallback if missing) ----\n",
    "if CFG.id_col not in df.columns:\n",
    "    print(f\"INFO: '{CFG.id_col}' not found. Creating a stable ID from SMILES…\")\n",
    "    assert CFG.smiles_col in df.columns, f\"SMILES column '{CFG.smiles_col}' missing.\"\n",
    "    def hash_smiles(s):\n",
    "        s = str(s).strip()\n",
    "        return hashlib.md5(s.encode(\"utf-8\")).hexdigest()[:16]\n",
    "    df[CFG.id_col] = df[CFG.smiles_col].apply(hash_smiles)\n",
    "else:\n",
    "    # basic duplicate check\n",
    "    dup_ids = df[CFG.id_col].duplicated().sum()\n",
    "    assert dup_ids == 0, f\"Found {dup_ids} duplicate {CFG.id_col} entries.\"\n",
    "\n",
    "print(f\"Unique {CFG.id_col}: {df[CFG.id_col].nunique()}\")\n",
    "\n",
    "# ---- SMILES basic checks ----\n",
    "assert CFG.smiles_col in df.columns, f\"SMILES column '{CFG.smiles_col}' missing.\"\n",
    "smiles_series = df[CFG.smiles_col].astype(str)\n",
    "n_empty_smiles = int((smiles_series.str.strip() == \"\").sum())\n",
    "assert n_empty_smiles == 0, f\"Found {n_empty_smiles} empty SMILES.\"\n",
    "\n",
    "# RDKit validity check (sample first 2000 for speed, fallback to all if issues)\n",
    "def smiles_valid_mask(s):\n",
    "    return s.apply(lambda x: Chem.MolFromSmiles(x) is not None)\n",
    "\n",
    "sample = smiles_series.sample(min(2000, len(smiles_series)), random_state=42)\n",
    "sample_valid = smiles_valid_mask(sample)\n",
    "if not sample_valid.all():\n",
    "    print(\"WARNING: Invalid SMILES detected in sample — checking full column…\")\n",
    "    all_valid = smiles_valid_mask(smiles_series)\n",
    "else:\n",
    "    all_valid = pd.Series(True, index=smiles_series.index)\n",
    "\n",
    "n_invalid = int((~all_valid).sum())\n",
    "assert n_invalid == 0, f\"Found {n_invalid} invalid SMILES.\"\n",
    "\n",
    "# Length vs max_smiles_len\n",
    "smiles_len = smiles_series.str.len()\n",
    "len_over = int((smiles_len > CFG.max_smiles_len).sum())\n",
    "print(f\"SMILES length > {CFG.max_smiles_len}: {len_over}\")\n",
    "\n",
    "# ---- Label integrity & mask handling ----\n",
    "missing_label_value = CFG.missing_label_value\n",
    "label_cols = list(CFG.label_cols)\n",
    "missing_label_cols = [c for c in label_cols if c not in df.columns]\n",
    "assert not missing_label_cols, f\"Missing label columns: {missing_label_cols}\"\n",
    "\n",
    "label_df = df[label_cols].copy()\n",
    "assert label_df.shape[1] == 12, \"Expected 12 Tox21 labels.\"\n",
    "\n",
    "# Check allowed values {0,1,-1}\n",
    "bad_vals = {}\n",
    "for col in label_cols:\n",
    "    unique_vals = set(pd.unique(label_df[col]))\n",
    "    if not unique_vals.issubset({0, 1, missing_label_value}):\n",
    "        bad_vals[col] = sorted([v for v in unique_vals if v not in (0,1,missing_label_value)])\n",
    "if bad_vals:\n",
    "    print(\"WARNING: some labels have unexpected values:\", bad_vals)\n",
    "\n",
    "def label_stats(series: pd.Series, missing_val: int = -1):\n",
    "    s = series.copy()\n",
    "    n_total = int(s.shape[0])\n",
    "    n_missing = int((s == missing_val).sum())\n",
    "    s_valid = s[s != missing_val]\n",
    "    n_valid = int(s_valid.shape[0])\n",
    "    n_pos = int((s_valid == 1).sum())\n",
    "    n_neg = int((s_valid == 0).sum())\n",
    "    prev = float(n_pos / n_valid) if n_valid > 0 else float(\"nan\")\n",
    "    return {\n",
    "        \"n_total\": n_total,\n",
    "        \"n_missing\": n_missing,\n",
    "        \"n_valid\": n_valid,\n",
    "        \"n_pos\": n_pos,\n",
    "        \"n_neg\": n_neg,\n",
    "        \"prevalence_valid\": prev\n",
    "    }\n",
    "\n",
    "per_label = {col: label_stats(label_df[col], missing_label_value) for col in label_cols}\n",
    "per_label_df = pd.DataFrame(per_label).T.loc[label_cols]  # keep canonical order\n",
    "\n",
    "print(\"\\nPer-label stats:\")\n",
    "print(per_label_df[[\"n_valid\",\"n_pos\",\"n_neg\",\"n_missing\",\"prevalence_valid\"]].round(4))\n",
    "\n",
    "# ---- Split detection ----\n",
    "split_col = None\n",
    "for cand in [\"split\", \"set\", \"subset\", \"fold\", \"Split\", \"Set\"]:\n",
    "    if cand in df.columns:\n",
    "        split_col = cand\n",
    "        break\n",
    "\n",
    "split_counts = {}\n",
    "if split_col:\n",
    "    split_counts = df[split_col].value_counts(dropna=False).to_dict()\n",
    "    print(f\"\\nDetected split column: '{split_col}' with counts:\", split_counts)\n",
    "else:\n",
    "    print(\"\\nNo split column detected (that's fine).\")\n",
    "\n",
    "# ---- Feature columns (descriptors/fingerprints etc.) ----\n",
    "protected_cols = set(label_cols + [CFG.smiles_col, CFG.id_col] + ([split_col] if split_col else []))\n",
    "feature_cols = [c for c in df.columns if c not in protected_cols]\n",
    "\n",
    "print(f\"\\nDetected {len(feature_cols)} feature columns (descriptors/fingerprints).\")\n",
    "print(\"Feature sample:\", feature_cols[:10])\n",
    "\n",
    "# ---- Co-occurrence (on valid-label rows per pair)\n",
    "def cooccurrence_matrix(labels_df: pd.DataFrame, missing_val: int = -1):\n",
    "    names = list(labels_df.columns)\n",
    "    n = len(names)\n",
    "    cooc = np.zeros((n, n), dtype=int)\n",
    "    pospos = np.zeros((n, n), dtype=int)\n",
    "    for i in range(n):\n",
    "        vi = labels_df[names[i]]\n",
    "        mask_i = (vi != missing_val)\n",
    "        for j in range(i, n):\n",
    "            vj = labels_df[names[j]]\n",
    "            mask_j = (vj != missing_val)\n",
    "            vmask = mask_i & mask_j\n",
    "            if vmask.sum() == 0:\n",
    "                continue\n",
    "            vi_v = vi[vmask].values\n",
    "            vj_v = vj[vmask].values\n",
    "            cooc_ij = vi_v.shape[0]\n",
    "            pp_ij = int(((vi_v == 1) & (vj_v == 1)).sum())\n",
    "            cooc[i, j] = cooc[j, i] = cooc_ij\n",
    "            pospos[i, j] = pospos[j, i] = pp_ij\n",
    "    cooc_df = pd.DataFrame(cooc, index=names, columns=names)\n",
    "    pospos_df = pd.DataFrame(pospos, index=names, columns=names)\n",
    "    return cooc_df, pospos_df\n",
    "\n",
    "cooc_df, pospos_df = cooccurrence_matrix(label_df, missing_label_value)\n",
    "\n",
    "# ---- Save artifacts ----\n",
    "data_card = {\n",
    "    \"n_rows\": int(df.shape[0]),\n",
    "    \"columns\": list(df.columns),\n",
    "    \"id_unique\": int(df[CFG.id_col].nunique()),\n",
    "    \"smiles\": {\n",
    "        \"n_empty\": n_empty_smiles,\n",
    "        \"n_invalid\": n_invalid,\n",
    "        \"len_over_max\": len_over,\n",
    "        \"max_len\": int(smiles_len.max()),\n",
    "        \"mean_len\": float(smiles_len.mean())\n",
    "    },\n",
    "    \"labels\": per_label,\n",
    "    \"split\": {\n",
    "        \"column\": split_col,\n",
    "        \"counts\": split_counts\n",
    "    },\n",
    "    \"n_features\": len(feature_cols),\n",
    "    \"feature_sample\": feature_cols[:20],\n",
    "}\n",
    "\n",
    "card_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_data_card.json\")\n",
    "with open(card_path, \"w\") as f:\n",
    "    json.dump(data_card, f, indent=2)\n",
    "\n",
    "per_label_csv = os.path.join(RES_DIR, f\"{CFG.run_tag}_per_label_stats.csv\")\n",
    "per_label_df.to_csv(per_label_csv, index=True)\n",
    "\n",
    "cooc_csv = os.path.join(RES_DIR, f\"{CFG.run_tag}_cooccurrence_counts.csv\")\n",
    "pospos_csv = os.path.join(RES_DIR, f\"{CFG.run_tag}_pospos_counts.csv\")\n",
    "cooc_df.to_csv(cooc_csv)\n",
    "pospos_df.to_csv(pospos_csv)\n",
    "\n",
    "# Save a copy with ensured ID column (no data leakage; just adds mol_id if needed)\n",
    "csv_with_id_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_dataset_with_id.csv\")\n",
    "df.to_csv(csv_with_id_path, index=False)\n",
    "\n",
    "print(\"\\nSaved:\")\n",
    "print(\" - Data card:\", card_path)\n",
    "print(\" - Per-label stats CSV:\", per_label_csv)\n",
    "print(\" - Co-occurrence counts CSV:\", cooc_csv)\n",
    "print(\" - Pos-Pos counts CSV:\", pospos_csv)\n",
    "print(\" - Dataset copy with ID:\", csv_with_id_path)\n",
    "print(\"\\nCell 3 ✅ — Dataset validated and summarized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2681ab6",
   "metadata": {},
   "source": [
    "## 4: Tokenizer + SMILES enumeration utilities + Descriptor scaler (fit on train only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796624d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: DeepChem/ChemBERTa-77M-MLM\n",
      "Tokenizer vocab size: 591\n",
      "Special tokens: {'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '[UNK]', 'sep_token': '[SEP]', 'pad_token': '[PAD]', 'cls_token': '[CLS]', 'mask_token': '[MASK]'}\n",
      "Encoded sample shapes: {'input_ids': (1, 256), 'attention_mask': (1, 256)}\n",
      "SMILES TTA example: ['CCOc1ccc2nc(S(N)(=O)=O)sc2c1', 'c12nc(sc2cc(OCC)cc1)S(=O)(=O)N', 'n1c(sc2c1ccc(OCC)c2)S(N)(=O)=O']\n",
      "\n",
      "Descriptor scaler fitted on TRAIN split:\n",
      " - Train rows (for scaler): 6265\n",
      " - Total descriptor cols: 257\n",
      " - Used numeric cols: 257\n",
      " - Dropped non-numeric: 0\n",
      " - NaNs before impute: 0 | after: 0\n",
      " - Saved scaler → d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v5\\rdkit_desc_scaler_v5.pkl\n",
      " - Saved feature names → d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v5\\v5_20250903_124631_feature_names.json\n",
      " - Saved tokenizer info → d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v5\\v5_20250903_124631_tokenizer_info.json\n",
      "\n",
      "Cell 4 ✅ — Tokenizer ready, SMILES TTA utils ready, descriptor scaler saved.\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, pickle, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import MolToSmiles\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# We rely on df, label_df, split_col, feature_cols from Cell 3.\n",
    "assert 'df' in globals(), \"Run Cell 3 first to load the dataset.\"\n",
    "assert 'feature_cols' in globals() and len(feature_cols) > 0, \"feature_cols missing; rerun Cell 3.\"\n",
    "assert CFG.smiles_col in df.columns, f\"SMILES column '{CFG.smiles_col}' missing.\"\n",
    "assert CFG.id_col in df.columns, f\"ID column '{CFG.id_col}' missing.\"\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Tokenizer load & checks\n",
    "# ---------------------------\n",
    "print(\"Loading tokenizer:\", CFG.text_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.text_model_name)\n",
    "print(\"Tokenizer vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Special tokens:\", tokenizer.special_tokens_map)\n",
    "\n",
    "sample_smiles = df[CFG.smiles_col].iloc[0]\n",
    "enc = tokenizer(\n",
    "    sample_smiles,\n",
    "    max_length=CFG.max_smiles_len,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "print(\"Encoded sample shapes:\", {k: tuple(v.shape) for k, v in enc.items()})\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 2) SMILES enumeration utilities (RandSMILES/TTA)\n",
    "# -------------------------------------------------\n",
    "def canonicalize_smiles(s: str) -> str:\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None:\n",
    "        return \"\"\n",
    "    return MolToSmiles(m, canonical=True)\n",
    "\n",
    "def randomize_smiles(s: str, max_tries: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Prefer RDKit's internal randomization. Fall back to safe renumbering\n",
    "    using a Python-int list (avoids numpy.int32 TypeError).\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None:\n",
    "        return \"\"\n",
    "    # Primary: RDKit random SMILES\n",
    "    for _ in range(max_tries):\n",
    "        try:\n",
    "            rs = MolToSmiles(mol, canonical=False, doRandom=True)\n",
    "            if rs:\n",
    "                return rs\n",
    "        except Exception:\n",
    "            pass\n",
    "        # Fallback: renumber with pure Python ints\n",
    "        n = mol.GetNumAtoms()\n",
    "        order = [int(i) for i in np.random.permutation(n).tolist()]\n",
    "        try:\n",
    "            renum = Chem.RenumberAtoms(mol, order)\n",
    "            rs = MolToSmiles(renum, canonical=False)\n",
    "            if rs:\n",
    "                return rs\n",
    "        except Exception:\n",
    "            continue\n",
    "    # Last resort: canonical\n",
    "    return MolToSmiles(mol, canonical=True)\n",
    "\n",
    "def enumerate_smiles_for_tta(s: str, n: int) -> list:\n",
    "    out = [canonicalize_smiles(s)]\n",
    "    for _ in range(max(0, n-1)):\n",
    "        out.append(randomize_smiles(s))\n",
    "    return out\n",
    "\n",
    "# quick enumeration smoke test\n",
    "test_enum = enumerate_smiles_for_tta(sample_smiles, n=min(3, CFG.tta_smiles_at_infer))\n",
    "print(\"SMILES TTA example:\", test_enum[:3])\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 3) Descriptor scaler (fit on TRAIN only, save)\n",
    "# -------------------------------------------------\n",
    "assert 'split_col' in globals(), \"split_col missing (from Cell 3).\"\n",
    "if split_col is None:\n",
    "    print(\"WARNING: No split column found; fitting scaler on ALL data (may leak).\")\n",
    "    df_train = df\n",
    "else:\n",
    "    df_train = df[df[split_col] == \"train\"].copy()\n",
    "    assert len(df_train) > 0, \"Empty train split.\"\n",
    "\n",
    "X_train_desc = df_train[feature_cols].copy()\n",
    "\n",
    "# Impute medians computed from TRAIN only\n",
    "medians = X_train_desc.median(numeric_only=True)\n",
    "nan_counts = X_train_desc.isna().sum().sum()\n",
    "X_train_desc = X_train_desc.fillna(medians)\n",
    "nan_counts_after = X_train_desc.isna().sum().sum()\n",
    "\n",
    "# Drop non-numeric columns (e.g., row_id)\n",
    "non_numeric_cols = [c for c in X_train_desc.columns if not np.issubdtype(X_train_desc[c].dtype, np.number)]\n",
    "if non_numeric_cols:\n",
    "    print(\"INFO: Dropping non-numeric descriptor columns:\", non_numeric_cols)\n",
    "    X_train_desc = X_train_desc.drop(columns=non_numeric_cols)\n",
    "\n",
    "# Save the numeric median map for inference imputation\n",
    "medians_numeric = X_train_desc.median(numeric_only=True)\n",
    "\n",
    "# Fit scaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_desc.values)\n",
    "\n",
    "# Persist scaler + metadata\n",
    "scaler_pack = {\n",
    "    \"feature_names\": list(X_train_desc.columns),\n",
    "    \"medians\": medians_numeric.to_dict(),\n",
    "    \"scaler_mean_\": scaler.mean_.tolist(),\n",
    "    \"scaler_scale_\": scaler.scale_.tolist(),\n",
    "}\n",
    "with open(CFG.desc_scaler_path, \"wb\") as f:\n",
    "    pickle.dump({\"sk_scaler\": scaler, \"pack\": scaler_pack}, f)\n",
    "\n",
    "# Also save a stable feature order file\n",
    "feature_names_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_feature_names.json\")\n",
    "with open(feature_names_path, \"w\") as f:\n",
    "    json.dump(scaler_pack[\"feature_names\"], f, indent=2)\n",
    "\n",
    "print(\"\\nDescriptor scaler fitted on TRAIN split:\")\n",
    "print(\" - Train rows (for scaler):\", len(df_train))\n",
    "print(\" - Total descriptor cols:\", len(feature_cols))\n",
    "print(\" - Used numeric cols:\", len(scaler_pack['feature_names']))\n",
    "print(\" - Dropped non-numeric:\", len(non_numeric_cols))\n",
    "print(\" - NaNs before impute:\", nan_counts, \"| after:\", nan_counts_after)\n",
    "print(\" - Saved scaler →\", os.path.abspath(CFG.desc_scaler_path))\n",
    "print(\" - Saved feature names →\", os.path.abspath(feature_names_path))\n",
    "\n",
    "# -------------------------------------------------\n",
    "# 4) Persist minimal tokenizer info (for reproducibility)\n",
    "# -------------------------------------------------\n",
    "tok_info_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_tokenizer_info.json\")\n",
    "with open(tok_info_path, \"w\") as f:\n",
    "    json.dump({\n",
    "        \"model_name\": CFG.text_model_name,\n",
    "        \"vocab_size\": tokenizer.vocab_size,\n",
    "        \"max_smiles_len\": CFG.max_smiles_len,\n",
    "        \"special_tokens\": tokenizer.special_tokens_map\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\" - Saved tokenizer info →\", os.path.abspath(tok_info_path))\n",
    "print(\"\\nCell 4 ✅ — Tokenizer ready, SMILES TTA utils ready, descriptor scaler saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5fe9c0",
   "metadata": {},
   "source": [
    "## 5: Rich atom/bond featurizers + graph builder (+ sanity checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f6469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atom feature length: 56 | Bond feature length: 12\n",
      "\n",
      "Graph mini-batch sanity:\n",
      " - #graphs: 16\n",
      " - nodes per graph (min/mean/max): 5/16.9/28\n",
      " - edges per graph (min/mean/max): 8/35.0/62\n",
      " - atom_feat_dim: 56 | bond_feat_dim: 12\n",
      " - virtual node enabled: True\n",
      " - Saved featurizer spec → d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v5\\v5_20250903_124631_featurizer_spec.json\n",
      "\n",
      "Cell 5 ✅ — Featurizers and graph builder ready.\n"
     ]
    }
   ],
   "source": [
    "import enum\n",
    "from typing import List, Dict, Any, Tuple, Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "\n",
    "# ----------------------------\n",
    "# 1) Enumerations & one-hot\n",
    "# ----------------------------\n",
    "class Hybrid(enum.Enum):\n",
    "    SP  = Chem.rdchem.HybridizationType.SP\n",
    "    SP2 = Chem.rdchem.HybridizationType.SP2\n",
    "    SP3 = Chem.rdchem.HybridizationType.SP3\n",
    "    SP3D = Chem.rdchem.HybridizationType.SP3D\n",
    "    SP3D2 = Chem.rdchem.HybridizationType.SP3D2\n",
    "    OTHER = 999\n",
    "\n",
    "HYB_SET = [Hybrid.SP, Hybrid.SP2, Hybrid.SP3, Hybrid.SP3D, Hybrid.SP3D2, Hybrid.OTHER]\n",
    "\n",
    "def one_hot(x, choices):\n",
    "    out = [0]*len(choices)\n",
    "    try:\n",
    "        idx = choices.index(x)\n",
    "        out[idx] = 1\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return out\n",
    "\n",
    "def clip_int(x: int, lo: int, hi: int) -> int:\n",
    "    return int(max(lo, min(hi, int(x))))\n",
    "\n",
    "# ----------------------------\n",
    "# 2) Atom featurizer (chemprop_plus)\n",
    "# ----------------------------\n",
    "ATOM_SYMS = [\n",
    "    'C','N','O','S','F','Cl','Br','I','P','B','Si','Se','Te','At','H','Li','Na','K','Ca','Mg'\n",
    "]\n",
    "def atom_features_chemprop_plus(atom: Chem.Atom) -> List[float]:\n",
    "    # Base categorical: symbol (cap rare as 'other'), degree(0–5), formal charge(-2..+2), implicit H(0–3), valence(0–6), hybridization, aromatic, in_ring\n",
    "    sym = atom.GetSymbol()\n",
    "    sym_onehot = [1 if sym == s else 0 for s in ATOM_SYMS] + [0]  # +1 slot for \"other\"\n",
    "    if sym not in ATOM_SYMS:\n",
    "        sym_onehot[-1] = 1\n",
    "\n",
    "    degree = clip_int(atom.GetDegree(), 0, 5)\n",
    "    degree_oh = one_hot(degree, list(range(0,6)))\n",
    "\n",
    "    formal = clip_int(atom.GetFormalCharge(), -2, 2)\n",
    "    formal_oh = one_hot(formal, [-2,-1,0,1,2])\n",
    "\n",
    "    impl_h = clip_int(atom.GetTotalNumHs(includeNeighbors=True), 0, 3)\n",
    "    impl_h_oh = one_hot(impl_h, [0,1,2,3])\n",
    "\n",
    "    valence = clip_int(atom.GetTotalValence(), 0, 6)\n",
    "    valence_oh = one_hot(valence, list(range(0,7)))\n",
    "\n",
    "    hyb = atom.GetHybridization()\n",
    "    hyb_enum = (hyb if hyb in [h.value for h in Hybrid] else Hybrid.OTHER.value)\n",
    "    hyb_map = [h.value for h in HYB_SET]\n",
    "    hyb_oh = one_hot(hyb_enum, hyb_map)\n",
    "\n",
    "    aromatic = [1 if atom.GetIsAromatic() else 0]\n",
    "    in_ring = [1 if atom.IsInRing() else 0]\n",
    "\n",
    "    # Numeric extras: atomic number (clipped), mass (scaled), pvalence (clipped)\n",
    "    anum = clip_int(atom.GetAtomicNum(), 1, 100)\n",
    "    mass = atom.GetMass() * 0.01  # simple scale to ~[1..20]\n",
    "    pval = clip_int(atom.GetImplicitValence() + atom.GetExplicitValence(), 0, 8)\n",
    "\n",
    "    # Chirality\n",
    "    chiral = [1 if atom.HasProp('_ChiralityPossible') else 0]\n",
    "    stereo_cent = [1 if str(atom.GetChiralTag()) != \"CHI_UNSPECIFIED\" else 0]\n",
    "\n",
    "    feats = sym_onehot + degree_oh + formal_oh + impl_h_oh + valence_oh + hyb_oh \\\n",
    "            + aromatic + in_ring + [anum, mass, pval] + chiral + stereo_cent\n",
    "    return [float(v) for v in feats]\n",
    "\n",
    "# ----------------------------\n",
    "# 3) Bond featurizer (chemprop_plus)\n",
    "# ----------------------------\n",
    "def bond_features_chemprop_plus(bond: Optional[Chem.Bond]) -> List[float]:\n",
    "    if bond is None:\n",
    "        # For self-loops (if ever added), return zeros of bond feature length\n",
    "        return [0.0]*10\n",
    "    btype = bond.GetBondType()\n",
    "    bt = [\n",
    "        1 if btype == Chem.rdchem.BondType.SINGLE else 0,\n",
    "        1 if btype == Chem.rdchem.BondType.DOUBLE else 0,\n",
    "        1 if btype == Chem.rdchem.BondType.TRIPLE else 0,\n",
    "        1 if btype == Chem.rdchem.BondType.AROMATIC else 0,\n",
    "    ]\n",
    "    conj = [1 if bond.GetIsConjugated() else 0]\n",
    "    in_ring = [1 if bond.IsInRing() else 0]\n",
    "    # Stereo (E/Z etc.)\n",
    "    stereo = bond.GetStereo()\n",
    "    stereo_oh = one_hot(int(stereo), list(range(0,6)))  # 0..5 covers RDKit enum\n",
    "    feats = bt + conj + in_ring + stereo_oh\n",
    "    return [float(v) for v in feats]\n",
    "\n",
    "# Compute fixed lengths for sanity\n",
    "_atom_len = len(atom_features_chemprop_plus(Chem.MolFromSmiles(\"C\").GetAtomWithIdx(0)))\n",
    "_bond_len = len(bond_features_chemprop_plus(Chem.MolFromSmiles(\"C=C\").GetBondWithIdx(0)))\n",
    "print(f\"Atom feature length: {_atom_len} | Bond feature length: {_bond_len}\")\n",
    "\n",
    "# ----------------------------\n",
    "# 4) SMILES → Graph dict\n",
    "# ----------------------------\n",
    "def smiles_to_graph(smiles: str,\n",
    "                    use_virtual_node: bool = True\n",
    "                   ) -> Dict[str, Any]:\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        raise ValueError(\"Invalid SMILES encountered in graph builder.\")\n",
    "\n",
    "    n = mol.GetNumAtoms()\n",
    "    # Nodes\n",
    "    x = np.zeros((n, _atom_len), dtype=np.float32)\n",
    "    for i, atom in enumerate(mol.GetAtoms()):\n",
    "        x[i, :] = np.array(atom_features_chemprop_plus(atom), dtype=np.float32)\n",
    "\n",
    "    # Edges (undirected → add both directions)\n",
    "    edges_src, edges_dst, eattr = [], [], []\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bf = bond_features_chemprop_plus(b)\n",
    "        edges_src += [i, j]\n",
    "        edges_dst += [j, i]\n",
    "        eattr += [bf, bf]\n",
    "\n",
    "    edge_index = np.vstack([np.array(edges_src, dtype=np.int64),\n",
    "                            np.array(edges_dst, dtype=np.int64)]) if edges_src else np.zeros((2,0), dtype=np.int64)\n",
    "    edge_attr  = np.array(eattr, dtype=np.float32) if eattr else np.zeros((0, _bond_len), dtype=np.float32)\n",
    "\n",
    "    # Optional: virtual node does not need to be added here structurally;\n",
    "    # we will handle a learnable virtual embedding in the model and aggregate with it.\n",
    "    g = {\n",
    "        \"x\": x,                       # (N, F_atom)\n",
    "        \"edge_index\": edge_index,     # (2, E)\n",
    "        \"edge_attr\": edge_attr,       # (E, F_bond)\n",
    "        \"num_nodes\": int(n),\n",
    "        \"use_virtual_node\": bool(use_virtual_node),\n",
    "    }\n",
    "    return g\n",
    "\n",
    "# ----------------------------\n",
    "# 5) Sanity checks on a mini-batch\n",
    "# ----------------------------\n",
    "# Use train split for quick sampling\n",
    "_df_train = df[df[split_col] == \"train\"].reset_index(drop=True) if split_col else df\n",
    "smiles_sample = _df_train[CFG.smiles_col].sample(16, random_state=42).tolist()\n",
    "\n",
    "graphs = [smiles_to_graph(s, use_virtual_node=CFG.use_virtual_node) for s in smiles_sample]\n",
    "\n",
    "# Validate shapes and types\n",
    "assert all(isinstance(g[\"x\"], np.ndarray) and g[\"x\"].dtype == np.float32 for g in graphs)\n",
    "assert all(isinstance(g[\"edge_index\"], np.ndarray) and g[\"edge_index\"].dtype == np.int64 for g in graphs)\n",
    "assert all(isinstance(g[\"edge_attr\"], np.ndarray) and g[\"edge_attr\"].dtype == np.float32 for g in graphs)\n",
    "\n",
    "node_counts = [g[\"num_nodes\"] for g in graphs]\n",
    "edge_counts = [g[\"edge_index\"].shape[1] for g in graphs]\n",
    "atom_feat_dim = graphs[0][\"x\"].shape[1]\n",
    "bond_feat_dim = graphs[0][\"edge_attr\"].shape[1] if edge_counts[0] > 0 else _bond_len\n",
    "\n",
    "print(f\"\\nGraph mini-batch sanity:\")\n",
    "print(f\" - #graphs: {len(graphs)}\")\n",
    "print(f\" - nodes per graph (min/mean/max): {min(node_counts)}/{np.mean(node_counts):.1f}/{max(node_counts)}\")\n",
    "print(f\" - edges per graph (min/mean/max): {min(edge_counts)}/{np.mean(edge_counts):.1f}/{max(edge_counts)}\")\n",
    "print(f\" - atom_feat_dim: {atom_feat_dim} | bond_feat_dim: {bond_feat_dim}\")\n",
    "print(f\" - virtual node enabled: {CFG.use_virtual_node}\")\n",
    "\n",
    "# Save featurizer spec for reproducibility\n",
    "featurizer_spec = {\n",
    "    \"atom_feature_len\": int(atom_feat_dim),\n",
    "    \"bond_feature_len\": int(bond_feat_dim),\n",
    "    \"atom_symbol_vocab\": ATOM_SYMS,\n",
    "    \"hybridization_set\": [h.name for h in HYB_SET],\n",
    "    \"use_virtual_node\": CFG.use_virtual_node,\n",
    "    \"feature_set\": {\n",
    "        \"atom\": CFG.atom_feature_set,\n",
    "        \"bond\": CFG.bond_feature_set\n",
    "    }\n",
    "}\n",
    "spec_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_featurizer_spec.json\")\n",
    "with open(spec_path, \"w\") as f:\n",
    "    json.dump(featurizer_spec, f, indent=2)\n",
    "print(\" - Saved featurizer spec →\", os.path.abspath(spec_path))\n",
    "\n",
    "print(\"\\nCell 5 ✅ — Featurizers and graph builder ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a3e8f6",
   "metadata": {},
   "source": [
    "## 6: Dataset, collate, positive-aware sampler, DataLoaders (+ sanity batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c79abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split sizes → train: 6265, val: 783, test: 783\n",
      "Using positive-aware WeightedRandomSampler on TRAIN.\n",
      "\n",
      "Sanity batch (train) — after patch:\n",
      " - batch_size: 32\n",
      " - text_ids: (32, 256) torch.int64\n",
      " - attn_mask: (32, 256) torch.int64\n",
      " - desc: (32, 257) torch.float32\n",
      " - graph.x: (766, 56) torch.float32\n",
      " - graph.edge_index: (2, 1664) torch.int64\n",
      " - graph.edge_attr: (1664, 12) torch.float32\n",
      " - graph.batch: (766,) torch.int64\n",
      " - y: (32, 12) torch.float32 | mask: (32, 12) torch.float32\n",
      " - ids[0]: 275017364c044555\n",
      "\n",
      "Patch ✅ — DataLoaders good to go.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# ---------- prerequisites from earlier cells ----------\n",
    "assert 'df' in globals() and 'split_col' in globals(), \"Run Cells 3–5 first.\"\n",
    "assert 'CFG' in globals(), \"Run Cell 2 first.\"\n",
    "assert 'tokenizer' in globals(), \"Run Cell 4 first.\"\n",
    "assert 'smiles_to_graph' in globals(), \"Run Cell 5 first.\"\n",
    "assert 'row_to_desc_vector' in globals(), \"Cell 6 (desc scaler utils) must be run first.\"\n",
    "assert 'canonicalize_smiles' in globals() and 'randomize_smiles' in globals(), \"Run Cell 4 first.\"\n",
    "assert 'collate_batch' in globals(), \"Run the original Cell 6 to define collate_batch.\"\n",
    "\n",
    "# ---------- dataset (fixed: use list(CFG.label_cols) for indexing) ----------\n",
    "class Tox21V5Dataset(Dataset):\n",
    "    def __init__(self, frame: pd.DataFrame, split: str):\n",
    "        self.df = frame.reset_index(drop=True)\n",
    "        self.split = split\n",
    "        self.labels = list(CFG.label_cols)\n",
    "        self.id_col = CFG.id_col\n",
    "        self.smiles_col = CFG.smiles_col\n",
    "        self.do_rand_smiles = (split == \"train\") and CFG.do_random_smiles_train\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def _get_smiles(self, s: str) -> str:\n",
    "        if self.do_rand_smiles:\n",
    "            return randomize_smiles(s)\n",
    "        return canonicalize_smiles(s)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        smi = self._get_smiles(str(row[self.smiles_col]))\n",
    "\n",
    "        # text branch\n",
    "        enc = tokenizer(\n",
    "            smi,\n",
    "            max_length=CFG.max_smiles_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=None\n",
    "        )\n",
    "        input_ids = np.asarray(enc[\"input_ids\"], dtype=np.int64)\n",
    "        attention_mask = np.asarray(enc[\"attention_mask\"], dtype=np.int64)\n",
    "\n",
    "        # graph branch\n",
    "        g = smiles_to_graph(smi, use_virtual_node=CFG.use_virtual_node)\n",
    "\n",
    "        # descriptor branch\n",
    "        desc_vec = row_to_desc_vector(row)\n",
    "\n",
    "        # labels & mask  --- FIXED: list(CFG.label_cols)\n",
    "        y_raw = row[list(CFG.label_cols)].values.astype(np.float32)     # {0,1,-1}\n",
    "        mask = (y_raw != CFG.missing_label_value).astype(np.float32)    # 1=valid, 0=ignore\n",
    "        y = np.where(y_raw == CFG.missing_label_value, 0.0, y_raw)      # replace -1→0 (masked in loss)\n",
    "\n",
    "        return {\n",
    "            \"id\": row[CFG.id_col],\n",
    "            \"smiles\": smi,\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": attention_mask,\n",
    "            \"x\": g[\"x\"],\n",
    "            \"edge_index\": g[\"edge_index\"],\n",
    "            \"edge_attr\": g[\"edge_attr\"],\n",
    "            \"num_nodes\": g[\"num_nodes\"],\n",
    "            \"desc\": desc_vec,\n",
    "            \"y\": y,\n",
    "            \"mask\": mask\n",
    "        }\n",
    "\n",
    "# ---------- positive-aware sample weights (re-define here for completeness) ----------\n",
    "def compute_positive_aware_weights(frame: pd.DataFrame) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    For each sample, weight = sum over labels where y=1 of (1 / pos_freq[label]) on valid labels.\n",
    "    Baseline weight given to all-negative samples to keep them in play.\n",
    "    \"\"\"\n",
    "    L = list(CFG.label_cols)\n",
    "    freqs = {}\n",
    "    for c in L:\n",
    "        col = frame[c].values\n",
    "        valid = col != CFG.missing_label_value\n",
    "        pos = (col == 1) & valid\n",
    "        n_valid = int(valid.sum())\n",
    "        pos_freq = (int(pos.sum()) / max(1, n_valid)) if n_valid > 0 else 1e-6\n",
    "        freqs[c] = max(pos_freq, 1e-6)\n",
    "\n",
    "    weights = np.zeros(len(frame), dtype=np.float64)\n",
    "    avg_pos = float(np.mean(list(freqs.values())))\n",
    "    baseline = 0.1 / (avg_pos + 1e-6)\n",
    "    for i, (_, row) in enumerate(frame.iterrows()):\n",
    "        w = 0.0\n",
    "        for c in L:\n",
    "            v = row[c]\n",
    "            if v == 1:\n",
    "                w += 1.0 / freqs[c]\n",
    "        if w == 0.0:\n",
    "            w = baseline\n",
    "        weights[i] = w\n",
    "\n",
    "    # normalize to mean=1\n",
    "    weights = weights / (weights.mean() + 1e-12)\n",
    "    return weights\n",
    "\n",
    "# ---------- rebuild splits ----------\n",
    "if split_col is None:\n",
    "    print(\"WARNING: No split column; creating 80/10/10 proxy split (deterministic).\")\n",
    "    df_shuf = df.sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "    n = len(df_shuf); n_val = int(0.1*n); n_test = int(0.1*n)\n",
    "    df_val = df_shuf.iloc[:n_val].copy()\n",
    "    df_test = df_shuf.iloc[n_val:n_val+n_test].copy()\n",
    "    df_train = df_shuf.iloc[n_val+n_test:].copy()\n",
    "else:\n",
    "    df_train = df[df[split_col] == \"train\"].reset_index(drop=True)\n",
    "    df_val   = df[df[split_col] == \"val\"].reset_index(drop=True)\n",
    "    df_test  = df[df[split_col] == \"test\"].reset_index(drop=True)\n",
    "\n",
    "print(f\"Split sizes → train: {len(df_train)}, val: {len(df_val)}, test: {len(df_test)}\")\n",
    "\n",
    "# ---------- datasets ----------\n",
    "ds_train = Tox21V5Dataset(df_train, split=\"train\")\n",
    "ds_val   = Tox21V5Dataset(df_val,   split=\"val\")\n",
    "ds_test  = Tox21V5Dataset(df_test,  split=\"test\")\n",
    "\n",
    "# ---------- DataLoader settings (Windows-safe) ----------\n",
    "CFG.num_workers = 0\n",
    "_PIN = False\n",
    "BATCH_TRAIN = min(32, CFG.batch_size)  # conservative sanity batch\n",
    "BATCH_EVAL  = BATCH_TRAIN\n",
    "\n",
    "# ---------- sampler (Windows-safe; optional) ----------\n",
    "USE_POSITIVE_AWARE_SAMPLER = True\n",
    "train_sampler = None\n",
    "if USE_POSITIVE_AWARE_SAMPLER:\n",
    "    try:\n",
    "        w = compute_positive_aware_weights(df_train)\n",
    "        train_sampler = WeightedRandomSampler(\n",
    "            weights=torch.as_tensor(w, dtype=torch.double),\n",
    "            num_samples=len(df_train),\n",
    "            replacement=True\n",
    "        )\n",
    "        print(\"Using positive-aware WeightedRandomSampler on TRAIN.\")\n",
    "    except Exception as e:\n",
    "        print(\"Sampler construction failed, fallback to shuffle. Reason:\", str(e))\n",
    "        train_sampler = None\n",
    "\n",
    "# ---------- loaders ----------\n",
    "loader_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=BATCH_TRAIN,\n",
    "    sampler=train_sampler,\n",
    "    shuffle=(train_sampler is None),\n",
    "    num_workers=CFG.num_workers,\n",
    "    collate_fn=collate_batch,\n",
    "    pin_memory=_PIN,\n",
    "    drop_last=True,\n",
    "    persistent_workers=False\n",
    ")\n",
    "loader_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=BATCH_EVAL,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.num_workers,\n",
    "    collate_fn=collate_batch,\n",
    "    pin_memory=_PIN,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "loader_test = DataLoader(\n",
    "    ds_test,\n",
    "    batch_size=BATCH_EVAL,\n",
    "    shuffle=False,\n",
    "    num_workers=CFG.num_workers,\n",
    "    collate_fn=collate_batch,\n",
    "    pin_memory=_PIN,\n",
    "    drop_last=False,\n",
    "    persistent_workers=False\n",
    ")\n",
    "\n",
    "# ---------- sanity batch ----------\n",
    "batch = next(iter(loader_train))\n",
    "text_ids = batch[\"text\"][\"input_ids\"]; text_mask = batch[\"text\"][\"attention_mask\"]\n",
    "gx = batch[\"graph\"][\"x\"]; gei = batch[\"graph\"][\"edge_index\"]; gea = batch[\"graph\"][\"edge_attr\"]; gb = batch[\"graph\"][\"batch\"]\n",
    "desc = batch[\"desc\"]; y = batch[\"y\"]; msk = batch[\"mask\"]\n",
    "\n",
    "print(\"\\nSanity batch (train) — after patch:\")\n",
    "print(\" - batch_size:\", BATCH_TRAIN)\n",
    "print(\" - text_ids:\", tuple(text_ids.shape), text_ids.dtype)\n",
    "print(\" - attn_mask:\", tuple(text_mask.shape), text_mask.dtype)\n",
    "print(\" - desc:\", tuple(desc.shape), desc.dtype)\n",
    "print(\" - graph.x:\", tuple(gx.shape), gx.dtype)\n",
    "print(\" - graph.edge_index:\", tuple(gei.shape), gei.dtype)\n",
    "print(\" - graph.edge_attr:\", tuple(gea.shape), gea.dtype)\n",
    "print(\" - graph.batch:\", tuple(gb.shape), gb.dtype)\n",
    "print(\" - y:\", tuple(y.shape), y.dtype, \"| mask:\", tuple(msk.shape), msk.dtype)\n",
    "print(\" - ids[0]:\", batch[\"ids\"][0])\n",
    "print(\"\\nPatch ✅ — DataLoaders good to go.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef88ba9",
   "metadata": {},
   "source": [
    "## 7: V5 model (Text + GIN-Virtual + Label-wise Gating + Co-Attn + Classifier) + sanity forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ca5f4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward sanity (after patch):\n",
      " - logits: (32, 12)\n",
      " - alpha (gates): (32, 12, 3)\n",
      " - h_text/h_graph/h_desc/h_co: (32, 256) (32, 256) (32, 256) (32, 256)\n",
      " - z_text/z_graph: (32, 64) (32, 64)\n",
      " - logits stats: min=-0.178 med=-0.015 max=0.229\n",
      " - gate example (sample 0, label 0) [α_text, α_graph, α_desc]: [0.354182   0.3151007  0.33071736]\n",
      "\n",
      "Cell 7 (patch) ✅ — Co-attention fixed and forward pass OK.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "assert 'model' in globals(), \"Run Cell 7 first to build the model.\"\n",
    "assert 'H' in globals() and 'device' in globals()\n",
    "\n",
    "class CoAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Stable bi-directional co-attention with proper multi-head math.\n",
    "\n",
    "    attend():\n",
    "      q_seq: (Lq, H)\n",
    "      k_seq: (Lk, H)\n",
    "      v_seq: (Lk, H)\n",
    "      key_mask: (Lk,) bool, 1=valid\n",
    "    returns:\n",
    "      out: (Lq, H)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=H, heads=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert dim % heads == 0, \"dim must be divisible by heads.\"\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.d = dim // heads\n",
    "        self.scale = self.d ** -0.5\n",
    "\n",
    "        # separate projections per direction\n",
    "        self.q_text  = nn.Linear(dim, dim, bias=False)\n",
    "        self.k_text  = nn.Linear(dim, dim, bias=False)\n",
    "        self.v_text  = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "        self.q_graph = nn.Linear(dim, dim, bias=False)\n",
    "        self.k_graph = nn.Linear(dim, dim, bias=False)\n",
    "        self.v_graph = nn.Linear(dim, dim, bias=False)\n",
    "\n",
    "        self.out_text  = nn.Linear(dim, dim)\n",
    "        self.out_graph = nn.Linear(dim, dim)\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def _mh_attend(self, q_seq, k_seq, v_seq, key_mask=None):\n",
    "        \"\"\"\n",
    "        Multi-head scaled dot-product attention.\n",
    "        q_seq: (Lq,H), k_seq: (Lk,H), v_seq: (Lk,H)\n",
    "        key_mask: (Lk,) bool\n",
    "        returns (Lq,H)\n",
    "        \"\"\"\n",
    "        Lq = q_seq.size(0); Lk = k_seq.size(0)\n",
    "        h  = self.heads; d = self.d\n",
    "\n",
    "        # reshape to (h, L, d)\n",
    "        q = q_seq.view(Lq, h, d).transpose(0, 1).contiguous()  # (h, Lq, d)\n",
    "        k = k_seq.view(Lk, h, d).transpose(0, 1).contiguous()  # (h, Lk, d)\n",
    "        v = v_seq.view(Lk, h, d).transpose(0, 1).contiguous()  # (h, Lk, d)\n",
    "\n",
    "        # (h, Lq, Lk)\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        if key_mask is not None:\n",
    "            # key_mask: (Lk,) -> (1,1,Lk)\n",
    "            mask = key_mask.bool().unsqueeze(0).unsqueeze(0)  # broadcast over heads and queries\n",
    "            scores = scores.masked_fill(~mask, float('-inf'))\n",
    "\n",
    "        attn = F.softmax(scores, dim=-1)\n",
    "        attn = self.drop(attn)  # (h, Lq, Lk)\n",
    "\n",
    "        out = torch.matmul(attn, v)  # (h, Lq, d)\n",
    "        out = out.transpose(0, 1).contiguous().view(Lq, h * d)  # (Lq,H)\n",
    "        return out\n",
    "\n",
    "    def forward_single(self, txt_seq, txt_mask, g_nodes, g_mask=None):\n",
    "        \"\"\"\n",
    "        txt_seq: (Lt,H), txt_mask: (Lt,) bool\n",
    "        g_nodes: (Lg,H), g_mask: (Lg,) bool or None\n",
    "        returns two pooled vectors (H),(H)\n",
    "        \"\"\"\n",
    "        # text attends to graph\n",
    "        q_t = self.q_text(txt_seq)\n",
    "        k_g = self.k_graph(g_nodes)\n",
    "        v_g = self.v_graph(g_nodes)\n",
    "        t2g = self._mh_attend(q_t, k_g, v_g, key_mask=g_mask)\n",
    "        t2g = self.out_text(t2g)\n",
    "\n",
    "        # graph attends to text\n",
    "        q_g = self.q_graph(g_nodes)\n",
    "        k_t = self.k_text(txt_seq)\n",
    "        v_t = self.v_text(txt_seq)\n",
    "        g2t = self._mh_attend(q_g, k_t, v_t, key_mask=txt_mask)\n",
    "        g2t = self.out_graph(g2t)\n",
    "\n",
    "        # mask-aware means\n",
    "        t_mask = txt_mask.float()\n",
    "        g_mask_f = g_mask.float() if g_mask is not None else torch.ones(g_nodes.size(0), device=g_nodes.device)\n",
    "        t_pooled = (t2g * t_mask.unsqueeze(-1)).sum(dim=0) / t_mask.sum().clamp_min(1e-6)\n",
    "        g_pooled = (g2t * g_mask_f.unsqueeze(-1)).sum(dim=0) / g_mask_f.sum().clamp_min(1e-6)\n",
    "        return t_pooled, g_pooled\n",
    "\n",
    "    def forward(self, txt_seq_b, txt_mask_b, g_nodes_b, g_batch_idx, num_graphs):\n",
    "        \"\"\"\n",
    "        txt_seq_b: (B,L,H), txt_mask_b: (B,L) bool\n",
    "        g_nodes_b: (N,H), g_batch_idx: (N,), num_graphs=B\n",
    "        returns (tpool, gpool): (B,H), (B,H)\n",
    "        \"\"\"\n",
    "        device = txt_seq_b.device\n",
    "        t_list, g_list = [], []\n",
    "        for b in range(num_graphs):\n",
    "            t_seq  = txt_seq_b[b]                 # (L,H)\n",
    "            t_mask = txt_mask_b[b].bool()         # (L,)\n",
    "            g_mask_nodes = (g_batch_idx == b)     # (N,)\n",
    "            g_nodes = g_nodes_b[g_mask_nodes]     # (Lg,H)\n",
    "\n",
    "            # handle pathological empty graph (shouldn't happen)\n",
    "            if g_nodes.size(0) == 0:\n",
    "                g_nodes = torch.zeros((1, self.dim), device=device)\n",
    "                g_mask = torch.ones((1,), dtype=torch.bool, device=device)\n",
    "            else:\n",
    "                g_mask = torch.ones((g_nodes.size(0),), dtype=torch.bool, device=device)\n",
    "\n",
    "            t_p, g_p = self.forward_single(t_seq, t_mask, g_nodes, g_mask)\n",
    "            t_list.append(t_p)\n",
    "            g_list.append(g_p)\n",
    "        return torch.stack(t_list, dim=0), torch.stack(g_list, dim=0)\n",
    "\n",
    "# ---- swap the old co-attention with the fixed one (no need to rebuild other parts) ----\n",
    "model.coattn = CoAttention(dim=H, heads=4, dropout=0.1).to(device)\n",
    "\n",
    "# ---- re-run the same sanity forward on one batch ----\n",
    "batch = next(iter(loader_train))\n",
    "with torch.no_grad():\n",
    "    out = model(batch, return_proj=True)\n",
    "\n",
    "logits = out[\"logits\"]; alpha = out[\"alpha\"]\n",
    "print(\"Forward sanity (after patch):\")\n",
    "print(\" - logits:\", tuple(logits.shape))          # (B,12)\n",
    "print(\" - alpha (gates):\", tuple(alpha.shape))    # (B,12,3)\n",
    "print(\" - h_text/h_graph/h_desc/h_co:\",\n",
    "      tuple(out[\"h_text\"].shape), tuple(out[\"h_graph\"].shape),\n",
    "      tuple(out[\"h_desc\"].shape), tuple(out[\"h_co\"].shape))\n",
    "if CFG.use_infonce:\n",
    "    print(\" - z_text/z_graph:\", tuple(out[\"z_text\"].shape), tuple(out[\"z_graph\"].shape))\n",
    "print(\" - logits stats: min={:.3f} med={:.3f} max={:.3f}\".format(\n",
    "    logits.min().item(), logits.median().item(), logits.max().item()\n",
    "))\n",
    "print(\" - gate example (sample 0, label 0) [α_text, α_graph, α_desc]:\",\n",
    "      alpha[0,0].detach().cpu().numpy())\n",
    "\n",
    "print(\"\\nCell 7 (patch) ✅ — Co-attention fixed and forward pass OK.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef017d2f",
   "metadata": {},
   "source": [
    "## 8: Losses (ASL + InfoNCE), EMA, optimizer/scheduler, one-batch backprop sanity + save init checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486a65a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-batch backprop sanity:\n",
      " - loss_cls: 0.3464\n",
      " - loss_nce: 3.6128 (weighted x0.5)\n",
      " - gate_L1:  0.0033\n",
      " - total:    2.1561\n",
      "Saved init checkpoint → d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v5\\v5_20250903_124631_init_sanity.pt\n",
      "\n",
      "Cell 8 ✅ — Training plumbing ready (ASL+InfoNCE+EMA). We can now script Stage A/B/C runs.\n"
     ]
    }
   ],
   "source": [
    "import math, os, json, time\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import AdamW\n",
    "\n",
    "assert 'model' in globals() and 'loader_train' in globals()\n",
    "assert 'CFG' in globals() and 'CKPT_DIR' in globals() and 'RES_DIR' in globals()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.train()\n",
    "\n",
    "# -------------------------\n",
    "# 1) ASL loss (multi-label)\n",
    "# -------------------------\n",
    "class AsymmetricLossMultiLabel(nn.Module):\n",
    "    \"\"\"\n",
    "    ASL for multi-label with logits input.\n",
    "    Supports positive/negative focusing and negative margin (m).\n",
    "    Respects label masks (mask=0 ignores a label).\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma_pos=0.0, gamma_neg=3.0, m=0.05, reduction=\"mean\"):\n",
    "        super().__init__()\n",
    "        self.gp = gamma_pos\n",
    "        self.gn = gamma_neg\n",
    "        self.m  = m\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, logits, targets, mask=None):\n",
    "        # logits/targets shape: (B, L)\n",
    "        # mask shape: (B, L) with 1 for valid, 0 to ignore\n",
    "        x = logits\n",
    "        y = targets\n",
    "        if mask is None:\n",
    "            mask = torch.ones_like(y)\n",
    "        # probability\n",
    "        x_sig = torch.sigmoid(x)\n",
    "        # apply margin to negatives: p' = clamp(p + m * (1-y), 0, 1)\n",
    "        if self.m > 0:\n",
    "            x_sig = torch.clamp(x_sig + self.m * (1.0 - y), min=0., max=1.)\n",
    "\n",
    "        # cross-entropy parts\n",
    "        loss_pos = - y * torch.log(x_sig.clamp_min(1e-8))\n",
    "        loss_neg = - (1.0 - y) * torch.log((1.0 - x_sig).clamp_min(1e-8))\n",
    "\n",
    "        # focusing\n",
    "        if self.gp > 0:\n",
    "            loss_pos *= torch.pow(1.0 - x_sig, self.gp)\n",
    "        if self.gn > 0:\n",
    "            loss_neg *= torch.pow(x_sig, self.gn)\n",
    "\n",
    "        loss = loss_pos + loss_neg\n",
    "        # mask invalid labels\n",
    "        loss = loss * mask\n",
    "\n",
    "        if self.reduction == \"mean\":\n",
    "            denom = mask.sum().clamp_min(1e-6)\n",
    "            return loss.sum() / denom\n",
    "        elif self.reduction == \"sum\":\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "asl = AsymmetricLossMultiLabel(\n",
    "    gamma_pos=CFG.asl_gamma_pos, gamma_neg=CFG.asl_gamma_neg, m=CFG.asl_m\n",
    ")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) InfoNCE (text↔graph alignment)\n",
    "# --------------------------------\n",
    "class InfoNCELoss(nn.Module):\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.t = temperature\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        # z1,z2: (B, D) normalized vectors\n",
    "        logits = (z1 @ z2.t()) / self.t              # (B,B)\n",
    "        labels = torch.arange(z1.size(0), device=z1.device)\n",
    "        return F.cross_entropy(logits, labels)\n",
    "\n",
    "infonce_loss_fn = InfoNCELoss(temperature=0.07)\n",
    "\n",
    "# --------------------------------\n",
    "# 3) EMA for model weights\n",
    "# --------------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[name] = p.data.clone()\n",
    "\n",
    "    def update(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad: \n",
    "                continue\n",
    "            assert name in self.shadow\n",
    "            new_avg = (1.0 - self.decay) * p.data + self.decay * self.shadow[name]\n",
    "            self.shadow[name] = new_avg.clone()\n",
    "\n",
    "    def apply_shadow(self, model):\n",
    "        self.backup = {}\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad: \n",
    "                continue\n",
    "            self.backup[name] = p.data.clone()\n",
    "            p.data = self.shadow[name]\n",
    "\n",
    "    def restore(self, model):\n",
    "        for name, p in model.named_parameters():\n",
    "            if not p.requires_grad: \n",
    "                continue\n",
    "            p.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "ema = EMA(model, decay=CFG.ema_decay)\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 4) Optimizer and cosine LR with warmup (simple)\n",
    "# ------------------------------------------------\n",
    "def build_optimizer(model, lr, wd):\n",
    "    # standard exclusions from weight decay\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\", \"ln.weight\", \"norm.weight\"]\n",
    "    param_groups = [\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() \n",
    "                       if p.requires_grad and not any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": wd,\n",
    "        },\n",
    "        {\n",
    "            \"params\": [p for n, p in model.named_parameters() \n",
    "                       if p.requires_grad and any(nd in n for nd in no_decay)],\n",
    "            \"weight_decay\": 0.0,\n",
    "        },\n",
    "    ]\n",
    "    return AdamW(param_groups, lr=lr)\n",
    "\n",
    "optimizer = build_optimizer(model, CFG.lr, CFG.weight_decay)\n",
    "\n",
    "# simple cosine schedule with warmup ratio = 0.05 over a nominal epoch size\n",
    "NOMINAL_STEPS = 2000  # we’ll replace later when we run real training\n",
    "WARMUP_STEPS = max(1, int(0.05 * NOMINAL_STEPS))\n",
    "TOTAL_STEPS  = NOMINAL_STEPS\n",
    "\n",
    "def lr_cosine(step):\n",
    "    if step < WARMUP_STEPS:\n",
    "        return step / float(max(1, WARMUP_STEPS))\n",
    "    # cosine from 1→0\n",
    "    progress = (step - WARMUP_STEPS) / float(max(1, TOTAL_STEPS - WARMUP_STEPS))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
    "\n",
    "def set_lr(optimizer, base_lr, scale):\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = base_lr * scale\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 5) One-batch backprop sanity (no full epoch yet)\n",
    "# ------------------------------------------------\n",
    "model.train()\n",
    "batch = next(iter(loader_train))\n",
    "\n",
    "# Move tensors to device\n",
    "batch_dev = {\n",
    "    \"text\": {\n",
    "        \"input_ids\": batch[\"text\"][\"input_ids\"].to(device),\n",
    "        \"attention_mask\": batch[\"text\"][\"attention_mask\"].to(device),\n",
    "    },\n",
    "    \"graph\": {\n",
    "        \"x\": batch[\"graph\"][\"x\"].to(device),\n",
    "        \"edge_index\": batch[\"graph\"][\"edge_index\"].to(device),\n",
    "        \"edge_attr\": batch[\"graph\"][\"edge_attr\"].to(device),\n",
    "        \"batch\": batch[\"graph\"][\"batch\"].to(device),\n",
    "    },\n",
    "    \"desc\": batch[\"desc\"].to(device),\n",
    "    \"y\": batch[\"y\"].to(device),\n",
    "    \"mask\": batch[\"mask\"].to(device),\n",
    "    \"ids\": batch[\"ids\"],\n",
    "    \"smiles\": batch[\"smiles\"],\n",
    "}\n",
    "\n",
    "optimizer.zero_grad(set_to_none=True)\n",
    "set_lr(optimizer, CFG.lr, lr_cosine(step=0))\n",
    "\n",
    "out = model(batch_dev, return_proj=True)\n",
    "logits = out[\"logits\"]        # (B,12)\n",
    "alpha  = out[\"alpha\"]         # (B,12,3)\n",
    "\n",
    "# main ASL loss with mask\n",
    "loss_cls = asl(logits, batch_dev[\"y\"], mask=batch_dev[\"mask\"])\n",
    "\n",
    "# InfoNCE (optional)\n",
    "loss_nce = torch.tensor(0.0, device=device)\n",
    "if CFG.use_infonce:\n",
    "    zt = out[\"z_text\"]   # (B,64)\n",
    "    zg = out[\"z_graph\"]  # (B,64)\n",
    "    loss_nce = infonce_loss_fn(zt, zg)\n",
    "\n",
    "# small L1 on gates to avoid overconfidence (optional)\n",
    "loss_gate_l1 = alpha.abs().mean() * 0.01\n",
    "\n",
    "loss = loss_cls + CFG.infonce_weight * loss_nce + loss_gate_l1\n",
    "loss.backward()\n",
    "\n",
    "# gradient clipping\n",
    "nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "\n",
    "optimizer.step()\n",
    "ema.update(model)\n",
    "\n",
    "print(\"One-batch backprop sanity:\")\n",
    "print(f\" - loss_cls: {loss_cls.item():.4f}\")\n",
    "if CFG.use_infonce:\n",
    "    print(f\" - loss_nce: {loss_nce.item():.4f} (weighted x{CFG.infonce_weight})\")\n",
    "print(f\" - gate_L1:  {loss_gate_l1.item():.4f}\")\n",
    "print(f\" - total:    {loss.item():.4f}\")\n",
    "\n",
    "# ------------------------------------------------\n",
    "# 6) Save an initial checkpoint (post-sanity step)\n",
    "# ------------------------------------------------\n",
    "ckpt_init = os.path.join(CKPT_DIR, f\"{CFG.run_tag}_init_sanity.pt\")\n",
    "torch.save({\n",
    "    \"model_state\": model.state_dict(),\n",
    "    \"optimizer_state\": optimizer.state_dict(),\n",
    "    \"ema_shadow\": ema.shadow,\n",
    "    \"config\": asdict(CFG),\n",
    "}, ckpt_init)\n",
    "print(\"Saved init checkpoint →\", os.path.abspath(ckpt_init))\n",
    "print(\"\\nCell 8 ✅ — Training plumbing ready (ASL+InfoNCE+EMA). We can now script Stage A/B/C runs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc3d2af",
   "metadata": {},
   "source": [
    "## 9: Full trainer (Stages A/B/C) + early stopping + EMA + SAM + rich VAL metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b848bdd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting V5 training (Stages A→B→C) with early stopping on macro PR-AUC.\n",
      "\n",
      "===== Stage A — start =====\n",
      "\n",
      "Epoch 1/6 — Stage A\n",
      "  step   50 | loss 1.9821 | cls 0.2940 | nce 3.370 | gate 0.0033\n",
      "  step  100 | loss 1.8697 | cls 0.2843 | nce 3.164 | gate 0.0033\n",
      "  step  150 | loss 1.7955 | cls 0.2763 | nce 3.032 | gate 0.0033\n",
      "VAL — macro PR: 0.1084 | ROC: 0.5137 | F1*: 0.1769 | R@1: 0.529 | R@3: 0.732\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageA_epoch01_best.pt\n",
      "\n",
      "Epoch 2/6 — Stage A\n",
      "  step   50 | loss 1.5690 | cls 0.2547 | nce 2.622 | gate 0.0033\n",
      "  step  100 | loss 1.5371 | cls 0.2508 | nce 2.566 | gate 0.0033\n",
      "  step  150 | loss 1.5132 | cls 0.2487 | nce 2.522 | gate 0.0033\n",
      "VAL — macro PR: 0.1628 | ROC: 0.6067 | F1*: 0.2355 | R@1: 0.581 | R@3: 0.797\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageA_epoch02_best.pt\n",
      "\n",
      "Epoch 3/6 — Stage A\n",
      "  step   50 | loss 1.4513 | cls 0.2432 | nce 2.410 | gate 0.0033\n",
      "  step  100 | loss 1.4338 | cls 0.2424 | nce 2.376 | gate 0.0033\n",
      "  step  150 | loss 1.4160 | cls 0.2430 | nce 2.339 | gate 0.0033\n",
      "VAL — macro PR: 0.2011 | ROC: 0.6507 | F1*: 0.2883 | R@1: 0.591 | R@3: 0.821\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageA_epoch03_best.pt\n",
      "\n",
      "Epoch 4/6 — Stage A\n",
      "  step   50 | loss 1.3862 | cls 0.2437 | nce 2.278 | gate 0.0033\n",
      "  step  100 | loss 1.3777 | cls 0.2431 | nce 2.263 | gate 0.0033\n",
      "  step  150 | loss 1.3688 | cls 0.2429 | nce 2.245 | gate 0.0033\n",
      "VAL — macro PR: 0.2273 | ROC: 0.6773 | F1*: 0.3130 | R@1: 0.584 | R@3: 0.845\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageA_epoch04_best.pt\n",
      "\n",
      "Epoch 5/6 — Stage A\n",
      "  step   50 | loss 1.3492 | cls 0.2396 | nce 2.212 | gate 0.0033\n",
      "  step  100 | loss 1.3375 | cls 0.2378 | nce 2.193 | gate 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:06:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:06:55] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step  150 | loss 1.3446 | cls 0.2387 | nce 2.205 | gate 0.0033\n",
      "VAL — macro PR: 0.2415 | ROC: 0.6881 | F1*: 0.3265 | R@1: 0.577 | R@3: 0.838\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageA_epoch05_best.pt\n",
      "\n",
      "Epoch 6/6 — Stage A\n",
      "  step   50 | loss 1.3328 | cls 0.2390 | nce 2.181 | gate 0.0033\n",
      "  step  100 | loss 1.3317 | cls 0.2368 | nce 2.183 | gate 0.0033\n",
      "  step  150 | loss 1.3241 | cls 0.2358 | nce 2.170 | gate 0.0033\n",
      "VAL — macro PR: 0.2486 | ROC: 0.6964 | F1*: 0.3312 | R@1: 0.574 | R@3: 0.825\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageA_epoch06_best.pt\n",
      "Saved history → tox21_dualenc_v1/results/v5\\v5_20250903_124631_stageA_history.json\n",
      "===== Stage A — done (best macro PR=0.2486 @ epoch 6) =====\n",
      "\n",
      "\n",
      "===== Stage B — start =====\n",
      "\n",
      "Epoch 1/10 — Stage B\n",
      "  step   50 | loss 1.2281 | cls 0.2350 | nce 1.980 | gate 0.0033\n",
      "  step  100 | loss 1.1183 | cls 0.2317 | nce 1.766 | gate 0.0033\n",
      "  step  150 | loss 1.0216 | cls 0.2294 | nce 1.578 | gate 0.0033\n",
      "VAL — macro PR: 0.2620 | ROC: 0.7043 | F1*: 0.3417 | R@1: 0.612 | R@3: 0.869\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageB_epoch01_best.pt\n",
      "\n",
      "Epoch 2/10 — Stage B\n",
      "  step   50 | loss 0.7772 | cls 0.2200 | nce 1.108 | gate 0.0033\n",
      "  step  100 | loss 0.7457 | cls 0.2164 | nce 1.052 | gate 0.0033\n",
      "  step  150 | loss 0.7245 | cls 0.2166 | nce 1.009 | gate 0.0033\n",
      "VAL — macro PR: 0.2691 | ROC: 0.6968 | F1*: 0.3489 | R@1: 0.622 | R@3: 0.863\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageB_epoch02_best.pt\n",
      "\n",
      "Epoch 3/10 — Stage B\n",
      "  step   50 | loss 0.6530 | cls 0.2103 | nce 0.879 | gate 0.0033\n",
      "  step  100 | loss 0.6574 | cls 0.2116 | nce 0.885 | gate 0.0033\n",
      "  step  150 | loss 0.6333 | cls 0.2115 | nce 0.837 | gate 0.0033\n",
      "VAL — macro PR: 0.2743 | ROC: 0.7001 | F1*: 0.3492 | R@1: 0.629 | R@3: 0.856\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageB_epoch03_best.pt\n",
      "\n",
      "Epoch 4/10 — Stage B\n",
      "  step   50 | loss 0.6299 | cls 0.2082 | nce 0.837 | gate 0.0033\n",
      "  step  100 | loss 0.6220 | cls 0.2102 | nce 0.817 | gate 0.0033\n",
      "  step  150 | loss 0.6206 | cls 0.2099 | nce 0.815 | gate 0.0033\n",
      "VAL — macro PR: 0.2825 | ROC: 0.7094 | F1*: 0.3549 | R@1: 0.639 | R@3: 0.856\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageB_epoch04_best.pt\n",
      "\n",
      "Epoch 5/10 — Stage B\n",
      "  step   50 | loss 0.6001 | cls 0.2114 | nce 0.771 | gate 0.0033\n",
      "  step  100 | loss 0.5966 | cls 0.2123 | nce 0.762 | gate 0.0033\n",
      "  step  150 | loss 0.5930 | cls 0.2099 | nce 0.760 | gate 0.0033\n",
      "VAL — macro PR: 0.2891 | ROC: 0.7152 | F1*: 0.3602 | R@1: 0.643 | R@3: 0.852\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageB_epoch05_best.pt\n",
      "\n",
      "Epoch 6/10 — Stage B\n",
      "  step   50 | loss 0.5659 | cls 0.2070 | nce 0.711 | gate 0.0033\n",
      "  step  100 | loss 0.5743 | cls 0.2075 | nce 0.727 | gate 0.0033\n",
      "  step  150 | loss 0.5798 | cls 0.2056 | nce 0.742 | gate 0.0033\n",
      "VAL — macro PR: 0.2909 | ROC: 0.7173 | F1*: 0.3640 | R@1: 0.639 | R@3: 0.859\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageB_epoch06_best.pt\n",
      "\n",
      "Epoch 7/10 — Stage B\n",
      "  step   50 | loss 0.5590 | cls 0.2077 | nce 0.696 | gate 0.0033\n",
      "  step  100 | loss 0.5662 | cls 0.2065 | nce 0.713 | gate 0.0033\n",
      "  step  150 | loss 0.5696 | cls 0.2084 | nce 0.716 | gate 0.0033\n",
      "VAL — macro PR: 0.2940 | ROC: 0.7217 | F1*: 0.3672 | R@1: 0.622 | R@3: 0.859\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageB_epoch07_best.pt\n",
      "\n",
      "Epoch 8/10 — Stage B\n",
      "  step   50 | loss 0.5842 | cls 0.2125 | nce 0.737 | gate 0.0033\n",
      "  step  100 | loss 0.5722 | cls 0.2087 | nce 0.720 | gate 0.0033\n",
      "  step  150 | loss 0.5674 | cls 0.2083 | nce 0.711 | gate 0.0033\n",
      "VAL — macro PR: 0.2938 | ROC: 0.7229 | F1*: 0.3670 | R@1: 0.622 | R@3: 0.856\n",
      "  no improvement (best @ epoch 7); patience 1/5\n",
      "\n",
      "Epoch 9/10 — Stage B\n",
      "  step   50 | loss 0.5962 | cls 0.2131 | nce 0.759 | gate 0.0033\n",
      "  step  100 | loss 0.5879 | cls 0.2101 | nce 0.749 | gate 0.0033\n",
      "  step  150 | loss 0.5833 | cls 0.2092 | nce 0.741 | gate 0.0033\n",
      "VAL — macro PR: 0.2965 | ROC: 0.7252 | F1*: 0.3716 | R@1: 0.615 | R@3: 0.852\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageB_epoch09_best.pt\n",
      "\n",
      "Epoch 10/10 — Stage B\n",
      "  step   50 | loss 0.5762 | cls 0.2100 | nce 0.726 | gate 0.0033\n",
      "  step  100 | loss 0.5691 | cls 0.2107 | nce 0.710 | gate 0.0033\n",
      "  step  150 | loss 0.5726 | cls 0.2105 | nce 0.718 | gate 0.0033\n",
      "VAL — macro PR: 0.2955 | ROC: 0.7256 | F1*: 0.3698 | R@1: 0.605 | R@3: 0.849\n",
      "  no improvement (best @ epoch 9); patience 1/5\n",
      "Saved history → tox21_dualenc_v1/results/v5\\v5_20250903_124631_stageB_history.json\n",
      "===== Stage B — done (best macro PR=0.2965 @ epoch 9) =====\n",
      "\n",
      "\n",
      "===== Stage C — start =====\n",
      "\n",
      "Epoch 1/12 — Stage C\n",
      "  step   50 | loss 0.6777 | cls 0.2071 | nce 0.935 | gate 0.0033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[13:15:30] WARNING: not removing hydrogen atom without neighbors\n",
      "[13:15:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  step  100 | loss 0.6544 | cls 0.2097 | nce 0.883 | gate 0.0033\n",
      "  step  150 | loss 0.6250 | cls 0.2076 | nce 0.828 | gate 0.0033\n",
      "VAL — macro PR: 0.2982 | ROC: 0.7275 | F1*: 0.3673 | R@1: 0.598 | R@3: 0.852\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch01_best.pt\n",
      "\n",
      "Epoch 2/12 — Stage C\n",
      "  step   50 | loss 0.5331 | cls 0.1993 | nce 0.661 | gate 0.0033\n",
      "  step  100 | loss 0.5091 | cls 0.1971 | nce 0.617 | gate 0.0033\n",
      "  step  150 | loss 0.4955 | cls 0.1977 | nce 0.589 | gate 0.0033\n",
      "VAL — macro PR: 0.3018 | ROC: 0.7351 | F1*: 0.3762 | R@1: 0.601 | R@3: 0.845\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch02_best.pt\n",
      "\n",
      "Epoch 3/12 — Stage C\n",
      "  step   50 | loss 0.4733 | cls 0.1921 | nce 0.556 | gate 0.0033\n",
      "  step  100 | loss 0.4720 | cls 0.1951 | nce 0.547 | gate 0.0033\n",
      "  step  150 | loss 0.4667 | cls 0.1962 | nce 0.534 | gate 0.0033\n",
      "VAL — macro PR: 0.3043 | ROC: 0.7378 | F1*: 0.3765 | R@1: 0.598 | R@3: 0.842\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch03_best.pt\n",
      "\n",
      "Epoch 4/12 — Stage C\n",
      "  step   50 | loss 0.4604 | cls 0.1931 | nce 0.528 | gate 0.0033\n",
      "  step  100 | loss 0.4539 | cls 0.1947 | nce 0.512 | gate 0.0033\n",
      "  step  150 | loss 0.4480 | cls 0.1938 | nce 0.502 | gate 0.0033\n",
      "VAL — macro PR: 0.3064 | ROC: 0.7374 | F1*: 0.3810 | R@1: 0.584 | R@3: 0.835\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch04_best.pt\n",
      "\n",
      "Epoch 5/12 — Stage C\n",
      "  step   50 | loss 0.4645 | cls 0.1924 | nce 0.538 | gate 0.0033\n",
      "  step  100 | loss 0.4579 | cls 0.1956 | nce 0.518 | gate 0.0033\n",
      "  step  150 | loss 0.4552 | cls 0.1954 | nce 0.513 | gate 0.0033\n",
      "VAL — macro PR: 0.3066 | ROC: 0.7365 | F1*: 0.3769 | R@1: 0.584 | R@3: 0.835\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch05_best.pt\n",
      "\n",
      "Epoch 6/12 — Stage C\n",
      "  step   50 | loss 0.4370 | cls 0.1899 | nce 0.488 | gate 0.0033\n",
      "  step  100 | loss 0.4327 | cls 0.1899 | nce 0.479 | gate 0.0033\n",
      "  step  150 | loss 0.4342 | cls 0.1910 | nce 0.480 | gate 0.0033\n",
      "VAL — macro PR: 0.3080 | ROC: 0.7385 | F1*: 0.3812 | R@1: 0.581 | R@3: 0.838\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch06_best.pt\n",
      "\n",
      "Epoch 7/12 — Stage C\n",
      "  step   50 | loss 0.4428 | cls 0.1968 | nce 0.485 | gate 0.0033\n",
      "  step  100 | loss 0.4345 | cls 0.1983 | nce 0.466 | gate 0.0033\n",
      "  step  150 | loss 0.4344 | cls 0.1978 | nce 0.467 | gate 0.0033\n",
      "VAL — macro PR: 0.3092 | ROC: 0.7382 | F1*: 0.3808 | R@1: 0.588 | R@3: 0.825\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch07_best.pt\n",
      "\n",
      "Epoch 8/12 — Stage C\n",
      "  step   50 | loss 0.4151 | cls 0.1924 | nce 0.439 | gate 0.0033\n",
      "  step  100 | loss 0.4177 | cls 0.1913 | nce 0.446 | gate 0.0033\n",
      "  step  150 | loss 0.4200 | cls 0.1905 | nce 0.453 | gate 0.0033\n",
      "VAL — macro PR: 0.3130 | ROC: 0.7376 | F1*: 0.3827 | R@1: 0.584 | R@3: 0.835\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch08_best.pt\n",
      "\n",
      "Epoch 9/12 — Stage C\n",
      "  step   50 | loss 0.4465 | cls 0.1929 | nce 0.500 | gate 0.0033\n",
      "  step  100 | loss 0.4472 | cls 0.1961 | nce 0.496 | gate 0.0033\n",
      "  step  150 | loss 0.4435 | cls 0.1940 | nce 0.492 | gate 0.0033\n",
      "VAL — macro PR: 0.3142 | ROC: 0.7382 | F1*: 0.3858 | R@1: 0.581 | R@3: 0.832\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch09_best.pt\n",
      "\n",
      "Epoch 10/12 — Stage C\n",
      "  step   50 | loss 0.4418 | cls 0.1905 | nce 0.496 | gate 0.0033\n",
      "  step  100 | loss 0.4389 | cls 0.1944 | nce 0.482 | gate 0.0033\n",
      "  step  150 | loss 0.4331 | cls 0.1942 | nce 0.471 | gate 0.0033\n",
      "VAL — macro PR: 0.3151 | ROC: 0.7386 | F1*: 0.3829 | R@1: 0.581 | R@3: 0.838\n",
      "  🔥 New best — saved: tox21_dualenc_v1/models/checkpoints_v5\\v5_20250903_124631_stageC_epoch10_best.pt\n",
      "\n",
      "Epoch 11/12 — Stage C\n",
      "  step   50 | loss 0.4288 | cls 0.1924 | nce 0.466 | gate 0.0033\n",
      "  step  100 | loss 0.4389 | cls 0.1933 | nce 0.485 | gate 0.0033\n",
      "  step  150 | loss 0.4383 | cls 0.1934 | nce 0.483 | gate 0.0033\n",
      "VAL — macro PR: 0.3146 | ROC: 0.7376 | F1*: 0.3832 | R@1: 0.577 | R@3: 0.838\n",
      "  no improvement (best @ epoch 10); patience 1/6\n",
      "\n",
      "Epoch 12/12 — Stage C\n",
      "  step   50 | loss 0.4262 | cls 0.1936 | nce 0.459 | gate 0.0033\n",
      "  step  100 | loss 0.4188 | cls 0.1918 | nce 0.447 | gate 0.0033\n",
      "  step  150 | loss 0.4271 | cls 0.1917 | nce 0.464 | gate 0.0033\n",
      "VAL — macro PR: 0.3150 | ROC: 0.7379 | F1*: 0.3848 | R@1: 0.577 | R@3: 0.835\n",
      "  no improvement (best @ epoch 10); patience 2/6\n",
      "Saved history → tox21_dualenc_v1/results/v5\\v5_20250903_124631_stageC_history.json\n",
      "===== Stage C — done (best macro PR=0.3151 @ epoch 10) =====\n",
      "\n",
      "Cell 9 ✅ — Training loop finished (check checkpoints & histories).\n"
     ]
    }
   ],
   "source": [
    "import os, math, time, json, pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_curve, f1_score\n",
    "\n",
    "assert 'model' in globals() and 'loader_train' in globals() and 'loader_val' in globals()\n",
    "assert 'CFG' in globals() and 'CKPT_DIR' in globals() and 'RES_DIR' in globals()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------\n",
    "# Hyperparams (stages)\n",
    "# -----------------------\n",
    "max_epochs_A = max(CFG.epochs_A, 6)   # give it a bit more room (you asked to extend; early stop guards)\n",
    "max_epochs_B = max(CFG.epochs_B, 10)\n",
    "max_epochs_C = max(CFG.epochs_C, 12)\n",
    "patience_A   = 4\n",
    "patience_B   = 5\n",
    "patience_C   = 6\n",
    "\n",
    "# -----------------------\n",
    "# SAM (optional)\n",
    "# -----------------------\n",
    "class SAM(torch.optim.Optimizer):\n",
    "    \"\"\"Sharpness-Aware Minimization (fore/aft step).\"\"\"\n",
    "    def __init__(self, params, base_optimizer, rho=0.05, adaptive=False, **kwargs):\n",
    "        assert rho >= 0.0, \"Invalid rho\"\n",
    "        defaults = dict(rho=rho, adaptive=adaptive, **kwargs)\n",
    "        super().__init__(params, defaults)\n",
    "        self.base_optimizer = base_optimizer(self.param_groups, **kwargs)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def first_step(self, zero_grad=True):\n",
    "        rho = self.defaults['rho']\n",
    "        adaptive = self.defaults['adaptive']\n",
    "        scale = 0.0\n",
    "        grad_norm = torch.norm(\n",
    "            torch.stack([\n",
    "                ((p.abs() if adaptive else 1.0) * p.grad).norm(p=2)\n",
    "                for group in self.param_groups for p in group['params']\n",
    "                if p.grad is not None\n",
    "            ]), p=2\n",
    "        )\n",
    "        scale = rho / (grad_norm + 1e-12)\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None: continue\n",
    "                e_w = (torch.pow(p, 2) if adaptive else 1.0) * p.grad * scale\n",
    "                p.add_(e_w)   # ascent step\n",
    "                self.state[p]['e_w'] = e_w\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def second_step(self, zero_grad=True):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None: continue\n",
    "                p.sub_(self.state[p]['e_w'])  # return to w\n",
    "        self.base_optimizer.step()\n",
    "        if zero_grad: self.zero_grad()\n",
    "\n",
    "    def step(self):  # not used\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.base_optimizer.zero_grad()\n",
    "\n",
    "# -----------------------\n",
    "# Loss objects (reuse)\n",
    "# -----------------------\n",
    "# ASL from Cell 8\n",
    "assert 'asl' in globals()\n",
    "assert 'infonce_loss_fn' in globals()\n",
    "assert 'ema' in globals()\n",
    "\n",
    "# -----------------------\n",
    "# LR schedule helpers\n",
    "# -----------------------\n",
    "def set_lr(optimizer, base_lr, scale):\n",
    "    for g in optimizer.param_groups:\n",
    "        g[\"lr\"] = base_lr * scale\n",
    "\n",
    "def cosine_scale(step, total, warmup=0.05):\n",
    "    w = max(1, int(warmup * total))\n",
    "    if step < w: return step / float(w)\n",
    "    prog = (step - w) / float(max(1, total - w))\n",
    "    return 0.5 * (1.0 + math.cos(math.pi * prog))\n",
    "\n",
    "# -----------------------\n",
    "# Metrics (mask-aware)\n",
    "# -----------------------\n",
    "def masked_metrics(y_true, y_prob, y_mask):\n",
    "    \"\"\"\n",
    "    y_true, y_prob, y_mask: (N, L)\n",
    "    Computes macro ROC-AUC, PR-AUC, best-F1 (via per-label threshold sweep),\n",
    "    and ranking R@1/R@3 (per-sample).\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.float32)\n",
    "    y_prob = y_prob.astype(np.float32)\n",
    "    y_mask = y_mask.astype(np.float32)\n",
    "\n",
    "    L = y_true.shape[1]\n",
    "    rocs, prs = [], []\n",
    "    f1s = []\n",
    "    for j in range(L):\n",
    "        mask = y_mask[:, j] > 0.5\n",
    "        if mask.sum() < 3 or (y_true[mask, j].sum() == 0) or (y_true[mask, j].sum() == mask.sum()):\n",
    "            # skip degenerate labels for ROC/PR\n",
    "            continue\n",
    "        try:\n",
    "            rocs.append(roc_auc_score(y_true[mask, j], y_prob[mask, j]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        try:\n",
    "            prs.append(average_precision_score(y_true[mask, j], y_prob[mask, j]))\n",
    "        except Exception:\n",
    "            pass\n",
    "        # best F1 on a sweep of thresholds\n",
    "        prec, rec, thr = precision_recall_curve(y_true[mask, j], y_prob[mask, j])\n",
    "        f1 = (2*prec*rec) / (prec+rec + 1e-12)\n",
    "        f1s.append(np.nanmax(f1))\n",
    "\n",
    "    macro_roc = float(np.nanmean(rocs)) if rocs else float('nan')\n",
    "    macro_pr  = float(np.nanmean(prs))  if prs  else float('nan')\n",
    "    macro_f1  = float(np.nanmean(f1s))  if f1s  else float('nan')\n",
    "\n",
    "    # R@k (ranking) — only on samples with at least one positive valid label\n",
    "    def recall_at_k(k):\n",
    "        ok = []\n",
    "        for i in range(y_true.shape[0]):\n",
    "            mask_row = y_mask[i] > 0.5\n",
    "            if mask_row.sum() == 0: \n",
    "                continue\n",
    "            true_pos = (y_true[i][mask_row] > 0.5)\n",
    "            if true_pos.sum() == 0:\n",
    "                continue\n",
    "            probs = y_prob[i][mask_row]\n",
    "            idx = np.argsort(-probs)[:k]\n",
    "            # Map back to mask indices\n",
    "            valid_indices = np.where(mask_row)[0][idx]\n",
    "            ok.append( (y_true[i, valid_indices] > 0.5).any() )\n",
    "        if not ok: return float('nan')\n",
    "        return float(np.mean(ok))\n",
    "    r1 = recall_at_k(1)\n",
    "    r3 = recall_at_k(3)\n",
    "\n",
    "    return {\n",
    "        \"macro_roc\": macro_roc,\n",
    "        \"macro_pr\": macro_pr,\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"R@1\": r1,\n",
    "        \"R@3\": r3\n",
    "    }\n",
    "\n",
    "@torch.no_grad()\n",
    "def run_eval(model, loader, use_ema=False):\n",
    "    model.eval()\n",
    "    if use_ema:\n",
    "        ema.apply_shadow(model)\n",
    "    probs_list, y_list, m_list = [], [], []\n",
    "    for batch in loader:\n",
    "        batch_dev = {\n",
    "            \"text\": {\n",
    "                \"input_ids\": batch[\"text\"][\"input_ids\"].to(device),\n",
    "                \"attention_mask\": batch[\"text\"][\"attention_mask\"].to(device),\n",
    "            },\n",
    "            \"graph\": {\n",
    "                \"x\": batch[\"graph\"][\"x\"].to(device),\n",
    "                \"edge_index\": batch[\"graph\"][\"edge_index\"].to(device),\n",
    "                \"edge_attr\": batch[\"graph\"][\"edge_attr\"].to(device),\n",
    "                \"batch\": batch[\"graph\"][\"batch\"].to(device),\n",
    "            },\n",
    "            \"desc\": batch[\"desc\"].to(device),\n",
    "            \"y\": batch[\"y\"].to(device),\n",
    "            \"mask\": batch[\"mask\"].to(device),\n",
    "        }\n",
    "        out = model(batch_dev, return_proj=False)\n",
    "        p = torch.sigmoid(out[\"logits\"]).detach().cpu().numpy()\n",
    "        probs_list.append(p)\n",
    "        y_list.append(batch_dev[\"y\"].cpu().numpy())\n",
    "        m_list.append(batch_dev[\"mask\"].cpu().numpy())\n",
    "    if use_ema:\n",
    "        ema.restore(model)\n",
    "    y_prob = np.concatenate(probs_list, axis=0)\n",
    "    y_true = np.concatenate(y_list, axis=0)\n",
    "    y_mask = np.concatenate(m_list, axis=0)\n",
    "    return masked_metrics(y_true, y_prob, y_mask)\n",
    "\n",
    "# -----------------------\n",
    "# Train step\n",
    "# -----------------------\n",
    "def build_optimizer():\n",
    "    # same grouping as Cell 8\n",
    "    no_decay = [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\", \"ln.weight\", \"norm.weight\"]\n",
    "    param_groups = [\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and not any(nd in n for nd in no_decay)], \"weight_decay\": CFG.weight_decay},\n",
    "        {\"params\": [p for n, p in model.named_parameters() if p.requires_grad and any(nd in n for nd in no_decay)],  \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    if CFG.use_sam:\n",
    "        opt = SAM(param_groups, base_optimizer=torch.optim.AdamW, lr=CFG.lr, rho=0.05, adaptive=False)\n",
    "    else:\n",
    "        opt = torch.optim.AdamW(param_groups, lr=CFG.lr)\n",
    "    return opt\n",
    "\n",
    "def train_one_epoch(loader, optimizer, epoch_idx, total_steps_est=1000):\n",
    "    model.train()\n",
    "    running = {\"loss\":0.0, \"cls\":0.0, \"nce\":0.0, \"gate\":0.0}\n",
    "    for step, batch in enumerate(loader):\n",
    "        step_global = epoch_idx * total_steps_est + step\n",
    "        scale = cosine_scale(step_global, total_steps_est*(epoch_idx+1), warmup=0.05)\n",
    "        set_lr(optimizer.base_optimizer if isinstance(optimizer, SAM) else optimizer, CFG.lr, scale)\n",
    "\n",
    "        # Move\n",
    "        batch_dev = {\n",
    "            \"text\": {\"input_ids\": batch[\"text\"][\"input_ids\"].to(device),\n",
    "                     \"attention_mask\": batch[\"text\"][\"attention_mask\"].to(device)},\n",
    "            \"graph\": {\"x\": batch[\"graph\"][\"x\"].to(device),\n",
    "                      \"edge_index\": batch[\"graph\"][\"edge_index\"].to(device),\n",
    "                      \"edge_attr\": batch[\"graph\"][\"edge_attr\"].to(device),\n",
    "                      \"batch\": batch[\"graph\"][\"batch\"].to(device)},\n",
    "            \"desc\": batch[\"desc\"].to(device),\n",
    "            \"y\": batch[\"y\"].to(device),\n",
    "            \"mask\": batch[\"mask\"].to(device),\n",
    "        }\n",
    "\n",
    "        def forward_and_loss():\n",
    "            out = model(batch_dev, return_proj=CFG.use_infonce)\n",
    "            logits = out[\"logits\"]\n",
    "            alpha  = out[\"alpha\"]\n",
    "            loss_cls = asl(logits, batch_dev[\"y\"], mask=batch_dev[\"mask\"])\n",
    "            loss_nce = torch.tensor(0.0, device=device)\n",
    "            if CFG.use_infonce:\n",
    "                zt = out[\"z_text\"]; zg = out[\"z_graph\"]\n",
    "                loss_nce = infonce_loss_fn(zt, zg)\n",
    "            loss_gate = alpha.abs().mean() * 0.01\n",
    "            loss = loss_cls + CFG.infonce_weight*loss_nce + loss_gate\n",
    "            return loss, loss_cls, loss_nce, loss_gate\n",
    "\n",
    "        if isinstance(optimizer, SAM):\n",
    "            # first forward-backward (ascent)\n",
    "            loss, l_cls, l_nce, l_gate = forward_and_loss()\n",
    "            (loss).backward()\n",
    "            optimizer.first_step(zero_grad=True)\n",
    "            # second forward-backward (descent)\n",
    "            loss2, l_cls2, l_nce2, l_gate2 = forward_and_loss()\n",
    "            (loss2).backward()\n",
    "            optimizer.second_step(zero_grad=True)\n",
    "            loss = loss2; l_cls = l_cls2; l_nce = l_nce2; l_gate = l_gate2\n",
    "        else:\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss, l_cls, l_nce, l_gate = forward_and_loss()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "\n",
    "        ema.update(model)\n",
    "\n",
    "        running[\"loss\"] += loss.item()\n",
    "        running[\"cls\"]  += l_cls.item()\n",
    "        running[\"nce\"]  += (l_nce.item() if CFG.use_infonce else 0.0)\n",
    "        running[\"gate\"] += l_gate.item()\n",
    "\n",
    "        if (step+1) % 50 == 0:\n",
    "            print(f\"  step {step+1:4d} | loss {running['loss']/(step+1):.4f} | cls {running['cls']/(step+1):.4f} | nce {running['nce']/(step+1):.3f} | gate {running['gate']/(step+1):.4f}\")\n",
    "\n",
    "    nsteps = max(1, step+1)\n",
    "    for k in running: running[k] /= nsteps\n",
    "    return running\n",
    "\n",
    "# -----------------------\n",
    "# Freeze/unfreeze helpers\n",
    "# -----------------------\n",
    "def freeze_text(backbone, until_layer=None):\n",
    "    for p in backbone.parameters():\n",
    "        p.requires_grad = False\n",
    "    if until_layer is not None:\n",
    "        # unfreeze last N transformer blocks\n",
    "        enc = backbone.encoder if hasattr(backbone, \"encoder\") else backbone.base_model.encoder\n",
    "        blocks = enc.layer if hasattr(enc, \"layer\") else enc.layers\n",
    "        for p in blocks[-until_layer:].parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "# -----------------------\n",
    "# Stage runner\n",
    "# -----------------------\n",
    "def run_stage(name, loader_tr, loader_ev, max_epochs, patience, unfreeze_last=None, adjust_for_stage=None):\n",
    "    print(f\"\\n===== Stage {name} — start =====\")\n",
    "    # Freeze or unfreeze text according to stage\n",
    "    if name == \"A\":\n",
    "        freeze_text(model.text_backbone, until_layer=None)  # fully frozen\n",
    "    else:\n",
    "        # enable all, then optionally LLRD by unfreezing last K\n",
    "        for p in model.text_backbone.parameters(): p.requires_grad = True\n",
    "        if unfreeze_last is not None:\n",
    "            freeze_text(model.text_backbone, until_layer=unfreeze_last)\n",
    "\n",
    "    # Stage-specific tweaks\n",
    "    if adjust_for_stage is not None:\n",
    "        adjust_for_stage()\n",
    "\n",
    "    optimizer = build_optimizer()\n",
    "\n",
    "    best_metric = -np.inf\n",
    "    best_epoch  = -1\n",
    "    patience_ctr = 0\n",
    "    hist = []\n",
    "\n",
    "    steps_per_epoch = max(1, len(loader_tr))\n",
    "    for epoch in range(1, max_epochs+1):\n",
    "        print(f\"\\nEpoch {epoch}/{max_epochs} — Stage {name}\")\n",
    "        tr_log = train_one_epoch(loader_tr, optimizer, epoch_idx=epoch-1, total_steps_est=steps_per_epoch)\n",
    "        val_log = run_eval(model, loader_ev, use_ema=True)  # EMA eval\n",
    "\n",
    "        metric = val_log[\"macro_pr\"]  # early-stop by macro PR-AUC\n",
    "        hist.append({\"epoch\": epoch, \"train\": tr_log, \"val\": val_log})\n",
    "\n",
    "        print(f\"VAL — macro PR: {val_log['macro_pr']:.4f} | ROC: {val_log['macro_roc']:.4f} | F1*: {val_log['macro_f1']:.4f} | R@1: {val_log['R@1']:.3f} | R@3: {val_log['R@3']:.3f}\")\n",
    "\n",
    "        if metric > best_metric + 1e-5:\n",
    "            best_metric = metric\n",
    "            best_epoch  = epoch\n",
    "            patience_ctr = 0\n",
    "            # save checkpoint\n",
    "            ckpt_path = os.path.join(CKPT_DIR, f\"{CFG.run_tag}_stage{name}_epoch{epoch:02d}_best.pt\")\n",
    "            torch.save({\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"ema_shadow\": ema.shadow,\n",
    "                \"config\": asdict(CFG),\n",
    "                \"stage\": name,\n",
    "                \"epoch\": epoch,\n",
    "                \"val\": val_log\n",
    "            }, ckpt_path)\n",
    "            print(\"  🔥 New best — saved:\", ckpt_path)\n",
    "        else:\n",
    "            patience_ctr += 1\n",
    "            print(f\"  no improvement (best @ epoch {best_epoch}); patience {patience_ctr}/{patience}\")\n",
    "            if patience_ctr >= patience:\n",
    "                print(f\"Early stopping Stage {name}.\")\n",
    "                break\n",
    "\n",
    "    # Persist stage history\n",
    "    stage_hist_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_stage{name}_history.json\")\n",
    "    with open(stage_hist_path, \"w\") as f:\n",
    "        json.dump(hist, f, indent=2)\n",
    "    print(\"Saved history →\", stage_hist_path)\n",
    "    print(f\"===== Stage {name} — done (best macro PR={best_metric:.4f} @ epoch {best_epoch}) =====\\n\")\n",
    "\n",
    "# -----------------------\n",
    "# Stage adjustments\n",
    "# -----------------------\n",
    "def stage_A_adjust():\n",
    "    # mild settings; keep current CFG as-is\n",
    "    pass\n",
    "\n",
    "def stage_B_adjust():\n",
    "    # unfreeze last 2–4 layers (LLRD idea): we already handle via run_stage arg\n",
    "    pass\n",
    "\n",
    "def stage_C_adjust():\n",
    "    # keep ASL; descriptor dropout already at 0.5 from CFG; InfoNCE weight stays 0.5\n",
    "    pass\n",
    "\n",
    "# -----------------------\n",
    "# Kick off all stages\n",
    "# -----------------------\n",
    "def run_all_stages():\n",
    "    # Stage A: freeze text, warm fusion/head\n",
    "    run_stage(\"A\", loader_train, loader_val, max_epochs_A, patience_A, unfreeze_last=None, adjust_for_stage=stage_A_adjust)\n",
    "    # Stage B: unfreeze last 4 transformer layers (LLRD-lite)\n",
    "    run_stage(\"B\", loader_train, loader_val, max_epochs_B, patience_B, unfreeze_last=4, adjust_for_stage=stage_B_adjust)\n",
    "    # Stage C: stronger regularization, sampler already active\n",
    "    run_stage(\"C\", loader_train, loader_val, max_epochs_C, patience_C, unfreeze_last=6, adjust_for_stage=stage_C_adjust)\n",
    "\n",
    "# -------------- RUN --------------\n",
    "print(\"Starting V5 training (Stages A→B→C) with early stopping on macro PR-AUC.\")\n",
    "run_all_stages()\n",
    "print(\"Cell 9 ✅ — Training loop finished (check checkpoints & histories).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44648e8",
   "metadata": {},
   "source": [
    "## 10: Collect logits, fit per-label calibration (Platt vs Temp), choose best by ECE, compute thresholds (max-F1 + policy), evaluate VAL/TEST, save artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665c4416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting VAL logits with EMA…\n",
      "Collecting TEST logits with EMA…\n",
      "Chosen calibration types per label:\n",
      "{'NR-AR': 'platt', 'NR-AR-LBD': 'platt', 'NR-AhR': 'platt', 'NR-Aromatase': 'platt', 'NR-ER': 'platt', 'NR-ER-LBD': 'platt', 'NR-PPAR-gamma': 'platt', 'SR-ARE': 'platt', 'SR-ATAD5': 'platt', 'SR-HSE': 'platt', 'SR-MMP': 'platt', 'SR-p53': 'platt'}\n",
      "\n",
      "VAL (calibrated):\n",
      " - maxF1 thresholds : {'macro_pr': 0.3150432217875227, 'macro_roc': 0.7379453147647883, 'macro_f1': 0.3848269893448413}\n",
      " - policy thresholds: {'macro_pr': 0.3150432217875227, 'macro_roc': 0.7379453147647883, 'macro_f1': 0.13746227419605175}\n",
      "\n",
      "TEST (calibrated):\n",
      " - maxF1 thresholds : {'macro_pr': 0.30229511258047653, 'macro_roc': 0.7479256714706223, 'macro_f1': 0.3070730393906951}\n",
      " - policy thresholds: {'macro_pr': 0.30229511258047653, 'macro_roc': 0.7479256714706223, 'macro_f1': 0.13756880238198216}\n",
      "\n",
      "Saved artifacts:\n",
      " - calibration: tox21_dualenc_v1/results/v5\\v5_20250903_124631_calibration_params.json\n",
      " - thresholds:  tox21_dualenc_v1/results/v5\\v5_20250903_124631_thresholds_maxF1.json | tox21_dualenc_v1/results/v5\\v5_20250903_124631_thresholds_rec0p60.json | tox21_dualenc_v1/results/v5\\v5_20250903_124631_thresholds_policy.json\n",
      " - probs/logits: *_val_*.npy, *_test_*.npy\n",
      " - summary: tox21_dualenc_v1/results/v5\\v5_20250903_124631_eval_summary.json\n",
      "\n",
      "Cell 10 ✅ — Calibration, thresholds, and VAL/TEST evaluation complete.\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "assert 'model' in globals() and 'ema' in globals()\n",
    "assert 'loader_val' in globals() and 'loader_test' in globals()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "LABELS = list(CFG.label_cols)\n",
    "L = len(LABELS)\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers: ECE, Brier, metrics\n",
    "# ---------------------------\n",
    "def ece_score(y_true, y_prob, n_bins=15):\n",
    "    # y_true/y_prob shape (N,)\n",
    "    bins = np.linspace(0., 1., n_bins+1)\n",
    "    ece = 0.0\n",
    "    for i in range(n_bins):\n",
    "        lo, hi = bins[i], bins[i+1]\n",
    "        m = (y_prob >= lo) & (y_prob < hi)\n",
    "        if m.sum() == 0: \n",
    "            continue\n",
    "        conf = y_prob[m].mean()\n",
    "        acc = (y_true[m] > 0.5).mean()\n",
    "        ece += (m.mean()) * abs(acc - conf)\n",
    "    return float(ece)\n",
    "\n",
    "def brier_score(y_true, y_prob):\n",
    "    return float(np.mean((y_prob - y_true)**2))\n",
    "\n",
    "# ---------------------------\n",
    "# Collect logits/probs (+ masks) with EMA\n",
    "# ---------------------------\n",
    "@torch.no_grad()\n",
    "def collect_logits(loader):\n",
    "    model.eval()\n",
    "    ema.apply_shadow(model)\n",
    "    logits_list, y_list, m_list = [], [], []\n",
    "    for batch in loader:\n",
    "        batch_dev = {\n",
    "            \"text\": {\"input_ids\": batch[\"text\"][\"input_ids\"].to(device),\n",
    "                     \"attention_mask\": batch[\"text\"][\"attention_mask\"].to(device)},\n",
    "            \"graph\": {\"x\": batch[\"graph\"][\"x\"].to(device),\n",
    "                      \"edge_index\": batch[\"graph\"][\"edge_index\"].to(device),\n",
    "                      \"edge_attr\": batch[\"graph\"][\"edge_attr\"].to(device),\n",
    "                      \"batch\": batch[\"graph\"][\"batch\"].to(device)},\n",
    "            \"desc\": batch[\"desc\"].to(device),\n",
    "            \"y\": batch[\"y\"].to(device),\n",
    "            \"mask\": batch[\"mask\"].to(device),\n",
    "        }\n",
    "        out = model(batch_dev, return_proj=False)\n",
    "        logits_list.append(out[\"logits\"].detach().cpu().numpy())\n",
    "        y_list.append(batch_dev[\"y\"].cpu().numpy())\n",
    "        m_list.append(batch_dev[\"mask\"].cpu().numpy())\n",
    "    ema.restore(model)\n",
    "    logits = np.concatenate(logits_list, axis=0)\n",
    "    y_true = np.concatenate(y_list, axis=0)\n",
    "    y_mask = np.concatenate(m_list, axis=0)\n",
    "    return logits, y_true, y_mask\n",
    "\n",
    "print(\"Collecting VAL logits with EMA…\")\n",
    "val_logits, val_true, val_mask = collect_logits(loader_val)\n",
    "val_prob_raw = 1/(1+np.exp(-val_logits))\n",
    "\n",
    "print(\"Collecting TEST logits with EMA…\")\n",
    "test_logits, test_true, test_mask = collect_logits(loader_test)\n",
    "test_prob_raw = 1/(1+np.exp(-test_logits))\n",
    "\n",
    "# ---------------------------\n",
    "# Per-label calibration: Platt vs Temp\n",
    "# ---------------------------\n",
    "def fit_platt(z, y):\n",
    "    # z: logits (n,), y: {0,1}\n",
    "    # L2-regularized logistic regression (small C) to avoid degenerate fits\n",
    "    lr = LogisticRegression(solver=\"lbfgs\", C=1.0, max_iter=1000)\n",
    "    lr.fit(z.reshape(-1,1), y)\n",
    "    a = float(lr.coef_[0][0])\n",
    "    b = float(lr.intercept_[0])\n",
    "    return {\"type\":\"platt\", \"a\":a, \"b\":b}\n",
    "\n",
    "def apply_platt(z, a, b):\n",
    "    return 1/(1+np.exp(-(a*z + b)))\n",
    "\n",
    "def fit_temp(z, y):\n",
    "    # optimize temperature T >= 1 to minimize NLL; start at 1.0\n",
    "    # work on logits; NLL = -[ y*log(sigmoid(z/T)) + (1-y)*log(1-sigmoid(z/T)) ]\n",
    "    T = 1.0\n",
    "    for _ in range(100):\n",
    "        p = 1/(1+np.exp(-(z/T)))\n",
    "        # derivative wrt T\n",
    "        grad = np.mean((p - y) * z / (T*T))\n",
    "        # simple line search\n",
    "        T_new = max(1.0, T - 0.1*grad)\n",
    "        if abs(T_new - T) < 1e-5:\n",
    "            break\n",
    "        T = T_new\n",
    "    return {\"type\":\"temp\", \"T\":float(T)}\n",
    "\n",
    "def apply_temp(z, T):\n",
    "    return 1/(1+np.exp(-(z / T)))\n",
    "\n",
    "calib_params = {}\n",
    "chosen_type = {}\n",
    "ece_table = {}\n",
    "\n",
    "for j, lab in enumerate(LABELS):\n",
    "    m = val_mask[:, j] > 0.5\n",
    "    if m.sum() < 50 or val_true[m, j].sum() == 0 or val_true[m, j].sum() == m.sum():\n",
    "        # degenerate → skip calibration, use raw\n",
    "        calib_params[lab] = {\"type\":\"none\"}\n",
    "        chosen_type[lab] = \"none\"\n",
    "        ece_table[lab] = {\"raw\": np.nan, \"chosen\": np.nan}\n",
    "        continue\n",
    "    z = val_logits[m, j]\n",
    "    y = val_true[m, j]\n",
    "\n",
    "    # raw ECE\n",
    "    ece_raw = ece_score(y, 1/(1+np.exp(-z)))\n",
    "\n",
    "    # Platt\n",
    "    pl = fit_platt(z, y)\n",
    "    p_pl = apply_platt(z, pl[\"a\"], pl[\"b\"])\n",
    "    ece_pl = ece_score(y, p_pl)\n",
    "\n",
    "    # Temp\n",
    "    ts = fit_temp(z, y)\n",
    "    p_ts = apply_temp(z, ts[\"T\"])\n",
    "    ece_ts = ece_score(y, p_ts)\n",
    "\n",
    "    # choose by ECE (tie-breaker: lower Brier)\n",
    "    if ece_pl < ece_ts:\n",
    "        chosen = pl\n",
    "        chosen_s = \"platt\"\n",
    "        chosen_p = p_pl\n",
    "    elif ece_ts < ece_pl:\n",
    "        chosen = ts\n",
    "        chosen_s = \"temp\"\n",
    "        chosen_p = p_ts\n",
    "    else:\n",
    "        # tie → compare Brier\n",
    "        if brier_score(y, p_pl) <= brier_score(y, p_ts):\n",
    "            chosen = pl; chosen_s = \"platt\"; chosen_p = p_pl\n",
    "        else:\n",
    "            chosen = ts; chosen_s = \"temp\"; chosen_p = p_ts\n",
    "\n",
    "    calib_params[lab] = chosen\n",
    "    chosen_type[lab] = chosen_s\n",
    "    ece_table[lab] = {\"raw\": float(ece_raw), \"chosen\": float(ece_score(y, chosen_p))}\n",
    "\n",
    "print(\"Chosen calibration types per label:\")\n",
    "print(chosen_type)\n",
    "\n",
    "# Apply calibration helper\n",
    "def apply_calibration(logits, params_per_label):\n",
    "    probs = np.zeros_like(logits, dtype=np.float32)\n",
    "    for j, lab in enumerate(LABELS):\n",
    "        pars = params_per_label[lab]\n",
    "        z = logits[:, j]\n",
    "        if pars[\"type\"] == \"platt\":\n",
    "            probs[:, j] = apply_platt(z, pars[\"a\"], pars[\"b\"])\n",
    "        elif pars[\"type\"] == \"temp\":\n",
    "            probs[:, j] = apply_temp(z, pars[\"T\"])\n",
    "        else:  # none\n",
    "            probs[:, j] = 1/(1+np.exp(-z))\n",
    "    return probs\n",
    "\n",
    "val_prob_cal = apply_calibration(val_logits, calib_params)\n",
    "test_prob_cal = apply_calibration(test_logits, calib_params)\n",
    "\n",
    "# ---------------------------\n",
    "# Thresholds (VAL-based)\n",
    "# ---------------------------\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def best_f1_threshold(y, p, mask):\n",
    "    m = mask > 0.5\n",
    "    if m.sum() < 3 or y[m].sum() == 0:\n",
    "        return 0.5  # fallback\n",
    "    prec, rec, thr = precision_recall_curve(y[m], p[m])\n",
    "    f1 = (2*prec*rec) / (prec+rec + 1e-12)\n",
    "    k = np.nanargmax(f1)\n",
    "    # precision_recall_curve returns thresholds of length len(prec)-1\n",
    "    return float(thr[max(0, min(k, len(thr)-1))])\n",
    "\n",
    "def threshold_for_recall(y, p, mask, target_recall):\n",
    "    m = mask > 0.5\n",
    "    if m.sum() < 3 or y[m].sum() == 0:\n",
    "        return 0.5\n",
    "    prec, rec, thr = precision_recall_curve(y[m], p[m])\n",
    "    # Find smallest threshold that achieves >= target_recall\n",
    "    idx = np.where(rec >= target_recall)[0]\n",
    "    if len(idx) == 0:\n",
    "        return float(thr[-1]) if len(thr)>0 else 0.5\n",
    "    # map idx to thresholds (shift by -1)\n",
    "    thr_idx = np.clip(idx-1, 0, len(thr)-1)\n",
    "    return float(thr[thr_idx[0]])\n",
    "\n",
    "thr_maxF1 = {}\n",
    "thr_rec060 = {}\n",
    "thr_policy = {}\n",
    "\n",
    "for j, lab in enumerate(LABELS):\n",
    "    y = val_true[:, j]\n",
    "    p = val_prob_cal[:, j]\n",
    "    m = val_mask[:, j]\n",
    "    thr_maxF1[lab] = best_f1_threshold(y, p, m)\n",
    "    base_target = 0.50 if lab in CFG.overrides_recall_50 else CFG.base_recall_target\n",
    "    thr_rec060[lab] = threshold_for_recall(y, p, m, target_recall=CFG.base_recall_target)\n",
    "    thr_policy[lab] = threshold_for_recall(y, p, m, target_recall=base_target)\n",
    "\n",
    "# ---------------------------\n",
    "# Evaluate with selected thresholds\n",
    "# ---------------------------\n",
    "def metrics_with_thresholds(y_true, y_prob, y_mask, thr_map):\n",
    "    L = y_true.shape[1]\n",
    "    pr_list, roc_list, f1_list = [], [], []\n",
    "    for j in range(L):\n",
    "        m = y_mask[:, j] > 0.5\n",
    "        y = y_true[m, j]\n",
    "        p = y_prob[m, j]\n",
    "        if m.sum() < 3 or y.sum() == 0 or y.sum() == m.sum():\n",
    "            continue\n",
    "        pr_list.append(average_precision_score(y, p))\n",
    "        roc_list.append(roc_auc_score(y, p))\n",
    "        y_hat = (p >= thr_map[LABELS[j]]).astype(np.int32)\n",
    "        f1_list.append(f1_score(y, y_hat, zero_division=0))\n",
    "    return {\n",
    "        \"macro_pr\": float(np.nanmean(pr_list)) if pr_list else float('nan'),\n",
    "        \"macro_roc\": float(np.nanmean(roc_list)) if roc_list else float('nan'),\n",
    "        \"macro_f1\": float(np.nanmean(f1_list)) if f1_list else float('nan'),\n",
    "    }\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "val_metrics_maxF1 = metrics_with_thresholds(val_true, val_prob_cal, val_mask, thr_maxF1)\n",
    "val_metrics_policy = metrics_with_thresholds(val_true, val_prob_cal, val_mask, thr_policy)\n",
    "\n",
    "test_metrics_maxF1 = metrics_with_thresholds(test_true, test_prob_cal, test_mask, thr_maxF1)\n",
    "test_metrics_policy = metrics_with_thresholds(test_true, test_prob_cal, test_mask, thr_policy)\n",
    "\n",
    "print(\"\\nVAL (calibrated):\")\n",
    "print(\" - maxF1 thresholds :\", val_metrics_maxF1)\n",
    "print(\" - policy thresholds:\", val_metrics_policy)\n",
    "\n",
    "print(\"\\nTEST (calibrated):\")\n",
    "print(\" - maxF1 thresholds :\", test_metrics_maxF1)\n",
    "print(\" - policy thresholds:\", test_metrics_policy)\n",
    "\n",
    "# ---------------------------\n",
    "# Save artifacts\n",
    "# ---------------------------\n",
    "calib_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_calibration_params.json\")\n",
    "with open(calib_path, \"w\") as f:\n",
    "    json.dump(calib_params, f, indent=2)\n",
    "\n",
    "thr_maxF1_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_thresholds_maxF1.json\")\n",
    "thr_rec060_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_thresholds_rec0p60.json\")\n",
    "thr_policy_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_thresholds_policy.json\")\n",
    "with open(thr_maxF1_path, \"w\") as f: json.dump(thr_maxF1, f, indent=2)\n",
    "with open(thr_rec060_path, \"w\") as f: json.dump(thr_rec060, f, indent=2)\n",
    "with open(thr_policy_path, \"w\") as f: json.dump(thr_policy, f, indent=2)\n",
    "\n",
    "np.save(os.path.join(RES_DIR, f\"{CFG.run_tag}_val_logits.npy\"), val_logits)\n",
    "np.save(os.path.join(RES_DIR, f\"{CFG.run_tag}_val_probs_raw.npy\"), val_prob_raw)\n",
    "np.save(os.path.join(RES_DIR, f\"{CFG.run_tag}_val_probs_cal.npy\"), val_prob_cal)\n",
    "np.save(os.path.join(RES_DIR, f\"{CFG.run_tag}_test_logits.npy\"), test_logits)\n",
    "np.save(os.path.join(RES_DIR, f\"{CFG.run_tag}_test_probs_raw.npy\"), test_prob_raw)\n",
    "np.save(os.path.join(RES_DIR, f\"{CFG.run_tag}_test_probs_cal.npy\"), test_prob_cal)\n",
    "\n",
    "summary = {\n",
    "    \"val\": {\n",
    "        \"macro_pr_RAW\": float(np.nanmean([average_precision_score(val_true[val_mask[:,j]>0.5, j],\n",
    "                                                val_prob_raw[val_mask[:,j]>0.5, j])\n",
    "                                          for j in range(L) if (val_mask[:,j]>0.5).sum()>2 and val_true[val_mask[:,j]>0.5, j].sum()>0 and val_true[val_mask[:,j]>0.5, j].sum()<(val_mask[:,j]>0.5).sum()] or [float('nan')])),\n",
    "        \"macro_pr_CAL\": float(np.nanmean([average_precision_score(val_true[val_mask[:,j]>0.5, j],\n",
    "                                                val_prob_cal[val_mask[:,j]>0.5, j])\n",
    "                                          for j in range(L) if (val_mask[:,j]>0.5).sum()>2 and val_true[val_mask[:,j]>0.5, j].sum()>0 and val_true[val_mask[:,j]>0.5, j].sum()<(val_mask[:,j]>0.5).sum()] or [float('nan')])),\n",
    "        \"maxF1\": val_metrics_maxF1,\n",
    "        \"policy\": val_metrics_policy,\n",
    "        \"chosen_calibration\": chosen_type,\n",
    "        \"ece_table\": ece_table,\n",
    "    },\n",
    "    \"test\": {\n",
    "        \"maxF1\": test_metrics_maxF1,\n",
    "        \"policy\": test_metrics_policy,\n",
    "    }\n",
    "}\n",
    "summary_path = os.path.join(RES_DIR, f\"{CFG.run_tag}_eval_summary.json\")\n",
    "with open(summary_path, \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\nSaved artifacts:\")\n",
    "print(\" - calibration:\", calib_path)\n",
    "print(\" - thresholds: \", thr_maxF1_path, \"|\", thr_rec060_path, \"|\", thr_policy_path)\n",
    "print(\" - probs/logits: *_val_*.npy, *_test_*.npy\")\n",
    "print(\" - summary:\", summary_path)\n",
    "print(\"\\nCell 10 ✅ — Calibration, thresholds, and VAL/TEST evaluation complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb0a542",
   "metadata": {},
   "source": [
    "## 11: VAL predictions preview (top-k table + one sample bar chart)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681dcf21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Excel: tox21_dualenc_v1/data/raw/Test.xlsx\n",
      "Columns: ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53', 'mol_id', 'smiles'] ...\n",
      "Using input column: 'smiles'\n",
      "\n",
      "Scanned 43 rows | used 43 valid SMILES | skipped 0 (invalid SMILES).\n",
      "\n",
      "Preview (first 10 rows):\n",
      "   idx                                             smiles  \\\n",
      "0    0                       CCOc1ccc2nc(S(N)(=O)=O)sc2c1   \n",
      "1    1                          CCN1C(=O)NC(c2ccccc2)C1=O   \n",
      "2    2  CC[C@]1(O)CC[C@H]2[C@@H]3CCC4=CCCC[C@@H]4[C@H]...   \n",
      "3    3                    CCCN(CC)C(CC)C(=O)Nc1c(C)cccc1C   \n",
      "4    4                          CC(O)(P(=O)(O)O)P(=O)(O)O   \n",
      "5    5               CC(C)(C)OOC(C)(C)CCC(C)(C)OOC(C)(C)C   \n",
      "6    6                                O=S(=O)(Cl)c1ccccc1   \n",
      "7    7             O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1   \n",
      "8    8                     OC[C@@H](O)[C@H](O)[C@@H](O)CO   \n",
      "9    9           CCCCCCCC(=O)[O-].CCCCCCCC(=O)[O-].[Zn+2]   \n",
      "\n",
      "                                                top3  \\\n",
      "0  [(NR-AhR, 0.4061), (SR-ARE, 0.1568), (SR-MMP, ...   \n",
      "1  [(SR-ARE, 0.1286), (NR-ER, 0.1233), (SR-MMP, 0...   \n",
      "2  [(NR-ER, 0.811), (NR-AR, 0.5061), (SR-MMP, 0.4...   \n",
      "3  [(SR-MMP, 0.1746), (NR-AhR, 0.1462), (SR-ARE, ...   \n",
      "4  [(SR-ARE, 0.1403), (NR-ER, 0.0718), (SR-MMP, 0...   \n",
      "5  [(SR-ARE, 0.1063), (SR-MMP, 0.0889), (NR-ER, 0...   \n",
      "6  [(SR-ARE, 0.1441), (NR-ER, 0.1119), (SR-MMP, 0...   \n",
      "7  [(SR-MMP, 0.2124), (SR-ARE, 0.169), (NR-ER, 0....   \n",
      "8  [(SR-ARE, 0.1267), (NR-ER, 0.0609), (SR-HSE, 0...   \n",
      "9  [(SR-ARE, 0.1471), (NR-ER, 0.077), (SR-HSE, 0....   \n",
      "\n",
      "                                         pred_labels  \\\n",
      "0  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "1  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "2  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "3  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "4  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "5  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "6  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "7  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "8  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "9  [NR-AR, NR-AR-LBD, NR-AhR, NR-Aromatase, NR-ER...   \n",
      "\n",
      "                                        pred_is_true  \\\n",
      "0  [False, False, True, False, False, False, Fals...   \n",
      "1  [False, False, False, False, False, False, Fal...   \n",
      "2  [False, False, False, False, False, False, Fal...   \n",
      "3  [False, False, False, False, False, False, Fal...   \n",
      "4  [False, False, False, False, False, False, Fal...   \n",
      "5  [False, False, False, False, False, False, Fal...   \n",
      "6  [False, False, False, False, False, False, Fal...   \n",
      "7  [False, False, False, False, True, False, Fals...   \n",
      "8  [False, False, False, False, False, False, Fal...   \n",
      "9  [False, False, False, False, False, False, Fal...   \n",
      "\n",
      "                          true_pos  \n",
      "0                 [NR-AhR, SR-ARE]  \n",
      "1                               []  \n",
      "2                               []  \n",
      "3                               []  \n",
      "4                               []  \n",
      "5                               []  \n",
      "6                               []  \n",
      "7  [NR-ER, SR-ARE, SR-HSE, SR-p53]  \n",
      "8                               []  \n",
      "9                               []  \n",
      "\n",
      "Overall metrics on this file (thresholds = policy ):\n",
      " - Micro  P/R/F1: 0.096 / 1.000 / 0.176\n",
      " - Macro  P/R/F1: 0.097 / 0.917 / 0.168\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "\n",
    "assert 'model' in globals() and 'ema' in globals()\n",
    "assert 'CFG' in globals() and 'LABELS' in globals()\n",
    "assert 'tokenizer' in globals() and 'smiles_to_graph' in globals()\n",
    "assert 'enumerate_smiles_for_tta' in globals() and 'canonicalize_smiles' in globals()\n",
    "assert 'calib_params' in globals() and 'thr_maxF1' in globals() and 'thr_policy' in globals()\n",
    "assert '_scaler' in globals() and 'DESC_ORDER' in globals() and 'DESC_MEDIANS' in globals()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LABELS = list(CFG.label_cols)\n",
    "\n",
    "# ---------- config ----------\n",
    "XLSX_PATH = \"tox21_dualenc_v1/data/raw/Test.xlsx\"\n",
    "INPUT_COL_CANDIDATES = [\"smarts\", \"SMILES\", \"smiles\"]   # we try in this order\n",
    "THRESHOLDS_CHOICE = \"policy\"  # \"policy\" | \"maxF1\"\n",
    "\n",
    "# ---------- helpers ----------\n",
    "def desc_for_smiles(smiles: str) -> np.ndarray:\n",
    "    \"\"\"Scaled descriptor vector (257,). Uses TRAIN means if SMILES not found in df.\"\"\"\n",
    "    c = canonicalize_smiles(smiles)\n",
    "    # Lazy build once: canonical SMILES -> row index\n",
    "    global SMI2ROW\n",
    "    if 'SMI2ROW' not in globals():\n",
    "        SMI2ROW = {}\n",
    "        for i, s in enumerate(df[CFG.smiles_col].astype(str)):\n",
    "            cs = canonicalize_smiles(s)\n",
    "            if cs and cs not in SMI2ROW:\n",
    "                SMI2ROW[cs] = i\n",
    "    if c in SMI2ROW:\n",
    "        row = df.iloc[SMI2ROW[c]]\n",
    "        vals = []\n",
    "        for col in DESC_ORDER:\n",
    "            v = row.get(col, np.nan)\n",
    "            if pd.isna(v):\n",
    "                v = DESC_MEDIANS.get(col, 0.0)\n",
    "            vals.append(float(v))\n",
    "        raw = np.asarray(vals, dtype=np.float32)[None, :]\n",
    "        return _scaler.transform(raw).astype(np.float32)[0]\n",
    "    else:\n",
    "        # neutral: TRAIN means -> scaled ≈ zeros\n",
    "        raw = _scaler.mean_[None, :].astype(np.float32)\n",
    "        return _scaler.transform(raw).astype(np.float32)[0]\n",
    "\n",
    "def apply_calibration_vector(logits_vec: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Apply per-label calibration to a 1D logits vector (12,).\"\"\"\n",
    "    probs = np.zeros_like(logits_vec, dtype=np.float32)\n",
    "    for j, lab in enumerate(LABELS):\n",
    "        pars = calib_params[lab]\n",
    "        z = logits_vec[j]\n",
    "        if pars[\"type\"] == \"platt\":\n",
    "            probs[j] = 1.0 / (1.0 + np.exp(-(pars[\"a\"]*z + pars[\"b\"])))\n",
    "        elif pars[\"type\"] == \"temp\":\n",
    "            probs[j] = 1.0 / (1.0 + np.exp(-(z / pars[\"T\"])))\n",
    "        else:\n",
    "            probs[j] = 1.0 / (1.0 + np.exp(-z))\n",
    "    return probs\n",
    "\n",
    "def predict_single_smiles(smi: str, tta_n: int) -> np.ndarray:\n",
    "    \"\"\"Return calibrated probabilities (12,) for a single SMILES with TTA.\"\"\"\n",
    "    # Build TTA forms\n",
    "    tta_smiles = enumerate_smiles_for_tta(smi, n=max(1, tta_n))\n",
    "    desc_vec = desc_for_smiles(smi)\n",
    "\n",
    "    logits_acc = np.zeros((len(LABELS),), dtype=np.float32)\n",
    "    for s in tta_smiles:\n",
    "        # text\n",
    "        enc = tokenizer(\n",
    "            s, max_length=CFG.max_smiles_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\"\n",
    "        )\n",
    "        # graph\n",
    "        g = smiles_to_graph(s, use_virtual_node=CFG.use_virtual_node)\n",
    "\n",
    "        batch_dev = {\n",
    "            \"text\": {\"input_ids\": enc[\"input_ids\"].to(device),\n",
    "                     \"attention_mask\": enc[\"attention_mask\"].to(device)},\n",
    "            \"graph\": {\"x\": torch.from_numpy(g[\"x\"]).to(device),\n",
    "                      \"edge_index\": torch.from_numpy(g[\"edge_index\"]).to(device),\n",
    "                      \"edge_attr\": torch.from_numpy(g[\"edge_attr\"]).to(device),\n",
    "                      \"batch\": torch.zeros((g[\"x\"].shape[0],), dtype=torch.long, device=device)},\n",
    "            \"desc\": torch.from_numpy(desc_vec).unsqueeze(0).to(device),\n",
    "            \"y\": torch.zeros((1, len(LABELS)), device=device),\n",
    "            \"mask\": torch.ones((1, len(LABELS)), device=device),\n",
    "        }\n",
    "        out = model(batch_dev, return_proj=False)\n",
    "        logits_acc += out[\"logits\"].squeeze(0).detach().cpu().numpy().astype(np.float32)\n",
    "\n",
    "    logits_avg = logits_acc / float(len(tta_smiles))\n",
    "    probs_cal = apply_calibration_vector(logits_avg)\n",
    "    return probs_cal\n",
    "\n",
    "def pick_input_column(df_in: pd.DataFrame) -> str:\n",
    "    for c in INPUT_COL_CANDIDATES:\n",
    "        if c in df_in.columns:\n",
    "            return c\n",
    "    raise KeyError(f\"None of the expected input columns found: {INPUT_COL_CANDIDATES}\")\n",
    "\n",
    "def compute_metrics_micro_macro(y_true_bin, y_pred_bin, mask_bin):\n",
    "    \"\"\"\n",
    "    y_* shape: (N, L), entries in {0,1}, mask 1=valid\n",
    "    Returns micro/macro Precision, Recall, F1 (ignoring masked labels).\n",
    "    \"\"\"\n",
    "    # Flatten masked\n",
    "    m = mask_bin.astype(bool)\n",
    "    y_t = y_true_bin[m]\n",
    "    y_p = y_pred_bin[m]\n",
    "    if y_t.size == 0:\n",
    "        return {\"micro\": {\"P\": np.nan, \"R\": np.nan, \"F1\": np.nan},\n",
    "                \"macro\": {\"P\": np.nan, \"R\": np.nan, \"F1\": np.nan}}\n",
    "    # Micro\n",
    "    tp = np.sum((y_t == 1) & (y_p == 1))\n",
    "    fp = np.sum((y_t == 0) & (y_p == 1))\n",
    "    fn = np.sum((y_t == 1) & (y_p == 0))\n",
    "    P_micro = tp / max(1, tp + fp)\n",
    "    R_micro = tp / max(1, tp + fn)\n",
    "    F1_micro = 2*P_micro*R_micro / max(1e-12, P_micro+R_micro)\n",
    "    # Macro\n",
    "    P_list, R_list, F1_list = [], [], []\n",
    "    L = y_true_bin.shape[1]\n",
    "    for j in range(L):\n",
    "        mj = m[:, j]\n",
    "        if mj.sum() == 0: \n",
    "            continue\n",
    "        yt = y_true_bin[mj, j]; yp = y_pred_bin[mj, j]\n",
    "        tp = np.sum((yt == 1) & (yp == 1))\n",
    "        fp = np.sum((yt == 0) & (yp == 1))\n",
    "        fn = np.sum((yt == 1) & (yp == 0))\n",
    "        P = tp / max(1, tp + fp)\n",
    "        R = tp / max(1, tp + fn)\n",
    "        F1 = 2*P*R / max(1e-12, P+R)\n",
    "        P_list.append(P); R_list.append(R); F1_list.append(F1)\n",
    "    return {\n",
    "        \"micro\": {\"P\": float(np.mean([P_micro])), \"R\": float(np.mean([R_micro])), \"F1\": float(np.mean([F1_micro]))},\n",
    "        \"macro\": {\"P\": float(np.mean(P_list)) if P_list else np.nan,\n",
    "                  \"R\": float(np.mean(R_list)) if R_list else np.nan,\n",
    "                  \"F1\": float(np.mean(F1_list)) if F1_list else np.nan},\n",
    "    }\n",
    "\n",
    "# ---------- load file ----------\n",
    "print(f\"Reading Excel: {XLSX_PATH}\")\n",
    "df_in = pd.read_excel(XLSX_PATH)\n",
    "print(\"Columns:\", list(df_in.columns)[:20], \"...\")\n",
    "\n",
    "in_col = pick_input_column(df_in)\n",
    "print(f\"Using input column: '{in_col}'\")\n",
    "\n",
    "# verify ground-truth columns exist\n",
    "missing_labels = [c for c in LABELS if c not in df_in.columns]\n",
    "assert not missing_labels, f\"Missing label columns in the file: {missing_labels}\"\n",
    "\n",
    "# ---------- run predictions ----------\n",
    "tta_n = int(CFG.tta_smiles_at_infer)\n",
    "thr_map = thr_policy if THRESHOLDS_CHOICE == \"policy\" else thr_maxF1\n",
    "\n",
    "model.eval()\n",
    "ema.apply_shadow(model)\n",
    "\n",
    "rows = []\n",
    "probs_all = []\n",
    "y_true_all = []\n",
    "mask_all = []\n",
    "n_total = 0\n",
    "n_used = 0\n",
    "n_skipped = 0\n",
    "\n",
    "for i, row in df_in.iterrows():\n",
    "    n_total += 1\n",
    "    s_raw = str(row[in_col])\n",
    "    # Try to parse as SMILES\n",
    "    if Chem.MolFromSmiles(s_raw) is None:\n",
    "        # Not a valid SMILES; skip with note (SMARTS often won't work with our featurizer)\n",
    "        n_skipped += 1\n",
    "        continue\n",
    "\n",
    "    smi = canonicalize_smiles(s_raw)\n",
    "    p = predict_single_smiles(smi, tta_n=tta_n)  # (12,)\n",
    "    probs_all.append(p)\n",
    "\n",
    "    # build truth + mask: treat -1 as missing\n",
    "    y_row = row[LABELS].values.astype(np.float32)\n",
    "    mask_row = (y_row != CFG.missing_label_value).astype(np.float32)\n",
    "    y_row = np.where(y_row == CFG.missing_label_value, 0.0, y_row)\n",
    "    y_true_all.append(y_row)\n",
    "    mask_all.append(mask_row)\n",
    "\n",
    "    # predicted labels under threshold\n",
    "    pred_labels = [lab for lab in LABELS if p[LABELS.index(lab)] >= thr_map[lab]]\n",
    "    # map to True/False vs ground truth (only count labels that are valid)\n",
    "    truth_pos_set = set([LABELS[j] for j in np.where((y_row > 0.5) & (mask_row > 0.5))[0]])\n",
    "    pred_tf = [ (lab in truth_pos_set) for lab in pred_labels ]\n",
    "\n",
    "    # top-3 for display\n",
    "    order = np.argsort(-p)[:3]\n",
    "    top3 = [(LABELS[j], float(p[j])) for j in order]\n",
    "\n",
    "    rows.append({\n",
    "        \"idx\": int(i),\n",
    "        \"smiles\": smi,\n",
    "        \"top3\": [(lab, round(prob, 4)) for lab, prob in top3],\n",
    "        \"pred_labels\": pred_labels,\n",
    "        \"pred_is_true\": pred_tf,\n",
    "        \"true_pos\": sorted(list(truth_pos_set))\n",
    "    })\n",
    "    n_used += 1\n",
    "\n",
    "ema.restore(model)\n",
    "\n",
    "# ---------- print preview ----------\n",
    "print(f\"\\nScanned {n_total} rows | used {n_used} valid SMILES | skipped {n_skipped} (invalid SMILES).\")\n",
    "preview = pd.DataFrame(rows)\n",
    "print(\"\\nPreview (first 10 rows):\")\n",
    "print(preview.head(10))\n",
    "\n",
    "# ---------- overall metrics on this file ----------\n",
    "if n_used > 0:\n",
    "    y_true_all = np.vstack(y_true_all)\n",
    "    mask_all = np.vstack(mask_all)\n",
    "    probs_all = np.vstack(probs_all)\n",
    "    # threshold to binary\n",
    "    y_pred_all = np.zeros_like(probs_all, dtype=np.int32)\n",
    "    for j, lab in enumerate(LABELS):\n",
    "        y_pred_all[:, j] = (probs_all[:, j] >= thr_map[lab]).astype(np.int32)\n",
    "\n",
    "    metrics = compute_metrics_micro_macro(y_true_all, y_pred_all, mask_all)\n",
    "    print(\"\\nOverall metrics on this file (thresholds =\", THRESHOLDS_CHOICE, \"):\")\n",
    "    print(\" - Micro  P/R/F1: {P:.3f} / {R:.3f} / {F1:.3f}\".format(**metrics[\"micro\"]))\n",
    "    print(\" - Macro  P/R/F1: {P:.3f} / {R:.3f} / {F1:.3f}\".format(**metrics[\"macro\"]))\n",
    "else:\n",
    "    print(\"\\nNo valid SMILES rows to score.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad531099",
   "metadata": {},
   "source": [
    "# v6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d01fb",
   "metadata": {},
   "source": [
    "## 1: Environment + Paths + Repro Seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7c6227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== V6 SETUP SUMMARY ===\n",
      "Time UTC: 2025-09-03 13:33:56\n",
      "Python: 3.11.9\n",
      "OS: Windows 10 | Windows-10-10.0.19045-SP0\n",
      "Torch: 2.6.0+cu124 | CUDA: available=True, device='NVIDIA GeForce RTX 4070 Ti'\n",
      "RDKit: 2022.09.5 | scikit-learn: 1.7.1 | transformers: 4.43.3\n",
      "Seed: 42\n",
      "Checkpoints dir: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\n",
      "Results dir    : D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\n",
      "\n",
      "OK: Environment looks good. Proceeding next will load the dataset and verify splits.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, sys, platform, time, random\n",
    "import numpy as np\n",
    "\n",
    "# ----- Reproducibility -----\n",
    "def seed_everything(seed: int = 42):\n",
    "    import torch\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # deterministic for cudnn\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return seed\n",
    "\n",
    "SEED = seed_everything(42)\n",
    "\n",
    "# ----- Project paths (created only if missing) -----\n",
    "CKPT_DIR = Path(\"tox21_dualenc_v1/models/checkpoints_v6\")\n",
    "RES_DIR  = Path(\"tox21_dualenc_v1/results/v6\")\n",
    "for p in [CKPT_DIR, RES_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----- Canonical Tox21 labels -----\n",
    "LABELS = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# ----- Environment report -----\n",
    "def get_version(mod_name):\n",
    "    try:\n",
    "        mod = __import__(mod_name)\n",
    "        return getattr(mod, \"__version__\", \"unknown\")\n",
    "    except Exception as e:\n",
    "        return f\"not found ({e})\"\n",
    "\n",
    "py_ver = sys.version.split()[0]\n",
    "torch_ver = get_version(\"torch\")\n",
    "rdkit_ver = get_version(\"rdkit\")\n",
    "sk_ver = get_version(\"sklearn\")\n",
    "transformers_ver = get_version(\"transformers\")\n",
    "\n",
    "# GPU info (torch optional)\n",
    "cuda_str = \"N/A\"\n",
    "try:\n",
    "    import torch\n",
    "    cuda_str = f\"available={torch.cuda.is_available()}\"\n",
    "    if torch.cuda.is_available():\n",
    "        cuda_str += f\", device='{torch.cuda.get_device_name(0)}'\"\n",
    "except Exception as e:\n",
    "    cuda_str = f\"torch not available ({e})\"\n",
    "\n",
    "print(\"=== V6 SETUP SUMMARY ===\")\n",
    "print(f\"Time UTC: {time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())}\")\n",
    "print(f\"Python: {py_ver}\")\n",
    "print(f\"OS: {platform.system()} {platform.release()} | {platform.platform()}\")\n",
    "print(f\"Torch: {torch_ver} | CUDA: {cuda_str}\")\n",
    "print(f\"RDKit: {rdkit_ver} | scikit-learn: {sk_ver} | transformers: {transformers_ver}\")\n",
    "print(f\"Seed: {SEED}\")\n",
    "print(f\"Checkpoints dir: {CKPT_DIR.resolve()}\")\n",
    "print(f\"Results dir    : {RES_DIR.resolve()}\")\n",
    "\n",
    "# Assert minimal expectations (we'll fail fast if core libs are missing)\n",
    "assert torch_ver != \"not found\", \"PyTorch is required.\"\n",
    "assert rdkit_ver != \"not found\", \"RDKit is required.\"\n",
    "assert sk_ver != \"not found\", \"scikit-learn is required.\"\n",
    "\n",
    "print(\"\\nOK: Environment looks good. Proceeding next will load the dataset and verify splits.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c635dc38",
   "metadata": {},
   "source": [
    "## 2: Data load and split verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fe0bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset from: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\data\\raw\\dataset_selected.csv\n",
      "Shape: (7831, 271)\n",
      "Columns (271): ['row_id', 'split', 'smiles', 'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ARE', 'SR-ATAD5'] ...\n",
      "\n",
      "Canonicalizing SMILES (this may take a moment)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[14:36:02] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kept 7831 / 7831 rows after removing invalid SMILES.\n",
      "mol_id not found — generating deterministic mol_id from canonical SMILES.\n",
      "\n",
      "Duplicates — mol_id: 0, smiles_canonical: 0\n",
      "\n",
      "Split sizes: {'train': 6265, 'test': 783, 'val': 783}\n",
      "\n",
      "Per-label validity / pos / neg / prevalence (valid only):\n",
      "NR-AR         valid=7265  pos= 309  neg=6956  prev=0.0425\n",
      "NR-AR-LBD     valid=6758  pos= 237  neg=6521  prev=0.0351\n",
      "NR-AhR        valid=6549  pos= 768  neg=5781  prev=0.1173\n",
      "NR-Aromatase  valid=5821  pos= 300  neg=5521  prev=0.0515\n",
      "NR-ER         valid=6193  pos= 793  neg=5400  prev=0.128\n",
      "NR-ER-LBD     valid=6955  pos= 350  neg=6605  prev=0.0503\n",
      "NR-PPAR-gamma  valid=6450  pos= 186  neg=6264  prev=0.0288\n",
      "SR-ARE        valid=5832  pos= 942  neg=4890  prev=0.1615\n",
      "SR-ATAD5      valid=7072  pos= 264  neg=6808  prev=0.0373\n",
      "SR-HSE        valid=6467  pos= 372  neg=6095  prev=0.0575\n",
      "SR-MMP        valid=5810  pos= 918  neg=4892  prev=0.158\n",
      "SR-p53        valid=6774  pos= 423  neg=6351  prev=0.0624\n",
      "\n",
      "SMILES spot-check: 1000 sampled, invalid=0\n",
      "\n",
      "OK: Data looks consistent. Ready to move on to descriptor scaling & caches (train-only fit).\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import hashlib\n",
    "from rdkit import Chem\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "PREFERRED_PATHS = [\n",
    "    Path(\"tox21_dualenc_v1/data/raw/dataset_selected.csv\"),\n",
    "    Path(\"tox21_dualenc_v1/data/raw/tox21.csv\"),\n",
    "    Path(\"tox21.csv\"),\n",
    "]\n",
    "SPLIT_MAP_PATH = Path(\"tox21_dualenc_v1/data/raw/split_map.csv\")  # optional fallback mapping\n",
    "\n",
    "REQUIRED_LABELS = set([\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "])\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def canonicalize_smiles(s: str) -> str | None:\n",
    "    \"\"\"Return RDKit canonical SMILES or None if invalid.\"\"\"\n",
    "    if pd.isna(s) or not isinstance(s, str) or s.strip() == \"\":\n",
    "        return None\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol is None:\n",
    "        return None\n",
    "    return Chem.MolToSmiles(mol, canonical=True)\n",
    "\n",
    "def make_mol_id_from_smiles(smi: str) -> str:\n",
    "    \"\"\"Deterministic mol_id from canonical SMILES using SHA1 (shortened).\"\"\"\n",
    "    h = hashlib.sha1(smi.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"mol_{h[:16]}\"\n",
    "\n",
    "def prevalence(y: pd.Series) -> float:\n",
    "    \"\"\"Prevalence among VALID (exclude NaN / -1) as fraction in [0,1].\"\"\"\n",
    "    valid = y[(~y.isna()) & (y != -1)]\n",
    "    if len(valid) == 0:\n",
    "        return np.nan\n",
    "    return float((valid == 1).sum() / len(valid))\n",
    "\n",
    "# -----------------------------\n",
    "# Load dataset from preferred paths\n",
    "# -----------------------------\n",
    "df = None\n",
    "loaded_path = None\n",
    "for p in PREFERRED_PATHS:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            df_try = pd.read_csv(p)\n",
    "            df = df_try\n",
    "            loaded_path = p\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"WARNING: Failed to read {p}: {e}\")\n",
    "\n",
    "if df is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not find any dataset. Looked for:\\n\"\n",
    "        + \"\\n\".join(str(p.resolve()) for p in PREFERRED_PATHS)\n",
    "    )\n",
    "\n",
    "print(f\"Loaded dataset from: {loaded_path.resolve()}\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns ({len(df.columns)}): {list(df.columns)[:12]}{' ...' if len(df.columns)>12 else ''}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Basic column checks\n",
    "# -----------------------------\n",
    "cols = set(df.columns)\n",
    "missing_labels = sorted(list(REQUIRED_LABELS - cols))\n",
    "if missing_labels:\n",
    "    raise ValueError(\n",
    "        \"Dataset is missing required Tox21 label columns:\\n\"\n",
    "        + \", \".join(missing_labels)\n",
    "    )\n",
    "\n",
    "if 'smiles' not in cols:\n",
    "    raise ValueError(\"Dataset must contain a 'smiles' column.\")\n",
    "\n",
    "has_mol_id = 'mol_id' in cols\n",
    "\n",
    "# -----------------------------\n",
    "# Canonicalize SMILES, build mol_id if needed\n",
    "# -----------------------------\n",
    "print(\"\\nCanonicalizing SMILES (this may take a moment)...\")\n",
    "df['smiles_canonical'] = df['smiles'].astype(str).map(canonicalize_smiles)\n",
    "n_invalid = df['smiles_canonical'].isna().sum()\n",
    "if n_invalid > 0:\n",
    "    print(f\"WARNING: {n_invalid} rows have invalid/empty SMILES; they will be excluded from training.\")\n",
    "\n",
    "# Drop rows with invalid SMILES now (keep a copy of original count)\n",
    "orig_n = len(df)\n",
    "df = df[~df['smiles_canonical'].isna()].copy()\n",
    "print(f\"Kept {len(df)} / {orig_n} rows after removing invalid SMILES.\")\n",
    "\n",
    "if not has_mol_id:\n",
    "    print(\"mol_id not found — generating deterministic mol_id from canonical SMILES.\")\n",
    "    df['mol_id'] = df['smiles_canonical'].map(make_mol_id_from_smiles)\n",
    "\n",
    "# -----------------------------\n",
    "# Verify/attach split\n",
    "# -----------------------------\n",
    "has_split_col = 'split' in df.columns\n",
    "if not has_split_col:\n",
    "    print(\"No 'split' column found in dataset. Looking for split map at:\", SPLIT_MAP_PATH)\n",
    "    if SPLIT_MAP_PATH.exists():\n",
    "        smap = pd.read_csv(SPLIT_MAP_PATH)\n",
    "        if not set(['mol_id','split']).issubset(smap.columns):\n",
    "            raise ValueError(\"split_map.csv must have columns: mol_id, split\")\n",
    "        # merge (inner) to avoid introducing new rows\n",
    "        before = len(df)\n",
    "        df = df.merge(smap[['mol_id','split']], on='mol_id', how='left')\n",
    "        missing_split = df['split'].isna().sum()\n",
    "        print(f\"Merged split map. Missing split for {missing_split} rows out of {len(df)}.\")\n",
    "        if missing_split > 0:\n",
    "            raise ValueError(\n",
    "                \"Some rows still missing split after merge. Ensure split_map covers all mol_id.\"\n",
    "            )\n",
    "        has_split_col = True\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            \"No 'split' column in dataset and split_map.csv was not found.\\n\"\n",
    "            \"Please place a split map at 'tox21_dualenc_v1/data/raw/split_map.csv' with columns: mol_id,split\\n\"\n",
    "            \"so we retain your existing V5 split.\"\n",
    "        )\n",
    "\n",
    "# Normalize split values\n",
    "df['split'] = df['split'].astype(str).str.lower().map({'train':'train','val':'val','valid':'val','test':'test'})\n",
    "if not set(df['split'].unique()).issubset({'train','val','test'}):\n",
    "    raise ValueError(\"Split column must contain only {'train','val','test'} (or 'valid' for val).\")\n",
    "\n",
    "# -----------------------------\n",
    "# Duplication checks\n",
    "# -----------------------------\n",
    "dup_mol = df.duplicated('mol_id').sum()\n",
    "dup_smi = df.duplicated('smiles_canonical').sum()\n",
    "print(f\"\\nDuplicates — mol_id: {dup_mol}, smiles_canonical: {dup_smi}\")\n",
    "if dup_mol > 0:\n",
    "    print(\"NOTE: Duplicate mol_id detected. Will keep first occurrence during training (handled later).\")\n",
    "if dup_smi > 0:\n",
    "    print(\"NOTE: Duplicate canonical SMILES detected. Consider de-dup for robustness (we will group later if needed).\")\n",
    "\n",
    "# -----------------------------\n",
    "# Split sizes & label prevalence\n",
    "# -----------------------------\n",
    "split_counts = df['split'].value_counts().to_dict()\n",
    "print(\"\\nSplit sizes:\", split_counts)\n",
    "\n",
    "label_stats = []\n",
    "for lab in sorted(REQUIRED_LABELS):\n",
    "    y = df[lab]\n",
    "    # Treat -1 as missing (mask)\n",
    "    valid_mask = (~y.isna()) & (y != -1)\n",
    "    pos = int((y[valid_mask] == 1).sum())\n",
    "    neg = int((y[valid_mask] == 0).sum())\n",
    "    prev = prevalence(y)\n",
    "    label_stats.append((lab, int(valid_mask.sum()), pos, neg, round(prev, 4) if prev==prev else None))\n",
    "\n",
    "print(\"\\nPer-label validity / pos / neg / prevalence (valid only):\")\n",
    "for lab, n_valid, pos, neg, prev in label_stats:\n",
    "    print(f\"{lab:12s}  valid={n_valid:4d}  pos={pos:4d}  neg={neg:4d}  prev={prev}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Quick SMILES parse spot-check (sample up to 1000)\n",
    "# -----------------------------\n",
    "sample_n = min(1000, len(df))\n",
    "samp = df.sample(sample_n, random_state=SEED)\n",
    "bad = sum(Chem.MolFromSmiles(s) is None for s in samp['smiles_canonical'])\n",
    "print(f\"\\nSMILES spot-check: {sample_n} sampled, invalid={bad}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Final minimal expectations\n",
    "# -----------------------------\n",
    "assert all(lab in df.columns for lab in REQUIRED_LABELS), \"Missing labels after processing.\"\n",
    "assert 'mol_id' in df.columns, \"mol_id must exist after processing.\"\n",
    "assert 'split' in df.columns, \"split must exist after processing.\"\n",
    "assert df['split'].isin(['train','val','test']).all(), \"Unexpected split values.\"\n",
    "\n",
    "print(\"\\nOK: Data looks consistent. Ready to move on to descriptor scaling & caches (train-only fit).\")\n",
    "\n",
    "# Keep in memory for the next cell\n",
    "DF_V6 = df  # global notebook variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ac1afd",
   "metadata": {},
   "source": [
    "## 3: Descriptor column selection, train-only imputer+scaler fit, sanity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91445a5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected descriptor columns: 256\n",
      "Sample descriptor cols: ['MolWt', 'TPSA', 'SlogP', 'HBD', 'HBA', 'NumRotBonds', 'RingCount', 'FractionCSP3', 'AromaticProportion', 'QED']\n",
      "\n",
      "Split row counts -> train:6265  val:783  test:783\n",
      "\n",
      "Train-only standardization: |mean|_avg=0.0000, |std-1|_avg=0.0000\n",
      "TRAIN: finite ratio = 1.000000\n",
      "VAL: finite ratio = 1.000000\n",
      "TEST: finite ratio = 1.000000\n",
      "\n",
      "Train missingness (pre-impute): columns with any NaN = 0, total NaNs = 0\n",
      "\n",
      "Saved imputer -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\rdkit_desc_imputer_v6.pkl\n",
      "Saved scaler  -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\rdkit_desc_scaler_v6.pkl\n",
      "Saved columns -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\rdkit_desc_columns_v6.txt (one name per line)\n",
      "\n",
      "OK: Descriptor pipeline primed (train-only fit). Next we’ll stage the model components and the per-label dataloaders.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "assert 'DF_V6' in globals(), \"DF_V6 not found. Please run Cell 2 first.\"\n",
    "\n",
    "df = DF_V6.copy()\n",
    "\n",
    "# -----------------------------\n",
    "# Define core / excluded columns\n",
    "# -----------------------------\n",
    "CORE_NON_DESC = {\n",
    "    'row_id', 'split', 'smiles', 'smiles_canonical', 'mol_id'\n",
    "}\n",
    "LABELS_SET = {\n",
    "    'NR-AR','NR-AR-LBD','NR-AhR','NR-Aromatase',\n",
    "    'NR-ER','NR-ER-LBD','NR-PPAR-gamma',\n",
    "    'SR-ARE','SR-ATAD5','SR-HSE','SR-MMP','SR-p53'\n",
    "}\n",
    "EXCLUDE = CORE_NON_DESC | LABELS_SET\n",
    "\n",
    "# -----------------------------\n",
    "# Detect numeric descriptor columns\n",
    "# -----------------------------\n",
    "numeric_cols = [c for c in df.columns if c not in EXCLUDE and pd.api.types.is_numeric_dtype(df[c])]\n",
    "non_numeric_ignored = [c for c in df.columns if c not in EXCLUDE and not pd.api.types.is_numeric_dtype(df[c])]\n",
    "\n",
    "DESC_COLS = numeric_cols\n",
    "print(f\"Detected descriptor columns: {len(DESC_COLS)}\")\n",
    "print(\"Sample descriptor cols:\", DESC_COLS[:10])\n",
    "if non_numeric_ignored:\n",
    "    print(f\"NOTE: Ignored {len(non_numeric_ignored)} non-numeric columns (not descriptors).\")\n",
    "\n",
    "# -----------------------------\n",
    "# Split indices\n",
    "# -----------------------------\n",
    "idx_train = df.index[df['split'] == 'train']\n",
    "idx_val   = df.index[df['split'] == 'val']\n",
    "idx_test  = df.index[df['split'] == 'test']\n",
    "print(f\"\\nSplit row counts -> train:{len(idx_train)}  val:{len(idx_val)}  test:{len(idx_test)}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train-only fit (imputer + scaler)\n",
    "# -----------------------------\n",
    "X_train = df.loc[idx_train, DESC_COLS].values\n",
    "X_val   = df.loc[idx_val,   DESC_COLS].values\n",
    "X_test  = df.loc[idx_test,  DESC_COLS].values\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "Xtr_imp = imputer.fit_transform(X_train)\n",
    "Xv_imp  = imputer.transform(X_val)\n",
    "Xte_imp = imputer.transform(X_test)\n",
    "\n",
    "# Sanity: no NaNs post-impute\n",
    "assert np.isfinite(Xtr_imp).all(), \"Non-finite values remain in TRAIN after imputation.\"\n",
    "assert np.isfinite(Xv_imp).all(),  \"Non-finite values remain in VAL after imputation.\"\n",
    "assert np.isfinite(Xte_imp).all(), \"Non-finite values remain in TEST after imputation.\"\n",
    "\n",
    "scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "Xtr_std = scaler.fit_transform(Xtr_imp)\n",
    "Xv_std  = scaler.transform(Xv_imp)\n",
    "Xte_std = scaler.transform(Xte_imp)\n",
    "\n",
    "# Sanity: TRAIN mean≈0, std≈1\n",
    "train_means = Xtr_std.mean(axis=0)\n",
    "train_stds  = Xtr_std.std(axis=0)\n",
    "mean_abs = float(np.abs(train_means).mean())\n",
    "std_dev  = float(np.abs(train_stds - 1.0).mean())\n",
    "print(f\"\\nTrain-only standardization: |mean|_avg={mean_abs:.4f}, |std-1|_avg={std_dev:.4f}\")\n",
    "\n",
    "# Sanity: no NaNs/inf post-scale\n",
    "for name, X in [('TRAIN', Xtr_std), ('VAL', Xv_std), ('TEST', Xte_std)]:\n",
    "    finite_ratio = np.isfinite(X).mean()\n",
    "    print(f\"{name}: finite ratio = {finite_ratio:.6f}\")\n",
    "    assert np.isfinite(X).all(), f\"{name} has non-finite after scaling.\"\n",
    "\n",
    "# Quick missingness report BEFORE impute (train only)\n",
    "missing_before = pd.Series(np.isnan(X_train).sum(axis=0), index=DESC_COLS)\n",
    "n_cols_any_missing = int((missing_before > 0).sum())\n",
    "n_missing_total = int(missing_before.sum())\n",
    "print(f\"\\nTrain missingness (pre-impute): columns with any NaN = {n_cols_any_missing}, total NaNs = {n_missing_total}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Save minimal, necessary artefacts\n",
    "# -----------------------------\n",
    "imputer_path = RES_DIR / \"rdkit_desc_imputer_v6.pkl\"\n",
    "scaler_path  = RES_DIR / \"rdkit_desc_scaler_v6.pkl\"\n",
    "cols_path    = RES_DIR / \"rdkit_desc_columns_v6.txt\"\n",
    "\n",
    "joblib.dump(imputer, imputer_path)\n",
    "joblib.dump(scaler, scaler_path)\n",
    "with open(cols_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    for c in DESC_COLS:\n",
    "        f.write(f\"{c}\\n\")\n",
    "\n",
    "print(f\"\\nSaved imputer -> {imputer_path.resolve()}\")\n",
    "print(f\"Saved scaler  -> {scaler_path.resolve()}\")\n",
    "print(f\"Saved columns -> {cols_path.resolve()} (one name per line)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Keep in memory for later cells\n",
    "# -----------------------------\n",
    "IMPUTER_V6 = imputer\n",
    "SCALER_V6  = scaler\n",
    "DESC_COLS_V6 = DESC_COLS\n",
    "\n",
    "print(\"\\nOK: Descriptor pipeline primed (train-only fit). Next we’ll stage the model components and the per-label dataloaders.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eb30ea",
   "metadata": {},
   "source": [
    "## 4: Per-label prevalence table + positive-aware sampler (+ sanity checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5630de3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Per-label prevalence and hyperparameters:\n",
      "        label  prevalence bucket  gamma_neg  gamma_pos  margin  desc_dropout\n",
      "        NR-AR      0.0425    mid        3.0        0.0   0.050          0.55\n",
      "    NR-AR-LBD      0.0351   rare        4.0        0.0   0.075          0.60\n",
      "       NR-AhR      0.1173 common        2.0        0.0   0.050          0.50\n",
      " NR-Aromatase      0.0515    mid        3.0        0.0   0.050          0.55\n",
      "        NR-ER      0.1280 common        2.0        0.0   0.050          0.50\n",
      "    NR-ER-LBD      0.0503    mid        3.0        0.0   0.050          0.55\n",
      "NR-PPAR-gamma      0.0288   rare        4.0        0.0   0.075          0.60\n",
      "       SR-ARE      0.1615 common        2.0        0.0   0.050          0.50\n",
      "     SR-ATAD5      0.0373   rare        4.0        0.0   0.075          0.60\n",
      "       SR-HSE      0.0575    mid        3.0        0.0   0.050          0.55\n",
      "       SR-MMP      0.1580 common        2.0        0.0   0.050          0.50\n",
      "       SR-p53      0.0624    mid        3.0        0.0   0.050          0.55\n",
      "\n",
      "Saved per-label hparams -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\per_label_hparams_v6.json\n",
      "\n",
      "[NR-AR] split sizes (valid only): train=5816, val=723, test=726\n",
      "Sampler check (3 batches):\n",
      "  batch size=64  pos=16  neg=48\n",
      "  batch size=64  pos=16  neg=48\n",
      "  batch size=64  pos=16  neg=48\n",
      "\n",
      "Per-label valid split sizes and buckets:\n",
      "        label  n_train  n_val  n_test  prevalence bucket\n",
      "        NR-AR     5816    723     726      0.0425    mid\n",
      "    NR-AR-LBD     5443    660     655      0.0351   rare\n",
      "       NR-AhR     5256    645     648      0.1173 common\n",
      " NR-Aromatase     4699    560     562      0.0515    mid\n",
      "        NR-ER     5020    586     587      0.1280 common\n",
      "    NR-ER-LBD     5582    680     693      0.0503    mid\n",
      "NR-PPAR-gamma     5217    611     622      0.0288   rare\n",
      "       SR-ARE     4751    538     543      0.1615 common\n",
      "     SR-ATAD5     5684    690     698      0.0373   rare\n",
      "       SR-HSE     5219    618     630      0.0575    mid\n",
      "       SR-MMP     4704    562     544      0.1580 common\n",
      "       SR-p53     5430    666     678      0.0624    mid\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from pathlib import Path\n",
    "\n",
    "assert 'DF_V6' in globals(), \"DF_V6 missing (run Cell 2).\"\n",
    "assert 'DESC_COLS_V6' in globals(), \"DESC_COLS_V6 missing (run Cell 3).\"\n",
    "\n",
    "df = DF_V6.copy()\n",
    "LABELS = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Per-label prevalence + derived hyperparams\n",
    "# -----------------------------\n",
    "def compute_prevalence(series: pd.Series) -> float:\n",
    "    valid = series[(~series.isna()) & (series != -1)]\n",
    "    return float((valid == 1).mean()) if len(valid) > 0 else float('nan')\n",
    "\n",
    "prev_map = {lab: compute_prevalence(df[lab]) for lab in LABELS}\n",
    "\n",
    "def label_bucket(prev: float) -> str:\n",
    "    if math.isnan(prev): return \"unknown\"\n",
    "    if prev <= 0.04: return \"rare\"\n",
    "    if prev <  0.10: return \"mid\"\n",
    "    return \"common\"\n",
    "\n",
    "def hyperparams_for(prev: float) -> Dict:\n",
    "    bucket = label_bucket(prev)\n",
    "    if bucket == \"rare\":\n",
    "        return {\"gamma_neg\": 4.0, \"gamma_pos\": 0.0, \"margin\": 0.075, \"desc_dropout\": 0.60}\n",
    "    if bucket == \"mid\":\n",
    "        return {\"gamma_neg\": 3.0, \"gamma_pos\": 0.0, \"margin\": 0.05,  \"desc_dropout\": 0.55}\n",
    "    if bucket == \"common\":\n",
    "        return {\"gamma_neg\": 2.0, \"gamma_pos\": 0.0, \"margin\": 0.05,  \"desc_dropout\": 0.50}\n",
    "    # fallback\n",
    "    return {\"gamma_neg\": 3.0, \"gamma_pos\": 0.0, \"margin\": 0.05, \"desc_dropout\": 0.55}\n",
    "\n",
    "hparams_table = []\n",
    "for lab in LABELS:\n",
    "    prev = prev_map[lab]\n",
    "    hp = hyperparams_for(prev)\n",
    "    hparams_table.append({\n",
    "        \"label\": lab,\n",
    "        \"prevalence\": round(prev, 4),\n",
    "        \"bucket\": label_bucket(prev),\n",
    "        **hp\n",
    "    })\n",
    "\n",
    "hp_df = pd.DataFrame(hparams_table).sort_values(\"label\").reset_index(drop=True)\n",
    "print(\"Per-label prevalence and hyperparameters:\")\n",
    "print(hp_df.to_string(index=False))\n",
    "\n",
    "# Save for reproducibility\n",
    "hp_path = RES_DIR / \"per_label_hparams_v6.json\"\n",
    "with open(hp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(hparams_table, f, indent=2)\n",
    "print(f\"\\nSaved per-label hparams -> {hp_path.resolve()}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Positive-aware batch sampler (with hard-negative hook)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class SamplerConfig:\n",
    "    label_name: str\n",
    "    batch_size: int = 64\n",
    "    pos_neg_ratio: Tuple[int,int] = (1,3)  # K_pos : K_neg\n",
    "    min_pos_per_batch: int = 8\n",
    "    hard_neg_frac: float = 0.33   # fraction of negatives drawn from hard-neg pool\n",
    "    seed: int = 42\n",
    "\n",
    "class PositiveAwareBatchSampler:\n",
    "    \"\"\"\n",
    "    Generates index batches with ~pos:neg ratio and min positives enforced.\n",
    "    Optional hard-negative pool to sample a fraction of negs.\n",
    "    \"\"\"\n",
    "    def __init__(self, y: np.ndarray, indices: np.ndarray, cfg: SamplerConfig,\n",
    "                 hard_neg_indices: Optional[np.ndarray] = None):\n",
    "        assert y.shape[0] == indices.shape[0], \"y and indices must align.\"\n",
    "        self.y = y.astype(int)\n",
    "        self.idx = indices.astype(int)\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "\n",
    "        # valid mask (exclude NaN/-1)\n",
    "        self.valid_mask = (self.y == 0) | (self.y == 1)\n",
    "        if not self.valid_mask.any():\n",
    "            raise ValueError(\"No valid labels for sampler.\")\n",
    "\n",
    "        self.pos_idx = self.idx[(self.y == 1) & self.valid_mask]\n",
    "        self.neg_idx = self.idx[(self.y == 0) & self.valid_mask]\n",
    "\n",
    "        self.hard_neg_idx = None\n",
    "        if hard_neg_indices is not None and len(hard_neg_indices) > 0:\n",
    "            # Keep only those present in our neg set\n",
    "            hard_set = set(hard_neg_indices.tolist())\n",
    "            self.hard_neg_idx = np.array([i for i in self.neg_idx if i in hard_set], dtype=int)\n",
    "            if len(self.hard_neg_idx) == 0:\n",
    "                self.hard_neg_idx = None\n",
    "\n",
    "        if len(self.pos_idx) == 0:\n",
    "            raise ValueError(f\"No positives available for label={cfg.label_name}.\")\n",
    "\n",
    "        # derive counts\n",
    "        self.k_pos, self.k_neg = cfg.pos_neg_ratio\n",
    "        self.bs = cfg.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        # compute desired counts\n",
    "        # start from ratio but enforce min positives\n",
    "        n_pos = max(self.cfg.min_pos_per_batch, int(round(self.bs * (self.k_pos / (self.k_pos + self.k_neg)))))\n",
    "        n_pos = min(n_pos, self.bs-1)  # leave room for at least 1 negative\n",
    "        n_neg = self.bs - n_pos\n",
    "\n",
    "        pos_sel = self.rng.choice(self.pos_idx, size=n_pos, replace=(n_pos > len(self.pos_idx)))\n",
    "\n",
    "        # Negatives: mix hard-negatives and regular negatives\n",
    "        if self.hard_neg_idx is not None and self.cfg.hard_neg_frac > 0:\n",
    "            n_hard = int(round(n_neg * self.cfg.hard_neg_frac))\n",
    "            n_easy = n_neg - n_hard\n",
    "            hard_sel = self.rng.choice(self.hard_neg_idx, size=n_hard, replace=(n_hard > len(self.hard_neg_idx)))\n",
    "            # draw easy negatives from remaining pool (avoid duplicates not necessary here)\n",
    "            easy_sel = self.rng.choice(self.neg_idx, size=n_easy, replace=(n_easy > len(self.neg_idx)))\n",
    "            neg_sel = np.concatenate([hard_sel, easy_sel])\n",
    "        else:\n",
    "            neg_sel = self.rng.choice(self.neg_idx, size=n_neg, replace=(n_neg > len(self.neg_idx)))\n",
    "\n",
    "        batch = np.concatenate([pos_sel, neg_sel])\n",
    "        self.rng.shuffle(batch)\n",
    "        return batch\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Minimal dataset scaffold (no tokeniser/GNN yet)\n",
    "# -----------------------------\n",
    "class PerLabelDataset:\n",
    "    \"\"\"\n",
    "    Light dataset wrapping df rows for a single label.\n",
    "    Returns dict with mol_id, smiles, descriptors (scaled later), and y.\n",
    "    \"\"\"\n",
    "    def __init__(self, df_part: pd.DataFrame, label: str, desc_cols: List[str]):\n",
    "        self.df = df_part.reset_index(drop=True)\n",
    "        self.label = label\n",
    "        self.desc_cols = desc_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict:\n",
    "        row = self.df.iloc[i]\n",
    "        y = row[self.label]\n",
    "        y = None if (pd.isna(y) or y == -1) else int(y)\n",
    "        return {\n",
    "            \"mol_id\": row[\"mol_id\"],\n",
    "            \"smiles\": row[\"smiles_canonical\"],\n",
    "            \"descriptors\": row[self.desc_cols].to_numpy(dtype=np.float32, copy=True),\n",
    "            \"y\": y\n",
    "        }\n",
    "\n",
    "def build_per_label_splits(label: str) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    valid_mask = (~df[label].isna()) & (df[label] != -1)\n",
    "    d = df[valid_mask]\n",
    "    return d[d['split']==\"train\"], d[d['split']==\"val\"], d[d['split']==\"test\"]\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Sanity run for one label (NR-AR) and a report for all labels\n",
    "# -----------------------------\n",
    "demo_label = \"NR-AR\"\n",
    "tr, va, te = build_per_label_splits(demo_label)\n",
    "\n",
    "print(f\"\\n[{demo_label}] split sizes (valid only): train={len(tr)}, val={len(va)}, test={len(te)}\")\n",
    "\n",
    "# Build y arrays aligned with indices\n",
    "y_train = tr[demo_label].to_numpy()\n",
    "idx_train = tr.index.to_numpy()\n",
    "\n",
    "# Instantiate sampler (no hard negs yet)\n",
    "sampler_cfg = SamplerConfig(label_name=demo_label, batch_size=64, pos_neg_ratio=(1,3), min_pos_per_batch=8, hard_neg_frac=0.33, seed=42)\n",
    "sampler = PositiveAwareBatchSampler(y=y_train, indices=idx_train, cfg=sampler_cfg, hard_neg_indices=None)\n",
    "\n",
    "# Draw a few batches to verify composition\n",
    "def batch_comp(batch_idx: np.ndarray, label: str) -> Tuple[int,int]:\n",
    "    lab_vals = df.loc[batch_idx, label].to_numpy()\n",
    "    pos = int((lab_vals == 1).sum())\n",
    "    neg = int((lab_vals == 0).sum())\n",
    "    return pos, neg\n",
    "\n",
    "print(\"Sampler check (3 batches):\")\n",
    "for _ in range(3):\n",
    "    b = next(sampler)\n",
    "    p, n = batch_comp(b, demo_label)\n",
    "    print(f\"  batch size={len(b):2d}  pos={p:2d}  neg={n:2d}\")\n",
    "\n",
    "# Quick per-label validity count and bucket\n",
    "summary_rows = []\n",
    "for lab in LABELS:\n",
    "    m = (~df[lab].isna()) & (df[lab] != -1)\n",
    "    n_tr = int(((df['split']==\"train\") & m).sum())\n",
    "    n_va = int(((df['split']==\"val\")   & m).sum())\n",
    "    n_te = int(((df['split']==\"test\")  & m).sum())\n",
    "    prev = prev_map[lab]\n",
    "    summary_rows.append([lab, n_tr, n_va, n_te, round(prev,4), label_bucket(prev)])\n",
    "\n",
    "sum_df = pd.DataFrame(summary_rows, columns=[\"label\",\"n_train\",\"n_val\",\"n_test\",\"prevalence\",\"bucket\"])\n",
    "print(\"\\nPer-label valid split sizes and buckets:\")\n",
    "print(sum_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d25c7f",
   "metadata": {},
   "source": [
    "## 5: Tokenizer, collate, and per-label DataLoaders (with sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9a9427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded tokenizer: DeepChem/ChemBERTa-77M-MLM\n",
      "[NR-AR] dataset sizes -> train:5816  val:723  test:726\n",
      "Train batch keys: ['input_ids', 'attention_mask', 'desc', 'y', 'mol_id', 'smiles']\n",
      "input_ids shape: (32, 153)\n",
      "attention_mask shape: (32, 153)\n",
      "desc shape: (32, 256)\n",
      "y shape: (32,)\n",
      "y counts: {-1: 0, 0: 24, 1: 8}\n",
      "Example smiles (augmented if train): ['Nc1ccc2cc(S(=O)(=O)[O-])cc(O)c2c1/N=N/c1ccccc1C(F)(F)F', 'CCC(C)C(C(=O)OCC[N+](C)(CC)CC)c1ccccc1', 'Cc1cc(-c2ccc(N)c(C)c2)ccc1N']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional, Dict, Tuple\n",
    "from transformers import AutoTokenizer\n",
    "from rdkit import Chem\n",
    "\n",
    "assert 'DF_V6' in globals(), \"DF_V6 missing (run Cell 2).\"\n",
    "assert 'DESC_COLS_V6' in globals(), \"DESC_COLS_V6 missing (run Cell 3).\"\n",
    "assert 'IMPUTER_V6' in globals() and 'SCALER_V6' in globals(), \"Imputer/Scaler missing (run Cell 3).\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = 256\n",
    "RANDOM_SMILES_TRAIN = True  # set False to disable augmentation\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load tokenizer (with fallback)\n",
    "# -----------------------------\n",
    "tok_name_candidates = [\n",
    "    \"DeepChem/ChemBERTa-77M-MLM\",\n",
    "    \"seyonec/ChemBERTa-zinc-base-v1\"\n",
    "]\n",
    "tokenizer = None\n",
    "tok_loaded_name = None\n",
    "for name in tok_name_candidates:\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(name)\n",
    "        tok_loaded_name = name\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: Failed to load tokenizer '{name}': {e}\")\n",
    "if tokenizer is None:\n",
    "    raise RuntimeError(\"Could not load a ChemBERTa tokenizer. Ensure internet/model cache is available.\")\n",
    "print(f\"Loaded tokenizer: {tok_loaded_name}\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Random SMILES utility (per molecule)\n",
    "# -----------------------------\n",
    "def smiles_randomize(smi: str) -> str:\n",
    "    \"\"\"Return a randomized SMILES using RDKit; fallback to canonical if randomization fails.\"\"\"\n",
    "    try:\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None:\n",
    "            return smi\n",
    "        return Chem.MolToSmiles(mol, canonical=False, doRandom=True)\n",
    "    except Exception:\n",
    "        return smi\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Dataset (per-label)\n",
    "# -----------------------------\n",
    "class PerLabelDataset:\n",
    "    \"\"\"\n",
    "    Light dataset wrapping df rows for a single label.\n",
    "    Returns dict with mol_id, smiles, descriptors (raw), and y.\n",
    "    \"\"\"\n",
    "    def __init__(self, df_part, label: str, desc_cols: List[str]):\n",
    "        self.df = df_part.reset_index(drop=True)\n",
    "        self.label = label\n",
    "        self.desc_cols = desc_cols\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i: int) -> Dict:\n",
    "        row = self.df.iloc[i]\n",
    "        y = row[self.label]\n",
    "        y = None if (pd.isna(y) or y == -1) else int(y)\n",
    "        return {\n",
    "            \"mol_id\": row[\"mol_id\"],\n",
    "            \"smiles\": row[\"smiles_canonical\"],\n",
    "            \"descriptors\": row[self.desc_cols].to_numpy(dtype=np.float32, copy=True),\n",
    "            \"y\": y\n",
    "        }\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Positive-aware batch sampler (with hard-neg hook)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class SamplerConfig:\n",
    "    label_name: str\n",
    "    batch_size: int = 64\n",
    "    pos_neg_ratio: Tuple[int,int] = (1,3)  # K_pos : K_neg\n",
    "    min_pos_per_batch: int = 8\n",
    "    hard_neg_frac: float = 0.33\n",
    "    seed: int = 42\n",
    "\n",
    "class PositiveAwareBatchSampler:\n",
    "    \"\"\"\n",
    "    Generates index batches with ~pos:neg ratio and min positives enforced.\n",
    "    Optional hard-negative pool to sample a fraction of negs.\n",
    "    \"\"\"\n",
    "    def __init__(self, y: np.ndarray, indices: np.ndarray, cfg: SamplerConfig,\n",
    "                 hard_neg_indices: Optional[np.ndarray] = None):\n",
    "        assert y.shape[0] == indices.shape[0], \"y and indices must align.\"\n",
    "        self.y = y.astype(int)\n",
    "        self.idx = indices.astype(int)\n",
    "        self.cfg = cfg\n",
    "        self.rng = np.random.default_rng(cfg.seed)\n",
    "\n",
    "        self.valid_mask = (self.y == 0) | (self.y == 1)\n",
    "        if not self.valid_mask.any():\n",
    "            raise ValueError(\"No valid labels for sampler.\")\n",
    "\n",
    "        self.pos_idx = self.idx[(self.y == 1) & self.valid_mask]\n",
    "        self.neg_idx = self.idx[(self.y == 0) & self.valid_mask]\n",
    "\n",
    "        self.hard_neg_idx = None\n",
    "        if hard_neg_indices is not None and len(hard_neg_indices) > 0:\n",
    "            hard_set = set(hard_neg_indices.tolist())\n",
    "            self.hard_neg_idx = np.array([i for i in self.neg_idx if i in hard_set], dtype=int)\n",
    "            if len(self.hard_neg_idx) == 0:\n",
    "                self.hard_neg_idx = None\n",
    "\n",
    "        if len(self.pos_idx) == 0:\n",
    "            raise ValueError(f\"No positives available for label={cfg.label_name}.\")\n",
    "\n",
    "        self.k_pos, self.k_neg = cfg.pos_neg_ratio\n",
    "        self.bs = cfg.batch_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        n_pos = max(self.cfg.min_pos_per_batch, int(round(self.bs * (self.k_pos / (self.k_pos + self.k_neg)))))\n",
    "        n_pos = min(n_pos, self.bs - 1)  # reserve at least 1 for neg\n",
    "        n_neg = self.bs - n_pos\n",
    "\n",
    "        pos_sel = self.rng.choice(self.pos_idx, size=n_pos, replace=(n_pos > len(self.pos_idx)))\n",
    "\n",
    "        if self.hard_neg_idx is not None and self.cfg.hard_neg_frac > 0:\n",
    "            n_hard = int(round(n_neg * self.cfg.hard_neg_frac))\n",
    "            n_easy = n_neg - n_hard\n",
    "            hard_sel = self.rng.choice(self.hard_neg_idx, size=n_hard, replace=(n_hard > len(self.hard_neg_idx)))\n",
    "            easy_sel = self.rng.choice(self.neg_idx, size=n_easy, replace=(n_easy > len(self.neg_idx)))\n",
    "            neg_sel = np.concatenate([hard_sel, easy_sel])\n",
    "        else:\n",
    "            neg_sel = self.rng.choice(self.neg_idx, size=n_neg, replace=(n_neg > len(self.neg_idx)))\n",
    "\n",
    "        batch = np.concatenate([pos_sel, neg_sel])\n",
    "        self.rng.shuffle(batch)\n",
    "        return batch\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Collate function (tokenize + impute/scale descriptors)\n",
    "# -----------------------------\n",
    "def collate_fn(batch, train_mode: bool):\n",
    "    \"\"\"\n",
    "    Returns dict with:\n",
    "      - input_ids, attention_mask (tokenized)\n",
    "      - desc (scaled)\n",
    "      - y (tensor with -1 for missing)\n",
    "      - mol_id, smiles (lists)\n",
    "    \"\"\"\n",
    "    mol_ids = [b[\"mol_id\"] for b in batch]\n",
    "    smiles_list = [b[\"smiles\"] for b in batch]\n",
    "    ys = [(-1 if b[\"y\"] is None else b[\"y\"]) for b in batch]\n",
    "    desc_mat = np.stack([b[\"descriptors\"] for b in batch], axis=0)  # (B, D)\n",
    "\n",
    "    if train_mode and RANDOM_SMILES_TRAIN:\n",
    "        smiles_tok = [smiles_randomize(s) for s in smiles_list]\n",
    "    else:\n",
    "        smiles_tok = smiles_list\n",
    "\n",
    "    tok = tokenizer(\n",
    "        smiles_tok,\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=MAX_LEN,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    X_imp = IMPUTER_V6.transform(desc_mat)\n",
    "    X_std = SCALER_V6.transform(X_imp)\n",
    "    X_std = torch.from_numpy(X_std).float()\n",
    "\n",
    "    y_tensor = torch.tensor(ys, dtype=torch.long)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": tok[\"input_ids\"],\n",
    "        \"attention_mask\": tok[\"attention_mask\"],\n",
    "        \"desc\": X_std,\n",
    "        \"y\": y_tensor,\n",
    "        \"mol_id\": mol_ids,\n",
    "        \"smiles\": smiles_list,\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Build DataLoaders for a given label (HOTFIX: dataset-local indices)\n",
    "# -----------------------------\n",
    "def build_dataloaders_for_label(label: str,\n",
    "                                batch_size: int = 64,\n",
    "                                pos_neg_ratio=(1,3),\n",
    "                                min_pos_per_batch: int = 8,\n",
    "                                hard_neg_indices=None,\n",
    "                                seed: int = 42):\n",
    "    # Valid subset for label\n",
    "    valid_mask = (~DF_V6[label].isna()) & (DF_V6[label] != -1)\n",
    "    d_all = DF_V6[valid_mask]\n",
    "    d_tr = d_all[d_all['split']==\"train\"].copy()\n",
    "    d_va = d_all[d_all['split']==\"val\"].copy()\n",
    "    d_te = d_all[d_all['split']==\"test\"].copy()\n",
    "\n",
    "    ds_tr = PerLabelDataset(d_tr, label, DESC_COLS_V6)\n",
    "    ds_va = PerLabelDataset(d_va, label, DESC_COLS_V6)\n",
    "    ds_te = PerLabelDataset(d_te, label, DESC_COLS_V6)\n",
    "\n",
    "    # Use dataset-local indices for sampler\n",
    "    y_tr = ds_tr.df[label].to_numpy()\n",
    "    idx_tr = np.arange(len(ds_tr), dtype=int)\n",
    "\n",
    "    s_cfg = SamplerConfig(\n",
    "        label_name=label,\n",
    "        batch_size=batch_size,\n",
    "        pos_neg_ratio=pos_neg_ratio,\n",
    "        min_pos_per_batch=min_pos_per_batch,\n",
    "        hard_neg_frac=0.33,\n",
    "        seed=seed\n",
    "    )\n",
    "    sampler = PositiveAwareBatchSampler(y=y_tr, indices=idx_tr, cfg=s_cfg, hard_neg_indices=hard_neg_indices)\n",
    "\n",
    "    dl_tr = DataLoader(\n",
    "        ds_tr,\n",
    "        batch_sampler=sampler,\n",
    "        num_workers=0,\n",
    "        collate_fn=lambda b: collate_fn(b, train_mode=True),\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    dl_va = DataLoader(\n",
    "        ds_va,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=lambda b: collate_fn(b, train_mode=False),\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    dl_te = DataLoader(\n",
    "        ds_te,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=0,\n",
    "        collate_fn=lambda b: collate_fn(b, train_mode=False),\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "    return dl_tr, dl_va, dl_te, len(ds_tr), len(ds_va), len(ds_te)\n",
    "\n",
    "# -----------------------------\n",
    "# 7) Sanity check on one label\n",
    "# -----------------------------\n",
    "demo_label = \"NR-AR\"\n",
    "dl_tr, dl_va, dl_te, ntr, nva, nte = build_dataloaders_for_label(demo_label, batch_size=32)\n",
    "print(f\"[{demo_label}] dataset sizes -> train:{ntr}  val:{nva}  test:{nte}\")\n",
    "\n",
    "batch = next(iter(dl_tr))\n",
    "print(\"Train batch keys:\", list(batch.keys()))\n",
    "print(\"input_ids shape:\", tuple(batch[\"input_ids\"].shape))\n",
    "print(\"attention_mask shape:\", tuple(batch[\"attention_mask\"].shape))\n",
    "print(\"desc shape:\", tuple(batch[\"desc\"].shape))\n",
    "print(\"y shape:\", tuple(batch[\"y\"].shape))\n",
    "print(\"y counts:\", {k:int((batch['y']==k).sum().item()) for k in [-1,0,1]})\n",
    "print(\"Example smiles (augmented if train):\", batch[\"smiles\"][:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1949b68",
   "metadata": {},
   "source": [
    "## 6: Graph featuriser + GIN encoder + Co-Attention + Model skeleton (dry forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeaaeb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atom dim ~ 36, Bond dim ~ 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dry forward OK.\n",
      "Logit shape: (8,) Prob shape: (8,)\n",
      "Gate weights (first item): [0.28767663 0.41927987 0.2930435 ]\n",
      "Emb shapes: cls_text (8, 256) graph_h (8, 256) desc_h (8, 256)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from typing import List, Tuple, Dict\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from rdkit import Chem\n",
    "\n",
    "# ---- Globals from earlier cells ----\n",
    "assert 'tokenizer' in globals(), \"Tokenizer not loaded (run Cell 5).\"\n",
    "assert 'DF_V6' in globals(), \"DF_V6 not found (run Cell 2).\"\n",
    "assert 'DESC_COLS_V6' in globals(), \"DESC_COLS_V6 not found (run Cell 3).\"\n",
    "\n",
    "H = 256  \n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# =========================================================\n",
    "# 1) RDKit -> graph tensors (atom + bond features, virtual node)\n",
    "# =========================================================\n",
    "\n",
    "# Atom featurisation helpers\n",
    "ATOM_LIST = list(range(1, 119))  # Z=1..118\n",
    "HYBRIDIZATION_LIST = [\n",
    "    Chem.rdchem.HybridizationType.SP,\n",
    "    Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3,\n",
    "    Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2,\n",
    "]\n",
    "\n",
    "def one_hot(x, choices):\n",
    "    v = [0]*len(choices)\n",
    "    try:\n",
    "        idx = choices.index(x)\n",
    "        v[idx] = 1\n",
    "    except ValueError:\n",
    "        pass\n",
    "    return v\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> List[float]:\n",
    "    z = atom.GetAtomicNum()\n",
    "    feats = []\n",
    "    # Atomic number (bucketed one-hot into ranges to keep dim modest)\n",
    "    # We'll bucket into: [H], [C], [N], [O], [F], [P], [S], [Cl], [Br], [I], [metal], [other]\n",
    "    sym = atom.GetSymbol()\n",
    "    heavy_map = {'H':0,'C':1,'N':2,'O':3,'F':4,'P':5,'S':6,'Cl':7,'Br':8,'I':9}\n",
    "    oh = [0]*12\n",
    "    oh[heavy_map[sym] if sym in heavy_map else (10 if z>20 else 11)] = 1\n",
    "    feats += oh\n",
    "    # Degree (0..5+)\n",
    "    deg = atom.GetDegree()\n",
    "    feats += one_hot(min(deg,5), list(range(6)))\n",
    "    # Formal charge (-2..+2 bucket)\n",
    "    fc = atom.GetFormalCharge()\n",
    "    feats += one_hot(int(max(-2, min(2, fc))+2), list(range(5)))\n",
    "    # Hybridization\n",
    "    feats += one_hot(atom.GetHybridization(), HYBRIDIZATION_LIST + [None])\n",
    "    # Aromatic\n",
    "    feats += [1.0 if atom.GetIsAromatic() else 0.0]\n",
    "    # Total H count (0..4+)\n",
    "    total_h = min(4, atom.GetTotalNumHs())\n",
    "    feats += one_hot(total_h, list(range(5)))\n",
    "    # In ring\n",
    "    feats += [1.0 if atom.IsInRing() else 0.0]\n",
    "    return feats  # ~12 + 6 + 5 + 6 + 1 + 5 + 1 = 36 dims\n",
    "\n",
    "def bond_features(bond: Chem.rdchem.Bond) -> List[float]:\n",
    "    bt = bond.GetBondType()\n",
    "    # Bond type one-hot: single, double, triple, aromatic\n",
    "    bt_oh = [\n",
    "        1.0 if bt == Chem.rdchem.BondType.SINGLE else 0.0,\n",
    "        1.0 if bt == Chem.rdchem.BondType.DOUBLE else 0.0,\n",
    "        1.0 if bt == Chem.rdchem.BondType.TRIPLE else 0.0,\n",
    "        1.0 if bt == Chem.rdchem.BondType.AROMATIC else 0.0,\n",
    "    ]\n",
    "    conj = 1.0 if bond.GetIsConjugated() else 0.0\n",
    "    ring = 1.0 if bond.IsInRing() else 0.0\n",
    "    stereo = 1.0 if bond.GetStereo() != Chem.rdchem.BondStereo.STEREONONE else 0.0\n",
    "    return bt_oh + [conj, ring, stereo]  # 4 + 3 = 7 dims\n",
    "\n",
    "ATOM_DIM = len(atom_features(Chem.MolFromSmiles(\"CC\").GetAtomWithIdx(0)))\n",
    "BOND_DIM = len(bond_features(Chem.MolFromSmiles(\"C=C\").GetBondWithIdx(0)))\n",
    "print(f\"Atom dim ~ {ATOM_DIM}, Bond dim ~ {BOND_DIM}\")\n",
    "\n",
    "def mol_to_graph(smi: str, add_virtual_node: bool = True) -> Dict[str, torch.Tensor]:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    assert mol is not None, f\"Invalid SMILES: {smi}\"\n",
    "\n",
    "    # Nodes\n",
    "    x = torch.tensor([atom_features(a) for a in mol.GetAtoms()], dtype=torch.float32)\n",
    "\n",
    "    # Edges (bidirectional)\n",
    "    src, dst, eattr = [], [], []\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        bf = bond_features(b)\n",
    "        src += [i, j]\n",
    "        dst += [j, i]\n",
    "        eattr += [bf, bf]\n",
    "    if len(src) == 0:\n",
    "        # Handle single-atom molecule: no bonds\n",
    "        src, dst = [0], [0]\n",
    "        eattr = [[1,0,0,0,0,0,0]]  # dummy single\n",
    "\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "    edge_attr  = torch.tensor(eattr, dtype=torch.float32)\n",
    "\n",
    "    if add_virtual_node:\n",
    "        n = x.shape[0]\n",
    "        # Append virtual node at index n\n",
    "        vfeat = torch.zeros((1, x.shape[1]), dtype=torch.float32)\n",
    "        x = torch.cat([x, vfeat], dim=0)\n",
    "        # Connect virtual node to all real nodes (both directions)\n",
    "        vn_src = list(range(n)) + [n]*n\n",
    "        vn_dst = [n]*n + list(range(n))\n",
    "        edge_index = torch.cat([edge_index, torch.tensor([vn_src, vn_dst])], dim=1)\n",
    "        vattr = torch.zeros((2*n, edge_attr.shape[1]), dtype=torch.float32)\n",
    "        edge_attr = torch.cat([edge_attr, vattr], dim=0)\n",
    "\n",
    "    return {\"x\": x, \"edge_index\": edge_index, \"edge_attr\": edge_attr}\n",
    "\n",
    "def build_graph_batch(smiles_list: List[str], add_virtual_node: bool = True) -> Dict[str, torch.Tensor]:\n",
    "    xs, eis, eas, batch_idx, n_nodes = [], [], [], [], []\n",
    "    node_offset = 0\n",
    "    for b_idx, smi in enumerate(smiles_list):\n",
    "        g = mol_to_graph(smi, add_virtual_node=add_virtual_node)\n",
    "        x, ei, ea = g[\"x\"], g[\"edge_index\"], g[\"edge_attr\"]\n",
    "        xs.append(x)\n",
    "        eis.append(ei + node_offset)\n",
    "        eas.append(ea)\n",
    "        n = x.shape[0]\n",
    "        n_nodes.append(n)\n",
    "        batch_idx.append(torch.full((n,), b_idx, dtype=torch.long))\n",
    "        node_offset += n\n",
    "    x_cat = torch.cat(xs, dim=0)\n",
    "    ei_cat = torch.cat(eis, dim=1)\n",
    "    ea_cat = torch.cat(eas, dim=0)\n",
    "    batch_cat = torch.cat(batch_idx, dim=0)\n",
    "    return {\n",
    "        \"x\": x_cat.to(DEVICE),\n",
    "        \"edge_index\": ei_cat.to(DEVICE),\n",
    "        \"edge_attr\": ea_cat.to(DEVICE),\n",
    "        \"batch\": batch_cat.to(DEVICE),\n",
    "        \"n_nodes\": torch.tensor(n_nodes, dtype=torch.long, device=DEVICE)\n",
    "    }\n",
    "\n",
    "# =========================================================\n",
    "# 2) Minimal GIN layer + encoder (no external libs)\n",
    "# =========================================================\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, out_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, dim, eps=0.0, dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor([eps], dtype=torch.float32))\n",
    "        self.mlp = MLP(dim, dim, dim, dropout)\n",
    "    def forward(self, x, edge_index):\n",
    "        # edge_index: [2, E] with src->dst\n",
    "        src, dst = edge_index[0], edge_index[1]\n",
    "        agg = torch.zeros_like(x)\n",
    "        agg.index_add_(0, dst, x[src])  # sum over neighbours\n",
    "        out = self.mlp((1 + self.eps) * x + agg)\n",
    "        return out\n",
    "\n",
    "class GINEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=H, layers=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(in_dim, hidden)\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden, eps=0.0, dropout=dropout) for _ in range(layers)])\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(hidden) for _ in range(layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x, edge_index, batch, n_nodes):\n",
    "        h = self.input(x)\n",
    "        for gin, ln in zip(self.layers, self.norms):\n",
    "            h = gin(h, edge_index)\n",
    "            h = ln(h)\n",
    "            h = F.relu(h, inplace=True)\n",
    "            h = self.dropout(h)\n",
    "        # graph-level pooling: mean per graph (excluding virtual node? we included it; mean is okay as a start)\n",
    "        # To exclude virtual nodes, we would need a mask; for now we keep them (their features are learnable zeros).\n",
    "        # Build pooled embeddings per graph:\n",
    "        B = int(n_nodes.shape[0])\n",
    "        # Compute sums per graph\n",
    "        pooled = torch.zeros((B, h.shape[1]), device=h.device)\n",
    "        pooled.index_add_(0, batch, h)\n",
    "        # divide by counts\n",
    "        counts = n_nodes.unsqueeze(1).clamp(min=1)\n",
    "        pooled = pooled / counts\n",
    "        return h, pooled  # node reps, graph reps\n",
    "\n",
    "# =========================================================\n",
    "# 3) Bi-directional token↔atom co-attention (mask-aware)\n",
    "# =========================================================\n",
    "class BiCoAttention(nn.Module):\n",
    "    def __init__(self, dim=H, heads=4):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.tok_q = nn.Linear(dim, dim)\n",
    "        self.tok_k = nn.Linear(dim, dim)\n",
    "        self.tok_v = nn.Linear(dim, dim)\n",
    "        self.node_q = nn.Linear(dim, dim)\n",
    "        self.node_k = nn.Linear(dim, dim)\n",
    "        self.node_v = nn.Linear(dim, dim)\n",
    "        self.proj_tok = nn.Linear(dim, dim)\n",
    "        self.proj_node = nn.Linear(dim, dim)\n",
    "\n",
    "    def _split_heads(self, x):\n",
    "        B, L, D = x.shape\n",
    "        H = self.heads\n",
    "        x = x.view(B, L, H, D // H).transpose(1, 2)  # (B, H, L, Dh)\n",
    "        return x\n",
    "\n",
    "    def _merge_heads(self, x):\n",
    "        B, H, L, Dh = x.shape\n",
    "        x = x.transpose(1, 2).contiguous().view(B, L, H * Dh)\n",
    "        return x\n",
    "\n",
    "    def forward(self, tok, tok_mask, node, node_mask):\n",
    "        \"\"\"\n",
    "        tok:  (B, T, D), tok_mask: (B, T) with 1 for tokens to keep\n",
    "        node: (B, N, D), node_mask:(B, N) with 1 for atoms to keep\n",
    "        Returns:\n",
    "          tok_ctx_pooled, node_ctx_pooled  (both (B, D))\n",
    "        \"\"\"\n",
    "        # Project\n",
    "        q_t = self._split_heads(self.tok_q(tok))\n",
    "        k_t = self._split_heads(self.tok_k(tok))\n",
    "        v_t = self._split_heads(self.tok_v(tok))\n",
    "\n",
    "        q_n = self._split_heads(self.node_q(node))\n",
    "        k_n = self._split_heads(self.node_k(node))\n",
    "        v_n = self._split_heads(self.node_v(node))\n",
    "\n",
    "        # tokens attending to nodes\n",
    "        attn_t2n = torch.matmul(q_t, k_n.transpose(-2, -1)) * self.scale  # (B,H,T,N)\n",
    "        # mask nodes\n",
    "        node_mask_ = node_mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,N)\n",
    "        attn_t2n = attn_t2n.masked_fill(node_mask_ == 0, float('-inf'))\n",
    "        w_t2n = torch.softmax(attn_t2n, dim=-1)\n",
    "        ctx_t = torch.matmul(w_t2n, v_n)  # (B,H,T,Dh)\n",
    "        ctx_t = self._merge_heads(ctx_t)\n",
    "        tok_enh = self.proj_tok(ctx_t)  # (B,T,D)\n",
    "\n",
    "        # nodes attending to tokens\n",
    "        attn_n2t = torch.matmul(q_n, k_t.transpose(-2, -1)) * self.scale  # (B,H,N,T)\n",
    "        tok_mask_ = tok_mask.unsqueeze(1).unsqueeze(2)  # (B,1,1,T)\n",
    "        attn_n2t = attn_n2t.masked_fill(tok_mask_ == 0, float('-inf'))\n",
    "        w_n2t = torch.softmax(attn_n2t, dim=-1)\n",
    "        ctx_n = torch.matmul(w_n2t, v_t)  # (B,H,N,Dh)\n",
    "        ctx_n = self._merge_heads(ctx_n)\n",
    "        node_enh = self.proj_node(ctx_n)  # (B,N,D)\n",
    "\n",
    "        # Pooled summaries\n",
    "        # mask-aware mean pooling\n",
    "        tok_mask_f = tok_mask.float().unsqueeze(-1).clamp(min=1e-6)\n",
    "        node_mask_f = node_mask.float().unsqueeze(-1).clamp(min=1e-6)\n",
    "        tok_ctx_pooled = (tok_enh * tok_mask_f).sum(dim=1) / tok_mask_f.sum(dim=1)\n",
    "        node_ctx_pooled = (node_enh * node_mask_f).sum(dim=1) / node_mask_f.sum(dim=1)\n",
    "\n",
    "        return tok_ctx_pooled, node_ctx_pooled\n",
    "\n",
    "# =========================================================\n",
    "# 4) Full Expert model skeleton (ChemBERTa text + GIN graph + co-attn + gate + head)\n",
    "# =========================================================\n",
    "from transformers import AutoModel\n",
    "\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=H, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, out_dim),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class V6Expert(nn.Module):\n",
    "    def __init__(self, text_model_name: str, desc_dim: int, desc_dropout: float = 0.5, heads: int = 4):\n",
    "        super().__init__()\n",
    "        self.text = AutoModel.from_pretrained(text_model_name)\n",
    "        text_hidden = self.text.config.hidden_size\n",
    "\n",
    "        self.text_proj = nn.Linear(text_hidden, H)\n",
    "        self.gnn = GINEncoder(in_dim=ATOM_DIM, hidden=H, layers=4, dropout=0.1)\n",
    "        self.coattn = BiCoAttention(dim=H, heads=heads)\n",
    "        self.desc_mlp = DescriptorMLP(desc_dim, out_dim=H, dropout=desc_dropout)\n",
    "\n",
    "        # label-wise gate over {text_cls, graph_pool, desc}\n",
    "        self.gate = nn.Linear(H, 3)  # we'll apply to a shared query vector (see below)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(H, H//2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(H//2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, batch, graph_batch):\n",
    "        # Text\n",
    "        out = self.text(input_ids=batch[\"input_ids\"].to(DEVICE),\n",
    "                        attention_mask=batch[\"attention_mask\"].to(DEVICE))\n",
    "        tok = out.last_hidden_state  # (B,T,hidden)\n",
    "        tok = self.text_proj(tok)    # (B,T,H)\n",
    "\n",
    "        # Build token-level masks\n",
    "        tok_mask = batch[\"attention_mask\"].to(DEVICE)  # (B,T)\n",
    "\n",
    "        # Graph\n",
    "        x, ei, ea, bvec, n_nodes = graph_batch[\"x\"], graph_batch[\"edge_index\"], graph_batch[\"edge_attr\"], graph_batch[\"batch\"], graph_batch[\"n_nodes\"]\n",
    "        node_h, graph_h = self.gnn(x, ei, bvec, n_nodes)  # (sumN,H), (B,H)\n",
    "\n",
    "        # Build padded node tensors for co-attention\n",
    "        B = tok.shape[0]\n",
    "        Ns = n_nodes.tolist()\n",
    "        Nmax = max(Ns)\n",
    "        node_pad = x.new_zeros((B, Nmax, H))\n",
    "        node_mask = torch.zeros((B, Nmax), dtype=torch.long, device=DEVICE)\n",
    "        start = 0\n",
    "        for i, n in enumerate(Ns):\n",
    "            node_pad[i, :n] = node_h[start:start+n]\n",
    "            node_mask[i, :n] = 1\n",
    "            start += n\n",
    "\n",
    "        # Co-attention pooled summaries\n",
    "        tok_ctx, node_ctx = self.coattn(tok, tok_mask, node_pad, node_mask)  # (B,H), (B,H)\n",
    "\n",
    "        # CLS token (projected)\n",
    "        cls_text = tok[:, 0, :]  # (B,H)\n",
    "\n",
    "        # Descriptors\n",
    "        desc = batch[\"desc\"].to(DEVICE)  # (B,D)\n",
    "        desc_h = self.desc_mlp(desc)     # (B,H)\n",
    "\n",
    "        # Fusion with gate over {text, graph, desc}; add co-attn residual\n",
    "        # Simple approach: use co-attn token ctx as the \"query\" for gating\n",
    "        gate_logits = self.gate(tok_ctx)              # (B,3)\n",
    "        gate_w = torch.softmax(gate_logits, dim=-1)   # (B,3)\n",
    "        fused = (gate_w[:,0:1] * cls_text\n",
    "                + gate_w[:,1:2] * graph_h\n",
    "                + gate_w[:,2:3] * desc_h)\n",
    "        fused = fused + 0.5 * (tok_ctx + node_ctx)    # residual from co-attn summaries\n",
    "\n",
    "        logit = self.classifier(fused).squeeze(-1)    # (B,)\n",
    "        return logit, {\"gate_w\": gate_w, \"cls_text\": cls_text, \"graph_h\": graph_h, \"desc_h\": desc_h,\n",
    "                       \"tok_ctx\": tok_ctx, \"node_ctx\": node_ctx}\n",
    "\n",
    "# =========================================================\n",
    "# 5) Dry forward on one batch\n",
    "# =========================================================\n",
    "# Build a fresh NR-AR batch from the dataloader defined in Cell 5\n",
    "demo_label = \"NR-AR\"\n",
    "dl_tr, dl_va, dl_te, ntr, nva, nte = build_dataloaders_for_label(demo_label, batch_size=8)\n",
    "batch = next(iter(dl_tr))\n",
    "\n",
    "# Build graph batch from SMILES in batch\n",
    "g_batch = build_graph_batch(batch[\"smiles\"], add_virtual_node=True)\n",
    "\n",
    "model = V6Expert(text_model_name=tok_loaded_name, desc_dim=len(DESC_COLS_V6), desc_dropout=0.5, heads=4).to(DEVICE)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logit, extra = model(batch, g_batch)\n",
    "    prob = torch.sigmoid(logit)\n",
    "\n",
    "print(\"Dry forward OK.\")\n",
    "print(\"Logit shape:\", tuple(logit.shape), \"Prob shape:\", tuple(prob.shape))\n",
    "print(\"Gate weights (first item):\", extra[\"gate_w\"][0].detach().cpu().numpy())\n",
    "print(\"Emb shapes: cls_text\", tuple(extra[\"cls_text\"].shape), \n",
    "      \"graph_h\", tuple(extra[\"graph_h\"].shape), \n",
    "      \"desc_h\", tuple(extra[\"desc_h\"].shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4d6ca1",
   "metadata": {},
   "source": [
    "## 7: Losses, Trainer (A/B/C), Checkpointing, Short Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14e0680",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Smoke test: NR-AR with Stage A=1 epoch, B/C=0 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=1 | SAM=False ===\n",
      "[A][ep 01] loss=0.0061 TR PR-AUC=0.2543 | VA PR-AUC=0.0982 (ROC=0.6441)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\NR-AR\\v6_20250903_135505_NR-AR_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AR\\v6_20250903_135505_NR-AR_history.json\n",
      "Smoke test complete. Last epoch record: {'stage': 'A', 'epoch': 1, 'train_loss': 0.006122, 'train_pr_auc': 0.254343, 'train_roc_auc': 0.50852, 'val_pr_auc': 0.098244, 'val_roc_auc': 0.644145}\n"
     ]
    }
   ],
   "source": [
    "import math, json, time, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, Optional\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# ---- Globals from earlier cells ----\n",
    "assert 'build_dataloaders_for_label' in globals()\n",
    "assert 'V6Expert' in globals()\n",
    "assert 'DESC_COLS_V6' in globals()\n",
    "assert 'tok_loaded_name' in globals()\n",
    "assert 'RES_DIR' in globals() and 'CKPT_DIR' in globals()\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Load per-label hyperparams from Cell 4 output\n",
    "# -----------------------------\n",
    "HP_JSON = RES_DIR / \"per_label_hparams_v6.json\"\n",
    "with open(HP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    PER_LABEL_HP = {row[\"label\"]: row for row in json.load(f)}\n",
    "\n",
    "def get_label_hparams(label: str) -> Dict[str, Any]:\n",
    "    hp = PER_LABEL_HP[label]\n",
    "    return {\n",
    "        \"gamma_neg\": float(hp[\"gamma_neg\"]),\n",
    "        \"gamma_pos\": float(hp[\"gamma_pos\"]),\n",
    "        \"margin\": float(hp[\"margin\"]),\n",
    "        \"desc_dropout\": float(hp[\"desc_dropout\"]),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# 2) ASL (binary) with margin + class-balanced positive weight\n",
    "# -----------------------------\n",
    "class ASLBceWithLogits(nn.Module):\n",
    "    \"\"\"\n",
    "    Asymmetric Loss for binary logits:\n",
    "      - margin m applied to negatives\n",
    "      - asymmetric focusing with gamma_neg, gamma_pos\n",
    "      - optional class-balanced positive weight 'w_pos' (scalar multiplier for positive samples)\n",
    "    \"\"\"\n",
    "    def __init__(self, gamma_pos=0.0, gamma_neg=4.0, margin=0.05, w_pos=1.0):\n",
    "        super().__init__()\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.margin = margin\n",
    "        self.w_pos = w_pos\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        \"\"\"\n",
    "        logits: (B,) raw scores\n",
    "        targets: (B,) in {0,1}\n",
    "        \"\"\"\n",
    "        x = logits\n",
    "        y = targets.float()\n",
    "\n",
    "        # Probabilities\n",
    "        x_sig = torch.sigmoid(x)\n",
    "\n",
    "        # Margin on negatives\n",
    "        if self.margin > 0:\n",
    "            x_sig = torch.where(y < 0.5, x_sig - self.margin, x_sig)\n",
    "\n",
    "        # Asymmetric focusing\n",
    "        pt = torch.where(y >= 0.5, x_sig, 1 - x_sig)\n",
    "        one_sided_gamma = torch.where(y >= 0.5, self.gamma_pos * torch.ones_like(pt), self.gamma_neg * torch.ones_like(pt))\n",
    "        focal_weight = (1 - pt) ** one_sided_gamma\n",
    "\n",
    "        # Standard BCE on logits\n",
    "        bce = F.binary_cross_entropy_with_logits(x, y, reduction='none')\n",
    "\n",
    "        # Reweight BCE\n",
    "        loss = focal_weight * bce\n",
    "\n",
    "        # Class-balanced positive upweight\n",
    "        if self.w_pos != 1.0:\n",
    "            w = torch.ones_like(y)\n",
    "            w = torch.where(y >= 0.5, torch.full_like(w, self.w_pos), w)\n",
    "            loss = loss * w\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "def effective_num_weight(n_pos: int, beta: float = 0.999) -> float:\n",
    "    \"\"\"Class-Balanced weight for positives.\"\"\"\n",
    "    if n_pos <= 0:\n",
    "        return 1.0\n",
    "    return float((1 - beta) / (1 - beta ** n_pos))\n",
    "\n",
    "# -----------------------------\n",
    "# 3) EMA (Exponential Moving Average) of model params\n",
    "# -----------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay=0.999):\n",
    "        self.shadow = {}\n",
    "        self.decay = decay\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[n] = p.detach().clone()\n",
    "\n",
    "    def update(self, model: nn.Module):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
    "\n",
    "    def apply_to(self, model: nn.Module):\n",
    "        self.backup = {}\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                self.backup[n] = p.detach().clone()\n",
    "                p.data.copy_(self.shadow[n].data)\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.backup:\n",
    "                p.data.copy_(self.backup[n].data)\n",
    "        self.backup = {}\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Simple SAM (two-step) optimizer wrapper\n",
    "# -----------------------------\n",
    "class SAMWrapper:\n",
    "    def __init__(self, model: nn.Module, base_optimizer: torch.optim.Optimizer, rho: float = 0.05, eps=1e-12):\n",
    "        self.model = model\n",
    "        self.opt = base_optimizer\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "\n",
    "    def first_step(self):\n",
    "        grad_norm = torch.norm(\n",
    "            torch.stack([p.grad.norm(p=2) for p in self.model.parameters() if p.requires_grad and p.grad is not None]),\n",
    "            p=2\n",
    "        )\n",
    "        scale = self.rho / (grad_norm + self.eps)\n",
    "        for p in self.model.parameters():\n",
    "            if p.requires_grad and p.grad is not None:\n",
    "                e_w = p.grad * scale\n",
    "                p.add_(e_w)  # perturb\n",
    "                p.state['e_w'] = e_w\n",
    "\n",
    "    def second_step(self):\n",
    "        for p in self.model.parameters():\n",
    "            if p.requires_grad and 'e_w' in p.state:\n",
    "                p.sub_(p.state['e_w'])  # unperturb\n",
    "                del p.state['e_w']\n",
    "        self.opt.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad(set_to_none=True)\n",
    "\n",
    "# -----------------------------\n",
    "# 5) Utilities: early stopping, metrics, freezing policy\n",
    "# -----------------------------\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=4, maximize=True):\n",
    "        self.best = None\n",
    "        self.count = 0\n",
    "        self.patience = patience\n",
    "        self.maximize = maximize\n",
    "\n",
    "    def step(self, value):\n",
    "        improved = False\n",
    "        if self.best is None:\n",
    "            improved = True\n",
    "        else:\n",
    "            improved = value > self.best if self.maximize else value < self.best\n",
    "        if improved:\n",
    "            self.best = value\n",
    "            self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "        return improved, self.count >= self.patience\n",
    "\n",
    "def compute_metrics_from_logits(logits: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
    "    # safe metrics\n",
    "    out = {}\n",
    "    try:\n",
    "        out[\"pr_auc\"] = float(average_precision_score(targets, 1/(1+np.exp(-logits))))\n",
    "    except Exception:\n",
    "        out[\"pr_auc\"] = float('nan')\n",
    "    try:\n",
    "        out[\"roc_auc\"] = float(roc_auc_score(targets, 1/(1+np.exp(-logits))))\n",
    "    except Exception:\n",
    "        out[\"roc_auc\"] = float('nan')\n",
    "    return out\n",
    "\n",
    "def set_text_trainable(model: V6Expert, stage: str):\n",
    "    # Stage A: freeze all text\n",
    "    # Stage B/C: unfreeze last 4 transformer blocks (Roberta encoder)\n",
    "    for p in model.text.parameters():\n",
    "        p.requires_grad = False\n",
    "    if stage in (\"B\", \"C\"):\n",
    "        try:\n",
    "            blocks = model.text.encoder.layer\n",
    "            for i in range(len(blocks)-4, len(blocks)):\n",
    "                for p in blocks[i].parameters():\n",
    "                    p.requires_grad = True\n",
    "            # Always allow the projection/gate/head/gnn/desc\n",
    "            for m in [model.text_proj, model.gnn, model.desc_mlp, model.coattn, model.gate, model.classifier]:\n",
    "                for p in m.parameters():\n",
    "                    p.requires_grad = True\n",
    "        except Exception:\n",
    "            # Fallback: if the structure differs, unfreeze all text (conservative)\n",
    "            for p in model.text.parameters():\n",
    "                p.requires_grad = True\n",
    "    else:\n",
    "        # Ensure non-text modules are trainable\n",
    "        for m in [model.text_proj, model.gnn, model.desc_mlp, model.coattn, model.gate, model.classifier]:\n",
    "            for p in m.parameters():\n",
    "                p.requires_grad = True\n",
    "\n",
    "def build_optimizer_and_sched(model, lr, weight_decay, total_steps, warmup_ratio=0.05):\n",
    "    # Simple param groups: text (trainable subset) + others\n",
    "    decay, no_decay = set(), set()\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\", \"layer_norm.bias\"]):\n",
    "            no_decay.add(n)\n",
    "        else:\n",
    "            decay.add(n)\n",
    "    param_groups = [\n",
    "        {\"params\": [p for n,p in model.named_parameters() if n in decay and p.requires_grad], \"lr\": lr, \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n,p in model.named_parameters() if n in no_decay and p.requires_grad], \"lr\": lr, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    opt = torch.optim.AdamW(param_groups, lr=lr, weight_decay=weight_decay)\n",
    "    warmup_steps = max(1, int(total_steps * warmup_ratio))\n",
    "    sched = get_cosine_schedule_with_warmup(opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    return opt, sched\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Trainer for one label\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class StageConfig:\n",
    "    name: str\n",
    "    epochs: int\n",
    "    use_sam: bool\n",
    "    patience: int\n",
    "\n",
    "def train_label(label: str,\n",
    "                batch_size: int = 64,\n",
    "                base_lr: float = 2e-4,\n",
    "                weight_decay: float = 1e-4,\n",
    "                ema_decay: float = 0.999,\n",
    "                seed: int = 42,\n",
    "                stageA_epochs: int = 3,\n",
    "                stageB_epochs: int = 6,\n",
    "                stageC_epochs: int = 8):\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "    # Data\n",
    "    dl_tr, dl_va, dl_te, ntr, nva, nte = build_dataloaders_for_label(label, batch_size=batch_size)\n",
    "    steps_per_epoch = max(1, math.ceil(ntr / batch_size))\n",
    "\n",
    "    # Per-label hparams\n",
    "    hp = get_label_hparams(label)\n",
    "    # Class-balanced positive weight from TRAIN\n",
    "    y_tr_all = []\n",
    "    for _, batch in zip(range(steps_per_epoch), dl_tr):\n",
    "        y_tr_all.extend([int(v) for v in batch[\"y\"].tolist()])\n",
    "    n_pos = sum(1 for v in y_tr_all if v == 1)\n",
    "    w_pos = effective_num_weight(n_pos, beta=0.999)\n",
    "\n",
    "    # Build model\n",
    "    model = V6Expert(text_model_name=tok_loaded_name, desc_dim=len(DESC_COLS_V6),\n",
    "                     desc_dropout=hp[\"desc_dropout\"], heads=4).to(DEVICE)\n",
    "\n",
    "    # EMA\n",
    "    ema = EMA(model, decay=ema_decay)\n",
    "\n",
    "    # Stages\n",
    "    stages = [\n",
    "        StageConfig(name=\"A\", epochs=stageA_epochs, use_sam=False, patience=4),\n",
    "        StageConfig(name=\"B\", epochs=stageB_epochs, use_sam=True,  patience=5),\n",
    "        StageConfig(name=\"C\", epochs=stageC_epochs, use_sam=True,  patience=6),\n",
    "    ]\n",
    "\n",
    "    # Run ID + dirs\n",
    "    run_id = time.strftime(\"v6_%Y%m%d_%H%M%S\", time.gmtime())\n",
    "    out_ckpt_dir = CKPT_DIR / label\n",
    "    out_res_dir  = RES_DIR  / label\n",
    "    out_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    history = {\"run_id\": run_id, \"label\": label, \"n_train\": ntr, \"n_val\": nva, \"n_test\": nte,\n",
    "               \"hparams\": hp, \"w_pos\": w_pos, \"epochs\": []}\n",
    "    best_val = -float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    for stage in stages:\n",
    "        if stage.epochs <= 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Stage {stage.name} | epochs={stage.epochs} | SAM={stage.use_sam} ===\")\n",
    "        set_text_trainable(model, stage.name)\n",
    "\n",
    "        total_steps = stage.epochs * steps_per_epoch\n",
    "        opt, sched = build_optimizer_and_sched(model, lr=base_lr, weight_decay=weight_decay,\n",
    "                                               total_steps=total_steps, warmup_ratio=0.05)\n",
    "        sam = SAMWrapper(model, opt, rho=0.05) if stage.use_sam else None\n",
    "        stopper = EarlyStopper(patience=stage.patience, maximize=True)\n",
    "\n",
    "        criterion = ASLBceWithLogits(gamma_pos=hp[\"gamma_pos\"], gamma_neg=hp[\"gamma_neg\"],\n",
    "                                     margin=hp[\"margin\"], w_pos=w_pos)\n",
    "\n",
    "        for ep in range(1, stage.epochs + 1):\n",
    "            # ---- Train ----\n",
    "            model.train()\n",
    "            tr_loss = 0.0\n",
    "            tr_logits, tr_targets = [], []\n",
    "\n",
    "            it = iter(dl_tr)\n",
    "            for _ in range(steps_per_epoch):\n",
    "                batch = next(it)\n",
    "                # Build graphs on the fly\n",
    "                g_batch = build_graph_batch(batch[\"smiles\"], add_virtual_node=True)\n",
    "\n",
    "                logits, extra = model(batch, g_batch)\n",
    "                y = batch[\"y\"].to(DEVICE).float()\n",
    "\n",
    "                # Gate L1 regulariser (on gate weights); small coefficient\n",
    "                gate_w = extra[\"gate_w\"]\n",
    "                gate_l1 = gate_w.abs().mean() * 0.01\n",
    "\n",
    "                loss = criterion(logits, y) + gate_l1\n",
    "\n",
    "                if sam is None:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                    opt.step()\n",
    "                else:\n",
    "                    # SAM first step\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                    sam.first_step()\n",
    "\n",
    "                    # second forward\n",
    "                    logits2, _ = model(batch, g_batch)\n",
    "                    loss2 = criterion(logits2, y) + gate_l1\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss2.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                    sam.second_step()\n",
    "\n",
    "                sched.step()\n",
    "                ema.update(model)\n",
    "\n",
    "                tr_loss += float(loss.detach().cpu().item())\n",
    "                tr_logits.append(logits.detach().cpu().numpy())\n",
    "                tr_targets.append(batch[\"y\"].numpy())\n",
    "\n",
    "            tr_logits = np.concatenate(tr_logits)\n",
    "            tr_targets = np.concatenate(tr_targets)\n",
    "            tr_metrics = compute_metrics_from_logits(tr_logits, tr_targets)\n",
    "\n",
    "            # ---- Validate (EMA weights) ----\n",
    "            model.eval()\n",
    "            ema.apply_to(model)\n",
    "            with torch.no_grad():\n",
    "                va_logits, va_targets = [], []\n",
    "                for batch in dl_va:\n",
    "                    g_batch = build_graph_batch(batch[\"smiles\"], add_virtual_node=True)\n",
    "                    logits, _ = model(batch, g_batch)\n",
    "                    va_logits.append(logits.detach().cpu().numpy())\n",
    "                    va_targets.append(batch[\"y\"].numpy())\n",
    "                va_logits = np.concatenate(va_logits) if va_logits else np.array([])\n",
    "                va_targets = np.concatenate(va_targets) if va_targets else np.array([])\n",
    "                va_metrics = compute_metrics_from_logits(va_logits, va_targets)\n",
    "            ema.restore(model)\n",
    "\n",
    "            ep_rec = {\n",
    "                \"stage\": stage.name,\n",
    "                \"epoch\": ep,\n",
    "                \"train_loss\": round(tr_loss / steps_per_epoch, 6),\n",
    "                \"train_pr_auc\": round(tr_metrics.get(\"pr_auc\", float('nan')), 6),\n",
    "                \"train_roc_auc\": round(tr_metrics.get(\"roc_auc\", float('nan')), 6),\n",
    "                \"val_pr_auc\": round(va_metrics.get(\"pr_auc\", float('nan')), 6),\n",
    "                \"val_roc_auc\": round(va_metrics.get(\"roc_auc\", float('nan')), 6),\n",
    "            }\n",
    "            history[\"epochs\"].append(ep_rec)\n",
    "            print(f\"[{stage.name}][ep {ep:02d}] loss={ep_rec['train_loss']:.4f} \"\n",
    "                  f\"TR PR-AUC={ep_rec['train_pr_auc']:.4f} | VA PR-AUC={ep_rec['val_pr_auc']:.4f} \"\n",
    "                  f\"(ROC={ep_rec['val_roc_auc']:.4f})\")\n",
    "\n",
    "            # Early stopping on VA PR-AUC\n",
    "            current = va_metrics.get(\"pr_auc\", -float('inf'))\n",
    "            improved, stop = stopper.step(current)\n",
    "            if improved and np.isfinite(current):\n",
    "                best_val = current\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "\n",
    "            if stop:\n",
    "                print(f\"Early stopping triggered on stage {stage.name} (patience={stage.patience}).\")\n",
    "                break\n",
    "\n",
    "    # ---- Save best checkpoint + history ----\n",
    "    if best_state is not None:\n",
    "        ckpt_path = out_ckpt_dir / f\"{history['run_id']}_{label}_best.pt\"\n",
    "        torch.save(best_state, ckpt_path)\n",
    "        print(f\"\\nSaved BEST checkpoint -> {ckpt_path.resolve()}\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: No best state captured; not saving checkpoint.\")\n",
    "\n",
    "    hist_path = out_res_dir / f\"{history['run_id']}_{label}_history.json\"\n",
    "    with open(hist_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"Saved history -> {hist_path.resolve()}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# -----------------------------\n",
    "# 7) SHORT SMOKE TEST (1 epoch only in Stage A) on a single label\n",
    "# -----------------------------\n",
    "print(\"\\n--- Smoke test: NR-AR with Stage A=1 epoch, B/C=0 ---\")\n",
    "hist = train_label(\n",
    "    label=\"NR-AR\",\n",
    "    batch_size=32,\n",
    "    base_lr=2e-4,\n",
    "    weight_decay=1e-4,\n",
    "    ema_decay=0.999,\n",
    "    seed=42,\n",
    "    stageA_epochs=1,  # quick check\n",
    "    stageB_epochs=0,\n",
    "    stageC_epochs=0\n",
    ")\n",
    "print(\"Smoke test complete. Last epoch record:\", hist[\"epochs\"][-1] if hist[\"epochs\"] else \"no epochs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd101d1",
   "metadata": {},
   "source": [
    "## 8 Full training harness (all labels, bucket-aware epochs, seeds configurable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e637b67a",
   "metadata": {},
   "source": [
    "### 8a) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68a455",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF32 enabled for faster matmul on CUDA.\n",
      "Set tokenizer MAX_LEN to 192\n",
      "Graph caching enabled.\n",
      "Prevalence buckets:\n",
      "  NR-AR         prev=0.0425  bucket=mid\n",
      "  NR-AR-LBD     prev=0.0351  bucket=rare\n",
      "  NR-AhR        prev=0.1173  bucket=common\n",
      "  NR-Aromatase  prev=0.0515  bucket=mid\n",
      "  NR-ER         prev=0.1280  bucket=common\n",
      "  NR-ER-LBD     prev=0.0503  bucket=mid\n",
      "  NR-PPAR-gamma  prev=0.0288  bucket=rare\n",
      "  SR-ARE        prev=0.1615  bucket=common\n",
      "  SR-ATAD5      prev=0.0373  bucket=rare\n",
      "  SR-HSE        prev=0.0575  bucket=mid\n",
      "  SR-MMP        prev=0.1580  bucket=common\n",
      "  SR-p53        prev=0.0624  bucket=mid\n",
      "Epoch plan: {'rare': {'A': 5, 'B': 10, 'C': 12}, 'mid': {'A': 4, 'B': 8, 'C': 10}, 'common': {'A': 4, 'B': 8, 'C': 10}, 'unknown': {'A': 3, 'B': 6, 'C': 8}}\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: NR-AR  | bucket=mid  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:53:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:53:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:53:35] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0110 TR PR-AUC=0.2533 | VA PR-AUC=0.0779 (ROC=0.4280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:53:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 02] loss=0.0041 TR PR-AUC=0.2973 | VA PR-AUC=0.0761 (ROC=0.4674)\n",
      "[A][ep 03] loss=0.0040 TR PR-AUC=0.5043 | VA PR-AUC=0.0749 (ROC=0.4558)\n",
      "[A][ep 04] loss=0.0040 TR PR-AUC=0.5525 | VA PR-AUC=0.0418 (ROC=0.4353)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n",
      "[B][ep 01] loss=0.0040 TR PR-AUC=0.6265 | VA PR-AUC=0.0759 (ROC=0.4853)\n",
      "[B][ep 02] loss=0.0039 TR PR-AUC=0.7373 | VA PR-AUC=0.1031 (ROC=0.6025)\n",
      "[B][ep 03] loss=0.0040 TR PR-AUC=0.7313 | VA PR-AUC=0.2702 (ROC=0.6656)\n",
      "[B][ep 04] loss=0.0040 TR PR-AUC=0.6205 | VA PR-AUC=0.2968 (ROC=0.6838)\n",
      "[B][ep 05] loss=0.0039 TR PR-AUC=0.7536 | VA PR-AUC=0.3234 (ROC=0.6984)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:54:38] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 06] loss=0.0038 TR PR-AUC=0.7874 | VA PR-AUC=0.3266 (ROC=0.7089)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:54:44] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 07] loss=0.0038 TR PR-AUC=0.7940 | VA PR-AUC=0.3191 (ROC=0.7135)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:54:51] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 08] loss=0.0039 TR PR-AUC=0.8076 | VA PR-AUC=0.3177 (ROC=0.7192)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n",
      "[C][ep 01] loss=0.0039 TR PR-AUC=0.7907 | VA PR-AUC=0.3193 (ROC=0.7232)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:55:06] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:55:07] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 02] loss=0.0038 TR PR-AUC=0.7952 | VA PR-AUC=0.3191 (ROC=0.7256)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:55:17] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 03] loss=0.0039 TR PR-AUC=0.7759 | VA PR-AUC=0.3201 (ROC=0.7259)\n",
      "[C][ep 04] loss=0.0039 TR PR-AUC=0.7271 | VA PR-AUC=0.3215 (ROC=0.7266)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:55:29] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 05] loss=0.0039 TR PR-AUC=0.7559 | VA PR-AUC=0.3231 (ROC=0.7273)\n",
      "[C][ep 06] loss=0.0039 TR PR-AUC=0.7695 | VA PR-AUC=0.3237 (ROC=0.7276)\n",
      "[C][ep 07] loss=0.0039 TR PR-AUC=0.8146 | VA PR-AUC=0.3238 (ROC=0.7263)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:55:53] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:55:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:55:56] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 08] loss=0.0038 TR PR-AUC=0.8381 | VA PR-AUC=0.3246 (ROC=0.7256)\n",
      "[C][ep 09] loss=0.0038 TR PR-AUC=0.8421 | VA PR-AUC=0.3269 (ROC=0.7244)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:56:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:56:10] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 10] loss=0.0038 TR PR-AUC=0.8435 | VA PR-AUC=0.3303 (ROC=0.7244)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\NR-AR\\v6_20250903_155333_NR-AR_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AR\\v6_20250903_155333_NR-AR_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: NR-AR-LBD  | bucket=rare  | epochs A/B/C = 5/10/12\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:56:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:56:14] WARNING: not removing hydrogen atom without neighbors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=5 | SAM=False ===\n",
      "[A][ep 01] loss=0.0067 TR PR-AUC=0.2594 | VA PR-AUC=0.0923 (ROC=0.5086)\n",
      "[A][ep 02] loss=0.0039 TR PR-AUC=0.4032 | VA PR-AUC=0.0959 (ROC=0.5551)\n",
      "[A][ep 03] loss=0.0039 TR PR-AUC=0.6736 | VA PR-AUC=0.0989 (ROC=0.5732)\n",
      "[A][ep 04] loss=0.0039 TR PR-AUC=0.6862 | VA PR-AUC=0.0626 (ROC=0.5640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:56:34] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:56:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 05] loss=0.0038 TR PR-AUC=0.7108 | VA PR-AUC=0.0626 (ROC=0.5692)\n",
      "\n",
      "=== Stage B | epochs=10 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:56:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:56:42] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 01] loss=0.0039 TR PR-AUC=0.5469 | VA PR-AUC=0.0629 (ROC=0.5924)\n",
      "[B][ep 02] loss=0.0038 TR PR-AUC=0.7319 | VA PR-AUC=0.1052 (ROC=0.6186)\n",
      "[B][ep 03] loss=0.0038 TR PR-AUC=0.8057 | VA PR-AUC=0.1286 (ROC=0.6495)\n",
      "[B][ep 04] loss=0.0038 TR PR-AUC=0.8444 | VA PR-AUC=0.1882 (ROC=0.6870)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:57:06] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 05] loss=0.0037 TR PR-AUC=0.8763 | VA PR-AUC=0.2522 (ROC=0.7143)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:57:16] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 06] loss=0.0038 TR PR-AUC=0.8991 | VA PR-AUC=0.3069 (ROC=0.7387)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:57:19] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 07] loss=0.0038 TR PR-AUC=0.8970 | VA PR-AUC=0.3146 (ROC=0.7593)\n",
      "[B][ep 08] loss=0.0037 TR PR-AUC=0.9137 | VA PR-AUC=0.3267 (ROC=0.7802)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:57:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:57:39] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 09] loss=0.0037 TR PR-AUC=0.9132 | VA PR-AUC=0.3249 (ROC=0.7919)\n",
      "[B][ep 10] loss=0.0037 TR PR-AUC=0.9116 | VA PR-AUC=0.3355 (ROC=0.8010)\n",
      "\n",
      "=== Stage C | epochs=12 | SAM=True ===\n",
      "[C][ep 01] loss=0.0037 TR PR-AUC=0.9028 | VA PR-AUC=0.3393 (ROC=0.8064)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:57:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:57:55] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:57:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 02] loss=0.0038 TR PR-AUC=0.9032 | VA PR-AUC=0.3348 (ROC=0.8128)\n",
      "[C][ep 03] loss=0.0037 TR PR-AUC=0.9176 | VA PR-AUC=0.3388 (ROC=0.8179)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:58:09] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 04] loss=0.0037 TR PR-AUC=0.9403 | VA PR-AUC=0.3451 (ROC=0.8210)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:58:16] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 05] loss=0.0037 TR PR-AUC=0.9438 | VA PR-AUC=0.3489 (ROC=0.8235)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:58:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 06] loss=0.0037 TR PR-AUC=0.9616 | VA PR-AUC=0.3456 (ROC=0.8255)\n",
      "[C][ep 07] loss=0.0037 TR PR-AUC=0.9528 | VA PR-AUC=0.3500 (ROC=0.8277)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:58:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 08] loss=0.0037 TR PR-AUC=0.9697 | VA PR-AUC=0.3513 (ROC=0.8283)\n",
      "[C][ep 09] loss=0.0037 TR PR-AUC=0.9669 | VA PR-AUC=0.3562 (ROC=0.8298)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:58:49] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:58:53] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 10] loss=0.0037 TR PR-AUC=0.9685 | VA PR-AUC=0.3569 (ROC=0.8308)\n",
      "[C][ep 11] loss=0.0037 TR PR-AUC=0.9746 | VA PR-AUC=0.3543 (ROC=0.8313)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:59:03] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 12] loss=0.0037 TR PR-AUC=0.9731 | VA PR-AUC=0.3670 (ROC=0.8325)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\NR-AR-LBD\\v6_20250903_155615_NR-AR-LBD_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AR-LBD\\v6_20250903_155615_NR-AR-LBD_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: NR-AhR  | bucket=common  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:59:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:59:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:59:13] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:59:14] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0210 TR PR-AUC=0.2473 | VA PR-AUC=0.0951 (ROC=0.4386)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:59:16] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 02] loss=0.0043 TR PR-AUC=0.2724 | VA PR-AUC=0.0899 (ROC=0.4006)\n",
      "[A][ep 03] loss=0.0043 TR PR-AUC=0.3365 | VA PR-AUC=0.0825 (ROC=0.3806)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:59:26] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:59:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 04] loss=0.0043 TR PR-AUC=0.3907 | VA PR-AUC=0.0748 (ROC=0.3346)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:59:33] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 01] loss=0.0043 TR PR-AUC=0.3802 | VA PR-AUC=0.0714 (ROC=0.3062)\n",
      "[B][ep 02] loss=0.0042 TR PR-AUC=0.5333 | VA PR-AUC=0.0836 (ROC=0.3989)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:59:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[16:59:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 03] loss=0.0042 TR PR-AUC=0.6129 | VA PR-AUC=0.2126 (ROC=0.7182)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[16:59:50] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 04] loss=0.0042 TR PR-AUC=0.6466 | VA PR-AUC=0.4072 (ROC=0.8430)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:00:01] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 05] loss=0.0042 TR PR-AUC=0.6765 | VA PR-AUC=0.4553 (ROC=0.8562)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:00:07] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 06] loss=0.0042 TR PR-AUC=0.7038 | VA PR-AUC=0.4778 (ROC=0.8596)\n",
      "[B][ep 07] loss=0.0042 TR PR-AUC=0.7263 | VA PR-AUC=0.4833 (ROC=0.8611)\n",
      "[B][ep 08] loss=0.0042 TR PR-AUC=0.7278 | VA PR-AUC=0.4853 (ROC=0.8617)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:00:22] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 01] loss=0.0042 TR PR-AUC=0.7207 | VA PR-AUC=0.4841 (ROC=0.8629)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:00:31] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 02] loss=0.0042 TR PR-AUC=0.6702 | VA PR-AUC=0.4857 (ROC=0.8644)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:00:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 03] loss=0.0042 TR PR-AUC=0.7278 | VA PR-AUC=0.4809 (ROC=0.8653)\n",
      "[C][ep 04] loss=0.0041 TR PR-AUC=0.7673 | VA PR-AUC=0.4861 (ROC=0.8660)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:00:49] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 05] loss=0.0041 TR PR-AUC=0.7491 | VA PR-AUC=0.4888 (ROC=0.8668)\n",
      "[C][ep 06] loss=0.0041 TR PR-AUC=0.8001 | VA PR-AUC=0.4962 (ROC=0.8669)\n",
      "[C][ep 07] loss=0.0041 TR PR-AUC=0.8008 | VA PR-AUC=0.5007 (ROC=0.8666)\n",
      "[C][ep 08] loss=0.0041 TR PR-AUC=0.8174 | VA PR-AUC=0.4969 (ROC=0.8663)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:01:18] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 09] loss=0.0041 TR PR-AUC=0.8401 | VA PR-AUC=0.4976 (ROC=0.8656)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:01:24] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:01:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 10] loss=0.0041 TR PR-AUC=0.8433 | VA PR-AUC=0.4968 (ROC=0.8649)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\NR-AhR\\v6_20250903_155910_NR-AhR_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AhR\\v6_20250903_155910_NR-AhR_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: NR-Aromatase  | bucket=mid  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:01:28] WARNING: not removing hydrogen atom without neighbors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:01:32] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0119 TR PR-AUC=0.2570 | VA PR-AUC=0.0764 (ROC=0.5296)\n",
      "[A][ep 02] loss=0.0041 TR PR-AUC=0.2621 | VA PR-AUC=0.0698 (ROC=0.5041)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:01:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 03] loss=0.0041 TR PR-AUC=0.2544 | VA PR-AUC=0.0593 (ROC=0.4437)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:01:44] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 04] loss=0.0041 TR PR-AUC=0.2643 | VA PR-AUC=0.0494 (ROC=0.3825)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:01:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 01] loss=0.0041 TR PR-AUC=0.3373 | VA PR-AUC=0.0477 (ROC=0.3663)\n",
      "[B][ep 02] loss=0.0041 TR PR-AUC=0.5276 | VA PR-AUC=0.0568 (ROC=0.4554)\n",
      "[B][ep 03] loss=0.0041 TR PR-AUC=0.6615 | VA PR-AUC=0.0848 (ROC=0.6062)\n",
      "[B][ep 04] loss=0.0040 TR PR-AUC=0.6913 | VA PR-AUC=0.1184 (ROC=0.6821)\n",
      "[B][ep 05] loss=0.0040 TR PR-AUC=0.7941 | VA PR-AUC=0.1446 (ROC=0.7203)\n",
      "[B][ep 06] loss=0.0040 TR PR-AUC=0.8366 | VA PR-AUC=0.1700 (ROC=0.7459)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:02:25] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 07] loss=0.0039 TR PR-AUC=0.8611 | VA PR-AUC=0.1995 (ROC=0.7633)\n",
      "[B][ep 08] loss=0.0039 TR PR-AUC=0.8640 | VA PR-AUC=0.2458 (ROC=0.7767)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:02:35] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:02:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 01] loss=0.0039 TR PR-AUC=0.8562 | VA PR-AUC=0.2735 (ROC=0.7845)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:02:46] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:02:47] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 02] loss=0.0039 TR PR-AUC=0.8757 | VA PR-AUC=0.3084 (ROC=0.7928)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:02:47] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:02:49] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 03] loss=0.0039 TR PR-AUC=0.8757 | VA PR-AUC=0.3407 (ROC=0.7951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:02:56] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 04] loss=0.0039 TR PR-AUC=0.9100 | VA PR-AUC=0.3607 (ROC=0.7951)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:03:04] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 05] loss=0.0039 TR PR-AUC=0.9074 | VA PR-AUC=0.3630 (ROC=0.7941)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:03:06] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 06] loss=0.0039 TR PR-AUC=0.9302 | VA PR-AUC=0.3704 (ROC=0.7926)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:03:14] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 07] loss=0.0038 TR PR-AUC=0.9282 | VA PR-AUC=0.3770 (ROC=0.7915)\n",
      "[C][ep 08] loss=0.0038 TR PR-AUC=0.9346 | VA PR-AUC=0.3827 (ROC=0.7889)\n",
      "[C][ep 09] loss=0.0038 TR PR-AUC=0.9345 | VA PR-AUC=0.3927 (ROC=0.7866)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:03:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 10] loss=0.0038 TR PR-AUC=0.9393 | VA PR-AUC=0.3911 (ROC=0.7842)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\NR-Aromatase\\v6_20250903_160130_NR-Aromatase_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-Aromatase\\v6_20250903_160130_NR-Aromatase_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: NR-ER  | bucket=common  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:03:36] WARNING: not removing hydrogen atom without neighbors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:03:40] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:03:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0213 TR PR-AUC=0.2557 | VA PR-AUC=0.1387 (ROC=0.4643)\n",
      "[A][ep 02] loss=0.0043 TR PR-AUC=0.2526 | VA PR-AUC=0.1389 (ROC=0.4768)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:03:50] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 03] loss=0.0043 TR PR-AUC=0.2574 | VA PR-AUC=0.1474 (ROC=0.4729)\n",
      "[A][ep 04] loss=0.0043 TR PR-AUC=0.2593 | VA PR-AUC=0.1249 (ROC=0.4420)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:03:58] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 01] loss=0.0043 TR PR-AUC=0.2880 | VA PR-AUC=0.1198 (ROC=0.4354)\n",
      "[B][ep 02] loss=0.0043 TR PR-AUC=0.4173 | VA PR-AUC=0.1379 (ROC=0.4653)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:04:11] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 03] loss=0.0043 TR PR-AUC=0.4617 | VA PR-AUC=0.1867 (ROC=0.5629)\n",
      "[B][ep 04] loss=0.0043 TR PR-AUC=0.5105 | VA PR-AUC=0.2428 (ROC=0.6556)\n",
      "[B][ep 05] loss=0.0042 TR PR-AUC=0.5293 | VA PR-AUC=0.2674 (ROC=0.6767)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:04:29] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 06] loss=0.0042 TR PR-AUC=0.5304 | VA PR-AUC=0.2782 (ROC=0.6839)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:04:35] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 07] loss=0.0042 TR PR-AUC=0.5632 | VA PR-AUC=0.2776 (ROC=0.6873)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:04:42] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:04:43] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 08] loss=0.0042 TR PR-AUC=0.5577 | VA PR-AUC=0.2786 (ROC=0.6917)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:04:47] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 01] loss=0.0042 TR PR-AUC=0.5813 | VA PR-AUC=0.2809 (ROC=0.6942)\n",
      "[C][ep 02] loss=0.0042 TR PR-AUC=0.5810 | VA PR-AUC=0.2852 (ROC=0.6971)\n",
      "[C][ep 03] loss=0.0042 TR PR-AUC=0.5415 | VA PR-AUC=0.2879 (ROC=0.7003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:05:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:05:10] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:05:11] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 04] loss=0.0042 TR PR-AUC=0.5693 | VA PR-AUC=0.2911 (ROC=0.7032)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:05:17] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 05] loss=0.0042 TR PR-AUC=0.6063 | VA PR-AUC=0.2922 (ROC=0.7041)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:05:21] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 06] loss=0.0042 TR PR-AUC=0.6600 | VA PR-AUC=0.2940 (ROC=0.7047)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:05:32] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 07] loss=0.0042 TR PR-AUC=0.6731 | VA PR-AUC=0.3055 (ROC=0.7034)\n",
      "[C][ep 08] loss=0.0042 TR PR-AUC=0.6767 | VA PR-AUC=0.3077 (ROC=0.7018)\n",
      "[C][ep 09] loss=0.0042 TR PR-AUC=0.6620 | VA PR-AUC=0.3126 (ROC=0.7000)\n",
      "[C][ep 10] loss=0.0041 TR PR-AUC=0.6874 | VA PR-AUC=0.3103 (ROC=0.6988)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\NR-ER\\v6_20250903_160338_NR-ER_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-ER\\v6_20250903_160338_NR-ER_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: NR-ER-LBD  | bucket=mid  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:05:56] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:05:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:05:57] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:05:59] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0111 TR PR-AUC=0.2480 | VA PR-AUC=0.1286 (ROC=0.5896)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:06:01] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 02] loss=0.0041 TR PR-AUC=0.2512 | VA PR-AUC=0.1137 (ROC=0.5908)\n",
      "[A][ep 03] loss=0.0041 TR PR-AUC=0.2898 | VA PR-AUC=0.0894 (ROC=0.5803)\n",
      "[A][ep 04] loss=0.0041 TR PR-AUC=0.2968 | VA PR-AUC=0.0486 (ROC=0.5474)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n",
      "[B][ep 01] loss=0.0041 TR PR-AUC=0.4061 | VA PR-AUC=0.0579 (ROC=0.5416)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:06:28] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 02] loss=0.0040 TR PR-AUC=0.5445 | VA PR-AUC=0.0904 (ROC=0.5658)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:06:29] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 03] loss=0.0040 TR PR-AUC=0.6659 | VA PR-AUC=0.1797 (ROC=0.5837)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:06:43] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 04] loss=0.0040 TR PR-AUC=0.7339 | VA PR-AUC=0.1939 (ROC=0.5994)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:06:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:06:50] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 05] loss=0.0040 TR PR-AUC=0.7519 | VA PR-AUC=0.2037 (ROC=0.6189)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:06:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 06] loss=0.0039 TR PR-AUC=0.8225 | VA PR-AUC=0.2090 (ROC=0.6368)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:07:04] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 07] loss=0.0039 TR PR-AUC=0.8225 | VA PR-AUC=0.2186 (ROC=0.6544)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:07:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:07:11] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 08] loss=0.0039 TR PR-AUC=0.8211 | VA PR-AUC=0.2234 (ROC=0.6654)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:07:20] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 01] loss=0.0039 TR PR-AUC=0.8255 | VA PR-AUC=0.2253 (ROC=0.6728)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:07:23] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 02] loss=0.0039 TR PR-AUC=0.8082 | VA PR-AUC=0.2322 (ROC=0.6824)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:07:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 03] loss=0.0039 TR PR-AUC=0.8331 | VA PR-AUC=0.2343 (ROC=0.6900)\n",
      "[C][ep 04] loss=0.0038 TR PR-AUC=0.8765 | VA PR-AUC=0.2388 (ROC=0.6955)\n",
      "[C][ep 05] loss=0.0039 TR PR-AUC=0.8563 | VA PR-AUC=0.2384 (ROC=0.6960)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:07:54] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:07:58] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:07:59] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 06] loss=0.0038 TR PR-AUC=0.8785 | VA PR-AUC=0.2485 (ROC=0.6989)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:08:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:08:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:08:05] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 07] loss=0.0038 TR PR-AUC=0.9003 | VA PR-AUC=0.2481 (ROC=0.6994)\n",
      "[C][ep 08] loss=0.0038 TR PR-AUC=0.8938 | VA PR-AUC=0.2487 (ROC=0.6998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:08:20] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 09] loss=0.0038 TR PR-AUC=0.9106 | VA PR-AUC=0.2479 (ROC=0.6997)\n",
      "[C][ep 10] loss=0.0038 TR PR-AUC=0.8996 | VA PR-AUC=0.2496 (ROC=0.7001)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\NR-ER-LBD\\v6_20250903_160555_NR-ER-LBD_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-ER-LBD\\v6_20250903_160555_NR-ER-LBD_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: NR-PPAR-gamma  | bucket=rare  | epochs A/B/C = 5/10/12\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=5 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:08:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:08:33] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:08:35] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0068 TR PR-AUC=0.2489 | VA PR-AUC=0.0601 (ROC=0.5286)\n",
      "[A][ep 02] loss=0.0039 TR PR-AUC=0.2591 | VA PR-AUC=0.0614 (ROC=0.5407)\n",
      "[A][ep 03] loss=0.0039 TR PR-AUC=0.3008 | VA PR-AUC=0.0512 (ROC=0.5542)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:08:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 04] loss=0.0039 TR PR-AUC=0.3295 | VA PR-AUC=0.0425 (ROC=0.5383)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:08:53] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 05] loss=0.0039 TR PR-AUC=0.3481 | VA PR-AUC=0.0399 (ROC=0.5097)\n",
      "\n",
      "=== Stage B | epochs=10 | SAM=True ===\n",
      "[B][ep 01] loss=0.0039 TR PR-AUC=0.3093 | VA PR-AUC=0.0416 (ROC=0.5214)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:01] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:09:04] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 02] loss=0.0039 TR PR-AUC=0.6146 | VA PR-AUC=0.0569 (ROC=0.5946)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:09] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 03] loss=0.0038 TR PR-AUC=0.8132 | VA PR-AUC=0.0885 (ROC=0.6728)\n",
      "[B][ep 04] loss=0.0038 TR PR-AUC=0.8673 | VA PR-AUC=0.1171 (ROC=0.7315)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:20] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:09:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 05] loss=0.0038 TR PR-AUC=0.9076 | VA PR-AUC=0.1691 (ROC=0.7740)\n",
      "[B][ep 06] loss=0.0038 TR PR-AUC=0.9230 | VA PR-AUC=0.2352 (ROC=0.7969)\n",
      "[B][ep 07] loss=0.0037 TR PR-AUC=0.9362 | VA PR-AUC=0.3114 (ROC=0.8106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:41] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:09:42] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 08] loss=0.0037 TR PR-AUC=0.9395 | VA PR-AUC=0.3317 (ROC=0.8132)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:51] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 09] loss=0.0037 TR PR-AUC=0.9454 | VA PR-AUC=0.3704 (ROC=0.8140)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:09:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 10] loss=0.0037 TR PR-AUC=0.9449 | VA PR-AUC=0.3816 (ROC=0.8129)\n",
      "\n",
      "=== Stage C | epochs=12 | SAM=True ===\n",
      "[C][ep 01] loss=0.0037 TR PR-AUC=0.9318 | VA PR-AUC=0.4173 (ROC=0.8106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:10:09] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 02] loss=0.0037 TR PR-AUC=0.9436 | VA PR-AUC=0.4304 (ROC=0.8084)\n",
      "[C][ep 03] loss=0.0037 TR PR-AUC=0.9460 | VA PR-AUC=0.4344 (ROC=0.8073)\n",
      "[C][ep 04] loss=0.0037 TR PR-AUC=0.9518 | VA PR-AUC=0.4421 (ROC=0.8045)\n",
      "[C][ep 05] loss=0.0037 TR PR-AUC=0.9635 | VA PR-AUC=0.4347 (ROC=0.8026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:10:38] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 06] loss=0.0037 TR PR-AUC=0.9593 | VA PR-AUC=0.4513 (ROC=0.8014)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:10:44] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:10:46] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 07] loss=0.0037 TR PR-AUC=0.9759 | VA PR-AUC=0.4536 (ROC=0.8004)\n",
      "[C][ep 08] loss=0.0037 TR PR-AUC=0.9768 | VA PR-AUC=0.4649 (ROC=0.7999)\n",
      "[C][ep 09] loss=0.0037 TR PR-AUC=0.9813 | VA PR-AUC=0.4689 (ROC=0.7994)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:11:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:11:06] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 10] loss=0.0037 TR PR-AUC=0.9805 | VA PR-AUC=0.4792 (ROC=0.7989)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:11:09] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 11] loss=0.0036 TR PR-AUC=0.9799 | VA PR-AUC=0.4800 (ROC=0.7989)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:11:16] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:11:18] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:11:19] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 12] loss=0.0036 TR PR-AUC=0.9826 | VA PR-AUC=0.4802 (ROC=0.7984)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\NR-PPAR-gamma\\v6_20250903_160832_NR-PPAR-gamma_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-PPAR-gamma\\v6_20250903_160832_NR-PPAR-gamma_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: SR-ARE  | bucket=common  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n",
      "[A][ep 01] loss=0.0222 TR PR-AUC=0.2510 | VA PR-AUC=0.2147 (ROC=0.5209)\n",
      "[A][ep 02] loss=0.0043 TR PR-AUC=0.2480 | VA PR-AUC=0.2186 (ROC=0.5201)\n",
      "[A][ep 03] loss=0.0043 TR PR-AUC=0.2537 | VA PR-AUC=0.2079 (ROC=0.5109)\n",
      "[A][ep 04] loss=0.0043 TR PR-AUC=0.2611 | VA PR-AUC=0.1840 (ROC=0.4714)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n",
      "[B][ep 01] loss=0.0043 TR PR-AUC=0.2764 | VA PR-AUC=0.1735 (ROC=0.4459)\n",
      "[B][ep 02] loss=0.0043 TR PR-AUC=0.3531 | VA PR-AUC=0.1819 (ROC=0.4661)\n",
      "[B][ep 03] loss=0.0043 TR PR-AUC=0.3524 | VA PR-AUC=0.2013 (ROC=0.5111)\n",
      "[B][ep 04] loss=0.0043 TR PR-AUC=0.4003 | VA PR-AUC=0.2418 (ROC=0.5736)\n",
      "[B][ep 05] loss=0.0043 TR PR-AUC=0.4346 | VA PR-AUC=0.3040 (ROC=0.6325)\n",
      "[B][ep 06] loss=0.0043 TR PR-AUC=0.4710 | VA PR-AUC=0.3435 (ROC=0.6658)\n",
      "[B][ep 07] loss=0.0043 TR PR-AUC=0.5025 | VA PR-AUC=0.3665 (ROC=0.6824)\n",
      "[B][ep 08] loss=0.0043 TR PR-AUC=0.5046 | VA PR-AUC=0.3803 (ROC=0.6935)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n",
      "[C][ep 01] loss=0.0043 TR PR-AUC=0.5059 | VA PR-AUC=0.3921 (ROC=0.7021)\n",
      "[C][ep 02] loss=0.0043 TR PR-AUC=0.4896 | VA PR-AUC=0.3970 (ROC=0.7074)\n",
      "[C][ep 03] loss=0.0043 TR PR-AUC=0.5714 | VA PR-AUC=0.4054 (ROC=0.7130)\n",
      "[C][ep 04] loss=0.0043 TR PR-AUC=0.5761 | VA PR-AUC=0.4117 (ROC=0.7172)\n",
      "[C][ep 05] loss=0.0042 TR PR-AUC=0.6127 | VA PR-AUC=0.4182 (ROC=0.7206)\n",
      "[C][ep 06] loss=0.0042 TR PR-AUC=0.6313 | VA PR-AUC=0.4240 (ROC=0.7240)\n",
      "[C][ep 07] loss=0.0042 TR PR-AUC=0.6430 | VA PR-AUC=0.4305 (ROC=0.7276)\n",
      "[C][ep 08] loss=0.0042 TR PR-AUC=0.6606 | VA PR-AUC=0.4359 (ROC=0.7307)\n",
      "[C][ep 09] loss=0.0042 TR PR-AUC=0.6798 | VA PR-AUC=0.4415 (ROC=0.7327)\n",
      "[C][ep 10] loss=0.0042 TR PR-AUC=0.6946 | VA PR-AUC=0.4489 (ROC=0.7350)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\SR-ARE\\v6_20250903_161122_SR-ARE_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-ARE\\v6_20250903_161122_SR-ARE_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: SR-ATAD5  | bucket=rare  | epochs A/B/C = 5/10/12\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:13:15] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:13:17] WARNING: not removing hydrogen atom without neighbors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=5 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:13:18] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0067 TR PR-AUC=0.2519 | VA PR-AUC=0.0591 (ROC=0.4746)\n",
      "[A][ep 02] loss=0.0039 TR PR-AUC=0.3031 | VA PR-AUC=0.0632 (ROC=0.4866)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:13:27] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:13:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 03] loss=0.0039 TR PR-AUC=0.3541 | VA PR-AUC=0.0454 (ROC=0.4742)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:13:33] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 04] loss=0.0039 TR PR-AUC=0.3642 | VA PR-AUC=0.0378 (ROC=0.4722)\n",
      "[A][ep 05] loss=0.0039 TR PR-AUC=0.3918 | VA PR-AUC=0.0404 (ROC=0.5008)\n",
      "\n",
      "=== Stage B | epochs=10 | SAM=True ===\n",
      "[B][ep 01] loss=0.0039 TR PR-AUC=0.3653 | VA PR-AUC=0.0614 (ROC=0.5732)\n",
      "[B][ep 02] loss=0.0039 TR PR-AUC=0.6042 | VA PR-AUC=0.0972 (ROC=0.6487)\n",
      "[B][ep 03] loss=0.0038 TR PR-AUC=0.7849 | VA PR-AUC=0.0958 (ROC=0.6771)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:14:07] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 04] loss=0.0038 TR PR-AUC=0.8444 | VA PR-AUC=0.1014 (ROC=0.6904)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:14:18] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 05] loss=0.0038 TR PR-AUC=0.8506 | VA PR-AUC=0.1101 (ROC=0.7003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:14:23] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 06] loss=0.0038 TR PR-AUC=0.8876 | VA PR-AUC=0.1124 (ROC=0.7110)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:14:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 07] loss=0.0038 TR PR-AUC=0.9027 | VA PR-AUC=0.1237 (ROC=0.7178)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:14:39] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 08] loss=0.0037 TR PR-AUC=0.9057 | VA PR-AUC=0.1377 (ROC=0.7260)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:14:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 09] loss=0.0037 TR PR-AUC=0.9124 | VA PR-AUC=0.1457 (ROC=0.7335)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:14:50] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 10] loss=0.0037 TR PR-AUC=0.9240 | VA PR-AUC=0.1591 (ROC=0.7393)\n",
      "\n",
      "=== Stage C | epochs=12 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:14:59] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 01] loss=0.0037 TR PR-AUC=0.9100 | VA PR-AUC=0.1699 (ROC=0.7436)\n",
      "[C][ep 02] loss=0.0037 TR PR-AUC=0.9121 | VA PR-AUC=0.1764 (ROC=0.7419)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:15:16] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 03] loss=0.0037 TR PR-AUC=0.9202 | VA PR-AUC=0.1829 (ROC=0.7378)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:15:23] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:15:29] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 04] loss=0.0037 TR PR-AUC=0.9256 | VA PR-AUC=0.1869 (ROC=0.7389)\n",
      "[C][ep 05] loss=0.0037 TR PR-AUC=0.9378 | VA PR-AUC=0.1874 (ROC=0.7397)\n",
      "[C][ep 06] loss=0.0037 TR PR-AUC=0.9494 | VA PR-AUC=0.1903 (ROC=0.7412)\n",
      "[C][ep 07] loss=0.0037 TR PR-AUC=0.9536 | VA PR-AUC=0.1962 (ROC=0.7462)\n",
      "[C][ep 08] loss=0.0037 TR PR-AUC=0.9560 | VA PR-AUC=0.1920 (ROC=0.7477)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:16:05] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:16:10] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 09] loss=0.0037 TR PR-AUC=0.9623 | VA PR-AUC=0.1849 (ROC=0.7490)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:16:16] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 10] loss=0.0037 TR PR-AUC=0.9650 | VA PR-AUC=0.1855 (ROC=0.7500)\n",
      "[C][ep 11] loss=0.0037 TR PR-AUC=0.9696 | VA PR-AUC=0.1926 (ROC=0.7503)\n",
      "[C][ep 12] loss=0.0037 TR PR-AUC=0.9674 | VA PR-AUC=0.1936 (ROC=0.7512)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\SR-ATAD5\\v6_20250903_161317_SR-ATAD5_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-ATAD5\\v6_20250903_161317_SR-ATAD5_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: SR-HSE  | bucket=mid  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:16:36] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:16:37] WARNING: not removing hydrogen atom without neighbors\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:16:40] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0115 TR PR-AUC=0.2521 | VA PR-AUC=0.0752 (ROC=0.4191)\n",
      "[A][ep 02] loss=0.0041 TR PR-AUC=0.2474 | VA PR-AUC=0.0739 (ROC=0.3998)\n",
      "[A][ep 03] loss=0.0041 TR PR-AUC=0.2507 | VA PR-AUC=0.0483 (ROC=0.3808)\n",
      "[A][ep 04] loss=0.0041 TR PR-AUC=0.2503 | VA PR-AUC=0.0397 (ROC=0.3462)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:16:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 01] loss=0.0041 TR PR-AUC=0.2696 | VA PR-AUC=0.0389 (ROC=0.3497)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:17:02] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:17:06] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 02] loss=0.0041 TR PR-AUC=0.4267 | VA PR-AUC=0.0442 (ROC=0.4268)\n",
      "[B][ep 03] loss=0.0041 TR PR-AUC=0.5520 | VA PR-AUC=0.0570 (ROC=0.5366)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:17:14] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 04] loss=0.0040 TR PR-AUC=0.5853 | VA PR-AUC=0.0815 (ROC=0.6304)\n",
      "[B][ep 05] loss=0.0040 TR PR-AUC=0.6469 | VA PR-AUC=0.1145 (ROC=0.6967)\n",
      "[B][ep 06] loss=0.0040 TR PR-AUC=0.7196 | VA PR-AUC=0.1359 (ROC=0.7241)\n",
      "[B][ep 07] loss=0.0040 TR PR-AUC=0.7103 | VA PR-AUC=0.1571 (ROC=0.7418)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:17:37] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:17:39] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 08] loss=0.0040 TR PR-AUC=0.7440 | VA PR-AUC=0.1755 (ROC=0.7555)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n",
      "[C][ep 01] loss=0.0040 TR PR-AUC=0.7398 | VA PR-AUC=0.1845 (ROC=0.7656)\n",
      "[C][ep 02] loss=0.0040 TR PR-AUC=0.7717 | VA PR-AUC=0.2008 (ROC=0.7708)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:17:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 03] loss=0.0040 TR PR-AUC=0.7772 | VA PR-AUC=0.2163 (ROC=0.7740)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:03] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 04] loss=0.0039 TR PR-AUC=0.8225 | VA PR-AUC=0.2241 (ROC=0.7776)\n",
      "[C][ep 05] loss=0.0039 TR PR-AUC=0.8538 | VA PR-AUC=0.2360 (ROC=0.7796)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:12] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 06] loss=0.0039 TR PR-AUC=0.8588 | VA PR-AUC=0.2544 (ROC=0.7775)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:18] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 07] loss=0.0039 TR PR-AUC=0.9015 | VA PR-AUC=0.2663 (ROC=0.7734)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:24] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 08] loss=0.0039 TR PR-AUC=0.8867 | VA PR-AUC=0.2795 (ROC=0.7707)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:31] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 09] loss=0.0039 TR PR-AUC=0.8901 | VA PR-AUC=0.3024 (ROC=0.7680)\n",
      "[C][ep 10] loss=0.0039 TR PR-AUC=0.8956 | VA PR-AUC=0.3109 (ROC=0.7648)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\SR-HSE\\v6_20250903_161638_SR-HSE_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-HSE\\v6_20250903_161638_SR-HSE_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: SR-MMP  | bucket=common  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:18:45] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:18:47] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0218 TR PR-AUC=0.2471 | VA PR-AUC=0.1662 (ROC=0.4787)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:49] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 02] loss=0.0044 TR PR-AUC=0.2542 | VA PR-AUC=0.1730 (ROC=0.4743)\n",
      "[A][ep 03] loss=0.0043 TR PR-AUC=0.2615 | VA PR-AUC=0.1636 (ROC=0.4487)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:55] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 04] loss=0.0043 TR PR-AUC=0.2571 | VA PR-AUC=0.1473 (ROC=0.4143)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:18:59] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 01] loss=0.0043 TR PR-AUC=0.3320 | VA PR-AUC=0.1283 (ROC=0.3783)\n",
      "[B][ep 02] loss=0.0043 TR PR-AUC=0.5423 | VA PR-AUC=0.1314 (ROC=0.4109)\n",
      "[B][ep 03] loss=0.0042 TR PR-AUC=0.6324 | VA PR-AUC=0.1760 (ROC=0.5269)\n",
      "[B][ep 04] loss=0.0042 TR PR-AUC=0.6777 | VA PR-AUC=0.2513 (ROC=0.6473)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:19:24] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 05] loss=0.0042 TR PR-AUC=0.6844 | VA PR-AUC=0.3410 (ROC=0.7408)\n",
      "[B][ep 06] loss=0.0042 TR PR-AUC=0.7377 | VA PR-AUC=0.4057 (ROC=0.7945)\n",
      "[B][ep 07] loss=0.0042 TR PR-AUC=0.7344 | VA PR-AUC=0.4333 (ROC=0.8166)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:19:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 08] loss=0.0042 TR PR-AUC=0.7548 | VA PR-AUC=0.4528 (ROC=0.8286)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n",
      "[C][ep 01] loss=0.0042 TR PR-AUC=0.7422 | VA PR-AUC=0.4687 (ROC=0.8361)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:19:55] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 02] loss=0.0042 TR PR-AUC=0.7337 | VA PR-AUC=0.4816 (ROC=0.8415)\n",
      "[C][ep 03] loss=0.0042 TR PR-AUC=0.7102 | VA PR-AUC=0.4952 (ROC=0.8463)\n",
      "[C][ep 04] loss=0.0042 TR PR-AUC=0.7451 | VA PR-AUC=0.5020 (ROC=0.8495)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:20:17] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 05] loss=0.0042 TR PR-AUC=0.7810 | VA PR-AUC=0.5099 (ROC=0.8523)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:20:20] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 06] loss=0.0042 TR PR-AUC=0.7873 | VA PR-AUC=0.5242 (ROC=0.8555)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:20:25] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:20:30] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:20:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 07] loss=0.0041 TR PR-AUC=0.8205 | VA PR-AUC=0.5355 (ROC=0.8581)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:20:33] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 08] loss=0.0041 TR PR-AUC=0.8273 | VA PR-AUC=0.5400 (ROC=0.8600)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:20:39] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:20:40] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 09] loss=0.0041 TR PR-AUC=0.8358 | VA PR-AUC=0.5491 (ROC=0.8618)\n",
      "[C][ep 10] loss=0.0041 TR PR-AUC=0.8471 | VA PR-AUC=0.5549 (ROC=0.8631)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\SR-MMP\\v6_20250903_161843_SR-MMP_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-MMP\\v6_20250903_161843_SR-MMP_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "================================================================================\n",
      "TRAINING LABEL: SR-p53  | bucket=mid  | epochs A/B/C = 4/8/10\n",
      "================================================================================\n",
      "\n",
      "--- Seed 20250903 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage A | epochs=4 | SAM=False ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:20:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 01] loss=0.0114 TR PR-AUC=0.2524 | VA PR-AUC=0.1157 (ROC=0.5552)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:20:59] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[A][ep 02] loss=0.0041 TR PR-AUC=0.2654 | VA PR-AUC=0.1049 (ROC=0.5198)\n",
      "[A][ep 03] loss=0.0041 TR PR-AUC=0.2984 | VA PR-AUC=0.0789 (ROC=0.4700)\n",
      "[A][ep 04] loss=0.0041 TR PR-AUC=0.3031 | VA PR-AUC=0.0616 (ROC=0.3916)\n",
      "\n",
      "=== Stage B | epochs=8 | SAM=True ===\n",
      "[B][ep 01] loss=0.0041 TR PR-AUC=0.3448 | VA PR-AUC=0.0596 (ROC=0.3814)\n",
      "[B][ep 02] loss=0.0041 TR PR-AUC=0.4383 | VA PR-AUC=0.0679 (ROC=0.4521)\n",
      "[B][ep 03] loss=0.0040 TR PR-AUC=0.5449 | VA PR-AUC=0.0885 (ROC=0.5729)\n",
      "[B][ep 04] loss=0.0040 TR PR-AUC=0.6360 | VA PR-AUC=0.1222 (ROC=0.6782)\n",
      "[B][ep 05] loss=0.0040 TR PR-AUC=0.7505 | VA PR-AUC=0.1512 (ROC=0.7313)\n",
      "[B][ep 06] loss=0.0040 TR PR-AUC=0.8006 | VA PR-AUC=0.1726 (ROC=0.7603)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:22:00] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 07] loss=0.0039 TR PR-AUC=0.8210 | VA PR-AUC=0.1906 (ROC=0.7807)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:22:07] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:22:10] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[B][ep 08] loss=0.0039 TR PR-AUC=0.8356 | VA PR-AUC=0.2068 (ROC=0.7909)\n",
      "\n",
      "=== Stage C | epochs=10 | SAM=True ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:22:11] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:22:13] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 01] loss=0.0039 TR PR-AUC=0.8351 | VA PR-AUC=0.2231 (ROC=0.7979)\n",
      "[C][ep 02] loss=0.0039 TR PR-AUC=0.8263 | VA PR-AUC=0.2336 (ROC=0.7999)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:22:29] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:22:30] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 03] loss=0.0039 TR PR-AUC=0.8548 | VA PR-AUC=0.2473 (ROC=0.8025)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:22:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 04] loss=0.0039 TR PR-AUC=0.8700 | VA PR-AUC=0.2573 (ROC=0.8027)\n",
      "[C][ep 05] loss=0.0039 TR PR-AUC=0.8911 | VA PR-AUC=0.2841 (ROC=0.8011)\n",
      "[C][ep 06] loss=0.0039 TR PR-AUC=0.9046 | VA PR-AUC=0.2993 (ROC=0.7997)\n",
      "[C][ep 07] loss=0.0039 TR PR-AUC=0.9159 | VA PR-AUC=0.3139 (ROC=0.7984)\n",
      "[C][ep 08] loss=0.0038 TR PR-AUC=0.9184 | VA PR-AUC=0.3226 (ROC=0.7962)\n",
      "[C][ep 09] loss=0.0038 TR PR-AUC=0.9201 | VA PR-AUC=0.3236 (ROC=0.7958)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[17:23:20] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:23:20] WARNING: not removing hydrogen atom without neighbors\n",
      "[17:23:22] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[C][ep 10] loss=0.0038 TR PR-AUC=0.9326 | VA PR-AUC=0.3232 (ROC=0.7947)\n",
      "\n",
      "Saved BEST checkpoint -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\models\\checkpoints_v6\\SR-p53\\v6_20250903_162052_SR-p53_best.pt\n",
      "Saved history -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-p53\\v6_20250903_162052_SR-p53_history.json\n",
      "\n",
      "Updated manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "All requested labels trained.\n",
      "Final manifest -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\v6_training_manifest.json\n",
      "\n",
      "Training harness finished. Manifest summary keys: dict_keys(['started_utc', 'seeds', 'epoch_plan', 'labels', 'finished_utc'])\n"
     ]
    }
   ],
   "source": [
    "# -----------------------------\n",
    "# Imports & globals\n",
    "# -----------------------------\n",
    "import json, time, math, copy\n",
    "from dataclasses import dataclass\n",
    "from typing import Dict, Any, List\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "\n",
    "# ---- Required context from earlier cells (assert to fail fast) ----\n",
    "assert 'DF_V6' in globals(), \"DF_V6 not found (run Cells 1–3).\"\n",
    "assert 'DESC_COLS_V6' in globals(), \"DESC_COLS_V6 not found (run Cell 3).\"\n",
    "assert 'build_dataloaders_for_label' in globals(), \"build_dataloaders_for_label missing (run Cell 5).\"\n",
    "assert 'V6Expert' in globals(), \"V6Expert missing (run Cell 6).\"\n",
    "assert 'tok_loaded_name' in globals(), \"Tokenizer/model name missing (run Cell 5).\"\n",
    "assert 'RES_DIR' in globals() and 'CKPT_DIR' in globals(), \"Paths not set (run Cell 1).\"\n",
    "assert 'mol_to_graph' in globals(), \"mol_to_graph missing (run Cell 6).\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# -----------------------------\n",
    "# Speed tweaks: TF32 + shorter sequence length\n",
    "# -----------------------------\n",
    "try:\n",
    "    if torch.cuda.is_available():\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.set_float32_matmul_precision('high')  # PyTorch 2.x\n",
    "        print(\"TF32 enabled for faster matmul on CUDA.\")\n",
    "except Exception as e:\n",
    "    print(\"TF32 tweak skipped:\", e)\n",
    "\n",
    "# Collate_fn from Cell 5 reads MAX_LEN from global scope -> update it\n",
    "MAX_LEN = 192\n",
    "print(\"Set tokenizer MAX_LEN to\", MAX_LEN)\n",
    "\n",
    "# -----------------------------\n",
    "# Graph cache (avoid rebuilding RDKit graphs every batch)\n",
    "# -----------------------------\n",
    "_GRAPH_CACHE: Dict[str, Dict[str, torch.Tensor]] = {}\n",
    "\n",
    "def mol_to_graph_cached(smi: str, add_virtual_node: bool = True) -> Dict[str, torch.Tensor]:\n",
    "    g = _GRAPH_CACHE.get(smi)\n",
    "    if g is not None:\n",
    "        return g\n",
    "    g0 = mol_to_graph(smi, add_virtual_node=add_virtual_node)\n",
    "    # Keep cached tensors on CPU; move to DEVICE during batching\n",
    "    g = {k: v.cpu().contiguous() for k, v in g0.items()}\n",
    "    _GRAPH_CACHE[smi] = g\n",
    "    return g\n",
    "\n",
    "def build_graph_batch(smiles_list: List[str], add_virtual_node: bool = True) -> Dict[str, torch.Tensor]:\n",
    "    xs, eis, eas, batch_idx, n_nodes = [], [], [], [], []\n",
    "    node_offset = 0\n",
    "    for b_idx, smi in enumerate(smiles_list):\n",
    "        g = mol_to_graph_cached(smi, add_virtual_node=add_virtual_node)\n",
    "        x, ei, ea = g[\"x\"], g[\"edge_index\"], g[\"edge_attr\"]\n",
    "        xs.append(x)\n",
    "        eis.append(ei + node_offset)\n",
    "        eas.append(ea)\n",
    "        n = x.shape[0]\n",
    "        n_nodes.append(n)\n",
    "        batch_idx.append(torch.full((n,), b_idx, dtype=torch.long))\n",
    "        node_offset += n\n",
    "    x_cat = torch.cat(xs, dim=0)\n",
    "    ei_cat = torch.cat(eis, dim=1)\n",
    "    ea_cat = torch.cat(eas, dim=0)\n",
    "    batch_cat = torch.cat(batch_idx, dim=0)\n",
    "    dev = DEVICE\n",
    "    return {\n",
    "        \"x\": x_cat.to(dev),\n",
    "        \"edge_index\": ei_cat.to(dev),\n",
    "        \"edge_attr\": ea_cat.to(dev),\n",
    "        \"batch\": batch_cat.to(dev),\n",
    "        \"n_nodes\": torch.tensor(n_nodes, dtype=torch.long, device=dev)\n",
    "    }\n",
    "\n",
    "print(\"Graph caching enabled.\")\n",
    "\n",
    "# -----------------------------\n",
    "# Per-label hyperparams (from Cell 4)\n",
    "# -----------------------------\n",
    "HP_JSON = RES_DIR / \"per_label_hparams_v6.json\"\n",
    "with open(HP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    PER_LABEL_HP = {row[\"label\"]: row for row in json.load(f)}\n",
    "\n",
    "def get_label_hparams(label: str) -> Dict[str, Any]:\n",
    "    hp = PER_LABEL_HP[label]\n",
    "    return {\n",
    "        \"gamma_neg\": float(hp[\"gamma_neg\"]),\n",
    "        \"gamma_pos\": float(hp[\"gamma_pos\"]),\n",
    "        \"margin\": float(hp[\"margin\"]),\n",
    "        \"desc_dropout\": float(hp[\"desc_dropout\"]),\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Loss + class-balanced positive upweight\n",
    "# -----------------------------\n",
    "class ASLBceWithLogits(nn.Module):\n",
    "    def __init__(self, gamma_pos=0.0, gamma_neg=4.0, margin=0.05, w_pos=1.0):\n",
    "        super().__init__()\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.margin = margin\n",
    "        self.w_pos = w_pos\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor):\n",
    "        y = targets.float()\n",
    "        x_sig = torch.sigmoid(logits)\n",
    "        if self.margin > 0:\n",
    "            x_sig = torch.where(y < 0.5, x_sig - self.margin, x_sig)\n",
    "        pt = torch.where(y >= 0.5, x_sig, 1 - x_sig)\n",
    "        one_g = torch.where(y >= 0.5,\n",
    "                            torch.full_like(pt, self.gamma_pos),\n",
    "                            torch.full_like(pt, self.gamma_neg))\n",
    "        focal = (1 - pt) ** one_g\n",
    "        bce = F.binary_cross_entropy_with_logits(logits, y, reduction='none')\n",
    "        loss = focal * bce\n",
    "        if self.w_pos != 1.0:\n",
    "            w = torch.where(y >= 0.5, torch.full_like(y, self.w_pos), torch.ones_like(y))\n",
    "            loss = loss * w\n",
    "        return loss.mean()\n",
    "\n",
    "def effective_num_weight(n_pos: int, beta: float = 0.999) -> float:\n",
    "    if n_pos <= 0:\n",
    "        return 1.0\n",
    "    return float((1 - beta) / (1 - beta ** n_pos))\n",
    "\n",
    "# -----------------------------\n",
    "# EMA\n",
    "# -----------------------------\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay=0.999):\n",
    "        self.shadow = {}\n",
    "        self.decay = decay\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[n] = p.detach().clone()\n",
    "\n",
    "    def update(self, model: nn.Module):\n",
    "        for n, p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
    "\n",
    "    def apply_to(self, model: nn.Module):\n",
    "        self.backup = {}\n",
    "        with torch.no_grad():\n",
    "            for n, p in model.named_parameters():\n",
    "                if p.requires_grad and n in self.shadow:\n",
    "                    self.backup[n] = p.detach().clone()\n",
    "                    p.data.copy_(self.shadow[n].data)\n",
    "\n",
    "    def restore(self, model: nn.Module):\n",
    "        with torch.no_grad():\n",
    "            for n, p in model.named_parameters():\n",
    "                if p.requires_grad and n in self.backup:\n",
    "                    p.data.copy_(self.backup[n].data)\n",
    "        self.backup = {}\n",
    "\n",
    "# -----------------------------\n",
    "# SAM wrapper (fixed): recompute full forward on second step\n",
    "# -----------------------------\n",
    "class SAMWrapper:\n",
    "    def __init__(self, model: nn.Module, base_optimizer: torch.optim.Optimizer, rho: float = 0.05, eps=1e-12):\n",
    "        self.model = model\n",
    "        self.opt = base_optimizer\n",
    "        self.rho = rho\n",
    "        self.eps = eps\n",
    "        self._ew = {}\n",
    "\n",
    "    def first_step(self):\n",
    "        grads = [p.grad for p in self.model.parameters() if p.requires_grad and p.grad is not None]\n",
    "        if not grads:\n",
    "            return\n",
    "        grad_norm = torch.norm(torch.stack([g.norm(p=2) for g in grads]), p=2)\n",
    "        scale = 0.0 if grad_norm == 0 else (self.rho / (grad_norm + self.eps))\n",
    "        self._ew.clear()\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters():\n",
    "                if p.requires_grad and p.grad is not None:\n",
    "                    e_w = p.grad * scale\n",
    "                    p.add_(e_w)\n",
    "                    self._ew[p] = e_w\n",
    "\n",
    "    def second_step(self):\n",
    "        with torch.no_grad():\n",
    "            for p, e_w in self._ew.items():\n",
    "                p.sub_(e_w)\n",
    "        self._ew.clear()\n",
    "        self.opt.step()\n",
    "\n",
    "    def zero_grad(self):\n",
    "        self.opt.zero_grad(set_to_none=True)\n",
    "\n",
    "# -----------------------------\n",
    "# Utils: early stopping, metrics, freezing policy, optim/sched\n",
    "# -----------------------------\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience=4, maximize=True):\n",
    "        self.best = None\n",
    "        self.patience = patience\n",
    "        self.count = 0\n",
    "        self.maximize = maximize\n",
    "\n",
    "    def step(self, value):\n",
    "        v = value\n",
    "        if self.maximize and (v is None or not np.isfinite(v)):\n",
    "            v = -float('inf')\n",
    "        improved = False\n",
    "        if self.best is None or (self.maximize and v > self.best) or (not self.maximize and v < self.best):\n",
    "            improved = True\n",
    "            self.best = v\n",
    "            self.count = 0\n",
    "        else:\n",
    "            self.count += 1\n",
    "        return improved, self.count >= self.patience\n",
    "\n",
    "def compute_metrics_from_logits(logits: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    try:\n",
    "        probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "        out[\"pr_auc\"] = float(average_precision_score(targets, probs))\n",
    "    except Exception:\n",
    "        out[\"pr_auc\"] = float('nan')\n",
    "    try:\n",
    "        probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "        out[\"roc_auc\"] = float(roc_auc_score(targets, probs))\n",
    "    except Exception:\n",
    "        out[\"roc_auc\"] = float('nan')\n",
    "    return out\n",
    "\n",
    "def set_text_trainable(model: nn.Module, stage: str):\n",
    "    # freeze all text by default\n",
    "    for p in model.text.parameters():\n",
    "        p.requires_grad = False\n",
    "    # ensure non-text modules are trainable\n",
    "    for m in [model.text_proj, model.gnn, model.desc_mlp, model.coattn, model.gate, model.classifier]:\n",
    "        for p in m.parameters():\n",
    "            p.requires_grad = True\n",
    "    # unfreeze last 4 blocks for stages B/C\n",
    "    if stage in (\"B\", \"C\"):\n",
    "        try:\n",
    "            blocks = model.text.encoder.layer\n",
    "            for i in range(len(blocks)-4, len(blocks)):\n",
    "                for p in blocks[i].parameters():\n",
    "                    p.requires_grad = True\n",
    "        except Exception:\n",
    "            for p in model.text.parameters():\n",
    "                p.requires_grad = True  # fallback if arch differs\n",
    "\n",
    "def build_optimizer_and_sched(model, lr, weight_decay, total_steps, warmup_ratio=0.05):\n",
    "    decay, no_decay = set(), set()\n",
    "    for n, p in model.named_parameters():\n",
    "        if not p.requires_grad:\n",
    "            continue\n",
    "        if any(nd in n for nd in [\"bias\", \"LayerNorm.weight\", \"layer_norm.weight\", \"layer_norm.bias\"]):\n",
    "            no_decay.add(n)\n",
    "        else:\n",
    "            decay.add(n)\n",
    "    param_groups = [\n",
    "        {\"params\": [p for n,p in model.named_parameters() if n in decay and p.requires_grad], \"lr\": lr, \"weight_decay\": weight_decay},\n",
    "        {\"params\": [p for n,p in model.named_parameters() if n in no_decay and p.requires_grad], \"lr\": lr, \"weight_decay\": 0.0},\n",
    "    ]\n",
    "    opt = torch.optim.AdamW(param_groups, lr=lr, weight_decay=weight_decay)\n",
    "    warmup_steps = max(1, int(total_steps * warmup_ratio))\n",
    "    sched = get_cosine_schedule_with_warmup(opt, num_warmup_steps=warmup_steps, num_training_steps=total_steps)\n",
    "    return opt, sched\n",
    "\n",
    "# -----------------------------\n",
    "# Trainer for one label (Stages A/B/C)\n",
    "# -----------------------------\n",
    "@dataclass\n",
    "class StageConfig:\n",
    "    name: str\n",
    "    epochs: int\n",
    "    use_sam: bool\n",
    "    patience: int\n",
    "\n",
    "def train_label(label: str,\n",
    "                batch_size: int = 64,\n",
    "                base_lr: float = 2e-4,\n",
    "                weight_decay: float = 1e-4,\n",
    "                ema_decay: float = 0.999,\n",
    "                seed: int = 42,\n",
    "                stageA_epochs: int = 4,\n",
    "                stageB_epochs: int = 8,\n",
    "                stageC_epochs: int = 10):\n",
    "    torch.manual_seed(seed); np.random.seed(seed)\n",
    "\n",
    "    # Data\n",
    "    dl_tr, dl_va, dl_te, ntr, nva, nte = build_dataloaders_for_label(label, batch_size=batch_size)\n",
    "    steps_per_epoch = max(1, math.ceil(ntr / batch_size))\n",
    "\n",
    "    # Per-label hparams\n",
    "    hp = get_label_hparams(label)\n",
    "\n",
    "    # Class-balanced positive weight (quick pass over one epoch of train)\n",
    "    y_tr_all = []\n",
    "    it_preview = iter(dl_tr)\n",
    "    for _ in range(steps_per_epoch):\n",
    "        try:\n",
    "            bprev = next(it_preview)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        y_tr_all.extend([int(v) for v in bprev[\"y\"].tolist()])\n",
    "    n_pos = sum(1 for v in y_tr_all if v == 1)\n",
    "    w_pos = effective_num_weight(n_pos, beta=0.999)\n",
    "\n",
    "    # Model\n",
    "    model = V6Expert(text_model_name=tok_loaded_name, desc_dim=len(DESC_COLS_V6),\n",
    "                     desc_dropout=hp[\"desc_dropout\"], heads=4).to(DEVICE)\n",
    "    ema = EMA(model, decay=ema_decay)\n",
    "\n",
    "    stages = [\n",
    "        StageConfig(name=\"A\", epochs=stageA_epochs, use_sam=False, patience=4),\n",
    "        StageConfig(name=\"B\", epochs=stageB_epochs, use_sam=True,  patience=5),\n",
    "        StageConfig(name=\"C\", epochs=stageC_epochs, use_sam=True,  patience=6),\n",
    "    ]\n",
    "\n",
    "    run_id = time.strftime(\"v6_%Y%m%d_%H%M%S\", time.gmtime())\n",
    "    out_ckpt_dir = CKPT_DIR / label\n",
    "    out_res_dir  = RES_DIR  / label\n",
    "    out_ckpt_dir.mkdir(parents=True, exist_ok=True)\n",
    "    out_res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    history = {\"run_id\": run_id, \"label\": label, \"n_train\": ntr, \"n_val\": nva, \"n_test\": nte,\n",
    "               \"hparams\": hp, \"w_pos\": w_pos, \"epochs\": []}\n",
    "    best_val = -float('inf')\n",
    "    best_state = None\n",
    "\n",
    "    for stage in stages:\n",
    "        if stage.epochs <= 0:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\n=== Stage {stage.name} | epochs={stage.epochs} | SAM={stage.use_sam} ===\")\n",
    "        set_text_trainable(model, stage.name)\n",
    "\n",
    "        total_steps = stage.epochs * steps_per_epoch\n",
    "        opt, sched = build_optimizer_and_sched(model, lr=base_lr, weight_decay=weight_decay,\n",
    "                                               total_steps=total_steps, warmup_ratio=0.05)\n",
    "        sam = SAMWrapper(model, opt, rho=0.05) if stage.use_sam else None\n",
    "        stopper = EarlyStopper(patience=stage.patience, maximize=True)\n",
    "        criterion = ASLBceWithLogits(gamma_pos=hp[\"gamma_pos\"], gamma_neg=hp[\"gamma_neg\"],\n",
    "                                     margin=hp[\"margin\"], w_pos=w_pos)\n",
    "\n",
    "        for ep in range(1, stage.epochs + 1):\n",
    "            # ---- Train ----\n",
    "            model.train()\n",
    "            tr_loss = 0.0\n",
    "            tr_logits, tr_targets = [], []\n",
    "\n",
    "            it = iter(dl_tr)\n",
    "            for _ in range(steps_per_epoch):\n",
    "                try:\n",
    "                    batch = next(it)\n",
    "                except StopIteration:\n",
    "                    break\n",
    "\n",
    "                g_batch = build_graph_batch(batch[\"smiles\"], add_virtual_node=True)\n",
    "                logits, extra = model(batch, g_batch)\n",
    "                y = batch[\"y\"].to(DEVICE).float()\n",
    "\n",
    "                gate_l1 = extra[\"gate_w\"].abs().mean() * 0.01\n",
    "                loss = criterion(logits, y) + gate_l1\n",
    "\n",
    "                if sam is None:\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                    opt.step()\n",
    "                else:\n",
    "                    # SAM first step\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                    sam.first_step()\n",
    "\n",
    "                    # SAM second step: recompute everything on new graph\n",
    "                    logits2, extra2 = model(batch, g_batch)\n",
    "                    gate_l1_2 = extra2[\"gate_w\"].abs().mean() * 0.01\n",
    "                    loss2 = criterion(logits2, y) + gate_l1_2\n",
    "                    opt.zero_grad(set_to_none=True)\n",
    "                    loss2.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "                    sam.second_step()\n",
    "\n",
    "                sched.step()\n",
    "                ema.update(model)\n",
    "\n",
    "                tr_loss += float(loss.detach().cpu().item())\n",
    "                tr_logits.append(logits.detach().cpu().numpy())\n",
    "                tr_targets.append(batch[\"y\"].numpy())\n",
    "\n",
    "            if tr_logits:\n",
    "                tr_logits = np.concatenate(tr_logits)\n",
    "                tr_targets = np.concatenate(tr_targets)\n",
    "                tr_metrics = compute_metrics_from_logits(tr_logits, tr_targets)\n",
    "            else:\n",
    "                tr_metrics = {\"pr_auc\": float('nan'), \"roc_auc\": float('nan')}\n",
    "\n",
    "            # ---- Validate with EMA weights ----\n",
    "            model.eval()\n",
    "            ema.apply_to(model)\n",
    "            with torch.no_grad():\n",
    "                va_logits, va_targets = [], []\n",
    "                for batch in dl_va:\n",
    "                    g_batch = build_graph_batch(batch[\"smiles\"], add_virtual_node=True)\n",
    "                    logits, _ = model(batch, g_batch)\n",
    "                    va_logits.append(logits.detach().cpu().numpy())\n",
    "                    va_targets.append(batch[\"y\"].numpy())\n",
    "                va_logits = np.concatenate(va_logits) if va_logits else np.array([])\n",
    "                va_targets = np.concatenate(va_targets) if va_targets else np.array([])\n",
    "                va_metrics = compute_metrics_from_logits(va_logits, va_targets)\n",
    "            ema.restore(model)\n",
    "\n",
    "            ep_rec = {\n",
    "                \"stage\": stage.name,\n",
    "                \"epoch\": ep,\n",
    "                \"train_loss\": round(tr_loss / max(1, steps_per_epoch), 6),\n",
    "                \"train_pr_auc\": round(tr_metrics.get(\"pr_auc\", float('nan')), 6),\n",
    "                \"train_roc_auc\": round(tr_metrics.get(\"roc_auc\", float('nan')), 6),\n",
    "                \"val_pr_auc\": round(va_metrics.get(\"pr_auc\", float('nan')), 6),\n",
    "                \"val_roc_auc\": round(va_metrics.get(\"roc_auc\", float('nan')), 6),\n",
    "            }\n",
    "            history[\"epochs\"].append(ep_rec)\n",
    "            print(f\"[{stage.name}][ep {ep:02d}] loss={ep_rec['train_loss']:.4f} \"\n",
    "                  f\"TR PR-AUC={ep_rec['train_pr_auc']:.4f} | VA PR-AUC={ep_rec['val_pr_auc']:.4f} \"\n",
    "                  f\"(ROC={ep_rec['val_roc_auc']:.4f})\")\n",
    "\n",
    "            current = va_metrics.get(\"pr_auc\", -float('inf'))\n",
    "            improved, stop = stopper.step(current)\n",
    "            if improved and np.isfinite(current):\n",
    "                best_val = current\n",
    "                best_state = copy.deepcopy(model.state_dict())\n",
    "            if stop:\n",
    "                print(f\"Early stopping triggered on stage {stage.name} (patience={stage.patience}).\")\n",
    "                break\n",
    "\n",
    "    # Save best checkpoint + history\n",
    "    if best_state is not None:\n",
    "        ckpt_path = (CKPT_DIR / label / f\"{history['run_id']}_{label}_best.pt\")\n",
    "        torch.save(best_state, ckpt_path)\n",
    "        print(f\"\\nSaved BEST checkpoint -> {ckpt_path.resolve()}\")\n",
    "    else:\n",
    "        print(\"\\nWARNING: No best state captured; not saving checkpoint.\")\n",
    "\n",
    "    hist_path = (RES_DIR / label / f\"{history['run_id']}_{label}_history.json\")\n",
    "    with open(hist_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(history, f, indent=2)\n",
    "    print(f\"Saved history -> {hist_path.resolve()}\")\n",
    "\n",
    "    # Free VRAM between labels\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return history\n",
    "\n",
    "# -----------------------------\n",
    "# Label list + buckets + epoch plan\n",
    "# -----------------------------\n",
    "LABELS = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "def _compute_prev(series):\n",
    "    v = series[(~series.isna()) & (series != -1)]\n",
    "    return float((v == 1).mean()) if len(v) else float('nan')\n",
    "\n",
    "prev_map = {lab: _compute_prev(DF_V6[lab]) for lab in LABELS}\n",
    "def _bucket(prev):\n",
    "    if prev != prev: return \"unknown\"\n",
    "    if prev <= 0.04: return \"rare\"\n",
    "    if prev < 0.10:  return \"mid\"\n",
    "    return \"common\"\n",
    "bucket_map = {lab: _bucket(prev_map[lab]) for lab in LABELS}\n",
    "\n",
    "print(\"Prevalence buckets:\")\n",
    "for lab in LABELS:\n",
    "    print(f\"  {lab:12s}  prev={prev_map[lab]:.4f}  bucket={bucket_map[lab]}\")\n",
    "\n",
    "# Extended epochs so models have runway; early stopping still guards overfit/compute\n",
    "EPOCHS_BY_BUCKET = {\n",
    "    \"rare\":   {\"A\": 5, \"B\": 10, \"C\": 12},\n",
    "    \"mid\":    {\"A\": 4, \"B\": 8,  \"C\": 10},\n",
    "    \"common\": {\"A\": 4, \"B\": 8,  \"C\": 10},\n",
    "    \"unknown\":{\"A\": 3, \"B\": 6,  \"C\": 8},\n",
    "}\n",
    "print(\"Epoch plan:\", EPOCHS_BY_BUCKET)\n",
    "\n",
    "# -----------------------------\n",
    "# Full training harness\n",
    "# -----------------------------\n",
    "def run_full_training(\n",
    "    labels=None,\n",
    "    seeds=(20250903,),   # later: (20250903,20250905,20250907) for ensembling\n",
    "    batch_size=64,\n",
    "    base_lr=2e-4,\n",
    "    weight_decay=1e-4,\n",
    "    ema_decay=0.999,\n",
    "):\n",
    "    if labels is None:\n",
    "        labels = LABELS\n",
    "\n",
    "    run_manifest = {\n",
    "        \"started_utc\": time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime()),\n",
    "        \"seeds\": list(seeds),\n",
    "        \"epoch_plan\": EPOCHS_BY_BUCKET,\n",
    "        \"labels\": [],\n",
    "    }\n",
    "\n",
    "    for lab in labels:\n",
    "        bkt = bucket_map[lab]\n",
    "        ep_cfg = EPOCHS_BY_BUCKET.get(bkt, EPOCHS_BY_BUCKET[\"unknown\"])\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(f\"TRAINING LABEL: {lab}  | bucket={bkt}  | epochs A/B/C = {ep_cfg['A']}/{ep_cfg['B']}/{ep_cfg['C']}\")\n",
    "        print(\"=\"*80)\n",
    "\n",
    "        lab_runs = []\n",
    "        for seed in seeds:\n",
    "            print(f\"\\n--- Seed {seed} ---\")\n",
    "            hist = train_label(\n",
    "                label=lab,\n",
    "                batch_size=batch_size,\n",
    "                base_lr=base_lr,\n",
    "                weight_decay=weight_decay,\n",
    "                ema_decay=ema_decay,\n",
    "                seed=seed,\n",
    "                stageA_epochs=ep_cfg[\"A\"],\n",
    "                stageB_epochs=ep_cfg[\"B\"],\n",
    "                stageC_epochs=ep_cfg[\"C\"],\n",
    "            )\n",
    "            run_id = hist[\"run_id\"]\n",
    "            ckpt_path = CKPT_DIR / lab / f\"{run_id}_{lab}_best.pt\"\n",
    "            hist_path = RES_DIR / lab / f\"{run_id}_{lab}_history.json\"\n",
    "            last = hist[\"epochs\"][-1] if hist[\"epochs\"] else {}\n",
    "\n",
    "            lab_runs.append({\n",
    "                \"seed\": seed,\n",
    "                \"run_id\": run_id,\n",
    "                \"ckpt_path\": str(ckpt_path.resolve()),\n",
    "                \"history_path\": str(hist_path.resolve()),\n",
    "                \"last_epoch\": last,\n",
    "            })\n",
    "\n",
    "        run_manifest[\"labels\"].append({\n",
    "            \"label\": lab,\n",
    "            \"bucket\": bkt,\n",
    "            \"prevalence\": prev_map[lab],\n",
    "            \"runs\": lab_runs,\n",
    "        })\n",
    "\n",
    "        manifest_path = RES_DIR / \"v6_training_manifest.json\"\n",
    "        with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(run_manifest, f, indent=2)\n",
    "        print(f\"\\nUpdated manifest -> {manifest_path.resolve()}\")\n",
    "\n",
    "    run_manifest[\"finished_utc\"] = time.strftime('%Y-%m-%d %H:%M:%S', time.gmtime())\n",
    "    manifest_path = RES_DIR / \"v6_training_manifest.json\"\n",
    "    with open(manifest_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(run_manifest, f, indent=2)\n",
    "    print(\"\\nAll requested labels trained.\")\n",
    "    print(f\"Final manifest -> {manifest_path.resolve()}\")\n",
    "    return run_manifest\n",
    "\n",
    "# -----------------------------\n",
    "# Kickoff\n",
    "# -----------------------------\n",
    "FULL_RUN_LABELS = tuple(LABELS)  # set e.g. ('NR-AR','SR-ARE') to iterate quickly\n",
    "manifest = run_full_training(\n",
    "    labels=FULL_RUN_LABELS,\n",
    "    seeds=(20250903,),\n",
    "    batch_size=64,\n",
    "    base_lr=2e-4,\n",
    "    weight_decay=1e-4,\n",
    "    ema_decay=0.999,\n",
    ")\n",
    "print(\"\\nTraining harness finished. Manifest summary keys:\", manifest.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3b7c4f",
   "metadata": {},
   "source": [
    "### 8b) Diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6993c96e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] NR-AR        | VAL PR-AUC=0.3539 | TEST PR-AUC=0.4573 | gate(avg)=[0.2736203074455261, 0.24625906348228455, 0.48012062907218933]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] NR-AR-LBD    | VAL PR-AUC=0.3637 | TEST PR-AUC=0.3924 | gate(avg)=[0.014401929453015327, 0.012224077247083187, 0.9733739495277405]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] NR-AhR       | VAL PR-AUC=0.5078 | TEST PR-AUC=0.5297 | gate(avg)=[0.035502225160598755, 0.7477993965148926, 0.21669836342334747]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] NR-Aromatase | VAL PR-AUC=0.4170 | TEST PR-AUC=0.3570 | gate(avg)=[0.012054522521793842, 0.02349117398262024, 0.9644543528556824]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] NR-ER        | VAL PR-AUC=0.2876 | TEST PR-AUC=0.3694 | gate(avg)=[0.12805557250976562, 0.4626104235649109, 0.4093339741230011]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] NR-ER-LBD    | VAL PR-AUC=0.2397 | TEST PR-AUC=0.3085 | gate(avg)=[0.026141121983528137, 0.03442065790295601, 0.9394382238388062]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] NR-PPAR-gamma | VAL PR-AUC=0.5268 | TEST PR-AUC=0.1521 | gate(avg)=[0.018302693963050842, 0.01768289878964424, 0.96401447057724]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SR-ARE       | VAL PR-AUC=0.4906 | TEST PR-AUC=0.4389 | gate(avg)=[0.02420538105070591, 0.8384400010108948, 0.13735456764698029]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SR-ATAD5     | VAL PR-AUC=0.2159 | TEST PR-AUC=0.1703 | gate(avg)=[0.009311230853199959, 0.012542759999632835, 0.9781460165977478]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SR-HSE       | VAL PR-AUC=0.3226 | TEST PR-AUC=0.4190 | gate(avg)=[0.04463708773255348, 0.056825682520866394, 0.898537278175354]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SR-MMP       | VAL PR-AUC=0.5847 | TEST PR-AUC=0.5832 | gate(avg)=[0.048996102064847946, 0.6696927547454834, 0.28131115436553955]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-77M-MLM and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] SR-p53       | VAL PR-AUC=0.3184 | TEST PR-AUC=0.3626 | gate(avg)=[0.015522181056439877, 0.022135786712169647, 0.9623420834541321]\n",
      "\n",
      "=== Pre-calibration diagnostics (sorted by VAL PR-AUC) ===\n",
      "        label             run_id  ckpt_exists  history_exists  val_pr_auc  val_roc_auc  val_f1@0.50  val_posrate@0.50  val_prob_mean  val_prob_std  test_pr_auc  test_roc_auc  test_f1@0.50  gate_text_mean  gate_graph_mean  gate_desc_mean\n",
      "       SR-MMP v6_20250903_161843         True            True    0.584702     0.872434     0.000000          0.000000       0.072196      0.041879     0.583154      0.897463      0.000000          0.0490           0.6697          0.2813\n",
      "NR-PPAR-gamma v6_20250903_160832         True            True    0.526842     0.796728     0.451613          0.014730       0.124619      0.096816     0.152066      0.713653      0.148148          0.0183           0.0177          0.9640\n",
      "       NR-AhR v6_20250903_155910         True            True    0.507752     0.862193     0.000000          0.000000       0.064141      0.041953     0.529731      0.897757      0.000000          0.0355           0.7478          0.2167\n",
      "       SR-ARE v6_20250903_161122         True            True    0.490621     0.761181     0.000000          0.000000       0.076395      0.033975     0.438877      0.783694      0.000000          0.0242           0.8384          0.1374\n",
      " NR-Aromatase v6_20250903_160130         True            True    0.417036     0.772477     0.200000          0.007143       0.085356      0.078155     0.356959      0.800996      0.125000          0.0121           0.0235          0.9645\n",
      "    NR-AR-LBD v6_20250903_155615         True            True    0.363690     0.837019     0.235294          0.004545       0.138238      0.064409     0.392436      0.771747      0.235294          0.0144           0.0122          0.9734\n",
      "        NR-AR v6_20250903_155333         True            True    0.353911     0.711257     0.086957          0.001383       0.108833      0.045149     0.457326      0.751593      0.074074          0.2736           0.2463          0.4801\n",
      "       SR-HSE v6_20250903_161638         True            True    0.322557     0.728050     0.058824          0.001618       0.100558      0.055282     0.419039      0.810341      0.041667          0.0446           0.0568          0.8985\n",
      "       SR-p53 v6_20250903_162052         True            True    0.318418     0.791782     0.074074          0.003003       0.097896      0.069950     0.362579      0.788403      0.032787          0.0155           0.0221          0.9623\n",
      "        NR-ER v6_20250903_160338         True            True    0.287560     0.668980     0.088235          0.005119       0.075879      0.050344     0.369415      0.647596      0.095238          0.1281           0.4626          0.4093\n",
      "    NR-ER-LBD v6_20250903_160555         True            True    0.239689     0.709413     0.222222          0.004412       0.090543      0.069463     0.308524      0.686036      0.275862          0.0261           0.0344          0.9394\n",
      "     SR-ATAD5 v6_20250903_161317         True            True    0.215857     0.765774     0.303030          0.011594       0.130798      0.096429     0.170337      0.788702      0.111111          0.0093           0.0125          0.9781\n",
      "\n",
      "Saved diagnostics JSON -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\diagnostics_precal_v6.json\n",
      "Saved diagnostics CSV  -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\diagnostics_precal_v6.csv\n",
      "\n",
      "Top-5 high-probability negatives (VAL) saved per label:\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AR\\v6_20250903_155333_NR-AR_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AR-LBD\\v6_20250903_155615_NR-AR-LBD_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AhR\\v6_20250903_155910_NR-AhR_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-Aromatase\\v6_20250903_160130_NR-Aromatase_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-ER\\v6_20250903_160338_NR-ER_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-ER-LBD\\v6_20250903_160555_NR-ER-LBD_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-PPAR-gamma\\v6_20250903_160832_NR-PPAR-gamma_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-ARE\\v6_20250903_161122_SR-ARE_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-ATAD5\\v6_20250903_161317_SR-ATAD5_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-HSE\\v6_20250903_161638_SR-HSE_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-MMP\\v6_20250903_161843_SR-MMP_val_topfp.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-p53\\v6_20250903_162052_SR-p53_val_topfp.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, glob, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "# ---- Requirements from earlier cells ----\n",
    "assert 'RES_DIR' in globals() and 'CKPT_DIR' in globals(), \"Run Cell 1.\"\n",
    "assert 'LABELS' in globals(), \"Labels list missing (run Cell 8).\"\n",
    "assert 'build_dataloaders_for_label' in globals(), \"Run Cell 5.\"\n",
    "assert 'V6Expert' in globals(), \"Run Cell 6.\"\n",
    "assert 'DESC_COLS_V6' in globals(), \"Run Cell 3.\"\n",
    "assert 'tok_loaded_name' in globals(), \"Run Cell 5.\"\n",
    "assert 'build_graph_batch' in globals(), \"Run Cell 8 (it overrides with cache).\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "def _safe_metrics(logits: np.ndarray, targets: np.ndarray, threshold: float = 0.5) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    try:\n",
    "        probs = 1/(1+np.exp(-logits))\n",
    "    except Exception:\n",
    "        probs = np.clip(logits, 0, 1)  # fallback if already probs\n",
    "    # PR/ROC\n",
    "    try: out['pr_auc'] = float(average_precision_score(targets, probs))\n",
    "    except Exception: out['pr_auc'] = float('nan')\n",
    "    try: out['roc_auc'] = float(roc_auc_score(targets, probs))\n",
    "    except Exception: out['roc_auc'] = float('nan')\n",
    "    # Thresholded quick sanity\n",
    "    preds = (probs >= threshold).astype(int)\n",
    "    try:\n",
    "        p, r, f1, _ = precision_recall_fscore_support(targets, preds, average='binary', zero_division=0)\n",
    "        out['f1@0.50'] = float(f1); out['prec@0.50'] = float(p); out['rec@0.50'] = float(r)\n",
    "    except Exception:\n",
    "        out['f1@0.50'] = out['prec@0.50'] = out['rec@0.50'] = float('nan')\n",
    "    out['pos_rate@0.50'] = float(preds.mean()) if len(preds) else float('nan')\n",
    "    out['prob_mean'] = float(probs.mean()) if len(probs) else float('nan')\n",
    "    out['prob_std']  = float(probs.std()) if len(probs) else float('nan')\n",
    "    return out\n",
    "\n",
    "def _list_histories_for_label(label: str) -> List[Path]:\n",
    "    return sorted((RES_DIR / label).glob(f\"v6_*_{label}_history.json\"))\n",
    "\n",
    "def _best_run_for_label(label: str) -> Tuple[str, Path, Path]:\n",
    "    \"\"\"\n",
    "    Return (run_id, hist_path, ckpt_path) for the run with the highest VAL PR-AUC observed in its history.\n",
    "    If multiple, picks the latest among best ties.\n",
    "    \"\"\"\n",
    "    candidates = _list_histories_for_label(label)\n",
    "    best = None\n",
    "    best_score = -float('inf')\n",
    "    best_hist = None\n",
    "    for hp in candidates:\n",
    "        try:\n",
    "            with open(hp, \"r\", encoding=\"utf-8\") as f:\n",
    "                hist = json.load(f)\n",
    "            # find max val_pr_auc in epochs\n",
    "            vals = [e.get(\"val_pr_auc\", float('nan')) for e in hist.get(\"epochs\", [])]\n",
    "            vals = [v for v in vals if isinstance(v, (int,float)) and np.isfinite(v)]\n",
    "            score = max(vals) if vals else -float('inf')\n",
    "            if score > best_score or (math.isclose(score, best_score) and hp.stat().st_mtime > (best_hist.stat().st_mtime if best_hist else 0)):\n",
    "                best_score = score\n",
    "                best = hist.get(\"run_id\", hp.stem.split(f\"_{label}_history\")[0])\n",
    "                best_hist = hp\n",
    "        except Exception:\n",
    "            continue\n",
    "    if best is None:\n",
    "        # fallback to newest checkpoint if no histories\n",
    "        ckpts = sorted((CKPT_DIR / label).glob(f\"v6_*_{label}_best.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if not ckpts:\n",
    "            raise FileNotFoundError(f\"No checkpoints/histories found for label={label}\")\n",
    "        run_id = ckpts[0].stem.split(f\"_{label}_best\")[0]\n",
    "        return run_id, None, ckpts[0]\n",
    "    ckpt = CKPT_DIR / label / f\"{best}_{label}_best.pt\"\n",
    "    return best, best_hist, ckpt\n",
    "\n",
    "def _load_model_for_label(label: str, run_id: str, desc_dim: int, desc_dropout: float) -> torch.nn.Module:\n",
    "    model = V6Expert(text_model_name=tok_loaded_name, desc_dim=desc_dim, desc_dropout=desc_dropout, heads=4).to(DEVICE)\n",
    "    ckpt_path = CKPT_DIR / label / f\"{run_id}_{label}_best.pt\"\n",
    "    sd = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    missing, unexpected = model.load_state_dict(sd, strict=False)\n",
    "    if missing or unexpected:\n",
    "        print(f\"[WARN] State dict mismatches for {label}: missing={len(missing)} unexpected={len(unexpected)}\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def _eval_loader(model, dl, collect_gate=False):\n",
    "    all_logits, all_targets, all_smiles, all_molids = [], [], [], []\n",
    "    gate_sum = torch.zeros(3, device=DEVICE)\n",
    "    gate_count = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in dl:\n",
    "            g_batch = build_graph_batch(batch[\"smiles\"], add_virtual_node=True)\n",
    "            logits, extra = model(batch, g_batch)\n",
    "            all_logits.append(logits.detach().cpu().numpy())\n",
    "            all_targets.append(batch[\"y\"].numpy())\n",
    "            if collect_gate:\n",
    "                gw = extra[\"gate_w\"]  # (B,3)\n",
    "                gate_sum += gw.sum(dim=0).detach()\n",
    "                gate_count += gw.shape[0]\n",
    "            all_smiles.extend(batch[\"smiles\"])\n",
    "            all_molids.extend(batch[\"mol_id\"])\n",
    "    logits = np.concatenate(all_logits) if all_logits else np.array([])\n",
    "    targets = np.concatenate(all_targets) if all_targets else np.array([])\n",
    "    gate_mean = (gate_sum / max(1, gate_count)).detach().cpu().numpy().tolist() if collect_gate else None\n",
    "    return logits, targets, all_smiles, all_molids, gate_mean\n",
    "\n",
    "# Pull per-label descriptor dropout from per_label_hparams_v6.json\n",
    "HP_JSON = RES_DIR / \"per_label_hparams_v6.json\"\n",
    "with open(HP_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "    PER_LABEL_HP = {row[\"label\"]: row for row in json.load(f)}\n",
    "\n",
    "summary_rows = []\n",
    "detailed_fp_paths = []\n",
    "labels_scanned = 0\n",
    "\n",
    "for label in LABELS:\n",
    "    try:\n",
    "        run_id, hist_path, ckpt_path = _best_run_for_label(label)\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"[SKIP] {label}: {e}\")\n",
    "        continue\n",
    "\n",
    "    hp = PER_LABEL_HP.get(label, {})\n",
    "    desc_dropout = float(hp.get(\"desc_dropout\", 0.5))\n",
    "\n",
    "    # Load dataloaders (we only need val/test)\n",
    "    dl_tr, dl_va, dl_te, ntr, nva, nte = build_dataloaders_for_label(label, batch_size=128)\n",
    "    model = _load_model_for_label(label, run_id, desc_dim=len(DESC_COLS_V6), desc_dropout=desc_dropout)\n",
    "\n",
    "    # Evaluate VAL with gate collection\n",
    "    val_logits, val_targets, val_smiles, val_molids, gate_mean = _eval_loader(model, dl_va, collect_gate=True)\n",
    "    val_metrics = _safe_metrics(val_logits, val_targets, threshold=0.5)\n",
    "\n",
    "    # Evaluate TEST\n",
    "    test_logits, test_targets, test_smiles, test_molids, _ = _eval_loader(model, dl_te, collect_gate=False)\n",
    "    test_metrics = _safe_metrics(test_logits, test_targets, threshold=0.5)\n",
    "\n",
    "    # Top-5 high-probability negatives in VAL (potential FPs)\n",
    "    try:\n",
    "        val_probs = 1/(1+np.exp(-val_logits))\n",
    "        neg_mask = (val_targets == 0)\n",
    "        cand_idx = np.argsort(val_probs[neg_mask])[::-1][:5]\n",
    "        neg_indices = np.nonzero(neg_mask)[0][cand_idx]\n",
    "        top_fp = []\n",
    "        for i in neg_indices:\n",
    "            top_fp.append({\n",
    "                \"mol_id\": val_molids[i],\n",
    "                \"smiles\": val_smiles[i],\n",
    "                \"prob\": float(val_probs[i])\n",
    "            })\n",
    "        fp_out = {\n",
    "            \"label\": label,\n",
    "            \"run_id\": run_id,\n",
    "            \"top5_val_highprob_negatives\": top_fp\n",
    "        }\n",
    "        out_fp_path = RES_DIR / label / f\"{run_id}_{label}_val_topfp.json\"\n",
    "        with open(out_fp_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(fp_out, f, indent=2)\n",
    "        detailed_fp_paths.append(str(out_fp_path.resolve()))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    row = {\n",
    "        \"label\": label,\n",
    "        \"run_id\": run_id,\n",
    "        \"ckpt_exists\": ckpt_path.exists(),\n",
    "        \"history_exists\": bool(hist_path and Path(hist_path).exists()),\n",
    "        \"val_pr_auc\": round(val_metrics[\"pr_auc\"], 6),\n",
    "        \"val_roc_auc\": round(val_metrics[\"roc_auc\"], 6),\n",
    "        \"val_f1@0.50\": round(val_metrics[\"f1@0.50\"], 6),\n",
    "        \"val_posrate@0.50\": round(val_metrics[\"pos_rate@0.50\"], 6),\n",
    "        \"val_prob_mean\": round(val_metrics[\"prob_mean\"], 6),\n",
    "        \"val_prob_std\": round(val_metrics[\"prob_std\"], 6),\n",
    "        \"test_pr_auc\": round(test_metrics[\"pr_auc\"], 6),\n",
    "        \"test_roc_auc\": round(test_metrics[\"roc_auc\"], 6),\n",
    "        \"test_f1@0.50\": round(test_metrics[\"f1@0.50\"], 6),\n",
    "        \"gate_text_mean\": round(gate_mean[0], 4) if gate_mean else None,\n",
    "        \"gate_graph_mean\": round(gate_mean[1], 4) if gate_mean else None,\n",
    "        \"gate_desc_mean\": round(gate_mean[2], 4) if gate_mean else None,\n",
    "    }\n",
    "    summary_rows.append(row)\n",
    "    labels_scanned += 1\n",
    "    print(f\"[OK] {label:12s} | VAL PR-AUC={row['val_pr_auc']:.4f} | TEST PR-AUC={row['test_pr_auc']:.4f} | gate(avg)={gate_mean}\")\n",
    "\n",
    "# Build summary table\n",
    "diag_df = pd.DataFrame(summary_rows).sort_values(\"val_pr_auc\", ascending=False)\n",
    "print(\"\\n=== Pre-calibration diagnostics (sorted by VAL PR-AUC) ===\")\n",
    "print(diag_df.to_string(index=False))\n",
    "\n",
    "# Save diagnostics\n",
    "diag_json_path = RES_DIR / \"diagnostics_precal_v6.json\"\n",
    "diag_csv_path  = RES_DIR / \"diagnostics_precal_v6.csv\"\n",
    "with open(diag_json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary_rows, f, indent=2)\n",
    "diag_df.to_csv(diag_csv_path, index=False)\n",
    "print(f\"\\nSaved diagnostics JSON -> {diag_json_path.resolve()}\")\n",
    "print(f\"Saved diagnostics CSV  -> {diag_csv_path.resolve()}\")\n",
    "\n",
    "if detailed_fp_paths:\n",
    "    print(\"\\nTop-5 high-probability negatives (VAL) saved per label:\")\n",
    "    for p in detailed_fp_paths:\n",
    "        print(\" -\", p)\n",
    "else:\n",
    "    print(\"\\nNo FP detail files written (unexpected or no negatives).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c6fc0a",
   "metadata": {},
   "source": [
    "## 9: Inference (pre-calibration): TTA, descriptors → probs for all 12 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ffadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math, json, time\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, rdMolDescriptors, Lipinski, Crippen, QED\n",
    "import joblib\n",
    "\n",
    "# Quiet down HF warnings about pooler weights, etc.\n",
    "try:\n",
    "    from transformers.utils import logging as hf_logging\n",
    "    hf_logging.set_verbosity_error()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---- Required context from earlier cells ----\n",
    "assert 'LABELS' in globals(), \"Run Cell 8 to define LABELS.\"\n",
    "assert 'RES_DIR' in globals() and 'CKPT_DIR' in globals(), \"Run Cell 1 to set paths.\"\n",
    "assert 'V6Expert' in globals(), \"Run Cell 6 to define the model.\"\n",
    "assert 'tokenizer' in globals() and 'tok_loaded_name' in globals(), \"Run Cell 5 to load tokenizer.\"\n",
    "assert 'build_graph_batch' in globals(), \"Run Cell 8 so graph batching uses the cache.\"\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = globals().get('MAX_LEN', 192)\n",
    "\n",
    "# ---------- Load descriptor pipeline (fit on TRAIN in Cell 3) ----------\n",
    "IMPUTER_PATH = RES_DIR / \"rdkit_desc_imputer_v6.pkl\"\n",
    "SCALER_PATH  = RES_DIR / \"rdkit_desc_scaler_v6.pkl\"\n",
    "COLS_PATH    = RES_DIR / \"rdkit_desc_columns_v6.txt\"\n",
    "\n",
    "imputer = joblib.load(IMPUTER_PATH)\n",
    "scaler  = joblib.load(SCALER_PATH)\n",
    "with open(COLS_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    DESC_ORDER = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# ---------- Helpers ----------\n",
    "def _best_run_for_label(label: str) -> Tuple[str, Path]:\n",
    "    # choose best run by max VAL PR-AUC from history; fallback to newest ckpt\n",
    "    hist_candidates = sorted((RES_DIR / label).glob(f\"v6_*_{label}_history.json\"))\n",
    "    best_run, best_score, best_time = None, -float('inf'), -1\n",
    "    for hp in hist_candidates:\n",
    "        try:\n",
    "            hist = json.loads(Path(hp).read_text())\n",
    "            vals = [e.get(\"val_pr_auc\", float('nan')) for e in hist.get(\"epochs\", [])]\n",
    "            vals = [v for v in vals if isinstance(v, (int,float)) and np.isfinite(v)]\n",
    "            score = max(vals) if vals else -float('inf')\n",
    "            t = hp.stat().st_mtime\n",
    "            if score > best_score or (math.isclose(score, best_score) and t > best_time):\n",
    "                best_score, best_time = score, t\n",
    "                best_run = hist.get(\"run_id\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if best_run is None:\n",
    "        ckpts = sorted((CKPT_DIR / label).glob(f\"v6_*_{label}_best.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if not ckpts:\n",
    "            raise FileNotFoundError(f\"No checkpoints for {label}\")\n",
    "        best_run = ckpts[0].stem.split(f\"_{label}_best\")[0]\n",
    "    ckpt = CKPT_DIR / label / f\"{best_run}_{label}_best.pt\"\n",
    "    return best_run, ckpt\n",
    "\n",
    "def _load_model_for_label(label: str):\n",
    "    # get label-specific desc_dropout if present (not critical for inference but keeps structure)\n",
    "    hp_json = RES_DIR / \"per_label_hparams_v6.json\"\n",
    "    desc_dropout = 0.5\n",
    "    if hp_json.exists():\n",
    "        lab2hp = {row[\"label\"]: row for row in json.loads(hp_json.read_text())}\n",
    "        if label in lab2hp:\n",
    "            desc_dropout = float(lab2hp[label].get(\"desc_dropout\", 0.5))\n",
    "    run_id, ckpt_path = _best_run_for_label(label)\n",
    "    model = V6Expert(text_model_name=tok_loaded_name, desc_dim=len(DESC_ORDER), desc_dropout=desc_dropout, heads=4).to(DEVICE)\n",
    "    sd = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    _ = model.load_state_dict(sd, strict=False)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def _desc_dict_for_mol(mol: Chem.Mol) -> Dict[str, float]:\n",
    "    d = {}\n",
    "    if mol is None:\n",
    "        return d\n",
    "    # a small, robust subset that aligns with your training set\n",
    "    try: d[\"MolWt\"] = float(Descriptors.MolWt(mol))\n",
    "    except: pass\n",
    "    try: d[\"TPSA\"] = float(rdMolDescriptors.CalcTPSA(mol))\n",
    "    except: pass\n",
    "    try: d[\"SlogP\"] = float(Crippen.MolLogP(mol))\n",
    "    except: pass\n",
    "    try: d[\"HBD\"] = float(Lipinski.NumHDonors(mol))\n",
    "    except: pass\n",
    "    try: d[\"HBA\"] = float(Lipinski.NumHAcceptors(mol))\n",
    "    except: pass\n",
    "    try: d[\"NumRotBonds\"] = float(rdMolDescriptors.CalcNumRotatableBonds(mol))\n",
    "    except: pass\n",
    "    try: d[\"RingCount\"] = float(rdMolDescriptors.CalcNumRings(mol))\n",
    "    except: pass\n",
    "    try:\n",
    "        heavy = mol.GetNumHeavyAtoms() or 1\n",
    "        arom = sum(1 for a in mol.GetAtoms() if a.GetIsAromatic())\n",
    "        d[\"AromaticProportion\"] = float(arom / heavy)\n",
    "    except: pass\n",
    "    try: d[\"FractionCSP3\"] = float(rdMolDescriptors.CalcFractionCSP3(mol))\n",
    "    except: pass\n",
    "    try: d[\"QED\"] = float(QED.qed(mol))\n",
    "    except: pass\n",
    "    return d\n",
    "\n",
    "def _compute_desc_df(smiles: List[str]) -> np.ndarray:\n",
    "    rows = []\n",
    "    for s in smiles:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        rows.append(_desc_dict_for_mol(mol))\n",
    "    df = pd.DataFrame(rows)\n",
    "    for col in DESC_ORDER:\n",
    "        if col not in df.columns:\n",
    "            df[col] = np.nan\n",
    "    df = df[DESC_ORDER]\n",
    "    X = imputer.transform(df.values)\n",
    "    X = scaler.transform(X)\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "def _enumerate_smiles(s: str, n: int = 8) -> List[str]:\n",
    "    # include canonical + random enumerations\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None:\n",
    "        return []\n",
    "    outs = set()\n",
    "    outs.add(Chem.MolToSmiles(m, canonical=True))\n",
    "    for _ in range(max(0, n-1)):\n",
    "        outs.add(Chem.MolToSmiles(m, canonical=False, doRandom=True))\n",
    "    outs = list(outs)\n",
    "    if len(outs) < n:\n",
    "        outs = outs + outs[: (n - len(outs))]\n",
    "    return outs[:n]\n",
    "\n",
    "def _tokenize(smiles_batch: List[str]):\n",
    "    enc = tokenizer(smiles_batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "# ---------- Simple one-call predictor ----------\n",
    "def predict_simple(smiles_input: List[str], tta: int = 8, threshold: float = 0.5) -> pd.DataFrame:\n",
    "    # Validate & keep only valid SMILES\n",
    "    base = []\n",
    "    for s in smiles_input:\n",
    "        s = (s or \"\").strip()\n",
    "        if not s:\n",
    "            continue\n",
    "        if Chem.MolFromSmiles(s) is None:\n",
    "            print(f\"[WARN] Invalid SMILES skipped: {s}\")\n",
    "            continue\n",
    "        base.append(s)\n",
    "    if not base:\n",
    "        raise ValueError(\"No valid SMILES provided.\")\n",
    "\n",
    "    # Precompute descriptors for base (standardized)\n",
    "    X_desc = _compute_desc_df(base)\n",
    "    # Use canonical strings to maximize graph-cache hits\n",
    "    canon = [Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True) for s in base]\n",
    "\n",
    "    results = [{\"smiles\": s} for s in base]\n",
    "\n",
    "    for label in LABELS:\n",
    "        model = _load_model_for_label(label)\n",
    "\n",
    "        # Build TTA list\n",
    "        tta_strings, graph_strings, desc_rows, index_map = [], [], [], []\n",
    "        for bi, s in enumerate(base):\n",
    "            variants = _enumerate_smiles(s, n=tta)\n",
    "            tta_strings.extend(variants)\n",
    "            graph_strings.extend([canon[bi] if canon[bi] else s] * tta)\n",
    "            desc_rows.extend([X_desc[bi]] * tta)\n",
    "            index_map.extend([bi] * tta)\n",
    "\n",
    "        logits_all = np.zeros((len(tta_strings),), dtype=np.float32)\n",
    "        bs = 64\n",
    "        with torch.no_grad():\n",
    "            for start in range(0, len(tta_strings), bs):\n",
    "                chunk = tta_strings[start:start+bs]\n",
    "                toks = _tokenize(chunk)\n",
    "                Xd = torch.tensor(np.stack(desc_rows[start:start+bs], axis=0), dtype=torch.float32, device=DEVICE)\n",
    "                g_batch = build_graph_batch(graph_strings[start:start+bs], add_virtual_node=True)\n",
    "                batch = {\n",
    "                    \"input_ids\": toks[\"input_ids\"],\n",
    "                    \"attention_mask\": toks[\"attention_mask\"],\n",
    "                    \"desc\": Xd,\n",
    "                    \"smiles\": chunk,\n",
    "                    \"mol_id\": [f\"infer_{start+i}\" for i in range(len(chunk))],\n",
    "                }\n",
    "                logits, _ = model(batch, g_batch)\n",
    "                logits_all[start:start+len(chunk)] = logits.detach().float().cpu().numpy()\n",
    "\n",
    "        probs = 1.0 / (1.0 + np.exp(-logits_all))\n",
    "        probs_per_mol = np.zeros((len(base),), dtype=np.float32)\n",
    "        counts = np.zeros((len(base),), dtype=np.int32)\n",
    "        for t_idx, b_idx in enumerate(index_map):\n",
    "            probs_per_mol[b_idx] += probs[t_idx]\n",
    "            counts[b_idx] += 1\n",
    "        probs_per_mol /= np.maximum(1, counts)\n",
    "\n",
    "        # Attach to result rows\n",
    "        for bi in range(len(base)):\n",
    "            results[bi][f\"{label}_prob\"] = float(probs_per_mol[bi])\n",
    "            results[bi][f\"{label}_pred\"] = int(probs_per_mol[bi] >= threshold)\n",
    "\n",
    "        # free VRAM\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    # Order: smiles, then label_prob columns, then label_pred columns\n",
    "    prob_cols = sorted([c for c in df.columns if c.endswith(\"_prob\")])\n",
    "    pred_cols = sorted([c for c in df.columns if c.endswith(\"_pred\")])\n",
    "    df = df[[\"smiles\"] + prob_cols + pred_cols]\n",
    "\n",
    "    # Show\n",
    "    try:\n",
    "        from IPython.display import display\n",
    "        display(df)\n",
    "    except Exception:\n",
    "        print(df.to_string(index=False))\n",
    "    return df\n",
    "\n",
    "# ======== Put your SMILES here and run the cell ========\n",
    "SMILES_INPUT = [\n",
    " \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc6d514",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_pred = predict_simple(SMILES_INPUT, tta=8, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be3568c",
   "metadata": {},
   "source": [
    "## 10: Per-label calibration (Platt / Temperature / Isotonic) + max-F1 thresholding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68244ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "Calibrating: NR-AR\n",
      "[NR-AR] TTA total=5744  dropped=40 | mismatch_chunks=5 (texts_total=496, graphs_total=536)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\NR-AR\\v6_20250903_155333_NR-AR_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\NR-AR\\v6_20250903_155333_NR-AR_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: NR-AR-LBD\n",
      "[NR-AR-LBD] TTA total=5230  dropped=50 | mismatch_chunks=5 (texts_total=494, graphs_total=544)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\NR-AR-LBD\\v6_20250903_155615_NR-AR-LBD_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\NR-AR-LBD\\v6_20250903_155615_NR-AR-LBD_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: NR-AhR\n",
      "[NR-AhR] TTA total=5120  dropped=40 | mismatch_chunks=4 (texts_total=472, graphs_total=512)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\NR-AhR\\v6_20250903_155910_NR-AhR_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\NR-AhR\\v6_20250903_155910_NR-AhR_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: NR-Aromatase\n",
      "[NR-Aromatase] TTA total=4444  dropped=36 | mismatch_chunks=4 (texts_total=476, graphs_total=512)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\NR-Aromatase\\v6_20250903_160130_NR-Aromatase_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\NR-Aromatase\\v6_20250903_160130_NR-Aromatase_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: NR-ER\n",
      "[NR-ER] TTA total=4648  dropped=40 | mismatch_chunks=4 (texts_total=424, graphs_total=464)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\NR-ER\\v6_20250903_160338_NR-ER_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\NR-ER\\v6_20250903_160338_NR-ER_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: NR-ER-LBD\n",
      "[NR-ER-LBD] TTA total=5400  dropped=40 | mismatch_chunks=5 (texts_total=536, graphs_total=576)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\NR-ER-LBD\\v6_20250903_160555_NR-ER-LBD_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\NR-ER-LBD\\v6_20250903_160555_NR-ER-LBD_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: NR-PPAR-gamma\n",
      "[NR-PPAR-gamma] TTA total=4852  dropped=36 | mismatch_chunks=4 (texts_total=372, graphs_total=408)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\NR-PPAR-gamma\\v6_20250903_160832_NR-PPAR-gamma_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\NR-PPAR-gamma\\v6_20250903_160832_NR-PPAR-gamma_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: SR-ARE\n",
      "[SR-ARE] TTA total=4256  dropped=48 | mismatch_chunks=4 (texts_total=416, graphs_total=464)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\SR-ARE\\v6_20250903_161122_SR-ARE_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\SR-ARE\\v6_20250903_161122_SR-ARE_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: SR-ATAD5\n",
      "[SR-ATAD5] TTA total=5474  dropped=46 | mismatch_chunks=5 (texts_total=482, graphs_total=528)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\SR-ATAD5\\v6_20250903_161317_SR-ATAD5_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\SR-ATAD5\\v6_20250903_161317_SR-ATAD5_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: SR-HSE\n",
      "[SR-HSE] TTA total=4900  dropped=44 | mismatch_chunks=3 (texts_total=292, graphs_total=336)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\SR-HSE\\v6_20250903_161638_SR-HSE_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\SR-HSE\\v6_20250903_161638_SR-HSE_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: SR-MMP\n",
      "[SR-MMP] TTA total=4452  dropped=44 | mismatch_chunks=5 (texts_total=484, graphs_total=528)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\SR-MMP\\v6_20250903_161843_SR-MMP_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\SR-MMP\\v6_20250903_161843_SR-MMP_thresholds_maxF1.json\n",
      "\n",
      "======================================================================\n",
      "Calibrating: SR-p53\n",
      "[SR-p53] TTA total=5280  dropped=48 | mismatch_chunks=5 (texts_total=544, graphs_total=592)\n",
      "Saved calibration -> tox21_dualenc_v1\\results\\v6\\SR-p53\\v6_20250903_162052_SR-p53_calibration.json\n",
      "Saved thresholds -> tox21_dualenc_v1\\results\\v6\\SR-p53\\v6_20250903_162052_SR-p53_thresholds_maxF1.json\n",
      "\n",
      "=== Calibration summary (sorted by calibrated ECE) ===\n",
      "        label             run_id   method  ece_cal  brier_cal  ece_raw  brier_raw  pr_auc_raw  pr_auc_cal  t_maxF1  F1_at_t  t_Fbeta  Fbeta@1.5  tta_dropped  tta_total  mismatch_chunks\n",
      "        NR-AR v6_20250903_155333 isotonic      0.0   0.024112 0.086147   0.032665    0.248020    0.262120 0.222222 0.390244 0.222222   0.379562           40       5744                5\n",
      "    NR-AR-LBD v6_20250903_155615 isotonic      0.0   0.016031 0.120961   0.033145    0.308087    0.309506 0.230769 0.344828 0.125000   0.379562           50       5230                5\n",
      "       NR-AhR v6_20250903_155910 isotonic      0.0   0.074709 0.046023   0.089202    0.365847    0.392930 0.230769 0.440945 0.156863   0.462712           40       5120                4\n",
      " NR-Aromatase v6_20250903_160130 isotonic      0.0   0.050108 0.033194   0.054034    0.304932    0.300693 0.173913 0.347826 0.173913   0.379562           36       4444                4\n",
      "        NR-ER v6_20250903_160338 isotonic      0.0   0.093336 0.040249   0.097249    0.199382    0.195684 0.187500 0.251656 0.100000   0.316387           40       4648                4\n",
      "    NR-ER-LBD v6_20250903_160555 isotonic      0.0   0.028860 0.057004   0.033524    0.230838    0.224006 0.166667 0.274510 0.125000   0.301546           40       5400                5\n",
      "NR-PPAR-gamma v6_20250903_160832 isotonic      0.0   0.022257 0.094546   0.035227    0.478729    0.466929 0.571429 0.555556 0.571429   0.511811           36       4852                4\n",
      "       SR-ARE v6_20250903_161122 isotonic      0.0   0.147665 0.128950   0.172367    0.293520    0.302330 0.239437 0.407018 0.239437   0.451497           48       4256                4\n",
      "     SR-ATAD5 v6_20250903_161317 isotonic      0.0   0.030987 0.091969   0.043510    0.202208    0.203614 0.153846 0.260870 0.105263   0.270130           46       5474                5\n",
      "       SR-HSE v6_20250903_161638 isotonic      0.0   0.045275 0.053701   0.050001    0.204698    0.211835 0.285714 0.310345 0.100000   0.301741           44       4900                3\n",
      "       SR-MMP v6_20250903_161843 isotonic      0.0   0.095358 0.075350   0.119804    0.453198    0.455156 0.446809 0.514970 0.217391   0.545959           44       4452                5\n",
      "       SR-p53 v6_20250903_162052 isotonic      0.0   0.062668 0.025181   0.066265    0.249664    0.252596 0.177778 0.314050 0.114286   0.346220           48       5280                5\n",
      "\n",
      "Saved calibration summary CSV -> D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\calibration_summary_v6.csv\n",
      "\n",
      "Per-label calibration JSONs:\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AR\\v6_20250903_155333_NR-AR_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AR-LBD\\v6_20250903_155615_NR-AR-LBD_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-AhR\\v6_20250903_155910_NR-AhR_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-Aromatase\\v6_20250903_160130_NR-Aromatase_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-ER\\v6_20250903_160338_NR-ER_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-ER-LBD\\v6_20250903_160555_NR-ER-LBD_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\NR-PPAR-gamma\\v6_20250903_160832_NR-PPAR-gamma_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-ARE\\v6_20250903_161122_SR-ARE_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-ATAD5\\v6_20250903_161317_SR-ATAD5_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-HSE\\v6_20250903_161638_SR-HSE_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-MMP\\v6_20250903_161843_SR-MMP_calibration.json\n",
      " - D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\SR-p53\\v6_20250903_162052_SR-p53_calibration.json\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import log_loss, f1_score, precision_recall_curve, average_precision_score\n",
    "\n",
    "# ---- Context asserts ----\n",
    "assert 'LABELS' in globals()\n",
    "assert 'RES_DIR' in globals() and 'CKPT_DIR' in globals()\n",
    "assert 'build_dataloaders_for_label' in globals()\n",
    "assert 'V6Expert' in globals()\n",
    "assert 'DESC_COLS_V6' in globals()\n",
    "assert 'tokenizer' in globals() and 'tok_loaded_name' in globals()\n",
    "assert 'build_graph_batch' in globals()  # we still use the cached per-mol builder internally\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = globals().get('MAX_LEN', 192)\n",
    "\n",
    "# Prefer cached graph builder if present\n",
    "_mol_to_graph = globals().get('mol_to_graph_cached', globals().get('mol_to_graph'))\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics + helpers\n",
    "# -----------------------------\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def ece_score(probs: np.ndarray, targets: np.ndarray, n_bins: int = 15) -> float:\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    inds = np.digitize(probs, bins) - 1\n",
    "    ece = 0.0\n",
    "    N = len(probs)\n",
    "    for b in range(n_bins):\n",
    "        mask = inds == b\n",
    "        if not np.any(mask): \n",
    "            continue\n",
    "        conf = probs[mask].mean()\n",
    "        acc  = targets[mask].mean()\n",
    "        ece += (np.sum(mask) / N) * abs(acc - conf)\n",
    "    return float(ece)\n",
    "\n",
    "def brier_score(probs: np.ndarray, targets: np.ndarray) -> float:\n",
    "    return float(np.mean((probs - targets) ** 2))\n",
    "\n",
    "def best_threshold_by_f1(probs: np.ndarray, targets: np.ndarray) -> Dict[str, float]:\n",
    "    precision, recall, thresh = precision_recall_curve(targets, probs)\n",
    "    best_f1, best_t = -1.0, 0.5\n",
    "    for t in thresh:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        f1 = f1_score(targets, preds, zero_division=0)\n",
    "        if f1 > best_f1:\n",
    "            best_f1, best_t = f1, float(t)\n",
    "    # Fbeta=1.5 for reference\n",
    "    beta = 1.5\n",
    "    best_fb, best_tb = -1.0, 0.5\n",
    "    for t in thresh:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        tp = ((preds == 1) & (targets == 1)).sum()\n",
    "        p  = tp / max(1, preds.sum())\n",
    "        r  = tp / max(1, (targets == 1).sum())\n",
    "        fb = (1 + beta**2) * (p * r) / max(1e-12, (beta**2) * p + r)\n",
    "        if fb > best_fb:\n",
    "            best_fb, best_tb = fb, float(t)\n",
    "    return {\"t_maxF1\": best_t, \"F1\": float(best_f1), \"t_Fbeta\": best_tb, \"Fbeta\": float(best_fb), \"beta\": beta}\n",
    "\n",
    "# -----------------------------\n",
    "# Model discovery / loading\n",
    "# -----------------------------\n",
    "def _best_run_for_label(label: str) -> Tuple[str, Path]:\n",
    "    hist_candidates = sorted((RES_DIR / label).glob(f\"v6_*_{label}_history.json\"))\n",
    "    best_run, best_score, best_time = None, -float('inf'), -1\n",
    "    for hp in hist_candidates:\n",
    "        try:\n",
    "            hist = json.loads(Path(hp).read_text())\n",
    "            vals = [e.get(\"val_pr_auc\", float('nan')) for e in hist.get(\"epochs\", [])]\n",
    "            vals = [v for v in vals if isinstance(v, (int,float)) and np.isfinite(v)]\n",
    "            score = max(vals) if vals else -float('inf')\n",
    "            t = hp.stat().st_mtime\n",
    "            if score > best_score or (math.isclose(score, best_score) and t > best_time):\n",
    "                best_score, best_time = score, t\n",
    "                best_run = hist.get(\"run_id\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if best_run is None:\n",
    "        ckpts = sorted((CKPT_DIR / label).glob(f\"v6_*_{label}_best.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if not ckpts:\n",
    "            raise FileNotFoundError(f\"No checkpoints for {label}\")\n",
    "        best_run = ckpts[0].stem.split(f\"_{label}_best\")[0]\n",
    "    ckpt = CKPT_DIR / label / f\"{best_run}_{label}_best.pt\"\n",
    "    return best_run, ckpt\n",
    "\n",
    "def _load_model_for_label(label: str):\n",
    "    # keep desc_dropout consistent with per-label hparams\n",
    "    hp_json = RES_DIR / \"per_label_hparams_v6.json\"\n",
    "    desc_dropout = 0.5\n",
    "    if hp_json.exists():\n",
    "        lab2hp = {row[\"label\"]: row for row in json.loads(hp_json.read_text())}\n",
    "        if label in lab2hp:\n",
    "            desc_dropout = float(lab2hp[label].get(\"desc_dropout\", 0.5))\n",
    "    run_id, ckpt_path = _best_run_for_label(label)\n",
    "    model = V6Expert(text_model_name=tok_loaded_name, desc_dim=len(DESC_COLS_V6), desc_dropout=desc_dropout, heads=4).to(DEVICE)\n",
    "    sd = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    _ = model.load_state_dict(sd, strict=False)\n",
    "    model.eval()\n",
    "    return model, run_id, ckpt_path\n",
    "\n",
    "# -----------------------------\n",
    "# Tokenizer + TTA + graph assembly\n",
    "# -----------------------------\n",
    "from rdkit import Chem\n",
    "\n",
    "def _tokenize(smiles_batch: List[str]):\n",
    "    enc = tokenizer(smiles_batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "def _enumerate_smiles(s: str, n: int = 8) -> List[str]:\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None:\n",
    "        return []\n",
    "    outs = set()\n",
    "    outs.add(Chem.MolToSmiles(m, canonical=True))\n",
    "    for _ in range(max(0, n-1)):\n",
    "        outs.add(Chem.MolToSmiles(m, canonical=False, doRandom=True))\n",
    "    outs = list(outs)\n",
    "    if len(outs) < n:\n",
    "        outs = outs + outs[: (n - len(outs))]\n",
    "    return outs[:n]\n",
    "\n",
    "def _assemble_graph_batch(graphs: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    xs, eis, eas, batch_idx, n_nodes = [], [], [], [], []\n",
    "    node_offset = 0\n",
    "    for b_idx, g in enumerate(graphs):\n",
    "        x = g[\"x\"].cpu(); ei = g[\"edge_index\"].cpu(); ea = g[\"edge_attr\"].cpu()\n",
    "        n = x.shape[0]\n",
    "        xs.append(x)\n",
    "        eis.append(ei + node_offset)\n",
    "        eas.append(ea)\n",
    "        batch_idx.append(torch.full((n,), b_idx, dtype=torch.long))\n",
    "        n_nodes.append(n)\n",
    "        node_offset += n\n",
    "    dev = DEVICE\n",
    "    return {\n",
    "        \"x\": torch.cat(xs, dim=0).to(dev),\n",
    "        \"edge_index\": torch.cat(eis, dim=1).to(dev),\n",
    "        \"edge_attr\": torch.cat(eas, dim=0).to(dev),\n",
    "        \"batch\": torch.cat(batch_idx, dim=0).to(dev),\n",
    "        \"n_nodes\": torch.tensor(n_nodes, dtype=torch.long, device=dev)\n",
    "    }\n",
    "\n",
    "# -----------------------------\n",
    "# Safe forward (trim to common K if #texts != #graphs)\n",
    "# -----------------------------\n",
    "def _trim_to_first_k(batch: Dict, g_batch: Dict, k: int) -> Tuple[Dict, Dict]:\n",
    "    # trim texts/desc\n",
    "    batch_trim = {\n",
    "        \"input_ids\": batch[\"input_ids\"][:k],\n",
    "        \"attention_mask\": batch[\"attention_mask\"][:k],\n",
    "        \"desc\": batch[\"desc\"][:k],\n",
    "        \"smiles\": batch.get(\"smiles\", [])[:k],\n",
    "        \"mol_id\": batch.get(\"mol_id\", [])[:k],\n",
    "    }\n",
    "    # trim graphs: keep first k graphs worth of nodes\n",
    "    n_nodes = g_batch[\"n_nodes\"].tolist()\n",
    "    k = min(k, len(n_nodes))\n",
    "    node_cut = int(np.sum(n_nodes[:k]))\n",
    "    x = g_batch[\"x\"][:node_cut]\n",
    "    batch_vec = g_batch[\"batch\"][:node_cut]\n",
    "    ea = g_batch[\"edge_attr\"]\n",
    "    ei = g_batch[\"edge_index\"]\n",
    "    mask_edges = (ei[0] < node_cut) & (ei[1] < node_cut)\n",
    "    ei = ei[:, mask_edges]\n",
    "    ea = ea[mask_edges]\n",
    "    g_trim = {\n",
    "        \"x\": x,\n",
    "        \"edge_index\": ei,\n",
    "        \"edge_attr\": ea,\n",
    "        \"batch\": batch_vec,\n",
    "        \"n_nodes\": torch.tensor(n_nodes[:k], dtype=torch.long, device=x.device)\n",
    "    }\n",
    "    return batch_trim, g_trim\n",
    "\n",
    "def forward_with_trim(model, batch: Dict, g_batch: Dict, debug_state: Dict) -> torch.Tensor:\n",
    "    B_text = int(batch[\"input_ids\"].shape[0])\n",
    "    B_graph = int(g_batch[\"n_nodes\"].shape[0])\n",
    "    if B_text != B_graph:\n",
    "        debug_state[\"mismatch_chunks\"] += 1\n",
    "        debug_state[\"texts_total\"] += B_text\n",
    "        debug_state[\"graphs_total\"] += B_graph\n",
    "        K = min(B_text, B_graph)\n",
    "        if K == 0:\n",
    "            return None  # skip this mini-batch\n",
    "        batch, g_batch = _trim_to_first_k(batch, g_batch, K)\n",
    "    logits, _extra = model(batch, g_batch)\n",
    "    return logits\n",
    "\n",
    "# -----------------------------\n",
    "# VAL TTA collection (guaranteed alignment + trimming)\n",
    "# -----------------------------\n",
    "def collect_val_logits_averaged(label: str, tta: int = 8, batch_size: int = 128) -> Tuple[np.ndarray, np.ndarray, Dict[str,int]]:\n",
    "    _, dl_va, _, _, _, _ = build_dataloaders_for_label(label, batch_size=128)\n",
    "    model, run_id, _ = _load_model_for_label(label)\n",
    "\n",
    "    dropped_tta = 0\n",
    "    total_tta = 0\n",
    "    dbg_mismatch = {\"mismatch_chunks\": 0, \"texts_total\": 0, \"graphs_total\": 0}\n",
    "\n",
    "    logits_batches, targets_batches = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in dl_va:\n",
    "            smiles = batch[\"smiles\"]\n",
    "            y      = batch[\"y\"].numpy()\n",
    "            desc   = batch[\"desc\"]  # standardized (B, D)\n",
    "\n",
    "            # Build all TTA variants for this VAL mini-batch\n",
    "            tta_strings, graph_strings, desc_rows, index_map = [], [], [], []\n",
    "            canon = [Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True) for s in smiles]\n",
    "            for bi, s in enumerate(smiles):\n",
    "                variants = _enumerate_smiles(s, n=tta)\n",
    "                tta_strings.extend(variants)\n",
    "                graph_strings.extend([canon[bi] if canon[bi] else s] * tta)\n",
    "                desc_rows.extend([desc[bi].cpu().numpy()] * tta)\n",
    "                index_map.extend([bi] * tta)\n",
    "\n",
    "            total_tta += len(tta_strings)\n",
    "\n",
    "            acc_logits, acc_bidx = [], []\n",
    "            for start in range(0, len(tta_strings), batch_size):\n",
    "                chunk = tta_strings[start:start+batch_size]\n",
    "                gchunk = graph_strings[start:start+batch_size]\n",
    "                dchunk = np.stack(desc_rows[start:start+batch_size], axis=0)\n",
    "                imap   = index_map[start:start+batch_size]\n",
    "\n",
    "                # Build aligned kept lists entry-by-entry\n",
    "                kept_graphs, kept_tokens, kept_desc, kept_imap = [], [], [], []\n",
    "                for jj, smi in enumerate(gchunk):\n",
    "                    try:\n",
    "                        g = _mol_to_graph(smi, add_virtual_node=True)\n",
    "                        x = g.get(\"x\", None)\n",
    "                        if x is None or x.numel() == 0 or x.shape[0] == 0:\n",
    "                            dropped_tta += 1\n",
    "                            continue\n",
    "                        kept_graphs.append(g)\n",
    "                        kept_tokens.append(chunk[jj])\n",
    "                        kept_desc.append(dchunk[jj])\n",
    "                        kept_imap.append(imap[jj])\n",
    "                    except Exception:\n",
    "                        dropped_tta += 1\n",
    "                        continue\n",
    "\n",
    "                if not kept_graphs:\n",
    "                    continue\n",
    "\n",
    "                toks = _tokenize(kept_tokens)\n",
    "                Xd = torch.tensor(np.stack(kept_desc, axis=0), dtype=torch.float32, device=DEVICE)\n",
    "                g_batch = _assemble_graph_batch(kept_graphs)\n",
    "                mini = {\n",
    "                    \"input_ids\": toks[\"input_ids\"],\n",
    "                    \"attention_mask\": toks[\"attention_mask\"],\n",
    "                    \"desc\": Xd,\n",
    "                    \"smiles\": kept_tokens,\n",
    "                    \"mol_id\": [f\"val_{start+i}\" for i in range(len(kept_tokens))],\n",
    "                }\n",
    "                logits = forward_with_trim(model, mini, g_batch, dbg_mismatch)\n",
    "                if logits is None:\n",
    "                    continue\n",
    "                acc_logits.extend(logits.detach().float().cpu().numpy().tolist())\n",
    "                acc_bidx.extend(kept_imap[: logits.shape[0]])  # safety slice\n",
    "\n",
    "            if len(smiles) > 0:\n",
    "                L_sum = np.zeros((len(smiles),), dtype=np.float32)\n",
    "                C = np.zeros((len(smiles),), dtype=np.int32)\n",
    "                for l, bi in zip(acc_logits, acc_bidx):\n",
    "                    if 0 <= bi < len(smiles):\n",
    "                        L_sum[bi] += l\n",
    "                        C[bi]     += 1\n",
    "                L_avg = L_sum / np.maximum(1, C)  # 0 kept TTA → logit 0 → prob 0.5\n",
    "                logits_batches.append(L_avg)\n",
    "                targets_batches.append(y)\n",
    "\n",
    "    logits = np.concatenate(logits_batches) if logits_batches else np.array([])\n",
    "    targets = np.concatenate(targets_batches) if targets_batches else np.array([])\n",
    "    debug = {\n",
    "        \"tta_total\": int(total_tta),\n",
    "        \"tta_dropped\": int(dropped_tta),\n",
    "        \"mismatch_chunks\": int(dbg_mismatch[\"mismatch_chunks\"]),\n",
    "        \"texts_total_in_mismatches\": int(dbg_mismatch[\"texts_total\"]),\n",
    "        \"graphs_total_in_mismatches\": int(dbg_mismatch[\"graphs_total\"]),\n",
    "    }\n",
    "    return logits, targets, debug\n",
    "\n",
    "# -----------------------------\n",
    "# Calibrators\n",
    "# -----------------------------\n",
    "def fit_platt(logits: np.ndarray, targets: np.ndarray) -> Dict:\n",
    "    X = logits.reshape(-1, 1); y = targets.astype(int)\n",
    "    lr = LogisticRegression(C=1e6, solver='lbfgs', max_iter=1000)\n",
    "    lr.fit(X, y)\n",
    "    a = float(lr.coef_[0][0]); b = float(lr.intercept_[0])\n",
    "    probs = sigmoid(a*logits + b)\n",
    "    return {\"a\": a, \"b\": b, \"probs\": probs}\n",
    "\n",
    "def fit_temperature(logits: np.ndarray, targets: np.ndarray, steps: int = 300, lr: float = 0.05) -> Dict:\n",
    "    t = torch.nn.Parameter(torch.tensor(1.0, device='cpu', dtype=torch.float32))\n",
    "    opt = torch.optim.LBFGS([t], lr=lr, max_iter=steps, line_search_fn=\"strong_wolfe\")\n",
    "    X = torch.tensor(logits.reshape(-1,1), dtype=torch.float32)\n",
    "    y = torch.tensor(targets.reshape(-1,1), dtype=torch.float32)\n",
    "    def closure():\n",
    "        opt.zero_grad()\n",
    "        T = torch.nn.functional.softplus(t) + 1e-6\n",
    "        z = X / T\n",
    "        loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)\n",
    "        loss.backward()\n",
    "        return loss\n",
    "    opt.step(closure)\n",
    "    T = float(torch.nn.functional.softplus(t).item() + 1e-6)\n",
    "    probs = sigmoid(logits / T)\n",
    "    return {\"T\": T, \"probs\": probs}\n",
    "\n",
    "def fit_isotonic(logits: np.ndarray, targets: np.ndarray) -> Dict:\n",
    "    raw_probs = sigmoid(logits)\n",
    "    ir = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    ir.fit(raw_probs, targets)\n",
    "    p_cal = ir.predict(raw_probs)\n",
    "    return {\"thresholds\": ir.X_thresholds_.tolist(), \"y_values\": ir.y_thresholds_.tolist(), \"probs\": p_cal}\n",
    "\n",
    "def choose_by_ece_brier(cands: Dict[str, Dict], targets: np.ndarray, bins: int = 15) -> Tuple[str, Dict, Dict[str, Dict]]:\n",
    "    scores = {}\n",
    "    for name, obj in cands.items():\n",
    "        probs = np.clip(obj[\"probs\"], 1e-8, 1 - 1e-8)\n",
    "        scores[name] = {\n",
    "            \"ece\": ece_score(probs, targets, n_bins=bins),\n",
    "            \"brier\": brier_score(probs, targets),\n",
    "            \"logloss\": float(log_loss(targets, probs, labels=[0,1])),\n",
    "        }\n",
    "    best = sorted(scores.items(), key=lambda kv: (kv[1][\"ece\"], kv[1][\"brier\"]))[0][0]\n",
    "    return best, scores[best], scores\n",
    "\n",
    "def apply_calibrated_probs(method: str, params: Dict, logits: np.ndarray) -> np.ndarray:\n",
    "    if method == \"platt\":\n",
    "        return sigmoid(params[\"a\"] * logits + params[\"b\"])\n",
    "    elif method == \"temperature\":\n",
    "        return sigmoid(logits / params[\"T\"])\n",
    "    elif method == \"isotonic\":\n",
    "        p = sigmoid(logits)\n",
    "        xs = np.array(params[\"thresholds\"], dtype=np.float64)\n",
    "        ys = np.array(params[\"y_values\"], dtype=np.float64)\n",
    "        return np.interp(p, xs, ys).astype(np.float32)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method {method}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Main calibration loop (+ debug summary)\n",
    "# -----------------------------\n",
    "summary_rows = []\n",
    "calib_paths = []\n",
    "\n",
    "for label in LABELS:\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"Calibrating: {label}\")\n",
    "    model, run_id, _ = _load_model_for_label(label)\n",
    "\n",
    "    logits, targets, dbg = collect_val_logits_averaged(label, tta=8, batch_size=128)\n",
    "    print(f\"[{label}] TTA total={dbg['tta_total']}  dropped={dbg['tta_dropped']} | mismatch_chunks={dbg['mismatch_chunks']} \"\n",
    "          f\"(texts_total={dbg['texts_total_in_mismatches']}, graphs_total={dbg['graphs_total_in_mismatches']})\")\n",
    "    if logits.size == 0:\n",
    "        print(f\"[WARN] No VAL data for {label}; skipping.\")\n",
    "        del model\n",
    "        if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        continue\n",
    "\n",
    "    # Fit calibrators\n",
    "    platt = fit_platt(logits, targets)\n",
    "    temp  = fit_temperature(logits, targets, steps=300, lr=0.05)\n",
    "    iso   = fit_isotonic(logits, targets)\n",
    "\n",
    "    # Choose by ECE (tie-break Brier)\n",
    "    best_name, best_score, all_scores = choose_by_ece_brier(\n",
    "        {\"platt\": platt, \"temperature\": temp, \"isotonic\": iso}, targets, bins=15\n",
    "    )\n",
    "    if best_name == \"platt\":\n",
    "        params = {\"a\": platt[\"a\"], \"b\": platt[\"b\"]}\n",
    "        probs_cal = np.clip(platt[\"probs\"], 1e-8, 1 - 1e-8)\n",
    "    elif best_name == \"temperature\":\n",
    "        params = {\"T\": temp[\"T\"]}\n",
    "        probs_cal = np.clip(temp[\"probs\"], 1e-8, 1 - 1e-8)\n",
    "    else:\n",
    "        params = {\"thresholds\": iso[\"thresholds\"], \"y_values\": iso[\"y_values\"]}\n",
    "        probs_cal = np.clip(iso[\"probs\"], 1e-8, 1 - 1e-8)\n",
    "\n",
    "    # Uncalibrated reference\n",
    "    probs_raw = sigmoid(logits)\n",
    "    ref_ece   = ece_score(probs_raw, targets, n_bins=15)\n",
    "    ref_brier = brier_score(probs_raw, targets)\n",
    "\n",
    "    # Thresholds (calibrated VAL probs)\n",
    "    th = best_threshold_by_f1(probs_cal, targets)\n",
    "\n",
    "    # Save per-label calibration + thresholds\n",
    "    out_dir = RES_DIR / label\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    calib_path = out_dir / f\"{run_id}_{label}_calibration.json\"\n",
    "    with open(calib_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\n",
    "            \"label\": label,\n",
    "            \"run_id\": run_id,\n",
    "            \"method\": best_name,\n",
    "            \"bins\": 15,\n",
    "            \"chosen_by\": {\"ece\": best_score[\"ece\"], \"brier\": best_score[\"brier\"]},\n",
    "            \"params\": params,\n",
    "            \"uncalibrated\": {\"ece\": ref_ece, \"brier\": ref_brier},\n",
    "            \"all_scores\": all_scores,\n",
    "            \"tta_debug\": dbg,\n",
    "        }, f, indent=2)\n",
    "    print(f\"Saved calibration -> {calib_path}\")\n",
    "    calib_paths.append(str(calib_path.resolve()))\n",
    "\n",
    "    thr_path = out_dir / f\"{run_id}_{label}_thresholds_maxF1.json\"\n",
    "    with open(thr_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(th, f, indent=2)\n",
    "    print(f\"Saved thresholds -> {thr_path}\")\n",
    "\n",
    "    # Summary row\n",
    "    pr_cal = float(average_precision_score(targets, probs_cal))\n",
    "    pr_raw = float(average_precision_score(targets, probs_raw))\n",
    "    summary_rows.append({\n",
    "        \"label\": label,\n",
    "        \"run_id\": run_id,\n",
    "        \"method\": best_name,\n",
    "        \"ece_cal\": round(best_score[\"ece\"], 6),\n",
    "        \"brier_cal\": round(best_score[\"brier\"], 6),\n",
    "        \"ece_raw\": round(ref_ece, 6),\n",
    "        \"brier_raw\": round(ref_brier, 6),\n",
    "        \"pr_auc_raw\": round(pr_raw, 6),\n",
    "        \"pr_auc_cal\": round(pr_cal, 6),\n",
    "        \"t_maxF1\": round(th[\"t_maxF1\"], 6),\n",
    "        \"F1_at_t\": round(th[\"F1\"], 6),\n",
    "        \"t_Fbeta\": round(th[\"t_Fbeta\"], 6),\n",
    "        \"Fbeta@1.5\": round(th[\"Fbeta\"], 6),\n",
    "        \"tta_dropped\": dbg[\"tta_dropped\"],\n",
    "        \"tta_total\": dbg[\"tta_total\"],\n",
    "        \"mismatch_chunks\": dbg[\"mismatch_chunks\"],\n",
    "    })\n",
    "\n",
    "    # cleanup\n",
    "    del model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# -----------------------------\n",
    "# Write summary\n",
    "# -----------------------------\n",
    "sum_df = pd.DataFrame(summary_rows).sort_values(\"ece_cal\", ascending=True)\n",
    "print(\"\\n=== Calibration summary (sorted by calibrated ECE) ===\")\n",
    "print(sum_df.to_string(index=False))\n",
    "\n",
    "out_csv = RES_DIR / \"calibration_summary_v6.csv\"\n",
    "sum_df.to_csv(out_csv, index=False)\n",
    "print(f\"\\nSaved calibration summary CSV -> {out_csv.resolve()}\")\n",
    "\n",
    "print(\"\\nPer-label calibration JSONs:\")\n",
    "for p in calib_paths:\n",
    "    print(\" -\", p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb197aa",
   "metadata": {},
   "source": [
    "## 11: Inference test V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d06887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== V6 inference report saved ===\n",
      "JSON: tox21_dualenc_v1\\results\\v6\\inference_v6_report_20250903_184650.json\n",
      "CSV : tox21_dualenc_v1\\results\\v6\\inference_v6_flat_20250903_184650.csv\n",
      "Descriptor coverage (non-NaN before impute): 92.2%\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, math, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Fragments, rdMolDescriptors as rdm\n",
    "from rdkit import DataStructs\n",
    "\n",
    "# ----------------- Required context or sensible defaults -----------------\n",
    "# These should exist from earlier cells; we also provide fallbacks.\n",
    "\n",
    "if 'LABELS' not in globals():\n",
    "    LABELS = ['NR-AR','NR-AR-LBD','NR-AhR','NR-Aromatase','NR-ER','NR-ER-LBD',\n",
    "              'NR-PPAR-gamma','SR-ARE','SR-ATAD5','SR-HSE','SR-MMP','SR-p53']\n",
    "\n",
    "if 'RES_DIR' not in globals():\n",
    "    RES_DIR = Path(\"tox21_dualenc_v1/results/v6\").resolve()\n",
    "else:\n",
    "    RES_DIR = Path(RES_DIR)\n",
    "\n",
    "if 'CKPT_DIR' not in globals():\n",
    "    CKPT_DIR = Path(\"tox21_dualenc_v1/models/checkpoints_v6\").resolve()\n",
    "else:\n",
    "    CKPT_DIR = Path(CKPT_DIR)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = globals().get('MAX_LEN', 192)\n",
    "\n",
    "# Tokenizer & model class expected from earlier cells:\n",
    "assert 'V6Expert' in globals(), \"V6Expert not found. Please run the model-def cell.\"\n",
    "assert 'tokenizer' in globals() and 'tok_loaded_name' in globals(), \"Tokenizer not found. Please run the tokenizer cell.\"\n",
    "\n",
    "# Try to get DESC_ORDER from memory; else load from saved file\n",
    "DESC_ORDER = globals().get('DESC_ORDER', None)\n",
    "if DESC_ORDER is None:\n",
    "    # try to read the columns file we saved in Cell 3\n",
    "    cols_path = RES_DIR / \"rdkit_desc_columns_v6.txt\"\n",
    "    if cols_path.exists():\n",
    "        DESC_ORDER = [ln.strip() for ln in cols_path.read_text().splitlines() if ln.strip()]\n",
    "    else:\n",
    "        raise RuntimeError(\"Descriptor column order (DESC_ORDER) not found in memory and rdkit_desc_columns_v6.txt is missing.\")\n",
    "\n",
    "# Load imputer & scaler from disk if not in memory\n",
    "if 'imputer' not in globals() or 'scaler' not in globals():\n",
    "    from joblib import load\n",
    "    imp_path = RES_DIR / \"rdkit_desc_imputer_v6.pkl\"\n",
    "    scl_path = RES_DIR / \"rdkit_desc_scaler_v6.pkl\"\n",
    "    assert imp_path.exists() and scl_path.exists(), \"Imputer/Scaler pickles not found in results/v6. Re-run the descriptor fit cell.\"\n",
    "    imputer = load(imp_path)\n",
    "    scaler  = load(scl_path)\n",
    "\n",
    "# Prefer cached graph builder if you defined it earlier\n",
    "_mol_to_graph = globals().get('mol_to_graph_cached', globals().get('mol_to_graph', None))\n",
    "assert _mol_to_graph is not None, \"mol_to_graph(_cached) not found. Please run the graph utils cell.\"\n",
    "\n",
    "# ----------------- Utilities -----------------\n",
    "def sigmoid(x): \n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def _best_run_for_label(label: str) -> Tuple[str, Path]:\n",
    "    # choose best run by max VAL PR-AUC; fallback to newest ckpt\n",
    "    hist_candidates = sorted((RES_DIR / label).glob(f\"v6_*_{label}_history.json\"))\n",
    "    best_run, best_score, best_time = None, -float('inf'), -1\n",
    "    for hp in hist_candidates:\n",
    "        try:\n",
    "            hist = json.loads(Path(hp).read_text())\n",
    "            vals = [e.get(\"val_pr_auc\", float('nan')) for e in hist.get(\"epochs\", [])]\n",
    "            vals = [v for v in vals if isinstance(v, (int,float)) and np.isfinite(v)]\n",
    "            score = max(vals) if vals else -float('inf')\n",
    "            t = hp.stat().st_mtime\n",
    "            if score > best_score or (math.isclose(score, best_score) and t > best_time):\n",
    "                best_score, best_time = score, t\n",
    "                best_run = hist.get(\"run_id\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if best_run is None:\n",
    "        ckpts = sorted((CKPT_DIR / label).glob(f\"v6_*_{label}_best.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if not ckpts:\n",
    "            raise FileNotFoundError(f\"No checkpoints for {label}\")\n",
    "        best_run = ckpts[0].stem.split(f\"_{label}_best\")[0]\n",
    "    ckpt = CKPT_DIR / label / f\"{best_run}_{label}_best.pt\"\n",
    "    return best_run, ckpt\n",
    "\n",
    "def _load_model_for_label(label: str):\n",
    "    # match desc dropout to training where possible\n",
    "    desc_dropout = 0.5\n",
    "    hp_json = RES_DIR / \"per_label_hparams_v6.json\"\n",
    "    if hp_json.exists():\n",
    "        try:\n",
    "            rows = json.loads(hp_json.read_text())\n",
    "            lab2hp = {row[\"label\"]: row for row in rows}\n",
    "            if label in lab2hp:\n",
    "                desc_dropout = float(lab2hp[label].get(\"desc_dropout\", 0.5))\n",
    "        except Exception:\n",
    "            pass\n",
    "    run_id, ckpt_path = _best_run_for_label(label)\n",
    "    model = V6Expert(text_model_name=tok_loaded_name, desc_dim=len(DESC_ORDER), desc_dropout=desc_dropout, heads=4).to(DEVICE)\n",
    "    sd = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    _ = model.load_state_dict(sd, strict=False)\n",
    "    model.eval()\n",
    "    return model, run_id, ckpt_path\n",
    "\n",
    "def _load_calibration_and_thresholds(label: str, run_id: str) -> Dict:\n",
    "    calib_path = RES_DIR / label / f\"{run_id}_{label}_calibration.json\"\n",
    "    thr_path   = RES_DIR / label / f\"{run_id}_{label}_thresholds_maxF1.json\"\n",
    "    if not calib_path.exists():\n",
    "        cand = sorted((RES_DIR / label).glob(f\"*_{label}_calibration.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if cand: calib_path = cand[0]\n",
    "    if not thr_path.exists():\n",
    "        cand = sorted((RES_DIR / label).glob(f\"*_{label}_thresholds_maxF1.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if cand: thr_path = cand[0]\n",
    "\n",
    "    out = {\"method\": \"temperature\", \"params\": {\"T\": 1.0}, \"t_maxF1\": 0.5, \"t_Fbeta\": 0.5, \"beta\": 1.5}\n",
    "    if calib_path.exists():\n",
    "        try:\n",
    "            j = json.loads(calib_path.read_text())\n",
    "            out[\"method\"] = j.get(\"method\", out[\"method\"])\n",
    "            out[\"params\"] = j.get(\"params\", out[\"params\"])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if thr_path.exists():\n",
    "        try:\n",
    "            tj = json.loads(thr_path.read_text())\n",
    "            out[\"t_maxF1\"] = float(tj.get(\"t_maxF1\", out[\"t_maxF1\"]))\n",
    "            out[\"t_Fbeta\"] = float(tj.get(\"t_Fbeta\", out[\"t_Fbeta\"]))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def _apply_calibration_by_mode(logits: np.ndarray, saved_method: str, saved_params: Dict, calib_mode: str) -> np.ndarray:\n",
    "    if calib_mode == \"raw\":\n",
    "        return sigmoid(logits)\n",
    "    if calib_mode == \"temperature\":\n",
    "        return sigmoid(logits / 1.0)  # same as raw\n",
    "    if calib_mode == \"platt\":\n",
    "        return sigmoid(1.0 * logits + 0.0)  # same as raw\n",
    "    # saved:\n",
    "    if saved_method == \"platt\":\n",
    "        return sigmoid(saved_params[\"a\"] * logits + saved_params[\"b\"])\n",
    "    elif saved_method == \"temperature\":\n",
    "        return sigmoid(logits / max(1e-6, saved_params.get(\"T\", 1.0)))\n",
    "    elif saved_method == \"isotonic\":\n",
    "        p = sigmoid(logits)\n",
    "        xs = np.array(saved_params[\"thresholds\"], dtype=np.float64)\n",
    "        ys = np.array(saved_params[\"y_values\"], dtype=np.float64)\n",
    "        p = np.clip(p, xs[0], xs[-1])\n",
    "        return np.interp(p, xs, ys).astype(np.float32)\n",
    "    else:\n",
    "        return sigmoid(logits)\n",
    "\n",
    "# ----------------- Descriptor parity: named RDKit + bit families -----------------\n",
    "bit_family_regex = re.compile(r'^(?P<prefix>[A-Za-z]+[A-Za-z0-9\\-]*)[_\\-]?(?P<idx>\\d{1,4})$')\n",
    "families: Dict[str, List[int]] = {}\n",
    "named_descriptors = []\n",
    "for col in DESC_ORDER:\n",
    "    m = bit_family_regex.match(col)\n",
    "    if m is None:\n",
    "        named_descriptors.append(col)\n",
    "    else:\n",
    "        prefix = m.group('prefix').lower()\n",
    "        idx = int(m.group('idx'))\n",
    "        families.setdefault(prefix, []).append(idx)\n",
    "families = {p: sorted(ix) for p, ix in families.items() if len(ix) >= 64}  # treat large groups as bit families\n",
    "\n",
    "_DESC_FUNCS = {name: func for name, func in Descriptors.descList}\n",
    "_FRAG_FUNCS = {name: getattr(Fragments, name) for name in dir(Fragments) if name.startswith(\"fr_\") and callable(getattr(Fragments, name))}\n",
    "\n",
    "def _safe(fn, mol):\n",
    "    try: return float(fn(mol))\n",
    "    except Exception: return float(\"nan\")\n",
    "\n",
    "_EXTRA_MAP = {\n",
    "    \"NumRotBonds\":            lambda m: _safe(rdm.CalcNumRotatableBonds, m),\n",
    "    \"NumRotatableBonds\":      lambda m: _safe(rdm.CalcNumRotatableBonds, m),\n",
    "    \"RingCount\":              lambda m: _safe(rdm.CalcNumRings, m),\n",
    "    \"NumAromaticRings\":       lambda m: _safe(rdm.CalcNumAromaticRings, m),\n",
    "    \"NumAliphaticRings\":      lambda m: _safe(rdm.CalcNumAliphaticRings, m),\n",
    "    \"NumSaturatedRings\":      lambda m: _safe(rdm.CalcNumSaturatedRings, m),\n",
    "    \"FractionCSP3\":           lambda m: _safe(rdm.CalcFractionCSP3, m),\n",
    "    \"TPSA\":                   lambda m: _safe(rdm.CalcTPSA, m),\n",
    "    \"ExactMolWt\":             lambda m: _safe(rdm.CalcExactMolWt, m),\n",
    "    \"HeavyAtomCount\":         lambda m: _safe(Descriptors.HeavyAtomCount, m),\n",
    "    \"AromaticProportion\":     lambda m: (_safe(lambda x: sum(int(a.GetIsAromatic()) for a in x.GetAtoms()), m) /\n",
    "                                         max(1.0, _safe(Descriptors.HeavyAtomCount, m))),\n",
    "    \"MolWt\":                  lambda m: _safe(Descriptors.MolWt, m),\n",
    "    \"SlogP\":                  lambda m: _safe(Descriptors.MolLogP, m),\n",
    "    \"HBD\":                    lambda m: _safe(Descriptors.NumHDonors, m),\n",
    "    \"HBA\":                    lambda m: _safe(Descriptors.NumHAcceptors, m),\n",
    "    \"QED\":                    lambda m: _safe(lambda x: __import__(\"rdkit.Chem.QED\", fromlist=[\"QED\"]).QED.qed(x), m),\n",
    "}\n",
    "\n",
    "def _compute_morgan_bits(mol: Chem.Mol, nBits: int, radius: int = 2) -> np.ndarray:\n",
    "    try:\n",
    "        bv = rdm.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, useChirality=True)\n",
    "        arr = np.zeros((nBits,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr\n",
    "    except Exception:\n",
    "        return np.zeros((nBits,), dtype=np.int8)\n",
    "\n",
    "def _compute_rdk_bits(mol: Chem.Mol, nBits: int) -> np.ndarray:\n",
    "    try:\n",
    "        bv = Chem.RDKFingerprint(mol, fpSize=nBits)\n",
    "        arr = np.zeros((nBits,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr\n",
    "    except Exception:\n",
    "        return np.zeros((nBits,), dtype=np.int8)\n",
    "\n",
    "def _compute_maccs_bits(mol: Chem.Mol, nBits: int) -> np.ndarray:\n",
    "    try:\n",
    "        bv = rdm.GetMACCSKeysFingerprint(mol)  # typically 167 bits with bit 0 unused\n",
    "        arr = np.zeros((bv.GetNumBits(),), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        if nBits == 166:\n",
    "            return arr[1:167].copy()\n",
    "        out = np.zeros((nBits,), dtype=np.int8)\n",
    "        use = min(nBits, arr.size)\n",
    "        out[:use] = arr[:use]\n",
    "        return out\n",
    "    except Exception:\n",
    "        return np.zeros((nBits,), dtype=np.int8)\n",
    "\n",
    "def _family_to_fn(prefix: str):\n",
    "    p = prefix.lower()\n",
    "    if 'maccs' in p:\n",
    "        return _compute_maccs_bits, {}\n",
    "    if 'ecfp' in p or 'morgan' in p:\n",
    "        return _compute_morgan_bits, dict(radius=2)\n",
    "    if 'rdk' in p or 'path' in p or 'topo' in p:\n",
    "        return _compute_rdk_bits, {}\n",
    "    if p in ('fp', 'bits', 'bit', 'fingerprint'):\n",
    "        return _compute_morgan_bits, dict(radius=2)\n",
    "    return _compute_morgan_bits, dict(radius=2)\n",
    "\n",
    "def _compute_named_descriptor(col: str, mol: Chem.Mol) -> float:\n",
    "    fn = _DESC_FUNCS.get(col, None)\n",
    "    if fn is not None:\n",
    "        try: return float(fn(mol))\n",
    "        except Exception: return float(\"nan\")\n",
    "    ffn = _FRAG_FUNCS.get(col, None)\n",
    "    if ffn is not None:\n",
    "        try: return float(ffn(mol))\n",
    "        except Exception: return float(\"nan\")\n",
    "    efn = _EXTRA_MAP.get(col, None)\n",
    "    if efn is not None:\n",
    "        return efn(mol)\n",
    "    return float(\"nan\")\n",
    "\n",
    "def _compute_desc_matrix(smiles: List[str]) -> Tuple[np.ndarray, float]:\n",
    "    \"\"\"Return (X, coverage_percent).\"\"\"\n",
    "    # Pre-build family info\n",
    "    family_info = {}\n",
    "    for p, ix in families.items():\n",
    "        fn, kwargs = _family_to_fn(p)\n",
    "        min_idx, max_idx = min(ix), max(ix)\n",
    "        contiguous = (ix == list(range(min_idx, max_idx+1)))\n",
    "        size = max_idx - min_idx + 1 if contiguous else (max_idx + 1)\n",
    "        family_info[p] = dict(fn=fn, kwargs=kwargs, size=size, indices=ix, min_idx=min_idx, contiguous=contiguous)\n",
    "\n",
    "    total_cells = len(smiles) * len(DESC_ORDER)\n",
    "    n_nonnan = 0\n",
    "    rows = []\n",
    "    for s in smiles:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        row: Dict[str, float] = {}\n",
    "\n",
    "        # named descriptors\n",
    "        for col in named_descriptors:\n",
    "            v = _compute_named_descriptor(col, mol) if mol is not None else float(\"nan\")\n",
    "            if isinstance(v, (int, float)) and not math.isnan(v): n_nonnan += 1\n",
    "            row[col] = v\n",
    "\n",
    "        # bit families\n",
    "        for p, info in family_info.items():\n",
    "            fn = info['fn']; kwargs = info['kwargs']; size = info['size']\n",
    "            min_idx = info['min_idx']; contiguous = info['contiguous']\n",
    "            bits = np.zeros((size,), dtype=np.int8)\n",
    "            if mol is not None:\n",
    "                try:\n",
    "                    bits = fn(mol, nBits=size, **kwargs)\n",
    "                except TypeError:\n",
    "                    bits = fn(mol, nBits=size)  # fn without kwargs\n",
    "                except Exception:\n",
    "                    bits = np.zeros((size,), dtype=np.int8)\n",
    "            for idx in info['indices']:\n",
    "                pos = (idx - min_idx) if contiguous else idx\n",
    "                if 0 <= pos < bits.size:\n",
    "                    val = float(bits[pos]); \n",
    "                    if val == val: n_nonnan += 1\n",
    "                    row[f\"{p}_{idx}\"] = val\n",
    "                else:\n",
    "                    row[f\"{p}_{idx}\"] = float(\"nan\")\n",
    "\n",
    "        # safety: any leftover columns → NaN\n",
    "        for col in DESC_ORDER:\n",
    "            if col not in row:\n",
    "                row[col] = float(\"nan\")\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=DESC_ORDER)\n",
    "    X = imputer.transform(df.values)\n",
    "    X = scaler.transform(X)\n",
    "    coverage = 100.0 * (n_nonnan / max(1, total_cells))\n",
    "    return X.astype(np.float32), coverage\n",
    "\n",
    "# ----------------- Tokenize / graphs / forward -----------------\n",
    "def _tokenize(smiles_batch: List[str]):\n",
    "    enc = tokenizer(smiles_batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "def _enumerate_smiles(s: str, n: int = 8) -> List[str]:\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None: return []\n",
    "    outs = set()\n",
    "    outs.add(Chem.MolToSmiles(m, canonical=True))\n",
    "    for _ in range(max(0, n-1)):\n",
    "        outs.add(Chem.MolToSmiles(m, canonical=False, doRandom=True))\n",
    "    outs = list(outs)\n",
    "    if len(outs) < n: outs = outs + outs[: (n - len(outs))]\n",
    "    return outs[:n]\n",
    "\n",
    "def _assemble_graph_batch(graphs: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    xs, eis, eas, batch_idx, n_nodes = [], [], [], [], []\n",
    "    node_offset = 0\n",
    "    for b_idx, g in enumerate(graphs):\n",
    "        x = g[\"x\"].cpu(); ei = g[\"edge_index\"].cpu(); ea = g[\"edge_attr\"].cpu()\n",
    "        n = x.shape[0]\n",
    "        xs.append(x); eis.append(ei + node_offset); eas.append(ea)\n",
    "        batch_idx.append(torch.full((n,), b_idx, dtype=torch.long))\n",
    "        n_nodes.append(n); node_offset += n\n",
    "    dev = DEVICE\n",
    "    return {\n",
    "        \"x\": torch.cat(xs, dim=0).to(dev),\n",
    "        \"edge_index\": torch.cat(eis, dim=1).to(dev),\n",
    "        \"edge_attr\": torch.cat(eas, dim=0).to(dev),\n",
    "        \"batch\": torch.cat(batch_idx, dim=0).to(dev),\n",
    "        \"n_nodes\": torch.tensor(n_nodes, dtype=torch.long, device=dev)\n",
    "    }\n",
    "\n",
    "def _trim_to_first_k(batch: Dict, g_batch: Dict, k: int) -> Tuple[Dict, Dict]:\n",
    "    batch_trim = {\n",
    "        \"input_ids\": batch[\"input_ids\"][:k],\n",
    "        \"attention_mask\": batch[\"attention_mask\"][:k],\n",
    "        \"desc\": batch[\"desc\"][:k],\n",
    "        \"smiles\": batch.get(\"smiles\", [])[:k],\n",
    "        \"mol_id\": batch.get(\"mol_id\", [])[:k],\n",
    "    }\n",
    "    n_nodes = g_batch[\"n_nodes\"].tolist()\n",
    "    k = min(k, len(n_nodes))\n",
    "    node_cut = int(np.sum(n_nodes[:k]))\n",
    "    x = g_batch[\"x\"][:node_cut]\n",
    "    batch_vec = g_batch[\"batch\"][:node_cut]\n",
    "    ea = g_batch[\"edge_attr\"]; ei = g_batch[\"edge_index\"]\n",
    "    mask_edges = (ei[0] < node_cut) & (ei[1] < node_cut)\n",
    "    ei = ei[:, mask_edges]; ea = ea[mask_edges]\n",
    "    g_trim = {\"x\": x, \"edge_index\": ei, \"edge_attr\": ea, \"batch\": batch_vec,\n",
    "              \"n_nodes\": torch.tensor(n_nodes[:k], dtype=torch.long, device=x.device)}\n",
    "    return batch_trim, g_trim\n",
    "\n",
    "def _forward_with_trim(model, batch: Dict, g_batch: Dict) -> Optional[torch.Tensor]:\n",
    "    B_text = int(batch[\"input_ids\"].shape[0])\n",
    "    B_graph = int(g_batch[\"n_nodes\"].shape[0])\n",
    "    if B_text != B_graph:\n",
    "        K = min(B_text, B_graph)\n",
    "        if K == 0: return None\n",
    "        batch, g_batch = _trim_to_first_k(batch, g_batch, K)\n",
    "    logits, _debug = model(batch, g_batch)\n",
    "    return logits\n",
    "\n",
    "# ----------------- Main runner -----------------\n",
    "def run_v6_inference_report(\n",
    "    smiles_input: List[str],\n",
    "    names: Optional[List[str]] = None,\n",
    "    tta: int = 8,\n",
    "    fixed_thr: float = 0.20\n",
    "):\n",
    "    # sanitize inputs\n",
    "    base, base_names = [], []\n",
    "    for i, s in enumerate(smiles_input):\n",
    "        s = (s or \"\").strip()\n",
    "        if not s: continue\n",
    "        if Chem.MolFromSmiles(s) is None:\n",
    "            continue\n",
    "        base.append(s)\n",
    "        nm = (names[i].strip() if names and i < len(names) else f\"mol_{len(base)}\")\n",
    "        base_names.append(nm)\n",
    "    assert len(base) > 0, \"No valid SMILES.\"\n",
    "\n",
    "    # Descriptors + coverage\n",
    "    X_desc, cov = _compute_desc_matrix(base)\n",
    "    canon = [Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True) for s in base]\n",
    "\n",
    "    # policies × calib\n",
    "    calib_modes = [\"saved\", \"raw\"]\n",
    "    policies = [(\"maxF1\", None), (\"Fbeta\", None), (\"fixed\", fixed_thr)]\n",
    "\n",
    "    meta = {\n",
    "        \"timestamp\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n",
    "        \"device\": DEVICE,\n",
    "        \"tta\": tta,\n",
    "        \"desc_coverage_percent\": cov,\n",
    "        \"n_molecules\": len(base),\n",
    "        \"labels\": LABELS,\n",
    "    }\n",
    "    molecules = [{\"name\": base_names[i], \"smiles\": base[i]} for i in range(len(base))]\n",
    "    flat_rows = []\n",
    "    big = {\"meta\": meta, \"molecules\": molecules, \"runs\": []}\n",
    "\n",
    "    for label in LABELS:\n",
    "        model, run_id, _ = _load_model_for_label(label)\n",
    "        calib = _load_calibration_and_thresholds(label, run_id)\n",
    "\n",
    "        # prebuild TTA packs that we can reuse for both calib modes/policies\n",
    "        tta_strings, graph_strings, desc_rows, index_map = [], [], [], []\n",
    "        for bi, s in enumerate(base):\n",
    "            variants = _enumerate_smiles(s, n=tta)\n",
    "            tta_strings.extend(variants)\n",
    "            graph_strings.extend([canon[bi] if canon[bi] else s] * tta)\n",
    "            desc_rows.extend([X_desc[bi]] * tta)\n",
    "            index_map.extend([bi] * tta)\n",
    "\n",
    "        # run once to get avg logits per molecule\n",
    "        logits_acc = np.zeros((len(base),), dtype=np.float32)\n",
    "        counts = np.zeros((len(base),), dtype=np.int32)\n",
    "        bs = 64\n",
    "        with torch.no_grad():\n",
    "            for start in range(0, len(tta_strings), bs):\n",
    "                chunk = tta_strings[start:start+bs]\n",
    "                gchunk = graph_strings[start:start+bs]\n",
    "                dchunk = np.stack(desc_rows[start:start+bs], axis=0)\n",
    "                imap   = index_map[start:start+bs]\n",
    "\n",
    "                kept_graphs, kept_tokens, kept_desc, kept_imap = [], [], [], []\n",
    "                for jj, smi in enumerate(gchunk):\n",
    "                    try:\n",
    "                        g = _mol_to_graph(smi, add_virtual_node=True)\n",
    "                        x = g.get(\"x\", None)\n",
    "                        if x is None or x.numel() == 0 or x.shape[0] == 0: \n",
    "                            continue\n",
    "                        kept_graphs.append(g)\n",
    "                        kept_tokens.append(chunk[jj])\n",
    "                        kept_desc.append(dchunk[jj])\n",
    "                        kept_imap.append(imap[jj])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                if not kept_graphs:\n",
    "                    continue\n",
    "                toks = _tokenize(kept_tokens)\n",
    "                Xd = torch.tensor(np.stack(kept_desc, axis=0), dtype=torch.float32, device=DEVICE)\n",
    "                g_batch = _assemble_graph_batch(kept_graphs)\n",
    "                mini = {\"input_ids\": toks[\"input_ids\"], \"attention_mask\": toks[\"attention_mask\"],\n",
    "                        \"desc\": Xd, \"smiles\": kept_tokens,\n",
    "                        \"mol_id\": [f\"infer_{start+i}\" for i in range(len(kept_tokens))]}\n",
    "                logits = _forward_with_trim(model, mini, g_batch)\n",
    "                if logits is None: \n",
    "                    continue\n",
    "                logits_np = logits.detach().float().cpu().numpy()\n",
    "                for l, bi in zip(logits_np, kept_imap[: len(logits_np)]):\n",
    "                    if 0 <= bi < len(base):\n",
    "                        logits_acc[bi] += l\n",
    "                        counts[bi]     += 1\n",
    "\n",
    "        # fallback canonical-only for zeros\n",
    "        need_fallback = np.where(counts == 0)[0]\n",
    "        if len(need_fallback) > 0:\n",
    "            with torch.no_grad():\n",
    "                kept_graphs, kept_tokens, kept_bi, kept_desc = [], [], [], []\n",
    "                for bi in need_fallback:\n",
    "                    smi = canon[bi] if canon[bi] else base[bi]\n",
    "                    try:\n",
    "                        g = _mol_to_graph(smi, add_virtual_node=True)\n",
    "                        x = g.get(\"x\", None)\n",
    "                        if x is None or x.numel() == 0 or x.shape[0] == 0: \n",
    "                            continue\n",
    "                        kept_graphs.append(g); kept_tokens.append(smi)\n",
    "                        kept_desc.append(X_desc[bi]); kept_bi.append(bi)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if kept_graphs:\n",
    "                    toks = _tokenize(kept_tokens)\n",
    "                    Xd = torch.tensor(np.stack(kept_desc, axis=0), dtype=torch.float32, device=DEVICE)\n",
    "                    g_batch = _assemble_graph_batch(kept_graphs)\n",
    "                    mini = {\"input_ids\": toks[\"input_ids\"], \"attention_mask\": toks[\"attention_mask\"],\n",
    "                            \"desc\": Xd, \"smiles\": kept_tokens,\n",
    "                            \"mol_id\": [f\"infer_fallback_{i}\" for i in range(len(kept_tokens))]}\n",
    "                    logits = _forward_with_trim(model, mini, g_batch)\n",
    "                    if logits is not None:\n",
    "                        logits_np = logits.detach().float().cpu().numpy()\n",
    "                        for l, bi in zip(logits_np, kept_bi[: len(logits_np)]):\n",
    "                            logits_acc[bi] += l; counts[bi] += 1\n",
    "\n",
    "        avg_logits = logits_acc / np.maximum(1, counts)\n",
    "\n",
    "        # store per-label base info for later thresholds\n",
    "        per_label_block = {\n",
    "            \"label\": label,\n",
    "            \"run_id\": run_id,\n",
    "            \"tta_kept_min\": int(counts.min()) if counts.size else 0,\n",
    "            \"tta_kept_med\": float(np.median(counts)) if counts.size else 0.0,\n",
    "            \"tta_kept_max\": int(counts.max()) if counts.size else 0,\n",
    "            \"tta_zero_kept\": int((counts==0).sum()),\n",
    "            \"per_mol_kept\": counts.tolist(),\n",
    "            \"calibration\": calib\n",
    "        }\n",
    "\n",
    "        # For each calibration mode & policy, compute probs & decisions and push flat rows\n",
    "        for cm in calib_modes:\n",
    "            probs = _apply_calibration_by_mode(avg_logits.copy(), calib[\"method\"], calib[\"params\"], cm)\n",
    "            for pol, thr in policies:\n",
    "                if pol == \"maxF1\":\n",
    "                    t = float(calib[\"t_maxF1\"])\n",
    "                elif pol == \"Fbeta\":\n",
    "                    t = float(calib[\"t_Fbeta\"])\n",
    "                else:\n",
    "                    t = float(thr)\n",
    "\n",
    "                preds = (probs >= t).astype(int).tolist()\n",
    "                # add to flat rows and runs list\n",
    "                for i in range(len(base)):\n",
    "                    flat_rows.append({\n",
    "                        \"calib_mode\": cm,\n",
    "                        \"policy\": pol if pol != \"fixed\" else f\"fixed@{t:.2f}\",\n",
    "                        \"name\": base_names[i],\n",
    "                        \"smiles\": base[i],\n",
    "                        \"label\": label,\n",
    "                        \"prob\": float(probs[i]),\n",
    "                        \"pred\": int(preds[i]),\n",
    "                    })\n",
    "\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        big[\"runs\"].append(per_label_block)\n",
    "\n",
    "    # Save files\n",
    "    out_dir = RES_DIR\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\", time.gmtime())\n",
    "    json_path = out_dir / f\"inference_v6_report_{ts}.json\"\n",
    "    csv_path  = out_dir / f\"inference_v6_flat_{ts}.csv\"\n",
    "\n",
    "    with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(big, f, indent=2)\n",
    "\n",
    "    flat_df = pd.DataFrame(flat_rows, columns=[\"calib_mode\",\"policy\",\"name\",\"smiles\",\"label\",\"prob\",\"pred\"])\n",
    "    flat_df.to_csv(csv_path, index=False)\n",
    "\n",
    "    print(\"\\n=== V6 inference report saved ===\")\n",
    "    print(f\"JSON: {json_path}\")\n",
    "    print(f\"CSV : {csv_path}\")\n",
    "    print(f\"Descriptor coverage (non-NaN before impute): {meta['desc_coverage_percent']:.1f}%\")\n",
    "    return str(json_path), str(csv_path)\n",
    "\n",
    "# ----------------- Your inputs (as requested) -----------------\n",
    "SMILES_INPUT = [\n",
    "   \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",\n",
    "   \"CCN1C(=O)NC(c2ccccc2)C1=O\",\n",
    "   \"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\",\n",
    "   \"CC(O)CNCC(C)O\",\n",
    "   \"O=c1cc(-c2ccccc2)oc2cc(O)cc(O)c12\"\n",
    "]\n",
    "NAMES_INPUT = [\n",
    "   \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",\n",
    "   \"CCN1C(=O)NC(c2ccccc2)C1=O\",\n",
    "   \"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\",\n",
    "   \"CC(O)CNCC(C)O\",\n",
    "   \"O=c1cc(-c2ccccc2)oc2cc(O)cc(O)c12\"\n",
    "]\n",
    "\n",
    "# ----------------- Run it -----------------\n",
    "json_file, csv_file = run_v6_inference_report(SMILES_INPUT, NAMES_INPUT, tta=8, fixed_thr=0.20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f05586a",
   "metadata": {},
   "source": [
    "## 12: inference + recall@0.70 thresholds + evaluation vs ground truth (XLSX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72ad1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using dataset CSV: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\data\\raw\\dataset_selected.csv\n",
      "[INFO] Truth XLSX: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\data\\raw\\Truth Lables.xlsx\n",
      "[INFO] Loaded truth (5 rows) from: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\data\\raw\\Truth Lables.xlsx\n",
      "\n",
      "=== Micro metrics over provided molecules (labels stacked) ===\n",
      "    pred_maxF1:  precision=1.000  recall=0.167  F1=0.286\n",
      "    pred_Fbeta:  precision=1.000  recall=0.167  F1=0.286\n",
      " pred_recall70:  precision=0.200  recall=1.000  F1=0.333\n",
      "   pred_screen:  precision=0.667  recall=0.333  F1=0.444\n",
      "\n",
      "Per-label recall (policy: recall70):\n",
      "  SR-ARE        recall=1.000  (P=3)\n",
      "  NR-AhR        recall=1.000  (P=2)\n",
      "  NR-ER         recall=1.000  (P=2)\n",
      "  NR-ER-LBD     recall=1.000  (P=1)\n",
      "  NR-PPAR-gamma  recall=1.000  (P=1)\n",
      "  SR-HSE        recall=1.000  (P=1)\n",
      "  SR-MMP        recall=1.000  (P=1)\n",
      "  SR-p53        recall=1.000  (P=1)\n",
      "  NR-AR         recall=nan  (P=0)\n",
      "  NR-AR-LBD     recall=nan  (P=0)\n",
      "  NR-Aromatase  recall=nan  (P=0)\n",
      "  SR-ATAD5      recall=nan  (P=0)\n",
      "\n",
      "[INFO] Joined molecules (n=5): ['CC(O)CNCC(C)O', 'CCN1C(=O)NC(c2ccccc2)C1=O', 'CCOc1ccc2nc(S(N)(=O)=O)sc2c1', 'O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1', 'O=c1cc(-c2ccccc2)oc2cc(O)cc(O)c12']\n",
      "\n",
      "=== Saved inference evaluation ===\n",
      "CSV : D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\inference_v6_eval_20250903_191952.csv\n",
      "JSON: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\tox21_dualenc_v1\\results\\v6\\inference_v6_eval_20250903_191952.json\n"
     ]
    }
   ],
   "source": [
    "import os, re, json, math, time\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Fragments, rdMolDescriptors as rdm\n",
    "from rdkit import DataStructs\n",
    "\n",
    "# -------------------- Config you can edit --------------------\n",
    "# Dataset (used to compute recall@0.70 thresholds on VAL)\n",
    "DATA_CSV_CANDIDATES = [\n",
    "    \"tox21_dualenc_v1/data/raw/dataset_selected.csv\",  # your V6 path\n",
    "    \"tox21.csv\"                                        # fallback if you copied it here\n",
    "]\n",
    "# >>> Explicit truth XLSX location (first in the list) <<<\n",
    "TRUTH_XLSX_CANDIDATES = [\n",
    "    \"tox21_dualenc_v1/data/raw/Truth Lables.xlsx\",     # your file\n",
    "    \"Truth Lables.xlsx\",\n",
    "    \"Truth Labels.xlsx\",\n",
    "    \"Truth_Lables.xlsx\",\n",
    "]\n",
    "\n",
    "SMILES_INPUT = [\n",
    "   \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",\n",
    "   \"CCN1C(=O)NC(c2ccccc2)C1=O\",\n",
    "   \"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\",\n",
    "   \"CC(O)CNCC(C)O\",\n",
    "   \"O=c1cc(-c2ccccc2)oc2cc(O)cc(O)c12\"\n",
    "]\n",
    "NAMES_INPUT = [\n",
    "   \"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\",\n",
    "   \"CCN1C(=O)NC(c2ccccc2)C1=O\",\n",
    "   \"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\",\n",
    "   \"CC(O)CNCC(C)O\",\n",
    "   \"O=c1cc(-c2ccccc2)oc2cc(O)cc(O)c12\"\n",
    "]\n",
    "\n",
    "TTA_VAL = 4   # TTA for VAL (threshold finding)\n",
    "TTA_INF = 8   # TTA for inference\n",
    "SCREEN_FIXED = {\"common\": 0.10, \"mid\": 0.12, \"rare\": 0.15}  # quick screening thresholds\n",
    "\n",
    "# -------------------- Required context from previous cells --------------------\n",
    "assert 'LABELS' in globals(), \"LABELS not found; run earlier setup.\"\n",
    "assert 'RES_DIR' in globals() and 'CKPT_DIR' in globals(), \"RES_DIR/CKPT_DIR missing; run setup.\"\n",
    "assert 'V6Expert' in globals(), \"Model class V6Expert missing; run model cell.\"\n",
    "assert 'tokenizer' in globals() and 'tok_loaded_name' in globals(), \"Tokenizer missing; run tokenizer cell.\"\n",
    "_mol_to_graph = globals().get('mol_to_graph_cached', globals().get('mol_to_graph', None))\n",
    "assert _mol_to_graph is not None, \"Graph builder mol_to_graph(_cached) missing; run graph utils cell.\"\n",
    "\n",
    "RES_DIR = Path(RES_DIR)\n",
    "CKPT_DIR = Path(CKPT_DIR)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_LEN = globals().get('MAX_LEN', 192)\n",
    "\n",
    "# Load descriptor pipeline + column order\n",
    "DESC_ORDER = globals().get('DESC_ORDER', None)\n",
    "if DESC_ORDER is None:\n",
    "    cols_path = RES_DIR / \"rdkit_desc_columns_v6.txt\"\n",
    "    assert cols_path.exists(), f\"Missing {cols_path}\"\n",
    "    DESC_ORDER = [ln.strip() for ln in cols_path.read_text().splitlines() if ln.strip()]\n",
    "\n",
    "if 'imputer' not in globals() or 'scaler' not in globals():\n",
    "    from joblib import load\n",
    "    imp_path = RES_DIR / \"rdkit_desc_imputer_v6.pkl\"\n",
    "    scl_path = RES_DIR / \"rdkit_desc_scaler_v6.pkl\"\n",
    "    assert imp_path.exists() and scl_path.exists(), \"Imputer/Scaler pickles not found in results/v6.\"\n",
    "    imputer = load(imp_path)\n",
    "    scaler  = load(scl_path)\n",
    "\n",
    "# -------------------- I/O helpers --------------------\n",
    "def _first_existing(paths: List[str]) -> Optional[Path]:\n",
    "    for p in paths:\n",
    "        q = Path(p)\n",
    "        if q.exists():\n",
    "            return q.resolve()\n",
    "    return None\n",
    "\n",
    "DATA_CSV = _first_existing(DATA_CSV_CANDIDATES)\n",
    "assert DATA_CSV is not None, f\"Couldn't find dataset. Set DATA_CSV_CANDIDATES to your dataset path.\"\n",
    "print(f\"[INFO] Using dataset CSV: {DATA_CSV}\")\n",
    "\n",
    "TRUTH_XLSX = _first_existing(TRUTH_XLSX_CANDIDATES)  # may be None; we handle gracefully\n",
    "print(f\"[INFO] Truth XLSX: {TRUTH_XLSX if TRUTH_XLSX is not None else 'not found (metrics skipped)'}\")\n",
    "\n",
    "# -------------------- Load per-label hyperparams (for buckets) --------------------\n",
    "bucket_map = {}   # label -> {'bucket': 'common'|'mid'|'rare', 'prevalence': float}\n",
    "hp_json = RES_DIR / \"per_label_hparams_v6.json\"\n",
    "if hp_json.exists():\n",
    "    try:\n",
    "        rows = json.loads(hp_json.read_text())\n",
    "        for r in rows:\n",
    "            lab = r['label']\n",
    "            bucket_map[lab] = {\n",
    "                \"bucket\": r.get(\"bucket\"),\n",
    "                \"prevalence\": r.get(\"prevalence\")\n",
    "            }\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def _best_run_for_label(label: str) -> Tuple[str, Path]:\n",
    "    hist_candidates = sorted((RES_DIR / label).glob(f\"v6_*_{label}_history.json\"))\n",
    "    best_run, best_score, best_time = None, -float('inf'), -1\n",
    "    for hp in hist_candidates:\n",
    "        try:\n",
    "            hist = json.loads(Path(hp).read_text())\n",
    "            vals = [e.get(\"val_pr_auc\", float('nan')) for e in hist.get(\"epochs\", [])]\n",
    "            vals = [v for v in vals if isinstance(v, (int,float)) and np.isfinite(v)]\n",
    "            score = max(vals) if vals else -float('inf')\n",
    "            t = hp.stat().st_mtime\n",
    "            if score > best_score or (math.isclose(score, best_score) and t > best_time):\n",
    "                best_score, best_time = score, t\n",
    "                best_run = hist.get(\"run_id\")\n",
    "        except Exception:\n",
    "            pass\n",
    "    if best_run is None:\n",
    "        ckpts = sorted((CKPT_DIR / label).glob(f\"v6_*_{label}_best.pt\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        assert ckpts, f\"No checkpoints for {label}\"\n",
    "        best_run = ckpts[0].stem.split(f\"_{label}_best\")[0]\n",
    "    ckpt = CKPT_DIR / label / f\"{best_run}_{label}_best.pt\"\n",
    "    return best_run, ckpt\n",
    "\n",
    "def _load_model_for_label(label: str):\n",
    "    desc_dropout = 0.5\n",
    "    if hp_json.exists():\n",
    "        try:\n",
    "            rows = json.loads(hp_json.read_text())\n",
    "            lab2hp = {row[\"label\"]: row for row in rows}\n",
    "            if label in lab2hp:\n",
    "                desc_dropout = float(lab2hp[label].get(\"desc_dropout\", 0.5))\n",
    "        except Exception:\n",
    "            pass\n",
    "    run_id, ckpt_path = _best_run_for_label(label)\n",
    "    model = V6Expert(text_model_name=tok_loaded_name, desc_dim=len(DESC_ORDER), desc_dropout=desc_dropout, heads=4).to(DEVICE)\n",
    "    sd = torch.load(ckpt_path, map_location=DEVICE)\n",
    "    _ = model.load_state_dict(sd, strict=False)\n",
    "    model.eval()\n",
    "    return model, run_id, ckpt_path\n",
    "\n",
    "def _load_calibration_and_thresholds(label: str, run_id: str) -> Dict:\n",
    "    calib_path = RES_DIR / label / f\"{run_id}_{label}_calibration.json\"\n",
    "    thr_path   = RES_DIR / label / f\"{run_id}_{label}_thresholds_maxF1.json\"\n",
    "    if not calib_path.exists():\n",
    "        cand = sorted((RES_DIR / label).glob(f\"*_{label}_calibration.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if cand: calib_path = cand[0]\n",
    "    if not thr_path.exists():\n",
    "        cand = sorted((RES_DIR / label).glob(f\"*_{label}_thresholds_maxF1.json\"), key=lambda p: p.stat().st_mtime, reverse=True)\n",
    "        if cand: thr_path = cand[0]\n",
    "\n",
    "    out = {\"method\": \"temperature\", \"params\": {\"T\": 1.0}, \"t_maxF1\": 0.5, \"t_Fbeta\": 0.5, \"beta\": 1.5}\n",
    "    if calib_path.exists():\n",
    "        try:\n",
    "            j = json.loads(calib_path.read_text())\n",
    "            out[\"method\"] = j.get(\"method\", out[\"method\"])\n",
    "            out[\"params\"] = j.get(\"params\", out[\"params\"])\n",
    "            out[\"beta\"]   = j.get(\"beta\", 1.5)\n",
    "        except Exception:\n",
    "            pass\n",
    "    if thr_path.exists():\n",
    "        try:\n",
    "            tj = json.loads(thr_path.read_text())\n",
    "            out[\"t_maxF1\"] = float(tj.get(\"t_maxF1\", out[\"t_maxF1\"]))\n",
    "            out[\"t_Fbeta\"] = float(tj.get(\"t_Fbeta\", out[\"t_Fbeta\"]))\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "def _apply_calibration_by_mode(logits: np.ndarray, saved_method: str, saved_params: Dict) -> np.ndarray:\n",
    "    if saved_method == \"platt\":\n",
    "        return 1.0 / (1.0 + np.exp(-(saved_params[\"a\"] * logits + saved_params[\"b\"])))\n",
    "    elif saved_method == \"temperature\":\n",
    "        T = max(1e-6, float(saved_params.get(\"T\", 1.0)))\n",
    "        return 1.0 / (1.0 + np.exp(-(logits / T)))\n",
    "    elif saved_method == \"isotonic\":\n",
    "        p = 1.0 / (1.0 + np.exp(-logits))\n",
    "        xs = np.array(saved_params[\"thresholds\"], dtype=np.float64)\n",
    "        ys = np.array(saved_params[\"y_values\"], dtype=np.float64)\n",
    "        p = np.clip(p, xs[0], xs[-1])\n",
    "        return np.interp(p, xs, ys).astype(np.float32)\n",
    "    else:\n",
    "        return 1.0 / (1.0 + np.exp(-logits))\n",
    "\n",
    "# -------------------- Descriptor parity (named + fingerprint families) --------------------\n",
    "bit_family_regex = re.compile(r'^(?P<prefix>[A-Za-z]+[A-Za-z0-9\\-]*)[_\\-]?(?P<idx>\\d{1,4})$')\n",
    "families: Dict[str, List[int]] = {}\n",
    "named_descriptors = []\n",
    "for col in DESC_ORDER:\n",
    "    m = bit_family_regex.match(col)\n",
    "    if m is None:\n",
    "        named_descriptors.append(col)\n",
    "    else:\n",
    "        prefix = m.group('prefix').lower()\n",
    "        idx = int(m.group('idx'))\n",
    "        families.setdefault(prefix, []).append(idx)\n",
    "families = {p: sorted(ix) for p, ix in families.items() if len(ix) >= 64}\n",
    "\n",
    "_DESC_FUNCS = {name: func for name, func in Descriptors.descList}\n",
    "_FRAG_FUNCS = {name: getattr(Fragments, name) for name in dir(Fragments) if name.startswith(\"fr_\") and callable(getattr(Fragments, name))}\n",
    "\n",
    "def _safe(fn, mol):\n",
    "    try: return float(fn(mol))\n",
    "    except Exception: return float(\"nan\")\n",
    "\n",
    "_EXTRA_MAP = {\n",
    "    \"NumRotBonds\":            lambda m: _safe(rdm.CalcNumRotatableBonds, m),\n",
    "    \"NumRotatableBonds\":      lambda m: _safe(rdm.CalcNumRotatableBonds, m),\n",
    "    \"RingCount\":              lambda m: _safe(rdm.CalcNumRings, m),\n",
    "    \"NumAromaticRings\":       lambda m: _safe(rdm.CalcNumAromaticRings, m),\n",
    "    \"NumAliphaticRings\":      lambda m: _safe(rdm.CalcNumAliphaticRings, m),\n",
    "    \"NumSaturatedRings\":      lambda m: _safe(rdm.CalcNumSaturatedRings, m),\n",
    "    \"FractionCSP3\":           lambda m: _safe(rdm.CalcFractionCSP3, m),\n",
    "    \"TPSA\":                   lambda m: _safe(rdm.CalcTPSA, m),\n",
    "    \"ExactMolWt\":             lambda m: _safe(rdm.CalcExactMolWt, m),\n",
    "    \"HeavyAtomCount\":         lambda m: _safe(Descriptors.HeavyAtomCount, m),\n",
    "    \"AromaticProportion\":     lambda m: (_safe(lambda x: sum(int(a.GetIsAromatic()) for a in x.GetAtoms()), m) /\n",
    "                                         max(1.0, _safe(Descriptors.HeavyAtomCount, m))),\n",
    "    \"MolWt\":                  lambda m: _safe(Descriptors.MolWt, m),\n",
    "    \"SlogP\":                  lambda m: _safe(Descriptors.MolLogP, m),\n",
    "    \"HBD\":                    lambda m: _safe(Descriptors.NumHDonors, m),\n",
    "    \"HBA\":                    lambda m: _safe(Descriptors.NumHAcceptors, m),\n",
    "    \"QED\":                    lambda m: _safe(lambda x: __import__(\"rdkit.Chem.QED\", fromlist=[\"QED\"]).QED.qed(x), m),\n",
    "}\n",
    "\n",
    "def _compute_morgan_bits(mol: Chem.Mol, nBits: int, radius: int = 2) -> np.ndarray:\n",
    "    try:\n",
    "        bv = rdm.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits, useChirality=True)\n",
    "        arr = np.zeros((nBits,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr\n",
    "    except Exception:\n",
    "        return np.zeros((nBits,), dtype=np.int8)\n",
    "\n",
    "def _compute_rdk_bits(mol: Chem.Mol, nBits: int) -> np.ndarray:\n",
    "    try:\n",
    "        bv = Chem.RDKFingerprint(mol, fpSize=nBits)\n",
    "        arr = np.zeros((nBits,), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        return arr\n",
    "    except Exception:\n",
    "        return np.zeros((nBits,), dtype=np.int8)\n",
    "\n",
    "def _compute_maccs_bits(mol: Chem.Mol, nBits: int) -> np.ndarray:\n",
    "    try:\n",
    "        bv = rdm.GetMACCSKeysFingerprint(mol)  # 167 bits with bit 0 unused\n",
    "        arr = np.zeros((bv.GetNumBits(),), dtype=np.int8)\n",
    "        DataStructs.ConvertToNumpyArray(bv, arr)\n",
    "        if nBits == 166:\n",
    "            return arr[1:167].copy()\n",
    "        out = np.zeros((nBits,), dtype=np.int8)\n",
    "        use = min(nBits, arr.size)\n",
    "        out[:use] = arr[:use]\n",
    "        return out\n",
    "    except Exception:\n",
    "        return np.zeros((nBits,), dtype=np.int8)\n",
    "\n",
    "def _family_to_fn(prefix: str):\n",
    "    p = prefix.lower()\n",
    "    if 'maccs' in p:\n",
    "        return _compute_maccs_bits, {}\n",
    "    if 'ecfp' in p or 'morgan' in p:\n",
    "        return _compute_morgan_bits, dict(radius=2)\n",
    "    if 'rdk' in p or 'path' in p or 'topo' in p:\n",
    "        return _compute_rdk_bits, {}\n",
    "    if p in ('fp', 'bits', 'bit', 'fingerprint'):\n",
    "        return _compute_morgan_bits, dict(radius=2)\n",
    "    return _compute_morgan_bits, dict(radius=2)\n",
    "\n",
    "def _compute_named_descriptor(col: str, mol: Chem.Mol) -> float:\n",
    "    fn = _DESC_FUNCS.get(col, None)\n",
    "    if fn is not None:\n",
    "        try: return float(fn(mol))\n",
    "        except Exception: return float(\"nan\")\n",
    "    ffn = _FRAG_FUNCS.get(col, None)\n",
    "    if ffn is not None:\n",
    "        try: return float(ffn(mol))\n",
    "        except Exception: return float(\"nan\")\n",
    "    efn = _EXTRA_MAP.get(col, None)\n",
    "    if efn is not None:\n",
    "        return efn(mol)\n",
    "    return float(\"nan\")\n",
    "\n",
    "def _compute_desc_matrix(smiles: List[str]) -> Tuple[np.ndarray, float]:\n",
    "    # Build family info\n",
    "    family_info = {}\n",
    "    for p, ix in families.items():\n",
    "        fn, kwargs = _family_to_fn(p)\n",
    "        min_idx, max_idx = min(ix), max(ix)\n",
    "        contiguous = (ix == list(range(min_idx, max_idx+1)))\n",
    "        size = max_idx - min_idx + 1 if contiguous else (max_idx + 1)\n",
    "        family_info[p] = dict(fn=fn, kwargs=kwargs, size=size, indices=ix, min_idx=min_idx, contiguous=contiguous)\n",
    "\n",
    "    total_cells = len(smiles) * len(DESC_ORDER)\n",
    "    n_nonnan = 0\n",
    "    rows = []\n",
    "    for s in smiles:\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        row: Dict[str, float] = {}\n",
    "\n",
    "        # named descriptors\n",
    "        for col in named_descriptors:\n",
    "            v = _compute_named_descriptor(col, mol) if mol is not None else float(\"nan\")\n",
    "            if isinstance(v, (int, float)) and not math.isnan(v): n_nonnan += 1\n",
    "            row[col] = v\n",
    "\n",
    "        # bit families\n",
    "        for p, info in family_info.items():\n",
    "            fn = info['fn']; kwargs = info['kwargs']; size = info['size']\n",
    "            min_idx = info['min_idx']; contiguous = info['contiguous']\n",
    "            bits = np.zeros((size,), dtype=np.int8)\n",
    "            if mol is not None:\n",
    "                try:\n",
    "                    bits = fn(mol, nBits=size, **kwargs)\n",
    "                except TypeError:\n",
    "                    bits = fn(mol, nBits=size)\n",
    "                except Exception:\n",
    "                    bits = np.zeros((size,), dtype=np.int8)\n",
    "            for idx in info['indices']:\n",
    "                pos = (idx - min_idx) if contiguous else idx\n",
    "                if 0 <= pos < bits.size:\n",
    "                    val = float(bits[pos]); \n",
    "                    if val == val: n_nonnan += 1\n",
    "                    row[f\"{p}_{idx}\"] = val\n",
    "                else:\n",
    "                    row[f\"{p}_{idx}\"] = float(\"nan\")\n",
    "\n",
    "        # fill any leftovers as NaN (will be imputed)\n",
    "        for col in DESC_ORDER:\n",
    "            if col not in row:\n",
    "                row[col] = float(\"nan\")\n",
    "\n",
    "        rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows, columns=DESC_ORDER)\n",
    "    X = imputer.transform(df.values)\n",
    "    X = scaler.transform(X)\n",
    "    coverage = 100.0 * (n_nonnan / max(1, total_cells))\n",
    "    return X.astype(np.float32), coverage\n",
    "\n",
    "# -------------------- Tokenization / graphs / forward --------------------\n",
    "def _tokenize(smiles_batch: List[str]):\n",
    "    enc = tokenizer(smiles_batch, padding=True, truncation=True, max_length=MAX_LEN, return_tensors=\"pt\")\n",
    "    return {k: v.to(DEVICE) for k, v in enc.items()}\n",
    "\n",
    "def _enumerate_smiles(s: str, n: int = 8) -> List[str]:\n",
    "    m = Chem.MolFromSmiles(s)\n",
    "    if m is None: return []\n",
    "    outs = set()\n",
    "    outs.add(Chem.MolToSmiles(m, canonical=True))\n",
    "    for _ in range(max(0, n-1)):\n",
    "        outs.add(Chem.MolToSmiles(m, canonical=False, doRandom=True))\n",
    "    outs = list(outs)\n",
    "    if len(outs) < n: outs = outs + outs[: (n - len(outs))]\n",
    "    return outs[:n]\n",
    "\n",
    "def _assemble_graph_batch(graphs: List[Dict[str, torch.Tensor]]) -> Dict[str, torch.Tensor]:\n",
    "    xs, eis, eas, batch_idx, n_nodes = [], [], [], [], []\n",
    "    node_offset = 0\n",
    "    for b_idx, g in enumerate(graphs):\n",
    "        x = g[\"x\"].cpu(); ei = g[\"edge_index\"].cpu(); ea = g[\"edge_attr\"].cpu()\n",
    "        n = x.shape[0]\n",
    "        xs.append(x); eis.append(ei + node_offset); eas.append(ea)\n",
    "        batch_idx.append(torch.full((n,), b_idx, dtype=torch.long))\n",
    "        n_nodes.append(n); node_offset += n\n",
    "    dev = DEVICE\n",
    "    return {\n",
    "        \"x\": torch.cat(xs, dim=0).to(dev),\n",
    "        \"edge_index\": torch.cat(eis, dim=1).to(dev),\n",
    "        \"edge_attr\": torch.cat(eas, dim=0).to(dev),\n",
    "        \"batch\": torch.cat(batch_idx, dim=0).to(dev),\n",
    "        \"n_nodes\": torch.tensor(n_nodes, dtype=torch.long, device=dev)\n",
    "    }\n",
    "\n",
    "def _trim_to_first_k(batch: Dict, g_batch: Dict, k: int) -> Tuple[Dict, Dict]:\n",
    "    batch_trim = {\n",
    "        \"input_ids\": batch[\"input_ids\"][:k],\n",
    "        \"attention_mask\": batch[\"attention_mask\"][:k],\n",
    "        \"desc\": batch[\"desc\"][:k],\n",
    "        \"smiles\": batch.get(\"smiles\", [])[:k],\n",
    "        \"mol_id\": batch.get(\"mol_id\", [])[:k],\n",
    "    }\n",
    "    n_nodes = g_batch[\"n_nodes\"].tolist()\n",
    "    k = min(k, len(n_nodes))\n",
    "    node_cut = int(np.sum(n_nodes[:k]))\n",
    "    x = g_batch[\"x\"][:node_cut]\n",
    "    batch_vec = g_batch[\"batch\"][:node_cut]\n",
    "    ea = g_batch[\"edge_attr\"]; ei = g_batch[\"edge_index\"]\n",
    "    mask_edges = (ei[0] < node_cut) & (ei[1] < node_cut)\n",
    "    ei = ei[:, mask_edges]; ea = ea[mask_edges]\n",
    "    g_trim = {\"x\": x, \"edge_index\": ei, \"edge_attr\": ea, \"batch\": batch_vec,\n",
    "              \"n_nodes\": torch.tensor(n_nodes[:k], dtype=torch.long, device=x.device)}\n",
    "    return batch_trim, g_trim\n",
    "\n",
    "def _forward_with_trim(model, batch: Dict, g_batch: Dict) -> Optional[torch.Tensor]:\n",
    "    B_text = int(batch[\"input_ids\"].shape[0])\n",
    "    B_graph = int(g_batch[\"n_nodes\"].shape[0])\n",
    "    if B_text != B_graph:\n",
    "        K = min(B_text, B_graph)\n",
    "        if K == 0: return None\n",
    "        batch, g_batch = _trim_to_first_k(batch, g_batch, K)\n",
    "    logits, _ = model(batch, g_batch)\n",
    "    return logits\n",
    "\n",
    "# -------------------- VAL probabilities (for recall@X thresholds) --------------------\n",
    "def _load_val_split(df_all: pd.DataFrame) -> pd.DataFrame:\n",
    "    if \"split\" not in df_all.columns:\n",
    "        raise RuntimeError(\"Dataset missing 'split' column needed for VAL thresholds.\")\n",
    "    dfv = df_all[df_all[\"split\"].astype(str).str.lower() == \"val\"].copy()\n",
    "    assert \"smiles\" in dfv.columns, \"VAL split missing 'smiles' column.\"\n",
    "    return dfv.reset_index(drop=True)\n",
    "\n",
    "def _collect_val_probs(label: str, dfv: pd.DataFrame, tta: int = 4) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    # y_true: binary with NaNs filtered out\n",
    "    y = dfv[label]\n",
    "    mask = y.notna() & y.isin([0,1])\n",
    "    y = y[mask].astype(int).values\n",
    "    smi = dfv.loc[mask, \"smiles\"].tolist()\n",
    "    if len(smi) == 0:\n",
    "        return np.zeros((0,), dtype=np.float32), np.zeros((0,), dtype=np.int32)\n",
    "\n",
    "    # descriptors + TTA predictions (avg logits)\n",
    "    Xd, _cov = _compute_desc_matrix(smi)\n",
    "    canon = [Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True) for s in smi]\n",
    "\n",
    "    model, run_id, _ = _load_model_for_label(label)\n",
    "    calib = _load_calibration_and_thresholds(label, run_id)\n",
    "\n",
    "    logits_acc = np.zeros((len(smi),), dtype=np.float32)\n",
    "    counts = np.zeros((len(smi),), dtype=np.int32)\n",
    "    bs = 64\n",
    "    with torch.no_grad():\n",
    "        tta_strings, graph_strings, desc_rows, index_map = [], [], [], []\n",
    "        for bi, s in enumerate(smi):\n",
    "            variants = _enumerate_smiles(s, n=tta)\n",
    "            tta_strings.extend(variants)\n",
    "            graph_strings.extend([canon[bi] if canon[bi] else s] * tta)\n",
    "            desc_rows.extend([Xd[bi]] * tta)\n",
    "            index_map.extend([bi] * tta)\n",
    "\n",
    "        for start in range(0, len(tta_strings), bs):\n",
    "            chunk = tta_strings[start:start+bs]\n",
    "            gchunk = graph_strings[start:start+bs]\n",
    "            dchunk = np.stack(desc_rows[start:start+bs], axis=0)\n",
    "            imap   = index_map[start:start+bs]\n",
    "\n",
    "            kept_graphs, kept_tokens, kept_desc, kept_imap = [], [], [], []\n",
    "            for jj, sc in enumerate(gchunk):\n",
    "                try:\n",
    "                    g = _mol_to_graph(sc, add_virtual_node=True)\n",
    "                    x = g.get(\"x\", None)\n",
    "                    if x is None or x.numel() == 0 or x.shape[0] == 0: \n",
    "                        continue\n",
    "                    kept_graphs.append(g)\n",
    "                    kept_tokens.append(chunk[jj])\n",
    "                    kept_desc.append(dchunk[jj])\n",
    "                    kept_imap.append(imap[jj])\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if not kept_graphs: \n",
    "                continue\n",
    "\n",
    "            toks = _tokenize(kept_tokens)\n",
    "            Xdesc = torch.tensor(np.stack(kept_desc, axis=0), dtype=torch.float32, device=DEVICE)\n",
    "            g_batch = _assemble_graph_batch(kept_graphs)\n",
    "            mini = {\"input_ids\": toks[\"input_ids\"], \"attention_mask\": toks[\"attention_mask\"],\n",
    "                    \"desc\": Xdesc, \"smiles\": kept_tokens,\n",
    "                    \"mol_id\": [f\"val_{start+i}\" for i in range(len(kept_tokens))]}\n",
    "            logits = _forward_with_trim(model, mini, g_batch)\n",
    "            if logits is None: continue\n",
    "            logits_np = logits.detach().float().cpu().numpy()\n",
    "            for l, bi in zip(logits_np, kept_imap[: len(logits_np)]):\n",
    "                if 0 <= bi < len(smi):\n",
    "                    logits_acc[bi] += l\n",
    "                    counts[bi]     += 1\n",
    "\n",
    "    # fallback canonical-only for zeros\n",
    "    need_fallback = np.where(counts == 0)[0]\n",
    "    if len(need_fallback) > 0:\n",
    "        with torch.no_grad():\n",
    "            kept_graphs, kept_tokens, kept_bi, kept_desc = [], [], [], []\n",
    "            for bi in need_fallback:\n",
    "                sc = canon[bi] if canon[bi] else smi[bi]\n",
    "                try:\n",
    "                    g = _mol_to_graph(sc, add_virtual_node=True)\n",
    "                    x = g.get(\"x\", None)\n",
    "                    if x is None or x.numel() == 0 or x.shape[0] == 0:\n",
    "                        continue\n",
    "                    kept_graphs.append(g); kept_tokens.append(sc)\n",
    "                    kept_desc.append(Xd[bi]); kept_bi.append(bi)\n",
    "                except Exception:\n",
    "                    continue\n",
    "            if kept_graphs:\n",
    "                toks = _tokenize(kept_tokens)\n",
    "                Xdesc = torch.tensor(np.stack(kept_desc, axis=0), dtype=torch.float32, device=DEVICE)\n",
    "                g_batch = _assemble_graph_batch(kept_graphs)\n",
    "                mini = {\"input_ids\": toks[\"input_ids\"], \"attention_mask\": toks[\"attention_mask\"],\n",
    "                        \"desc\": Xdesc, \"smiles\": kept_tokens,\n",
    "                        \"mol_id\": [f\"val_fallback_{i}\" for i in range(len(kept_tokens))]}\n",
    "                logits = _forward_with_trim(model, mini, g_batch)\n",
    "                if logits is not None:\n",
    "                    logits_np = logits.detach().float().cpu().numpy()\n",
    "                    for l, bi in zip(logits_np, kept_bi[: len(logits_np)]):\n",
    "                        logits_acc[bi] += l; counts[bi] += 1\n",
    "\n",
    "    avg_logits = logits_acc / np.maximum(1, counts)\n",
    "    probs = _apply_calibration_by_mode(avg_logits, calib[\"method\"], calib[\"params\"])\n",
    "    return probs.astype(np.float32), y.astype(np.int32)\n",
    "\n",
    "def _threshold_for_recall(y_true: np.ndarray, probs: np.ndarray, target_recall: float = 0.70) -> float:\n",
    "    # Return smallest threshold t s.t. recall >= target_recall; if no positives, return 0.5\n",
    "    P = int((y_true == 1).sum())\n",
    "    if P == 0:\n",
    "        return 0.5\n",
    "    # Evaluate recall at unique probability cutoffs\n",
    "    order = np.argsort(probs)[::-1]\n",
    "    y_sorted = y_true[order]\n",
    "    probs_sorted = probs[order]\n",
    "    tp_cum = np.cumsum(y_sorted)\n",
    "    recall_cum = tp_cum / max(1, P)\n",
    "    idx = np.searchsorted(recall_cum, target_recall, side='left')\n",
    "    if idx >= len(probs_sorted):\n",
    "        # need the minimum threshold to flag all positives\n",
    "        return float(np.nextafter(0.0, -1.0))  # effectively 0\n",
    "    t = float(probs_sorted[idx])\n",
    "    return max(0.0, min(1.0, t - 1e-8))\n",
    "\n",
    "# -------------------- Build recall@0.70 thresholds + prevalence buckets --------------------\n",
    "df_all = pd.read_csv(DATA_CSV)\n",
    "df_val = _load_val_split(df_all)\n",
    "\n",
    "# prevalence from file if available, else compute\n",
    "prevalence = {}\n",
    "for lab in LABELS:\n",
    "    if lab in bucket_map and bucket_map[lab].get(\"prevalence\") is not None:\n",
    "        prevalence[lab] = float(bucket_map[lab][\"prevalence\"])\n",
    "    else:\n",
    "        y_all = df_all[lab]\n",
    "        m = y_all.notna() & y_all.isin([0,1])\n",
    "        prevalence[lab] = float((y_all[m] == 1).mean())\n",
    "\n",
    "def _bucket_of(lab: str) -> str:\n",
    "    if lab in bucket_map and bucket_map[lab].get(\"bucket\") in (\"common\",\"mid\",\"rare\"):\n",
    "        return bucket_map[lab][\"bucket\"]\n",
    "    p = prevalence.get(lab, 0.0)\n",
    "    if p >= 0.10: return \"common\"\n",
    "    if p >= 0.04: return \"mid\"\n",
    "    return \"rare\"\n",
    "\n",
    "recall70 = {}\n",
    "for lab in LABELS:\n",
    "    probs_v, y_v = _collect_val_probs(lab, df_val, tta=TTA_VAL)\n",
    "    if (y_v == 1).sum() == 0 or probs_v.size == 0:\n",
    "        recall70[lab] = 0.50\n",
    "    else:\n",
    "        recall70[lab] = _threshold_for_recall(y_v, probs_v, target_recall=0.70)\n",
    "\n",
    "# -------------------- Inference on your SMILES under four policies --------------------\n",
    "def _infer_calibrated(smiles: List[str], names: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "    base, base_names = [], []\n",
    "    for i, s in enumerate(smiles):\n",
    "        s = (s or \"\").strip()\n",
    "        if not s: continue\n",
    "        mol = Chem.MolFromSmiles(s)\n",
    "        if mol is None: \n",
    "            print(f\"[WARN] skipping invalid SMILES: {s}\")\n",
    "            continue\n",
    "        base.append(s)\n",
    "        base_names.append(names[i] if names and i < len(names) else f\"mol_{len(base)}\")\n",
    "\n",
    "    X_desc, coverage = _compute_desc_matrix(base)\n",
    "    canon = [Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True) for s in base]\n",
    "    rows = []\n",
    "\n",
    "    for lab in LABELS:\n",
    "        model, run_id, _ = _load_model_for_label(lab)\n",
    "        calib = _load_calibration_and_thresholds(lab, run_id)\n",
    "\n",
    "        logits_acc = np.zeros((len(base),), dtype=np.float32)\n",
    "        counts = np.zeros((len(base),), dtype=np.int32)\n",
    "\n",
    "        # Build TTA pools\n",
    "        tta_strings, graph_strings, desc_rows, index_map = [], [], [], []\n",
    "        for bi, s in enumerate(base):\n",
    "            variants = _enumerate_smiles(s, n=TTA_INF)\n",
    "            tta_strings.extend(variants)\n",
    "            graph_strings.extend([canon[bi] if canon[bi] else s] * TTA_INF)\n",
    "            desc_rows.extend([X_desc[bi]] * TTA_INF)\n",
    "            index_map.extend([bi] * TTA_INF)\n",
    "\n",
    "        bs = 64\n",
    "        with torch.no_grad():\n",
    "            for start in range(0, len(tta_strings), bs):\n",
    "                chunk = tta_strings[start:start+bs]\n",
    "                gchunk = graph_strings[start:start+bs]\n",
    "                dchunk = np.stack(desc_rows[start:start+bs], axis=0)\n",
    "                imap   = index_map[start:start+bs]\n",
    "\n",
    "                kept_graphs, kept_tokens, kept_desc, kept_imap = [], [], [], []\n",
    "                for jj, sc in enumerate(gchunk):\n",
    "                    try:\n",
    "                        g = _mol_to_graph(sc, add_virtual_node=True)\n",
    "                        x = g.get(\"x\", None)\n",
    "                        if x is None or x.numel() == 0 or x.shape[0] == 0: \n",
    "                            continue\n",
    "                        kept_graphs.append(g)\n",
    "                        kept_tokens.append(chunk[jj])\n",
    "                        kept_desc.append(dchunk[jj])\n",
    "                        kept_imap.append(imap[jj])\n",
    "                    except Exception:\n",
    "                        continue\n",
    "                if not kept_graphs:\n",
    "                    continue\n",
    "\n",
    "                toks = _tokenize(kept_tokens)\n",
    "                Xd = torch.tensor(np.stack(kept_desc, axis=0), dtype=torch.float32, device=DEVICE)\n",
    "                g_batch = _assemble_graph_batch(kept_graphs)\n",
    "                mini = {\"input_ids\": toks[\"input_ids\"], \"attention_mask\": toks[\"attention_mask\"],\n",
    "                        \"desc\": Xd, \"smiles\": kept_tokens,\n",
    "                        \"mol_id\": [f\"infer_{start+i}\" for i in range(len(kept_tokens))]}\n",
    "                logits = _forward_with_trim(model, mini, g_batch)\n",
    "                if logits is None: continue\n",
    "                ln = logits.detach().float().cpu().numpy()\n",
    "                for l, bi in zip(ln, kept_imap[: len(ln)]):\n",
    "                    if 0 <= bi < len(base):\n",
    "                        logits_acc[bi] += l; counts[bi] += 1\n",
    "\n",
    "        # fallback canonical-only for zeros\n",
    "        need_fallback = np.where(counts == 0)[0]\n",
    "        if len(need_fallback) > 0:\n",
    "            with torch.no_grad():\n",
    "                kept_graphs, kept_tokens, kept_bi, kept_desc = [], [], [], []\n",
    "                for bi in need_fallback:\n",
    "                    sc = canon[bi] if canon[bi] else base[bi]\n",
    "                    try:\n",
    "                        g = _mol_to_graph(sc, add_virtual_node=True)\n",
    "                        x = g.get(\"x\", None)\n",
    "                        if x is None or x.numel() == 0 or x.shape[0] == 0: \n",
    "                            continue\n",
    "                        kept_graphs.append(g); kept_tokens.append(sc)\n",
    "                        kept_desc.append(X_desc[bi]); kept_bi.append(bi)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "            if len(need_fallback) > 0 and kept_graphs:\n",
    "                toks = _tokenize(kept_tokens)\n",
    "                Xd = torch.tensor(np.stack(kept_desc, axis=0), dtype=torch.float32, device=DEVICE)\n",
    "                g_batch = _assemble_graph_batch(kept_graphs)\n",
    "                mini = {\"input_ids\": toks[\"input_ids\"], \"attention_mask\": toks[\"attention_mask\"],\n",
    "                        \"desc\": Xd, \"smiles\": kept_tokens,\n",
    "                        \"mol_id\": [f\"infer_fallback_{i}\" for i in range(len(kept_tokens))]}\n",
    "                logits = _forward_with_trim(model, mini, g_batch)\n",
    "                if logits is not None:\n",
    "                    ln = logits.detach().float().cpu().numpy()\n",
    "                    for l, bi in zip(ln, kept_bi[: len(ln)]):\n",
    "                        logits_acc[bi] += l; counts[bi] += 1\n",
    "\n",
    "        avg_logits = logits_acc / np.maximum(1, counts)\n",
    "        probs = _apply_calibration_by_mode(avg_logits, calib[\"method\"], calib[\"params\"])\n",
    "\n",
    "        # thresholds\n",
    "        t_maxF1 = float(calib[\"t_maxF1\"])\n",
    "        t_Fb    = float(calib[\"t_Fbeta\"])\n",
    "        t_rec70 = float(recall70.get(lab, 0.5))\n",
    "        bucket  = _bucket_of(lab)\n",
    "        t_fix   = float(SCREEN_FIXED[bucket])\n",
    "\n",
    "        for i in range(len(base)):\n",
    "            rows.append({\n",
    "                \"name\": base_names[i], \"smiles\": base[i], \"label\": lab, \"bucket\": bucket,\n",
    "                \"prob\": float(probs[i]),\n",
    "                \"pred_maxF1\": int(probs[i] >= t_maxF1),\n",
    "                \"pred_Fbeta\": int(probs[i] >= t_Fb),\n",
    "                \"pred_recall70\": int(probs[i] >= t_rec70),\n",
    "                \"pred_screen\": int(probs[i] >= t_fix),\n",
    "                \"thr_maxF1\": t_maxF1, \"thr_Fbeta\": t_Fb, \"thr_recall70\": t_rec70, \"thr_screen\": t_fix\n",
    "            })\n",
    "        del model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    # Canonicalize for joining with truth\n",
    "    df[\"smiles_canon\"] = df[\"smiles\"].apply(lambda s: Chem.MolToSmiles(Chem.MolFromSmiles(s), canonical=True) if isinstance(s,str) else s)\n",
    "    return df\n",
    "\n",
    "df_inf = _infer_calibrated(SMILES_INPUT, NAMES_INPUT)\n",
    "\n",
    "# -------------------- Load truth and evaluate (robust label normalization) --------------------\n",
    "truth_df = None\n",
    "if TRUTH_XLSX is not None:\n",
    "    try:\n",
    "        truth_df = pd.read_excel(TRUTH_XLSX)\n",
    "        print(f\"[INFO] Loaded truth ({len(truth_df)} rows) from: {TRUTH_XLSX}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] Failed to read truth file {TRUTH_XLSX}: {e}\")\n",
    "\n",
    "def _canon(s):\n",
    "    try: return Chem.MolToSmiles(Chem.MolFromSmiles(str(s)), canonical=True)\n",
    "    except Exception: return np.nan\n",
    "\n",
    "if truth_df is not None:\n",
    "    assert \"smiles\" in truth_df.columns, \"Truth Excel must have a 'smile' column.\"\n",
    "    truth_df[\"smiles_canon\"] = truth_df[\"smiles\"].apply(_canon)\n",
    "    truth_df = truth_df.dropna(subset=[\"smiles_canon\"]).copy()\n",
    "\n",
    "    # Normalize truth label headers to exactly LABELS\n",
    "    def _normalize_label(name: str) -> str:\n",
    "        if name in (\"smiles\",\"smiles_canon\"): return name\n",
    "        norm = name\n",
    "        norm = norm.replace(\"_\",\"-\").replace(\"–\",\"-\").replace(\"—\",\"-\")\n",
    "        norm = re.sub(r\"\\s+\",\"\", norm)\n",
    "        norm = norm.replace(\"PPARgamma\",\"PPAR-gamma\").replace(\"PPAR-γ\",\"PPAR-gamma\")\n",
    "        # try to coerce to canonical\n",
    "        if norm not in LABELS:\n",
    "            norm = re.sub(r\"^(SR)(-?)(MMP)$\", r\"\\1-\\3\", norm)     # SRMMP -> SR-MMP\n",
    "            norm = re.sub(r\"^(SRARE)$\", \"SR-ARE\", norm)           # SRARE -> SR-ARE\n",
    "            norm = re.sub(r\"^(SRATAD5)$\", \"SR-ATAD5\", norm)\n",
    "            norm = re.sub(r\"^(SRe?HSE)$\", \"SR-HSE\", norm)\n",
    "            norm = re.sub(r\"^(NRERLBD)$\", \"NR-ER-LBD\", norm)\n",
    "            norm = re.sub(r\"^(NRARLBD)$\", \"NR-AR-LBD\", norm)\n",
    "            norm = re.sub(r\"^(NRPPARgamma|NRPPAR-gamma)$\", \"NR-PPAR-gamma\", norm)\n",
    "        return norm\n",
    "\n",
    "    colmap = {c: _normalize_label(c) for c in truth_df.columns}\n",
    "    truth_df = truth_df.rename(columns=colmap)\n",
    "\n",
    "    missing = [lab for lab in LABELS if lab not in truth_df.columns]\n",
    "    if missing:\n",
    "        print(f\"[WARN] Truth file missing label columns (after normalization): {missing}\")\n",
    "\n",
    "    keep_cols = [\"smiles_canon\"] + [lab for lab in LABELS if lab in truth_df.columns]\n",
    "    truth_df = truth_df[keep_cols].copy()\n",
    "\n",
    "    # coerce to 0/1 ints where present\n",
    "    for lab in LABELS:\n",
    "        if lab in truth_df.columns:\n",
    "            truth_df[lab] = pd.to_numeric(truth_df[lab], errors=\"coerce\").fillna(0).clip(0,1).astype(int)\n",
    "\n",
    "    truth_long = truth_df.melt(id_vars=[\"smiles_canon\"], value_vars=[c for c in truth_df.columns if c != \"smiles_canon\"],\n",
    "                               var_name=\"label\", value_name=\"y_true\")\n",
    "    truth_long = truth_long[truth_long[\"label\"].isin(LABELS)]\n",
    "    eval_df = df_inf.merge(truth_long, on=[\"smiles_canon\",\"label\"], how=\"left\")\n",
    "else:\n",
    "    eval_df = df_inf.copy()\n",
    "    eval_df[\"y_true\"] = np.nan\n",
    "    print(\"[INFO] Truth Excel not found; skipping metrics. CSV/JSON will still be saved.\")\n",
    "\n",
    "# Compute metrics for each policy (micro)\n",
    "def _micro_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Tuple[float,float,float]:\n",
    "    y_true = y_true.astype(int)\n",
    "    y_pred = y_pred.astype(int)\n",
    "    tp = int(((y_true==1) & (y_pred==1)).sum())\n",
    "    fp = int(((y_true==0) & (y_pred==1)).sum())\n",
    "    fn = int(((y_true==1) & (y_pred==0)).sum())\n",
    "    prec = tp / max(1, tp+fp)\n",
    "    rec  = tp / max(1, tp+fn)\n",
    "    f1 = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "    return prec, rec, f1\n",
    "\n",
    "if eval_df[\"y_true\"].notna().any():\n",
    "    policies = [\"pred_maxF1\", \"pred_Fbeta\", \"pred_recall70\", \"pred_screen\"]\n",
    "    print(\"\\n=== Micro metrics over provided molecules (labels stacked) ===\")\n",
    "    sub_all = eval_df.dropna(subset=[\"y_true\"]).copy()\n",
    "    for pol in policies:\n",
    "        p,r,f1 = _micro_metrics(sub_all[\"y_true\"].values, sub_all[pol].values)\n",
    "        print(f\"{pol:>14}:  precision={p:.3f}  recall={r:.3f}  F1={f1:.3f}\")\n",
    "    # Per-label recall (useful to see which labels still struggle)\n",
    "    print(\"\\nPer-label recall (policy: recall70):\")\n",
    "    perlab = []\n",
    "    for lab in LABELS:\n",
    "        sub = sub_all[sub_all[\"label\"]==lab]\n",
    "        P = int((sub[\"y_true\"]==1).sum())\n",
    "        tp = int(((sub[\"y_true\"]==1) & (sub[\"pred_recall70\"]==1)).sum())\n",
    "        rec = (tp / P) if P>0 else np.nan\n",
    "        perlab.append((lab, rec, P))\n",
    "    perlab_sorted = sorted(perlab, key=lambda t: (np.nan_to_num(t[1], nan=-1.0), t[2]), reverse=True)\n",
    "    for lab, rec, P in perlab_sorted:\n",
    "        print(f\"  {lab:12}  recall={('nan' if pd.isna(rec) else f'{rec:.3f}')}  (P={P})\")\n",
    "    # Quick join health\n",
    "    names_joined = sorted(set(eval_df[\"name\"]))\n",
    "    print(f\"\\n[INFO] Joined molecules (n={len(names_joined)}): {names_joined}\")\n",
    "else:\n",
    "    print(\"\\n[INFO] No y_true values merged; metrics skipped. Check truth headers and SMILES canonicalization.\")\n",
    "\n",
    "# -------------------- Save artifacts --------------------\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\", time.gmtime())\n",
    "out_csv = RES_DIR / f\"inference_v6_eval_{ts}.csv\"\n",
    "out_json = RES_DIR / f\"inference_v6_eval_{ts}.json\"\n",
    "\n",
    "# Tidy wide CSV: one row per (name, label)\n",
    "wide = eval_df[[\n",
    "    \"name\",\"smiles\",\"smiles_canon\",\"label\",\"bucket\",\"prob\",\n",
    "    \"pred_maxF1\",\"pred_Fbeta\",\"pred_recall70\",\"pred_screen\",\n",
    "    \"thr_maxF1\",\"thr_Fbeta\",\"thr_recall70\",\"thr_screen\",\"y_true\"\n",
    "]].copy()\n",
    "wide.to_csv(out_csv, index=False)\n",
    "\n",
    "# JSON summary\n",
    "summary = {\n",
    "    \"timestamp_utc\": time.strftime(\"%Y-%m-%d %H:%M:%S\", time.gmtime()),\n",
    "    \"device\": DEVICE,\n",
    "    \"tta_val\": TTA_VAL,\n",
    "    \"tta_infer\": TTA_INF,\n",
    "    \"screen_fixed\": SCREEN_FIXED,\n",
    "    \"recall70_thresholds\": recall70,\n",
    "    \"prevalence\": prevalence,\n",
    "    \"buckets\": {lab: _bucket_of(lab) for lab in LABELS},\n",
    "    \"n_molecules\": len(SMILES_INPUT),\n",
    "    \"files\": {\"csv\": str(out_csv), \"json\": str(out_json)},\n",
    "}\n",
    "# Add micro metrics if truth available\n",
    "if eval_df[\"y_true\"].notna().any():\n",
    "    policies = [\"pred_maxF1\", \"pred_Fbeta\", \"pred_recall70\", \"pred_screen\"]\n",
    "    sub_all = eval_df.dropna(subset=[\"y_true\"])\n",
    "    summary[\"micro_metrics\"] = {}\n",
    "    for pol in policies:\n",
    "        p,r,f1 = _micro_metrics(sub_all[\"y_true\"].values, sub_all[pol].values)\n",
    "        summary[\"micro_metrics\"][pol] = {\"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "with open(out_json, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"\\n=== Saved inference evaluation ===\")\n",
    "print(f\"CSV : {out_csv}\")\n",
    "print(f\"JSON: {out_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c734706e",
   "metadata": {},
   "source": [
    "# V7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca5260b1",
   "metadata": {},
   "source": [
    "## Phase 1 (Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2eec11",
   "metadata": {},
   "source": [
    "### Load & Inspect the Dataset (V7 Initial Checks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "090a50e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded dataset with shape: (7831, 14)\n",
      "🔎 Running sanity checks...\n",
      "🔁 Duplicates → mol_id: 0, smiles: 0\n",
      "✅ Sanity checks passed. Metadata saved to:\n",
      "  • Preview: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\meta\\preview.csv\n",
      "  • Label stats: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\meta\\label_stats.csv\n",
      "  • Schema: D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\meta\\schema.json\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# ------------------------------\n",
    "# Step 1: Setup paths\n",
    "# ------------------------------\n",
    "DATA_PATH = Path(\"v7/data/tox21.csv\").resolve()\n",
    "META_DIR = DATA_PATH.parent / \"meta\"\n",
    "META_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# ------------------------------\n",
    "# Step 2: Load the dataset\n",
    "# ------------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(f\"✅ Loaded dataset with shape: {df.shape}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 3: Infer column roles\n",
    "# ------------------------------\n",
    "# Assume 12 labels, then mol_id and smiles\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# Confirm labels exist\n",
    "missing_labels = [c for c in expected_labels if c not in df.columns]\n",
    "assert not missing_labels, f\"❌ Missing label columns: {missing_labels}\"\n",
    "\n",
    "# Detect mol_id and smiles\n",
    "assert 'mol_id' in df.columns, \"❌ Missing 'mol_id' column\"\n",
    "assert 'smiles' in df.columns, \"❌ Missing 'smiles' column\"\n",
    "\n",
    "# ------------------------------\n",
    "# Step 4: Sanity checks\n",
    "# ------------------------------\n",
    "print(\"🔎 Running sanity checks...\")\n",
    "\n",
    "# Check for missing or empty SMILES\n",
    "num_missing_smiles = df['smiles'].isna().sum()\n",
    "num_empty_smiles = (df['smiles'].astype(str).str.strip() == \"\").sum()\n",
    "assert num_missing_smiles == 0, f\"❌ {num_missing_smiles} missing SMILES\"\n",
    "assert num_empty_smiles == 0, f\"❌ {num_empty_smiles} empty SMILES\"\n",
    "\n",
    "# Check label values are 0, 1, or NaN\n",
    "bad_values = {}\n",
    "for col in expected_labels:\n",
    "    unique_vals = df[col].dropna().unique()\n",
    "    bad_vals = [v for v in unique_vals if v not in [0, 1, 0.0, 1.0]]\n",
    "    if bad_vals:\n",
    "        bad_values[col] = bad_vals\n",
    "\n",
    "assert not bad_values, f\"❌ Invalid label values detected: {bad_values}\"\n",
    "\n",
    "# Check duplicates\n",
    "num_dup_mol = df['mol_id'].duplicated().sum()\n",
    "num_dup_smiles = df['smiles'].duplicated().sum()\n",
    "print(f\"🔁 Duplicates → mol_id: {num_dup_mol}, smiles: {num_dup_smiles}\")\n",
    "\n",
    "# ------------------------------\n",
    "# Step 5: Save metadata outputs\n",
    "# ------------------------------\n",
    "# Preview CSV\n",
    "df.head(5).to_csv(META_DIR / \"preview.csv\", index=False)\n",
    "\n",
    "# Label stats\n",
    "label_stats = []\n",
    "for col in expected_labels:\n",
    "    total = df[col].notna().sum()\n",
    "    pos = int((df[col] == 1).sum())\n",
    "    neg = int((df[col] == 0).sum())\n",
    "    missing = int(df[col].isna().sum())\n",
    "    prevalence = pos / total if total > 0 else 0\n",
    "    label_stats.append({\n",
    "        \"label\": col,\n",
    "        \"n_samples\": total,\n",
    "        \"n_positive\": pos,\n",
    "        \"n_negative\": neg,\n",
    "        \"n_missing\": missing,\n",
    "        \"positive_rate\": round(prevalence, 5)\n",
    "    })\n",
    "\n",
    "pd.DataFrame(label_stats).to_csv(META_DIR / \"label_stats.csv\", index=False)\n",
    "\n",
    "# Schema summary\n",
    "schema = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"n_rows\": len(df),\n",
    "    \"n_cols\": df.shape[1],\n",
    "    \"mol_id_column\": \"mol_id\",\n",
    "    \"smiles_column\": \"smiles\",\n",
    "    \"label_columns\": expected_labels,\n",
    "    \"has_duplicates\": {\n",
    "        \"mol_id\": bool(num_dup_mol),\n",
    "        \"smiles\": bool(num_dup_smiles)\n",
    "    }\n",
    "}\n",
    "with open(META_DIR / \"schema.json\", \"w\") as f:\n",
    "    json.dump(schema, f, indent=2)\n",
    "\n",
    "print(\"✅ Sanity checks passed. Metadata saved to:\")\n",
    "print(f\"  • Preview: {META_DIR / 'preview.csv'}\")\n",
    "print(f\"  • Label stats: {META_DIR / 'label_stats.csv'}\")\n",
    "print(f\"  • Schema: {META_DIR / 'schema.json'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a3566e",
   "metadata": {},
   "source": [
    "### 2: RDKit Descriptor Generation and Cleaning\n",
    "\n",
    "This cell computes RDKit descriptors (~300 per molecule) from SMILES strings, then:\n",
    "- Drops molecules where descriptor generation fails\n",
    "- Replaces infinite/extreme values with NaN\n",
    "- Applies median imputation and StandardScaler\n",
    "- Saves:\n",
    "  - Cleaned descriptors → `X_rdkit.npy`\n",
    "  - Binary labels → `Y.npy`\n",
    "  - Metadata: `smiles.npy`, `mol_ids.npy`\n",
    "  - Feature names → `feature_names.txt`\n",
    "  - Fitted imputer and scaler (for reuse in training/inference)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f31ed52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:41:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:41:09] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:41:09] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RDKit descriptors computed. Failures: 0\n",
      "🔍 Total NaNs after sanitization: 2966\n",
      "✅ RDKit descriptors: Imputed and scaled\n",
      "🧩 Descriptor shape: (7831, 208)\n",
      "🧬 Label shape: (7831, 12)\n",
      "\n",
      "📁 Saved:\n",
      "• Features       → D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\X_rdkit.npy\n",
      "• Labels         → D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\Y.npy\n",
      "• SMILES         → D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\smiles.npy\n",
      "• mol_ids        → D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\mol_ids.npy\n",
      "• Feature names  → D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\feature_names.txt\n",
      "• Scaler         → D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\scaler.joblib\n",
      "• Imputer        → D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\imputer.joblib\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# -------------------------\n",
    "# Paths\n",
    "# -------------------------\n",
    "DATA_PATH = Path(\"v7/data/tox21.csv\").resolve()\n",
    "META_DIR = DATA_PATH.parent / \"meta\"\n",
    "DESC_DIR = DATA_PATH.parent / \"descriptors\"\n",
    "DESC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# -------------------------\n",
    "# Load the dataset\n",
    "# -------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# -------------------------\n",
    "# Setup RDKit descriptor calculator (~200–300 descriptors)\n",
    "# -------------------------\n",
    "desc_names = [desc[0] for desc in Descriptors._descList]\n",
    "desc_calculator = MoleculeDescriptors.MolecularDescriptorCalculator(desc_names)\n",
    "\n",
    "# -------------------------\n",
    "# Compute descriptors from SMILES\n",
    "# -------------------------\n",
    "features, labels = [], []\n",
    "smiles_list, mol_ids = [], []\n",
    "failed_count = 0\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    smi = row['smiles']\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        failed_count += 1\n",
    "        continue\n",
    "    try:\n",
    "        desc = desc_calculator.CalcDescriptors(mol)\n",
    "        features.append(desc)\n",
    "        labels.append(row[expected_labels].values)\n",
    "        mol_ids.append(row['mol_id'])\n",
    "        smiles_list.append(smi)\n",
    "    except:\n",
    "        failed_count += 1\n",
    "\n",
    "print(f\"✅ RDKit descriptors computed. Failures: {failed_count}\")\n",
    "\n",
    "X = np.array(features, dtype=np.float64)\n",
    "Y = np.array(labels).astype(float)\n",
    "mol_ids = np.array(mol_ids)\n",
    "smiles_list = np.array(smiles_list)\n",
    "\n",
    "# -------------------------\n",
    "# Clean descriptor matrix: replace inf/extreme with NaN\n",
    "# -------------------------\n",
    "X[~np.isfinite(X)] = np.nan\n",
    "X[np.abs(X) > 1e6] = np.nan\n",
    "\n",
    "n_total_nan = np.isnan(X).sum()\n",
    "print(f\"🔍 Total NaNs after sanitization: {n_total_nan}\")\n",
    "\n",
    "# -------------------------\n",
    "# Impute and Scale\n",
    "# -------------------------\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "X_scaled = scaler.fit_transform(X_imputed)\n",
    "\n",
    "print(\"✅ RDKit descriptors: Imputed and scaled\")\n",
    "print(f\"🧩 Descriptor shape: {X_scaled.shape}\")\n",
    "print(f\"🧬 Label shape: {Y.shape}\")\n",
    "\n",
    "# -------------------------\n",
    "# Save outputs\n",
    "# -------------------------\n",
    "np.save(DESC_DIR / \"X_rdkit.npy\", X_scaled)\n",
    "np.save(DESC_DIR / \"Y.npy\", Y)\n",
    "np.save(DESC_DIR / \"smiles.npy\", smiles_list)\n",
    "np.save(DESC_DIR / \"mol_ids.npy\", mol_ids)\n",
    "\n",
    "with open(DESC_DIR / \"feature_names.txt\", \"w\") as f:\n",
    "    for name in desc_names:\n",
    "        f.write(name + \"\\n\")\n",
    "\n",
    "joblib.dump(imputer, DESC_DIR / \"imputer.joblib\")\n",
    "joblib.dump(scaler, DESC_DIR / \"scaler.joblib\")\n",
    "\n",
    "print(\"\\n📁 Saved:\")\n",
    "print(f\"• Features       → {DESC_DIR / 'X_rdkit.npy'}\")\n",
    "print(f\"• Labels         → {DESC_DIR / 'Y.npy'}\")\n",
    "print(f\"• SMILES         → {DESC_DIR / 'smiles.npy'}\")\n",
    "print(f\"• mol_ids        → {DESC_DIR / 'mol_ids.npy'}\")\n",
    "print(f\"• Feature names  → {DESC_DIR / 'feature_names.txt'}\")\n",
    "print(f\"• Scaler         → {DESC_DIR / 'scaler.joblib'}\")\n",
    "print(f\"• Imputer        → {DESC_DIR / 'imputer.joblib'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeea21e5",
   "metadata": {},
   "source": [
    "#### 2b) Save *sanitized raw* RDKit descriptors (no scaling)\n",
    "\n",
    "Recompute RDKit descriptors and only sanitize (replace inf/±inf/extremes with NaN).  \n",
    "This gives us `X_rdkit_raw.npy` so we can fit imputer/scaler **on the train split only** in the next cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16772bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:41:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:41:48] WARNING: not removing hydrogen atom without neighbors\n",
      "[23:41:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ RDKit descriptors recomputed. Failures: 0\n",
      "🔍 NaNs after sanitization: 2966 | Shape: (7831, 208)\n",
      "📁 Saved sanitized raw → D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\v7\\data\\descriptors\\X_rdkit_raw.npy\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "from rdkit.ML.Descriptors import MoleculeDescriptors\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "DATA_PATH = Path(\"v7/data/tox21.csv\").resolve()\n",
    "DESC_DIR = DATA_PATH.parent / \"descriptors\"\n",
    "DESC_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# RDKit descriptor setup\n",
    "desc_names = [d[0] for d in Descriptors._descList]\n",
    "calc = MoleculeDescriptors.MolecularDescriptorCalculator(desc_names)\n",
    "\n",
    "features, labels, smiles_list, mol_ids = [], [], [], []\n",
    "fail = 0\n",
    "for _, row in df.iterrows():\n",
    "    smi = row[\"smiles\"]\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        fail += 1\n",
    "        continue\n",
    "    try:\n",
    "        desc = calc.CalcDescriptors(mol)\n",
    "        features.append(desc)\n",
    "        labels.append(row[expected_labels].values)\n",
    "        smiles_list.append(smi)\n",
    "        mol_ids.append(row[\"mol_id\"])\n",
    "    except:\n",
    "        fail += 1\n",
    "\n",
    "print(f\"✅ RDKit descriptors recomputed. Failures: {fail}\")\n",
    "\n",
    "X_raw = np.array(features, dtype=np.float64)\n",
    "Y = np.array(labels, dtype=np.float64)\n",
    "smiles_arr = np.array(smiles_list)\n",
    "mol_ids_arr = np.array(mol_ids)\n",
    "\n",
    "# Sanitize only (no impute/scale)\n",
    "X_raw[~np.isfinite(X_raw)] = np.nan\n",
    "X_raw[np.abs(X_raw) > 1e6] = np.nan\n",
    "print(f\"🔍 NaNs after sanitization: {np.isnan(X_raw).sum()} | Shape: {X_raw.shape}\")\n",
    "\n",
    "# Save sanitized raw\n",
    "np.save(DESC_DIR / \"X_rdkit_raw.npy\", X_raw)\n",
    "np.save(DESC_DIR / \"Y.npy\", Y)  # overwrite same labels to keep in sync\n",
    "np.save(DESC_DIR / \"smiles.npy\", smiles_arr)\n",
    "np.save(DESC_DIR / \"mol_ids.npy\", mol_ids_arr)\n",
    "print(\"📁 Saved sanitized raw →\", DESC_DIR / \"X_rdkit_raw.npy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b9348f",
   "metadata": {},
   "source": [
    "### 3: Deterministic Scaffold-Based Train/Val/Test Split (80/10/10)\n",
    "\n",
    "This cell:\n",
    "- Computes RDKit **Bemis–Murcko scaffolds** for each SMILES.\n",
    "- **Fallback:** If a scaffold is empty (acyclic molecules), uses the molecule’s **canonical SMILES** as a pseudo-scaffold so every molecule is assigned.\n",
    "- Groups molecules by scaffold and performs an **80/10/10** split **by scaffold**, deterministic and balanced by group size.\n",
    "- Saves:\n",
    "  - Index masks: `v7/data/splits/train.npy`, `val.npy`, `test.npy`\n",
    "  - Metadata: `v7/data/splits/scaffold_split.csv` (mol_id, smiles, scaffold, split)\n",
    "  - Label distribution per split: `v7/data/splits/label_distribution.csv`\n",
    "  - Split summary: `v7/data/splits/split_summary.json`\n",
    "\n",
    "Sanity checks ensure:\n",
    "- All molecules are assigned and covered exactly once\n",
    "- Split sizes match the target proportions (±1 due to rounding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71bd047",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Split complete → Train: 6265, Val: 783, Test: 783\n",
      "🎯 Targets       → Train: 6265, Val: 783, Test: 783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:28] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📁 Saved:\n",
      "• Index masks       → v7\\data\\splits\n",
      "• Scaffold metadata → v7\\data\\splits\\scaffold_split.csv\n",
      "• Label distribution→ v7\\data\\splits\\label_distribution.csv\n",
      "• Summary JSON      → v7\\data\\splits\\split_summary.json\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, OrderedDict\n",
    "import json\n",
    "import random\n",
    "\n",
    "# ------------------------\n",
    "# Paths & constants\n",
    "# ------------------------\n",
    "DATA_DIR = Path(\"v7/data\")\n",
    "DESC_DIR = DATA_DIR / \"descriptors\"\n",
    "SPLIT_DIR = DATA_DIR / \"splits\"\n",
    "SPLIT_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "mol_ids = np.load(DESC_DIR / \"mol_ids.npy\", allow_pickle=True)\n",
    "smiles = np.load(DESC_DIR / \"smiles.npy\", allow_pickle=True)\n",
    "Y = np.load(DESC_DIR / \"Y.npy\")  # shape: [N, 12]\n",
    "\n",
    "# For reporting label-wise stats with names\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "\n",
    "# Split ratios and seed\n",
    "train_frac, val_frac, test_frac = 0.80, 0.10, 0.10\n",
    "rng_seed = 42\n",
    "random.seed(rng_seed)\n",
    "\n",
    "# ------------------------\n",
    "# Helper: compute scaffold or fallback to canonical SMILES\n",
    "# ------------------------\n",
    "def scaffold_key_from_smiles(smi: str) -> str:\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        return \"__INVALID__\"\n",
    "    scaf = MurckoScaffold.MurckoScaffoldSmiles(mol=mol, includeChirality=False)\n",
    "    if not scaf or scaf.strip() == \"\":\n",
    "        # Fallback: canonical SMILES as pseudo-scaffold for acyclic molecules\n",
    "        scaf = Chem.MolToSmiles(mol, isomericSmiles=False)\n",
    "    return scaf\n",
    "\n",
    "# ------------------------\n",
    "# Group indices by scaffold\n",
    "# ------------------------\n",
    "scaffold_to_indices = defaultdict(list)\n",
    "for idx, smi in enumerate(smiles):\n",
    "    key = scaffold_key_from_smiles(str(smi))\n",
    "    scaffold_to_indices[key].append(idx)\n",
    "\n",
    "# Shuffle groups of equal size in a deterministic way\n",
    "size_to_groups = defaultdict(list)\n",
    "for scaf, idxs in scaffold_to_indices.items():\n",
    "    size_to_groups[len(idxs)].append(idxs)\n",
    "\n",
    "for size in size_to_groups:\n",
    "    random.shuffle(size_to_groups[size])  # deterministic shuffle due to rng_seed\n",
    "\n",
    "# Build ordered list of groups: large -> small, with shuffled ties\n",
    "ordered_groups = []\n",
    "for size in sorted(size_to_groups.keys(), reverse=True):\n",
    "    ordered_groups.extend(size_to_groups[size])\n",
    "\n",
    "# ------------------------\n",
    "# Allocate groups to splits (80/10/10) by total assigned molecules\n",
    "# ------------------------\n",
    "N = len(smiles)\n",
    "N_grouped = sum(len(g) for g in ordered_groups)\n",
    "assert N_grouped == N, f\"Grouping mismatch: grouped {N_grouped} != total {N}\"\n",
    "\n",
    "target_train = int(round(train_frac * N))\n",
    "target_val = int(round(val_frac * N))\n",
    "target_test = N - target_train - target_val  # ensure sums to N\n",
    "\n",
    "train_idx, val_idx, test_idx = [], [], []\n",
    "cnt_train = cnt_val = cnt_test = 0\n",
    "\n",
    "for group in ordered_groups:\n",
    "    gsize = len(group)\n",
    "\n",
    "    # Greedy fill towards targets\n",
    "    if cnt_train + gsize <= target_train:\n",
    "        train_idx.extend(group); cnt_train += gsize\n",
    "    elif cnt_val + gsize <= target_val:\n",
    "        val_idx.extend(group); cnt_val += gsize\n",
    "    else:\n",
    "        test_idx.extend(group); cnt_test += gsize\n",
    "\n",
    "# If due to rounding we still have leftovers to meet exact target sizes, rebalance\n",
    "remaining = set(range(N)) - set(train_idx) - set(val_idx) - set(test_idx)\n",
    "if remaining:\n",
    "    # Fill val, then test, then train (in that order) to hit targets\n",
    "    for idx in list(remaining):\n",
    "        if cnt_val < target_val:\n",
    "            val_idx.append(idx); cnt_val += 1\n",
    "        elif cnt_test < target_test:\n",
    "            test_idx.append(idx); cnt_test += 1\n",
    "        else:\n",
    "            train_idx.append(idx); cnt_train += 1\n",
    "\n",
    "# Final sanity checks\n",
    "all_assigned = set(train_idx) | set(val_idx) | set(test_idx)\n",
    "assert len(all_assigned) == N, \"Not all molecules assigned to a split.\"\n",
    "assert len(set(train_idx) & set(val_idx)) == 0, \"Overlap between train and val.\"\n",
    "assert len(set(train_idx) & set(test_idx)) == 0, \"Overlap between train and test.\"\n",
    "assert len(set(val_idx) & set(test_idx)) == 0, \"Overlap between val and test.\"\n",
    "\n",
    "# Sort indices for neatness\n",
    "train_idx = np.array(sorted(train_idx))\n",
    "val_idx = np.array(sorted(val_idx))\n",
    "test_idx = np.array(sorted(test_idx))\n",
    "\n",
    "print(f\"✅ Split complete → Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "print(f\"🎯 Targets       → Train: {target_train}, Val: {target_val}, Test: {target_test}\")\n",
    "\n",
    "# ------------------------\n",
    "# Save index masks\n",
    "# ------------------------\n",
    "np.save(SPLIT_DIR / \"train.npy\", train_idx)\n",
    "np.save(SPLIT_DIR / \"val.npy\", val_idx)\n",
    "np.save(SPLIT_DIR / \"test.npy\", test_idx)\n",
    "\n",
    "# ------------------------\n",
    "# Save metadata CSV\n",
    "# ------------------------\n",
    "def compute_scaffolds_for_indices(idxs):\n",
    "    return [scaffold_key_from_smiles(str(smiles[i])) for i in idxs]\n",
    "\n",
    "meta_df = pd.DataFrame({\n",
    "    \"idx\": np.concatenate([train_idx, val_idx, test_idx]),\n",
    "    \"mol_id\": mol_ids[np.concatenate([train_idx, val_idx, test_idx])],\n",
    "    \"smiles\": smiles[np.concatenate([train_idx, val_idx, test_idx])],\n",
    "    \"scaffold\": compute_scaffolds_for_indices(np.concatenate([train_idx, val_idx, test_idx])),\n",
    "    \"split\": ([\"train\"] * len(train_idx)) + ([\"val\"] * len(val_idx)) + ([\"test\"] * len(test_idx)),\n",
    "})\n",
    "meta_df.to_csv(SPLIT_DIR / \"scaffold_split.csv\", index=False)\n",
    "\n",
    "# ------------------------\n",
    "# Label distribution per split (positives and non-missing)\n",
    "# ------------------------\n",
    "def split_label_stats(Y, idxs, split_name):\n",
    "    sub = Y[idxs]\n",
    "    pos = np.nansum(sub == 1, axis=0).astype(int)\n",
    "    non_missing = np.sum(~np.isnan(sub), axis=0).astype(int)\n",
    "    prev = np.divide(pos, np.maximum(non_missing, 1))  # avoid div0\n",
    "    return pd.DataFrame({\n",
    "        \"split\": [split_name] * len(expected_labels),\n",
    "        \"label\": expected_labels,\n",
    "        \"n_non_missing\": non_missing,\n",
    "        \"n_positive\": pos,\n",
    "        \"prevalence\": np.round(prev, 5),\n",
    "    })\n",
    "\n",
    "stats_df = pd.concat([\n",
    "    split_label_stats(Y, train_idx, \"train\"),\n",
    "    split_label_stats(Y, val_idx, \"val\"),\n",
    "    split_label_stats(Y, test_idx, \"test\"),\n",
    "], ignore_index=True)\n",
    "stats_df.to_csv(SPLIT_DIR / \"label_distribution.csv\", index=False)\n",
    "\n",
    "# ------------------------\n",
    "# Split summary JSON\n",
    "# ------------------------\n",
    "summary = {\n",
    "    \"seed\": rng_seed,\n",
    "    \"N_total\": int(N),\n",
    "    \"sizes\": {\n",
    "        \"train\": int(len(train_idx)),\n",
    "        \"val\": int(len(val_idx)),\n",
    "        \"test\": int(len(test_idx)),\n",
    "    },\n",
    "    \"targets\": {\n",
    "        \"train\": int(target_train),\n",
    "        \"val\": int(target_val),\n",
    "        \"test\": int(target_test),\n",
    "    },\n",
    "    \"paths\": {\n",
    "        \"train_idx\": str(SPLIT_DIR / \"train.npy\"),\n",
    "        \"val_idx\": str(SPLIT_DIR / \"val.npy\"),\n",
    "        \"test_idx\": str(SPLIT_DIR / \"test.npy\"),\n",
    "        \"scaffold_meta\": str(SPLIT_DIR / \"scaffold_split.csv\"),\n",
    "        \"label_distribution\": str(SPLIT_DIR / \"label_distribution.csv\"),\n",
    "    }\n",
    "}\n",
    "(Path(SPLIT_DIR) / \"split_summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"📁 Saved:\")\n",
    "print(f\"• Index masks       → {SPLIT_DIR}\")\n",
    "print(f\"• Scaffold metadata → {SPLIT_DIR / 'scaffold_split.csv'}\")\n",
    "print(f\"• Label distribution→ {SPLIT_DIR / 'label_distribution.csv'}\")\n",
    "print(f\"• Summary JSON      → {SPLIT_DIR / 'split_summary.json'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225e6d0a",
   "metadata": {},
   "source": [
    "### 4: Train-only impute/scale → package train/val/test\n",
    "\n",
    "- Load `X_rdkit_raw.npy` (sanitized, unscaled)\n",
    "- Fit `SimpleImputer(median)` and `StandardScaler` **on train only**\n",
    "- Transform train/val/test\n",
    "- Save:\n",
    "  - NPZ bundles: `v7/data/prepared/{train,val,test}.npz`\n",
    "  - Train-fitted artifacts: `imputer_train.joblib`, `scaler_train.joblib`\n",
    "  - `dataset_manifest.json`\n",
    "- Sanity checks: coverage, overlap, finiteness, label validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ca196e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Train-only impute/scale complete.\n",
      "Shapes → train (6265, 208) val (783, 208) test (783, 208)\n",
      "📁 Saved bundles & manifest → v7\\data\\prepared\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "DATA_DIR = Path(\"v7/data\")\n",
    "DESC_DIR = DATA_DIR / \"descriptors\"\n",
    "SPLIT_DIR = DATA_DIR / \"splits\"\n",
    "PREP_DIR  = DATA_DIR / \"prepared\"\n",
    "PREP_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Load sanitized RAW descriptors and metadata\n",
    "X_raw   = np.load(DESC_DIR / \"X_rdkit_raw.npy\")   # sanitized, may contain NaNs\n",
    "Y       = np.load(DESC_DIR / \"Y.npy\")\n",
    "smiles  = np.load(DESC_DIR / \"smiles.npy\", allow_pickle=True)\n",
    "mol_ids = np.load(DESC_DIR / \"mol_ids.npy\", allow_pickle=True)\n",
    "\n",
    "train_idx = np.load(SPLIT_DIR / \"train.npy\")\n",
    "val_idx   = np.load(SPLIT_DIR / \"val.npy\")\n",
    "test_idx  = np.load(SPLIT_DIR / \"test.npy\")\n",
    "\n",
    "N, F = X_raw.shape\n",
    "assert Y.shape[0] == N == smiles.shape[0] == mol_ids.shape[0], \"Row mismatch across arrays.\"\n",
    "\n",
    "# --- Sanity: coverage & overlap\n",
    "assigned = set(train_idx.tolist() + val_idx.tolist() + test_idx.tolist())\n",
    "assert len(assigned) == N, f\"Not all rows assigned: {len(assigned)} != {N}\"\n",
    "assert not (set(train_idx) & set(val_idx)),  \"Overlap train/val\"\n",
    "assert not (set(train_idx) & set(test_idx)), \"Overlap train/test\"\n",
    "assert not (set(val_idx) & set(test_idx)),   \"Overlap val/test\"\n",
    "\n",
    "# --- Fit imputer & scaler on TRAIN ONLY\n",
    "imputer_tr = SimpleImputer(strategy=\"median\")\n",
    "scaler_tr  = StandardScaler()\n",
    "\n",
    "X_tr_imp = imputer_tr.fit_transform(X_raw[train_idx])\n",
    "X_tr     = scaler_tr.fit_transform(X_tr_imp)\n",
    "\n",
    "def transform_split(indices: np.ndarray, name: str):\n",
    "    X_imp = imputer_tr.transform(X_raw[indices])\n",
    "    X_s   = scaler_tr.transform(X_imp)\n",
    "    Y_s   = Y[indices].astype(np.float32, copy=False)\n",
    "    s_s   = smiles[indices]\n",
    "    m_s   = mol_ids[indices]\n",
    "    y_missing = np.isnan(Y_s)\n",
    "    # checks\n",
    "    assert np.isfinite(X_s).all(), f\"Non-finite features in {name} split after transform.\"\n",
    "    return X_s.astype(np.float32), Y_s, s_s, m_s, y_missing\n",
    "\n",
    "X_train, Y_train, smi_train, mid_train, miss_train = transform_split(train_idx, \"train\")\n",
    "X_val,   Y_val,   smi_val,   mid_val,   miss_val   = transform_split(val_idx,   \"val\")\n",
    "X_test,  Y_test,  smi_test,  mid_test,  miss_test  = transform_split(test_idx,  \"test\")\n",
    "\n",
    "print(\"✅ Train-only impute/scale complete.\")\n",
    "print(\"Shapes →\",\n",
    "      \"train\", X_train.shape, \n",
    "      \"val\",   X_val.shape, \n",
    "      \"test\",  X_test.shape)\n",
    "\n",
    "# --- Save bundles\n",
    "def save_npz(name, Xs, Ys, smi, mids, idxs, miss):\n",
    "    out = PREP_DIR / f\"{name}.npz\"\n",
    "    np.savez_compressed(out, X=Xs, Y=Ys, smiles=smi, mol_id=mids, indices=idxs, y_missing_mask=miss)\n",
    "    return str(out)\n",
    "\n",
    "p_train = save_npz(\"train\", X_train, Y_train, smi_train, mid_train, train_idx, miss_train)\n",
    "p_val   = save_npz(\"val\",   X_val,   Y_val,   smi_val,   mid_val,   val_idx,   miss_val)\n",
    "p_test  = save_npz(\"test\",  X_test,  Y_test,  smi_test,  mid_test,  test_idx,  miss_test)\n",
    "\n",
    "# --- Save train-fitted artifacts\n",
    "joblib.dump(imputer_tr, DESC_DIR / \"imputer_train.joblib\")\n",
    "joblib.dump(scaler_tr,  DESC_DIR / \"scaler_train.joblib\")\n",
    "\n",
    "# --- Manifest\n",
    "expected_labels = [\n",
    "    'NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase',\n",
    "    'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "    'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53'\n",
    "]\n",
    "manifest = {\n",
    "    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n",
    "    \"n_total\": int(N),\n",
    "    \"n_features\": int(F),\n",
    "    \"labels\": expected_labels,\n",
    "    \"artifacts\": {\n",
    "        \"prepared_dir\": str(PREP_DIR),\n",
    "        \"train_npz\": p_train, \"val_npz\": p_val, \"test_npz\": p_test,\n",
    "        \"imputer_train\": str(DESC_DIR / \"imputer_train.joblib\"),\n",
    "        \"scaler_train\": str(DESC_DIR / \"scaler_train.joblib\"),\n",
    "        \"splits_dir\": str(SPLIT_DIR),\n",
    "        \"descriptors_dir\": str(DESC_DIR),\n",
    "        \"raw_descriptors\": str(DESC_DIR / \"X_rdkit_raw.npy\"),\n",
    "    },\n",
    "    \"splits\": {\n",
    "        \"train\": {\"size\": int(len(train_idx))},\n",
    "        \"val\":   {\"size\": int(len(val_idx))},\n",
    "        \"test\":  {\"size\": int(len(test_idx))}\n",
    "    }\n",
    "}\n",
    "(PREP_DIR / \"dataset_manifest.json\").write_text(json.dumps(manifest, indent=2))\n",
    "print(\"📁 Saved bundles & manifest →\", PREP_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9e93f2",
   "metadata": {},
   "source": [
    "## Phase 2 (Model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b93b42",
   "metadata": {},
   "source": [
    "### 1 : Cross-Attention Fusion Core + Sanity Test\n",
    "\n",
    "This defines the **novel fusion** block:\n",
    "- SMILES token embeddings (queries) attend over graph node embeddings (keys/values)\n",
    "- Residual + LayerNorm\n",
    "- Masked mean pooling for text and graph streams (fixed broadcasting)\n",
    "- Descriptor MLP to align RDKit features with the model dimension\n",
    "- Classifier head → 12 toxicity logits\n",
    "\n",
    "A synthetic sanity test checks tensor shapes and masks end-to-end (no encoders yet).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "165ed21a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded manifest. N_LABELS=12, DESC_IN_DIM=208\n",
      "[Sanity] logits shape: (4, 12) (expected: 4 x 12)\n",
      "✅ Fusion core defined & sanity-checked.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# -----------------------------\n",
    "# Repro & device\n",
    "# -----------------------------\n",
    "def seed_everything(seed: int = 42):\n",
    "    import random, numpy as np\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load descriptor dim & labels from manifest\n",
    "# -----------------------------\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "manifest_path = PREP_DIR / \"dataset_manifest.json\"\n",
    "assert manifest_path.exists(), f\"Missing manifest at {manifest_path}. Run Phase 1, Cell 4 first.\"\n",
    "\n",
    "with open(manifest_path) as f:\n",
    "    manifest = json.load(f)\n",
    "\n",
    "LABEL_NAMES = manifest[\"labels\"]\n",
    "N_LABELS = len(LABEL_NAMES)\n",
    "DESC_IN_DIM = manifest[\"n_features\"]  # RDKit feature count (e.g., 208)\n",
    "\n",
    "print(f\"Loaded manifest. N_LABELS={N_LABELS}, DESC_IN_DIM={DESC_IN_DIM}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Mask helpers (fixed broadcasting)\n",
    "# -----------------------------\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    x:    (B, L, D)\n",
    "    mask: (B, L) with 1 for valid, 0 for pad\n",
    "    returns: (B, D)\n",
    "    \"\"\"\n",
    "    # ensure same device/dtype\n",
    "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "    # keepdim=True to make denom shape (B,1), so it broadcasts over D\n",
    "    denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)  # (B,1)\n",
    "    num = (x * mask.unsqueeze(-1)).sum(dim=dim)              # (B,D)\n",
    "    return num / denom                                       # (B,D)\n",
    "\n",
    "def lengths_from_mask(mask: torch.Tensor) -> torch.Tensor:\n",
    "    return mask.long().sum(dim=1)\n",
    "\n",
    "# -----------------------------\n",
    "# Modules\n",
    "# -----------------------------\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int, hidden: int = 256, p: float = 0.2):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):  # (B, in_dim)\n",
    "        return self.net(x) # (B, out_dim)\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Single cross-attention layer: text queries attend to graph keys/values.\n",
    "    Uses PyTorch MultiheadAttention. Expects masks for graph nodes.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n_heads: int = 4, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(embed_dim=dim, num_heads=n_heads, dropout=p, batch_first=False)\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.ln = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "        \"\"\"\n",
    "        text_tokens: (B, L, D)\n",
    "        text_mask:   (B, L)  1=valid, 0=pad\n",
    "        graph_nodes: (B, N, D)\n",
    "        graph_mask:  (B, N)  1=valid, 0=pad\n",
    "        Returns:\n",
    "          text_out: (B, L, D) after cross-attn + residual + LN\n",
    "        \"\"\"\n",
    "        B, L, D = text_tokens.shape\n",
    "        N = graph_nodes.size(1)\n",
    "\n",
    "        # Convert to (S, B, D) for MHA\n",
    "        Q = text_tokens.transpose(0, 1)   # (L, B, D)\n",
    "        K = graph_nodes.transpose(0, 1)   # (N, B, D)\n",
    "        V = graph_nodes.transpose(0, 1)   # (N, B, D)\n",
    "\n",
    "        # key_padding_mask: (B, N) with True for positions to ignore\n",
    "        key_padding_mask = (graph_mask == 0)  # bool\n",
    "        attn_out, _ = self.mha(Q, K, V, key_padding_mask=key_padding_mask)  # (L, B, D)\n",
    "\n",
    "        # Residual + LN\n",
    "        attn_out = attn_out.transpose(0, 1)   # (B, L, D)\n",
    "        text_out = self.ln(text_tokens + self.dropout(attn_out))\n",
    "        return text_out\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Pools attended text and graph streams, fuses with descriptor embedding, predicts 12 labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int, n_labels: int, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim * 3, dim * 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(dim * 2, n_labels),\n",
    "        )\n",
    "\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask, desc_embed):\n",
    "        # Masked mean pools (fixed)\n",
    "        text_pool = masked_mean(text_tokens, text_mask, dim=1)   # (B, D)\n",
    "        graph_pool = masked_mean(graph_nodes, graph_mask, dim=1) # (B, D)\n",
    "\n",
    "        fused = torch.cat([text_pool, graph_pool, desc_embed], dim=-1)  # (B, 3D)\n",
    "        fused = self.dropout(fused)\n",
    "        logits = self.mlp(fused)  # (B, n_labels)\n",
    "        return logits\n",
    "\n",
    "class V7FusionCore(nn.Module):\n",
    "    \"\"\"\n",
    "    Novel fusion core (no encoders here).\n",
    "    Expects:\n",
    "      - text_tokens: (B, L, D)\n",
    "      - text_mask:   (B, L) 1/0 (valid/pad)\n",
    "      - graph_nodes: (B, N, D)\n",
    "      - graph_mask:  (B, N) 1/0\n",
    "      - desc_feats:  (B, DESC_IN_DIM)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim: int = 256, n_heads: int = 4, n_labels: int = 12,\n",
    "                 desc_in_dim: int = DESC_IN_DIM, desc_hidden: int = 256,\n",
    "                 p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.cross = CrossAttentionBlock(dim=dim, n_heads=n_heads, p=p)\n",
    "        self.desc_mlp = DescriptorMLP(in_dim=desc_in_dim, out_dim=dim, hidden=desc_hidden, p=p)\n",
    "        self.classifier = FusionClassifier(dim=dim, n_labels=n_labels, p=p)\n",
    "\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask, desc_feats):\n",
    "        text_attn = self.cross(text_tokens, text_mask, graph_nodes, graph_mask)  # (B,L,D)\n",
    "        desc_embed = self.desc_mlp(desc_feats)                                   # (B,D)\n",
    "        logits = self.classifier(text_attn, text_mask, graph_nodes, graph_mask, desc_embed)  # (B, n_labels)\n",
    "        return logits\n",
    "\n",
    "# -----------------------------\n",
    "# 🔎 Sanity test with synthetic tensors (no encoders yet)\n",
    "# -----------------------------\n",
    "def _sanity_test():\n",
    "    B, L, N, D = 4, 64, 48, 256\n",
    "    desc_in = DESC_IN_DIM\n",
    "\n",
    "    text_tokens = torch.randn(B, L, D, device=device)\n",
    "    graph_nodes = torch.randn(B, N, D, device=device)\n",
    "\n",
    "    # Build masks with at least 1 valid token/node per item\n",
    "    text_mask = (torch.rand(B, L, device=device) > 0.1).int()\n",
    "    graph_mask = (torch.rand(B, N, device=device) > 0.1).int()\n",
    "    for b in range(B):\n",
    "        if text_mask[b].sum() == 0:\n",
    "            text_mask[b, 0] = 1\n",
    "        if graph_mask[b].sum() == 0:\n",
    "            graph_mask[b, 0] = 1\n",
    "\n",
    "    desc_feats = torch.randn(B, desc_in, device=device)\n",
    "\n",
    "    model = V7FusionCore(dim=256, n_heads=4, n_labels=N_LABELS, desc_in_dim=desc_in, desc_hidden=256, p=0.1).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(text_tokens, text_mask, graph_nodes, graph_mask, desc_feats)\n",
    "    print(f\"[Sanity] logits shape: {tuple(logits.shape)} (expected: {B} x {N_LABELS})\")\n",
    "\n",
    "_sanity_test()\n",
    "print(\"✅ Fusion core defined & sanity-checked.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859ed99",
   "metadata": {},
   "source": [
    "### 2: ChemBERTa Text Encoder Wrapper (+ config saved under `v7/model`)\n",
    "\n",
    "This cell defines a lightweight wrapper around a ChemBERTa checkpoint to produce\n",
    "token-level embeddings aligned to the fusion dimension (default **256**).\n",
    "\n",
    "**What it does**\n",
    "- Loads a SMILES-aware tokenizer & model (ChemBERTa: `seyonec/ChemBERTa-zinc-base-v1`)\n",
    "- Projects hidden size → `fusion_dim` (256) for compatibility with the fusion core\n",
    "- Returns:\n",
    "  - `text_tokens`: `(B, L, 256)` token embeddings\n",
    "  - `text_mask`: `(B, L)` with 1=valid, 0=pad (compatible with fusion core)\n",
    "- Utilities:\n",
    "  - `freeze_backbone(n_unfrozen_layers=0)` for staged fine-tuning\n",
    "  - Optional gradient checkpointing toggle\n",
    "- Saves a minimal **encoder manifest** to `v7/model/text_encoder/config.json`\n",
    "- Runs a **small sanity test** using a few SMILES from your prepared train split\n",
    "\n",
    "> Notes:\n",
    "> - Default max length = **256 tokens**; adjust via `max_length` when calling.\n",
    "> - Ensure `transformers` is installed (>=4.30 recommended).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb897853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "📝 Saved text encoder manifest → v7\\model\\text_encoder\\config.json\n",
      "Sanity:\n",
      "  input batch: 4\n",
      "  tokens: (4, 45, 256) (B, L, 256)\n",
      "  mask:   (4, 45) (B, L), 1=valid, 0=pad\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & constants\n",
    "# -----------------------------\n",
    "MODEL_DIR = Path(\"v7/model\")\n",
    "TEXT_DIR  = MODEL_DIR / \"text_encoder\"\n",
    "TEXT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "train_npz = PREP_DIR / \"train.npz\"\n",
    "assert train_npz.exists(), \"Missing train split. Please run Phase 1, Cells 3–4.\"\n",
    "\n",
    "# -----------------------------\n",
    "# Config you can tweak\n",
    "# -----------------------------\n",
    "CHEMBERTA_CKPT = \"seyonec/ChemBERTa-zinc-base-v1\"   # or: \"DeepChem/ChemBERTa-77M-MLM\"\n",
    "FUSION_DIM     = 256\n",
    "DROPOUT_PROB   = 0.1\n",
    "MAX_SEQ_LEN    = 256    # default when encoding batches\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Text encoder wrapper\n",
    "# -----------------------------\n",
    "class ChemBERTaEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a SMILES-aware ChemBERTa to produce token embeddings aligned to fusion dim.\n",
    "    Returns (text_tokens, text_mask):\n",
    "      - text_tokens: (B, L, FUSION_DIM)\n",
    "      - text_mask:   (B, L) int {0,1}\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ckpt_name: str = CHEMBERTA_CKPT,\n",
    "        fusion_dim: int = FUSION_DIM,\n",
    "        dropout_p: float = DROPOUT_PROB,\n",
    "        gradient_checkpointing: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.ckpt_name = ckpt_name\n",
    "\n",
    "        # Tokenizer/Model\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "        self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "\n",
    "        hidden_size = self.backbone.config.hidden_size\n",
    "        self.proj = nn.Sequential(\n",
    "            nn.Dropout(dropout_p),\n",
    "            nn.Linear(hidden_size, fusion_dim),\n",
    "        )\n",
    "        self.ln = nn.LayerNorm(fusion_dim)\n",
    "\n",
    "        if gradient_checkpointing and hasattr(self.backbone, \"gradient_checkpointing_enable\"):\n",
    "            self.backbone.gradient_checkpointing_enable()\n",
    "\n",
    "        # Keep mask semantics explicit: 1=valid, 0=pad\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id\n",
    "        if self.pad_token_id is None:\n",
    "            # Some Roberta tokenizers don't have pad by default; set to eos\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "            self.pad_token_id = self.tokenizer.pad_token_id\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(\n",
    "        self,\n",
    "        smiles_list: List[str],\n",
    "        max_length: int = MAX_SEQ_LEN,\n",
    "        add_special_tokens: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Fast no-grad encode → (text_tokens, text_mask)\n",
    "        \"\"\"\n",
    "        self.eval()\n",
    "        return self.forward(smiles_list, max_length, add_special_tokens)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        smiles_list: List[str],\n",
    "        max_length: int = MAX_SEQ_LEN,\n",
    "        add_special_tokens: bool = True,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Forward pass → (text_tokens, text_mask)\n",
    "        \"\"\"\n",
    "        enc = self.tokenizer(\n",
    "            list(smiles_list),\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            add_special_tokens=add_special_tokens,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids      = enc[\"input_ids\"].to(device)\n",
    "        attention_mask = enc[\"attention_mask\"].to(device)  # 1=valid, 0=pad\n",
    "\n",
    "        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden = outputs.last_hidden_state  # (B, L, H)\n",
    "\n",
    "        tokens = self.proj(last_hidden)          # (B, L, fusion_dim)\n",
    "        tokens = self.ln(tokens)                 # (B, L, fusion_dim)\n",
    "\n",
    "        # Return mask as int {0,1} to match fusion core expectations\n",
    "        mask = attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "        return tokens, mask\n",
    "\n",
    "    def freeze_backbone(self, n_unfrozen_layers: int = 0) -> None:\n",
    "        \"\"\"\n",
    "        Freeze full backbone; optionally unfreeze the last `n_unfrozen_layers` transformer blocks.\n",
    "        \"\"\"\n",
    "        # Freeze all backbone parameters\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        if n_unfrozen_layers > 0:\n",
    "            assert hasattr(self.backbone, \"encoder\") or hasattr(self.backbone, \"roberta\"), \\\n",
    "                \"Unexpected backbone structure; adjust unfreezing logic.\"\n",
    "            # For RoBERTa-like models in HF, layers live under .encoder.layer (or .roberta.encoder.layer)\n",
    "            encoder = getattr(self.backbone, \"encoder\", None)\n",
    "            if encoder is None and hasattr(self.backbone, \"roberta\"):\n",
    "                encoder = self.backbone.roberta.encoder\n",
    "\n",
    "            if encoder is not None and hasattr(encoder, \"layer\"):\n",
    "                L = len(encoder.layer)\n",
    "                for idx in range(L - n_unfrozen_layers, L):\n",
    "                    for p in encoder.layer[idx].parameters():\n",
    "                        p.requires_grad = True\n",
    "\n",
    "        # Always keep projection & layernorm trainable\n",
    "        for p in self.proj.parameters():\n",
    "            p.requires_grad = True\n",
    "        for p in self.ln.parameters():\n",
    "            p.requires_grad = True\n",
    "\n",
    "# -----------------------------\n",
    "# Build & save a minimal manifest\n",
    "# -----------------------------\n",
    "text_encoder = ChemBERTaEncoder(\n",
    "    ckpt_name=CHEMBERTA_CKPT,\n",
    "    fusion_dim=FUSION_DIM,\n",
    "    dropout_p=DROPOUT_PROB,\n",
    "    gradient_checkpointing=False,  # set True if you need memory savings\n",
    ").to(device)\n",
    "\n",
    "manifest = {\n",
    "    \"checkpoint\": CHEMBERTA_CKPT,\n",
    "    \"fusion_dim\": FUSION_DIM,\n",
    "    \"dropout_p\": DROPOUT_PROB,\n",
    "    \"max_seq_len_default\": MAX_SEQ_LEN,\n",
    "    \"pad_token_id\": text_encoder.pad_token_id,\n",
    "    \"hidden_size\": int(text_encoder.backbone.config.hidden_size),\n",
    "    \"device\": str(device),\n",
    "}\n",
    "(TEXT_DIR / \"config.json\").write_text(json.dumps(manifest, indent=2))\n",
    "print(\"📝 Saved text encoder manifest →\", TEXT_DIR / \"config.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# 🔎 Quick sanity test on real SMILES from train split\n",
    "# -----------------------------\n",
    "batch = np.load(train_npz, allow_pickle=True)\n",
    "sample_smiles = [str(s) for s in batch[\"smiles\"][:4].tolist()]  # small batch of real strings\n",
    "\n",
    "with torch.no_grad():\n",
    "    toks, mask = text_encoder.encode(sample_smiles, max_length=MAX_SEQ_LEN)\n",
    "\n",
    "print(\"Sanity:\")\n",
    "print(\"  input batch:\", len(sample_smiles))\n",
    "print(\"  tokens:\", tuple(toks.shape), \"(B, L, 256)\")\n",
    "print(\"  mask:  \", tuple(mask.shape), \"(B, L), 1=valid, 0=pad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4b4961",
   "metadata": {},
   "source": [
    "### 3: Graph Encoder (RDKit → GIN)\n",
    "\n",
    "This cell builds a lightweight GIN graph encoder without external GNN libs:\n",
    "- RDKit featurisation → padded tensors\n",
    "- Pure-PyTorch GIN layers (sum aggregation + MLP + residual + LayerNorm)\n",
    "- Outputs per-node embeddings (dim=256) + masks, ready for fusion\n",
    "\n",
    "Saves:\n",
    "- `v7/model/graph_encoder/config.json`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbb6e580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "📝 Saved graph encoder manifest → v7\\model\\graph_encoder\\config.json\n",
      "Sanity:\n",
      "  input batch: 4\n",
      "  nodes: (4, 21, 256) (B, N, 256)\n",
      "  mask:  (4, 21) (B, N), 1=valid, 0=pad\n",
      "  valid node counts: [16, 15, 21, 20]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from rdkit import Chem\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & constants\n",
    "# -----------------------------\n",
    "MODEL_DIR = Path(\"v7/model\")\n",
    "GRAPH_DIR = MODEL_DIR / \"graph_encoder\"\n",
    "GRAPH_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "train_npz = PREP_DIR / \"train.npz\"\n",
    "assert train_npz.exists(), \"Missing train split. Please run Phase 1 Cells 3–4.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "FUSION_DIM   = 256\n",
    "MAX_NODES    = 128   # cap for very large molecules (rare in Tox21)\n",
    "DROPOUT_PROB = 0.1\n",
    "GIN_LAYERS   = 4\n",
    "GIN_HIDDEN   = 256   # keep equal to fusion dim\n",
    "\n",
    "# -----------------------------\n",
    "# Atom featurisation\n",
    "# -----------------------------\n",
    "# Common organic set; everything else -> \"other\"\n",
    "ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "HYB_LIST  = [\n",
    "    Chem.rdchem.HybridizationType.S,\n",
    "    Chem.rdchem.HybridizationType.SP,\n",
    "    Chem.rdchem.HybridizationType.SP2,\n",
    "    Chem.rdchem.HybridizationType.SP3,\n",
    "    Chem.rdchem.HybridizationType.SP3D,\n",
    "    Chem.rdchem.HybridizationType.SP3D2,\n",
    "]\n",
    "CHIRAL_LIST = [\n",
    "    Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "    Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "    Chem.rdchem.ChiralType.CHI_OTHER,\n",
    "]\n",
    "\n",
    "def one_hot(value, choices):\n",
    "    vec = [0]*len(choices)\n",
    "    if value in choices:\n",
    "        vec[choices.index(value)] = 1\n",
    "    return vec\n",
    "\n",
    "def clamp_one_hot_int(value: int, lo: int, hi: int) -> List[int]:\n",
    "    \"\"\"One-hot for integer value clamped to [lo, hi]; extra bucket if outside range.\"\"\"\n",
    "    # buckets: lo..hi and an \"other\"\n",
    "    buckets = list(range(lo, hi+1))\n",
    "    if value < lo or value > hi:\n",
    "        return [0]*(len(buckets)) + [1]\n",
    "    out = [0]*(len(buckets)+1)\n",
    "    out[value - lo] = 1\n",
    "    return out\n",
    "\n",
    "def atom_features(atom: Chem.rdchem.Atom) -> List[float]:\n",
    "    sym = atom.GetSymbol()\n",
    "    atom_type = one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST + [\"other\"])\n",
    "\n",
    "    degree = clamp_one_hot_int(atom.GetDegree(), 0, 5)              # 7 dims (0..5 + other)\n",
    "    formal = clamp_one_hot_int(atom.GetFormalCharge(), -2, 2)       # 6 dims (-2..2 + other)\n",
    "    hyb    = one_hot(atom.GetHybridization(), HYB_LIST) + [0]       # +1 \"other\"\n",
    "    aromatic = [1 if atom.GetIsAromatic() else 0]\n",
    "    in_ring  = [1 if atom.IsInRing() else 0]\n",
    "    chiral   = one_hot(atom.GetChiralTag(), CHIRAL_LIST)\n",
    "\n",
    "    total_h  = clamp_one_hot_int(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)  # 6 dims\n",
    "    valence  = clamp_one_hot_int(atom.GetTotalValence(), 0, 5)                     # 7 dims\n",
    "    mass     = [atom.GetMass() / 200.0]  # scale roughly into [0, 1]\n",
    "\n",
    "    feat = atom_type + degree + formal + hyb + aromatic + in_ring + chiral + total_h + valence + mass\n",
    "    return feat\n",
    "\n",
    "def smiles_to_graph(smi: str, max_nodes: int = MAX_NODES) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      x:   (N, F_node)\n",
    "      adj: (N, N) binary adjacency (no self loops)\n",
    "    Truncates to max_nodes if needed.\n",
    "    \"\"\"\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None or mol.GetNumAtoms() == 0:\n",
    "        return np.zeros((0,0), dtype=np.float32), np.zeros((0,0), dtype=np.float32)\n",
    "\n",
    "    # Atom features\n",
    "    feats = [atom_features(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "    x = np.asarray(feats, dtype=np.float32)\n",
    "\n",
    "    # Adjacency (no self-loops here; we’ll handle in GIN)\n",
    "    N = mol.GetNumAtoms()\n",
    "    adj = np.zeros((N, N), dtype=np.float32)\n",
    "    for bond in mol.GetBonds():\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        adj[i, j] = 1.0\n",
    "        adj[j, i] = 1.0\n",
    "\n",
    "    # Truncate if needed\n",
    "    if N > max_nodes:\n",
    "        x = x[:max_nodes]\n",
    "        adj = adj[:max_nodes, :max_nodes]\n",
    "\n",
    "    return x, adj\n",
    "\n",
    "def collate_graphs(smiles_list: List[str], max_nodes: int = MAX_NODES) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Build a padded batch:\n",
    "      X:     (B, N_max, F_node)\n",
    "      A:     (B, N_max, N_max)\n",
    "      mask:  (B, N_max) 1=valid, 0=pad\n",
    "    \"\"\"\n",
    "    graphs = [smiles_to_graph(s, max_nodes) for s in smiles_list]\n",
    "    N_max = max([g[0].shape[0] for g in graphs] + [1])\n",
    "\n",
    "    # Node feature dim\n",
    "    F_node = graphs[0][0].shape[1] if graphs[0][0].size > 0 else len(atom_features(Chem.MolFromSmiles(\"C\").GetAtomWithIdx(0)))\n",
    "\n",
    "    B = len(graphs)\n",
    "    X = np.zeros((B, N_max, F_node), dtype=np.float32)\n",
    "    A = np.zeros((B, N_max, N_max), dtype=np.float32)\n",
    "    M = np.zeros((B, N_max), dtype=np.int64)\n",
    "\n",
    "    for i, (x, adj) in enumerate(graphs):\n",
    "        n = x.shape[0]\n",
    "        if n == 0:\n",
    "            continue\n",
    "        X[i, :n, :] = x\n",
    "        A[i, :n, :n] = adj\n",
    "        M[i, :n] = 1\n",
    "\n",
    "    # to tensors\n",
    "    X = torch.from_numpy(X)\n",
    "    A = torch.from_numpy(A)\n",
    "    M = torch.from_numpy(M)\n",
    "    return X, A, M\n",
    "\n",
    "# -----------------------------\n",
    "# Lightweight GIN (pure torch)\n",
    "# -----------------------------\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, eps_init: float = 0.0, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(eps_init, dtype=torch.float32))\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(hidden_dim),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, adj: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        x:    (B, N, D)\n",
    "        adj:  (B, N, N)  binary adjacency (no self loops)\n",
    "        mask: (B, N)     1=valid, 0=pad\n",
    "        \"\"\"\n",
    "        # Add self term explicitly: (1+eps) * x  +  sum_neighbors(x)\n",
    "        neigh = torch.matmul(adj, x)  # (B, N, D)\n",
    "        out = (1.0 + self.eps) * x + neigh\n",
    "        out = self.mlp(out)\n",
    "\n",
    "        # Zero-out padded nodes\n",
    "        out = out * mask.unsqueeze(-1).to(out.dtype)\n",
    "        return out\n",
    "\n",
    "class GraphGINEncoder(nn.Module):\n",
    "    def __init__(self, node_in_dim: int, hidden_dim: int = GIN_HIDDEN, n_layers: int = GIN_LAYERS, dropout: float = DROPOUT_PROB):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(\n",
    "            nn.Linear(node_in_dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden_dim, eps_init=0.0, dropout=dropout) for _ in range(n_layers)])\n",
    "        self.out_ln = nn.LayerNorm(hidden_dim)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def encode(self, smiles_list: List[str], max_nodes: int = MAX_NODES) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        self.eval()\n",
    "        return self.forward(smiles_list, max_nodes)\n",
    "\n",
    "    def forward(self, smiles_list: List[str], max_nodes: int = MAX_NODES) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          node_embeddings: (B, N, hidden_dim)\n",
    "          mask:            (B, N) int {0,1}\n",
    "        \"\"\"\n",
    "        X, A, M = collate_graphs(smiles_list, max_nodes)  # (B,N,F), (B,N,N), (B,N)\n",
    "        X = X.to(device)\n",
    "        A = A.to(device)\n",
    "        M = M.to(device)\n",
    "\n",
    "        h = self.inp(X)  # (B,N,D)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, A, M)\n",
    "        h = self.out_ln(h)\n",
    "\n",
    "        # final mask as int {0,1}\n",
    "        mask = M.to(dtype=torch.int32)\n",
    "        return h, mask\n",
    "\n",
    "# -----------------------------\n",
    "# Build encoder & save manifest\n",
    "# -----------------------------\n",
    "# Infer node feature dim from a simple atom (or compute from a real SMILES)\n",
    "probe_x, _ = smiles_to_graph(\"CCO\")\n",
    "node_in_dim = int(probe_x.shape[1]) if probe_x.size > 0 else 64  # fallback\n",
    "\n",
    "graph_encoder = GraphGINEncoder(node_in_dim=node_in_dim, hidden_dim=FUSION_DIM, n_layers=GIN_LAYERS, dropout=DROPOUT_PROB).to(device)\n",
    "\n",
    "manifest = {\n",
    "    \"fusion_dim\": FUSION_DIM,\n",
    "    \"hidden_dim\": FUSION_DIM,\n",
    "    \"n_layers\": GIN_LAYERS,\n",
    "    \"dropout_p\": DROPOUT_PROB,\n",
    "    \"max_nodes\": MAX_NODES,\n",
    "    \"node_in_dim\": node_in_dim,\n",
    "    \"atom_types\": ATOM_LIST + [\"other\"],\n",
    "    \"hybridizations\": [str(h) for h in HYB_LIST] + [\"other\"],\n",
    "    \"chiral_types\": [int(c) for c in CHIRAL_LIST],\n",
    "    \"device\": str(device),\n",
    "}\n",
    "(GRAPH_DIR / \"config.json\").write_text(json.dumps(manifest, indent=2))\n",
    "print(\"📝 Saved graph encoder manifest →\", GRAPH_DIR / \"config.json\")\n",
    "\n",
    "# -----------------------------\n",
    "# 🔎 Sanity test: 4 real SMILES from train split\n",
    "# -----------------------------\n",
    "batch = np.load(train_npz, allow_pickle=True)\n",
    "sample_smiles = [str(s) for s in batch[\"smiles\"][:4].tolist()]\n",
    "\n",
    "with torch.no_grad():\n",
    "    nodes, mask = graph_encoder.encode(sample_smiles, max_nodes=MAX_NODES)\n",
    "\n",
    "print(\"Sanity:\")\n",
    "print(\"  input batch:\", len(sample_smiles))\n",
    "print(\"  nodes:\", tuple(nodes.shape), \"(B, N, 256)\")\n",
    "print(\"  mask: \", tuple(mask.shape), \"(B, N), 1=valid, 0=pad\")\n",
    "print(\"  valid node counts:\", mask.sum(dim=1).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d695c26",
   "metadata": {},
   "source": [
    "### 4: Full V7 Fusion Model + Label-Specialist Heads\n",
    "\n",
    "This cell assembles the full V7 model:\n",
    "- Text encoder: ChemBERTa (from Cell 2)\n",
    "- Graph encoder: lightweight GIN (from Cell 3)\n",
    "- Descriptors branch: MLP (inside fusion)\n",
    "- Fusion: **cross-attention** (SMILES tokens query graph nodes), masked pooling\n",
    "- Heads:\n",
    "  - **Shared multi-label head** (12 logits)\n",
    "  - **Optionally**: 12 **label-specialist** heads (one-vs-rest heads) for ensembling\n",
    "\n",
    "What it saves/creates:\n",
    "- Model manifest: `v7/model/v7_fusion/config.json`\n",
    "- Ensemble folders (empty, ready for training): `v7/model/ensembles/<LABEL>/`\n",
    "\n",
    "Sanity test (no training):\n",
    "- Takes a tiny batch from `v7/data/prepared/train.npz`\n",
    "- Runs both **shared** and **specialist** forward passes\n",
    "- Prints tensor shapes and checks for NaNs\n",
    "\n",
    "> You can freeze/unfreeze encoders via helper methods for staged training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf2e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "📝 Saved fusion manifest → v7\\model\\v7_fusion\\config.json\n",
      "📁 Ensemble label folders ready under: v7\\model\\ensembles\n",
      "Sanity:\n",
      "  shared logits:      (4, 12)  (B, 12)\n",
      "  specialist logits:  (4, 12)  (B, 12)\n",
      "  fused vector shape: (4, 768)  (B, 768)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# -----------------------------\n",
    "# Paths & constants\n",
    "# -----------------------------\n",
    "MODEL_DIR = Path(\"v7/model\")\n",
    "FUSION_DIR = MODEL_DIR / \"v7_fusion\"\n",
    "FUSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ENSEMBLE_DIR = MODEL_DIR / \"ensembles\"\n",
    "ENSEMBLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "TEXT_DIR = MODEL_DIR / \"text_encoder\"\n",
    "GRAPH_DIR = MODEL_DIR / \"graph_encoder\"\n",
    "\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "train_npz_path = PREP_DIR / \"train.npz\"\n",
    "manifest_path = PREP_DIR / \"dataset_manifest.json\"\n",
    "assert train_npz_path.exists() and manifest_path.exists(), \"Missing prepared data or manifest.\"\n",
    "\n",
    "with open(manifest_path) as f:\n",
    "    ds_manifest = json.load(f)\n",
    "LABEL_NAMES = ds_manifest[\"labels\"]\n",
    "N_LABELS = len(LABEL_NAMES)\n",
    "DESC_IN_DIM = ds_manifest[\"n_features\"]    # 208 features from RDKit\n",
    "FUSION_DIM = 256\n",
    "MAX_SEQ_LEN = 256\n",
    "MAX_NODES = 128\n",
    "DROPOUT = 0.1\n",
    "N_HEADS = 4\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Label-specialist head\n",
    "# -----------------------------\n",
    "class LabelHead(nn.Module):\n",
    "    \"\"\"Small MLP head for a single label (binary logit). Input: fused (3*D).\"\"\"\n",
    "    def __init__(self, fused_dim: int, hidden: int = 256, p: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(fused_dim, hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(hidden, 1),\n",
    "        )\n",
    "    def forward(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        # z: (B, fused_dim) -> (B, 1)\n",
    "        return self.net(z)\n",
    "\n",
    "# -----------------------------\n",
    "# Full Fusion Model\n",
    "# -----------------------------\n",
    "class V7FusionModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Full model that composes:\n",
    "      - text_encoder: ChemBERTaEncoder\n",
    "      - graph_encoder: GraphGINEncoder\n",
    "      - cross-attn + desc MLP + pooling to produce fused vector\n",
    "      - either shared multi-label head OR 12 specialist heads\n",
    "\n",
    "    Modes:\n",
    "      specialist=False (default): shared multi-label head → (B,12)\n",
    "      specialist=True:  12 label heads (one-vs-rest) → concat → (B,12)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        text_encoder: \"ChemBERTaEncoder\",\n",
    "        graph_encoder: \"GraphGINEncoder\",\n",
    "        desc_in_dim: int = DESC_IN_DIM,\n",
    "        dim: int = FUSION_DIM,\n",
    "        n_heads: int = N_HEADS,\n",
    "        n_labels: int = N_LABELS,\n",
    "        dropout: float = DROPOUT,\n",
    "        specialist: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.text_encoder = text_encoder\n",
    "        self.graph_encoder = graph_encoder\n",
    "        self.dim = dim\n",
    "        self.n_labels = n_labels\n",
    "        self.specialist = specialist\n",
    "\n",
    "        # Fusion components (reusing the exact modules from Cell 1 design)\n",
    "        self.cross = CrossAttentionBlock(dim=dim, n_heads=n_heads, p=dropout)\n",
    "        self.desc_mlp = DescriptorMLP(in_dim=desc_in_dim, out_dim=dim, hidden=256, p=dropout)\n",
    "\n",
    "        fused_dim = dim * 3  # [text_pool ; graph_pool ; desc_embed]\n",
    "\n",
    "        if specialist:\n",
    "            # 12 label-wise heads\n",
    "            self.label_heads = nn.ModuleList([LabelHead(fused_dim=fused_dim, hidden=dim, p=dropout) for _ in range(n_labels)])\n",
    "            self.shared_head = None\n",
    "        else:\n",
    "            # Shared multi-label head\n",
    "            self.shared_head = FusionClassifier(dim=dim, n_labels=n_labels, p=dropout)\n",
    "            self.label_heads = None\n",
    "\n",
    "    def freeze_text_backbone(self, n_unfrozen_layers: int = 0):\n",
    "        \"\"\"Freeze ChemBERTa backbone; keep proj/LN trainable; optionally unfreeze last N layers.\"\"\"\n",
    "        self.text_encoder.freeze_backbone(n_unfrozen_layers=n_unfrozen_layers)\n",
    "\n",
    "    def freeze_graph(self, freeze: bool = True):\n",
    "        for p in self.graph_encoder.parameters():\n",
    "            p.requires_grad = not freeze\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        smiles_list: List[str],\n",
    "        desc_feats: torch.Tensor,\n",
    "        max_seq_len: int = MAX_SEQ_LEN,\n",
    "        max_nodes: int = MAX_NODES,\n",
    "        return_intermediates: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[dict]]:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "          smiles_list: list[str] length B\n",
    "          desc_feats:  (B, DESC_IN_DIM) float tensor (already imputed/scaled)\n",
    "        Returns:\n",
    "          logits: (B, 12)\n",
    "          intermediates (optional dict)\n",
    "        \"\"\"\n",
    "        # Encode text & graph\n",
    "        text_tokens, text_mask = self.text_encoder(smiles_list, max_length=max_seq_len)  # (B,L,D), (B,L)\n",
    "        graph_nodes, graph_mask = self.graph_encoder(smiles_list, max_nodes=max_nodes)    # (B,N,D), (B,N)\n",
    "\n",
    "        # Ensure tensors on same device\n",
    "        text_tokens = text_tokens.to(device)\n",
    "        text_mask   = text_mask.to(device)\n",
    "        graph_nodes = graph_nodes.to(device)\n",
    "        graph_mask  = graph_mask.to(device)\n",
    "        desc_feats  = desc_feats.to(device)\n",
    "\n",
    "        # Cross-attention update of text tokens with graph context\n",
    "        text_attn = self.cross(text_tokens, text_mask, graph_nodes, graph_mask)  # (B,L,D)\n",
    "\n",
    "        # Descriptor embedding\n",
    "        desc_embed = self.desc_mlp(desc_feats)  # (B,D)\n",
    "\n",
    "        # Masked pools\n",
    "        text_pool  = masked_mean(text_attn,   text_mask,  dim=1)  # (B,D)\n",
    "        graph_pool = masked_mean(graph_nodes, graph_mask, dim=1)  # (B,D)\n",
    "        fused = torch.cat([text_pool, graph_pool, desc_embed], dim=-1)  # (B, 3D)\n",
    "\n",
    "        # Heads\n",
    "        if self.specialist:\n",
    "            logits_list = [head(fused) for head in self.label_heads]  # list of (B,1)\n",
    "            logits = torch.cat(logits_list, dim=1)                    # (B,12)\n",
    "        else:\n",
    "            logits = self.shared_head(text_attn, text_mask, graph_nodes, graph_mask, desc_embed)  # (B,12)\n",
    "\n",
    "        aux = None\n",
    "        if return_intermediates:\n",
    "            aux = {\n",
    "                \"text_tokens\": text_tokens,\n",
    "                \"text_attended\": text_attn,\n",
    "                \"graph_nodes\": graph_nodes,\n",
    "                \"desc_embed\": desc_embed,\n",
    "                \"text_pool\": text_pool,\n",
    "                \"graph_pool\": graph_pool,\n",
    "                \"fused\": fused,\n",
    "            }\n",
    "        return logits, aux\n",
    "\n",
    "# -----------------------------\n",
    "# Build text/graph encoders from previous cells\n",
    "# -----------------------------\n",
    "# These objects should exist if you ran Cell 2 and Cell 3; otherwise re-instantiate:\n",
    "try:\n",
    "    text_encoder\n",
    "except NameError:\n",
    "    # fallback: rebuild with defaults\n",
    "    text_encoder = ChemBERTaEncoder().to(device)\n",
    "\n",
    "try:\n",
    "    graph_encoder\n",
    "except NameError:\n",
    "    # probe node_in_dim like in Cell 3 if needed\n",
    "    from rdkit import Chem\n",
    "    def _probe_node_in_dim():\n",
    "        from rdkit.Chem.Scaffolds import MurckoScaffold\n",
    "        mol = Chem.MolFromSmiles(\"CCO\")\n",
    "        from math import isfinite\n",
    "        return 51  # fallback from previous cell config\n",
    "    graph_encoder = GraphGINEncoder(node_in_dim=_probe_node_in_dim(), hidden_dim=FUSION_DIM, n_layers=4, dropout=0.1).to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Instantiate both variants (shared & specialist)\n",
    "# -----------------------------\n",
    "v7_shared = V7FusionModel(\n",
    "    text_encoder=text_encoder,\n",
    "    graph_encoder=graph_encoder,\n",
    "    desc_in_dim=DESC_IN_DIM,\n",
    "    dim=FUSION_DIM,\n",
    "    n_heads=N_HEADS,\n",
    "    n_labels=N_LABELS,\n",
    "    dropout=DROPOUT,\n",
    "    specialist=False,\n",
    ").to(device)\n",
    "\n",
    "v7_specialist = V7FusionModel(\n",
    "    text_encoder=text_encoder,\n",
    "    graph_encoder=graph_encoder,\n",
    "    desc_in_dim=DESC_IN_DIM,\n",
    "    dim=FUSION_DIM,\n",
    "    n_heads=N_HEADS,\n",
    "    n_labels=N_LABELS,\n",
    "    dropout=DROPOUT,\n",
    "    specialist=True,\n",
    ").to(device)\n",
    "\n",
    "# -----------------------------\n",
    "# Save fusion config & create ensemble folders\n",
    "# -----------------------------\n",
    "fusion_manifest = {\n",
    "    \"labels\": LABEL_NAMES,\n",
    "    \"n_labels\": N_LABELS,\n",
    "    \"desc_in_dim\": DESC_IN_DIM,\n",
    "    \"fusion_dim\": FUSION_DIM,\n",
    "    \"n_heads\": N_HEADS,\n",
    "    \"dropout\": DROPOUT,\n",
    "    \"max_seq_len\": MAX_SEQ_LEN,\n",
    "    \"max_nodes\": MAX_NODES,\n",
    "    \"modes\": [\"shared\", \"specialist\"],\n",
    "    \"paths\": {\n",
    "        \"text_encoder_config\": str(TEXT_DIR / \"config.json\"),\n",
    "        \"graph_encoder_config\": str(GRAPH_DIR / \"config.json\"),\n",
    "        \"ensemble_root\": str(ENSEMBLE_DIR),\n",
    "    }\n",
    "}\n",
    "(FUSION_DIR / \"config.json\").write_text(json.dumps(fusion_manifest, indent=2))\n",
    "print(\"📝 Saved fusion manifest →\", FUSION_DIR / \"config.json\")\n",
    "\n",
    "# Create per-label ensemble directories (empty for now)\n",
    "for label in LABEL_NAMES:\n",
    "    (ENSEMBLE_DIR / label).mkdir(parents=True, exist_ok=True)\n",
    "print(\"📁 Ensemble label folders ready under:\", ENSEMBLE_DIR)\n",
    "\n",
    "# -----------------------------\n",
    "# 🔎 Sanity: small forward on real data (no training)\n",
    "# -----------------------------\n",
    "batch = np.load(train_npz_path, allow_pickle=True)\n",
    "smiles_batch = [str(s) for s in batch[\"smiles\"][:4].tolist()]\n",
    "desc_batch = torch.tensor(batch[\"X\"][:4], dtype=torch.float32, device=device)  # imputed+scaled\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Shared head\n",
    "    logits_shared, aux_shared = v7_shared(smiles_batch, desc_batch, return_intermediates=True)\n",
    "    # Specialist heads\n",
    "    logits_spec,  aux_spec   = v7_specialist(smiles_batch, desc_batch, return_intermediates=False)\n",
    "\n",
    "print(\"Sanity:\")\n",
    "print(\"  shared logits:     \", tuple(logits_shared.shape), \" (B, 12)\")\n",
    "print(\"  specialist logits: \", tuple(logits_spec.shape),   \" (B, 12)\")\n",
    "assert torch.isfinite(logits_shared).all() and torch.isfinite(logits_spec).all(), \"Found non-finite logits.\"\n",
    "\n",
    "print(\"  fused vector shape:\", tuple(aux_shared[\"fused\"].shape), \" (B, 768)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9412f57",
   "metadata": {},
   "source": [
    "## Phase 3 (Train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a761e734",
   "metadata": {},
   "source": [
    "### 0: Hardware & Throughput Probe (fp32 vs AMP)\n",
    "\n",
    "This cell profiles:\n",
    "- GPU name/VRAM/CUDA/torch versions\n",
    "- Full-model forward throughput at batch sizes `[8, 16, 24, 32]`\n",
    "- Mixed precision (AMP) vs fp32\n",
    "- A quick fwd+backward step (1 iter) to estimate step time\n",
    "- Saves results:\n",
    "  - `v7/results/meta/hw_probe.json`\n",
    "  - `v7/results/meta/throughput_probe.csv`\n",
    "\n",
    "**Output:** A recommended batch size & precision mode for training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe1b771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW: {\n",
      "  \"torch_version\": \"2.6.0+cu124\",\n",
      "  \"cuda_available\": true,\n",
      "  \"cuda_version\": \"12.4\",\n",
      "  \"device_name\": \"NVIDIA GeForce RTX 4070 Ti\",\n",
      "  \"total_vram_gb\": 11.99\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_2928\\3958611543.py:200: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else nullcontext()\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_2928\\3958611543.py:219: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_2928\\3958611543.py:220: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else nullcontext()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BS=8 | fwd fp32 0.0060s | fwd amp 0.0068s | step fp32 0.0507s | step amp 0.0336s | peak 0.87 GB\n",
      "BS=16 | fwd fp32 0.0100s | fwd amp 0.0095s | step fp32 0.0349s | step amp 0.0291s | peak 0.88 GB\n",
      "BS=24 | fwd fp32 0.0134s | fwd amp 0.0125s | step fp32 0.0439s | step amp 0.0400s | peak 0.98 GB\n",
      "BS=32 | fwd fp32 0.0167s | fwd amp 0.0133s | step fp32 0.0529s | step amp 0.0462s | peak 1.24 GB\n",
      "\n",
      "=== Probe Summary ===\n",
      "{\n",
      "  \"batch_size\": 32,\n",
      "  \"precision\": \"amp\",\n",
      "  \"note\": \"Use grad accumulation to reach higher effective batch if needed.\"\n",
      "}\n",
      "Results saved to: v7\\results\\meta\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "from contextlib import nullcontext\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    v7_shared\n",
    "except NameError:\n",
    "    print(\"⚠️ v7_shared not found in memory — rebuilding minimal defaults.\")\n",
    "    # Minimal rebuild (assumes Phase 2 cells are available; else raise)\n",
    "    try:\n",
    "        text_encoder\n",
    "    except NameError:\n",
    "        from transformers import AutoTokenizer, AutoModel\n",
    "        class ChemBERTaEncoder(nn.Module):\n",
    "            def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "                super().__init__()\n",
    "                self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "                self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "                self.pad_token_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "                self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "                self.ln = nn.LayerNorm(fusion_dim)\n",
    "            def forward(self, smiles_list, max_length=256, add_special_tokens=True):\n",
    "                enc = self.tokenizer(list(smiles_list), padding=True, truncation=True, max_length=max_length, add_special_tokens=add_special_tokens, return_tensors=\"pt\")\n",
    "                input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "                out = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state\n",
    "                toks = self.ln(self.proj(out))\n",
    "                return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "    try:\n",
    "        graph_encoder\n",
    "    except NameError:\n",
    "        from rdkit import Chem\n",
    "        # Minimal graph encoder using same interfaces as before\n",
    "        ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "        def one_hot(v, choices): \n",
    "            z=[0]*len(choices); \n",
    "            if v in choices: z[choices.index(v)]=1\n",
    "            return z\n",
    "        def clamp_oh(v, lo, hi):\n",
    "            buckets=list(range(lo,hi+1))\n",
    "            if v<lo or v>hi: return [0]*len(buckets)+[1]\n",
    "            o=[0]*(len(buckets)+1); o[v-lo]=1; return o\n",
    "        def atom_features(atom):\n",
    "            hybs=[Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "            chir=[Chem.rdchem.ChiralType.CHI_UNSPECIFIED, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW, Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "            sym = atom.GetSymbol()\n",
    "            feat = one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST+[\"other\"])\n",
    "            feat += clamp_oh(atom.GetDegree(),0,5)\n",
    "            feat += clamp_oh(atom.GetFormalCharge(),-2,2)\n",
    "            feat += (one_hot(atom.GetHybridization(), hybs)+[0])\n",
    "            feat += [int(atom.GetIsAromatic())]\n",
    "            feat += [int(atom.IsInRing())]\n",
    "            feat += one_hot(atom.GetChiralTag(), chir)\n",
    "            feat += clamp_oh(atom.GetTotalNumHs(includeNeighbors=True),0,4)\n",
    "            feat += clamp_oh(atom.GetTotalValence(),0,5)\n",
    "            feat += [atom.GetMass()/200.0]\n",
    "            return feat\n",
    "        def smiles_to_graph(smi, max_nodes=128):\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None or mol.GetNumAtoms()==0:\n",
    "                return np.zeros((0,0),dtype=np.float32), np.zeros((0,0),dtype=np.float32)\n",
    "            feats = [atom_features(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "            x = np.asarray(feats, dtype=np.float32)\n",
    "            N = mol.GetNumAtoms()\n",
    "            adj = np.zeros((N,N), dtype=np.float32)\n",
    "            for b in mol.GetBonds():\n",
    "                i,j=b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "                adj[i,j]=1.0; adj[j,i]=1.0\n",
    "            if N>128: x=x[:128]; adj=adj[:128,:128]\n",
    "            return x, adj\n",
    "        def collate_graphs(smiles_batch):\n",
    "            graphs=[smiles_to_graph(s) for s in smiles_batch]\n",
    "            Nmax=max([g[0].shape[0] for g in graphs] + [1])\n",
    "            Fnode=graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "            B=len(graphs)\n",
    "            X=np.zeros((B,Nmax,Fnode),dtype=np.float32)\n",
    "            A=np.zeros((B,Nmax,Nmax),dtype=np.float32)\n",
    "            M=np.zeros((B,Nmax),dtype=np.int64)\n",
    "            for i,(x,a) in enumerate(graphs):\n",
    "                n=x.shape[0]; \n",
    "                if n==0: continue\n",
    "                X[i,:n,:]=x; A[i,:n,:n]=a; M[i,:n]=1\n",
    "            return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "        class GINLayer(nn.Module):\n",
    "            def __init__(self, h=256, p=0.1):\n",
    "                super().__init__()\n",
    "                self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "                self.mlp = nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "            def forward(self, x, adj, mask):\n",
    "                out=(1.0+self.eps)*x + torch.matmul(adj,x)\n",
    "                out=self.mlp(out)\n",
    "                return out*mask.unsqueeze(-1).to(out.dtype)\n",
    "        class GraphGINEncoder(nn.Module):\n",
    "            def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "                super().__init__()\n",
    "                self.inp=nn.Sequential(nn.Linear(node_in_dim,hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "                self.layers=nn.ModuleList([GINLayer(hidden_dim,p) for _ in range(n_layers)])\n",
    "                self.ln=nn.LayerNorm(hidden_dim)\n",
    "            def forward(self, smiles_list, max_nodes=128):\n",
    "                X,A,M=collate_graphs(smiles_list)\n",
    "                h=self.inp(X)\n",
    "                for layer in self.layers: h=layer(h,A,M)\n",
    "                return self.ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "    # Fusion pieces (masked_mean) minimal copy\n",
    "    def masked_mean(x, mask, dim=1):\n",
    "        mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "        denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "        return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "            self.ln  = nn.LayerNorm(dim)\n",
    "            self.do  = nn.Dropout(p)\n",
    "        def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "            Q=text_tokens.transpose(0,1); K=graph_nodes.transpose(0,1); V=graph_nodes.transpose(0,1)\n",
    "            kpm=(graph_mask==0)\n",
    "            attn,_=self.mha(Q,K,V, key_padding_mask=kpm)\n",
    "            attn=attn.transpose(0,1)\n",
    "            return self.ln(text_tokens + self.do(attn))\n",
    "\n",
    "    class DescriptorMLP(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.net=nn.Sequential(nn.Linear(in_dim,hidden), nn.GELU(), nn.Dropout(p), nn.Linear(hidden,out_dim), nn.GELU(), nn.Dropout(p))\n",
    "        def forward(self,x): return self.net(x)\n",
    "\n",
    "    class FusionClassifier(nn.Module):\n",
    "        def __init__(self, dim=256, n_labels=12, p=0.1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p), nn.Linear(dim*2, n_labels))\n",
    "        def forward(self, text_tokens, text_mask, graph_nodes, graph_mask, desc_embed):\n",
    "            text_pool = masked_mean(text_tokens, text_mask, 1)\n",
    "            graph_pool= masked_mean(graph_nodes, graph_mask, 1)\n",
    "            return self.net(torch.cat([text_pool, graph_pool, desc_embed], dim=-1))\n",
    "\n",
    "    class V7FusionModel(nn.Module):\n",
    "        def __init__(self, text_encoder, graph_encoder, desc_in_dim=208, dim=256, n_labels=12, n_heads=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.text_encoder=text_encoder\n",
    "            self.graph_encoder=graph_encoder\n",
    "            self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "            self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "            self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "        def forward(self, smiles_list, desc_feats, max_seq_len=256, max_nodes=128):\n",
    "            tt, tm = self.text_encoder(smiles_list, max_length=max_seq_len)\n",
    "            gn, gm = self.graph_encoder(smiles_list, max_nodes=max_nodes)\n",
    "            tt, tm, gn, gm, desc_feats = tt.to(device), tm.to(device), gn.to(device), gm.to(device), desc_feats.to(device)\n",
    "            tta = self.cross(tt, tm, gn, gm)\n",
    "            de  = self.desc_mlp(desc_feats)\n",
    "            logits = self.shared_head(tta, tm, gn, gm, de)\n",
    "            return logits\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    text_encoder = ChemBERTaEncoder().to(device)\n",
    "    graph_encoder= GraphGINEncoder().to(device)\n",
    "    v7_shared    = V7FusionModel(text_encoder, graph_encoder).to(device)\n",
    "\n",
    "# ---------------- Paths & data ----------------\n",
    "PREP_DIR = Path(\"v7/data/prepared\")\n",
    "RES_DIR  = Path(\"v7/results/meta\"); RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "train_npz = np.load(PREP_DIR / \"train.npz\", allow_pickle=True)\n",
    "\n",
    "smiles_train = [str(s) for s in train_npz[\"smiles\"].tolist()]\n",
    "X_train = torch.tensor(train_npz[\"X\"], dtype=torch.float32)\n",
    "\n",
    "# ---------------- HW info --------------------\n",
    "hw = {\n",
    "    \"torch_version\": torch.__version__,\n",
    "    \"cuda_available\": torch.cuda.is_available(),\n",
    "    \"cuda_version\": torch.version.cuda,\n",
    "    \"device_name\": torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"cpu\",\n",
    "    \"total_vram_gb\": round(torch.cuda.get_device_properties(0).total_memory/1024**3,2) if torch.cuda.is_available() else None,\n",
    "}\n",
    "(Path(RES_DIR / \"hw_probe.json\")).write_text(json.dumps(hw, indent=2))\n",
    "print(\"HW:\", json.dumps(hw, indent=2))\n",
    "\n",
    "# --------------- benchmark helpers -----------\n",
    "# --- Patch: unwrap model outputs to logits ---\n",
    "def _to_logits(out):\n",
    "    return out[0] if isinstance(out, (tuple, list)) else out\n",
    "\n",
    "def bench_forward(model, smiles_batch, desc_batch, amp=False, iters=10, warmup=3):\n",
    "    from contextlib import nullcontext\n",
    "    model.eval()\n",
    "    use_amp = amp and torch.cuda.is_available()\n",
    "    ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else nullcontext()\n",
    "\n",
    "    with torch.no_grad(), ctx:\n",
    "        for _ in range(warmup):\n",
    "            _ = _to_logits(model(smiles_batch, desc_batch))\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "\n",
    "    t0 = time.time()\n",
    "    with torch.no_grad(), ctx:\n",
    "        for _ in range(iters):\n",
    "            _ = _to_logits(model(smiles_batch, desc_batch))\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "    return (time.time() - t0) / iters\n",
    "\n",
    "def bench_step(model, smiles_batch, desc_batch, amp=False):\n",
    "    from contextlib import nullcontext\n",
    "    model.train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "    use_amp = amp and torch.cuda.is_available()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "    ctx = torch.cuda.amp.autocast(dtype=torch.float16) if use_amp else nullcontext()\n",
    "\n",
    "    # dummy targets for timing only\n",
    "    y = torch.rand((desc_batch.size(0), 12), device=desc_batch.device)\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "    t0 = time.time()\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "    with ctx:\n",
    "        logits = _to_logits(model(smiles_batch, desc_batch))\n",
    "        loss = loss_fn(logits, y)\n",
    "    if scaler.is_enabled():\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(opt); scaler.update()\n",
    "    else:\n",
    "        loss.backward(); opt.step()\n",
    "    if torch.cuda.is_available(): torch.cuda.synchronize()\n",
    "    return float(loss.item()), (time.time() - t0)\n",
    "\n",
    "# --- Re-run the driver loop & save artifacts again ---\n",
    "batch_sizes = [8, 16, 24, 32]\n",
    "results = []\n",
    "max_nodes_seen = 0\n",
    "\n",
    "for bs in batch_sizes:\n",
    "    try:\n",
    "        smiles_bs = smiles_train[:bs]\n",
    "        X_bs = X_train[:bs].to(next(v7_shared.parameters()).device, non_blocking=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            gn, gm = graph_encoder(smiles_bs, max_nodes=128)\n",
    "            max_nodes_seen = max(max_nodes_seen, int(gm.sum(dim=1).max().item()))\n",
    "\n",
    "        t_f32 = bench_forward(v7_shared, smiles_bs, X_bs, amp=False, iters=10, warmup=3)\n",
    "        t_amp = bench_forward(v7_shared, smiles_bs, X_bs, amp=True,  iters=10, warmup=3)\n",
    "\n",
    "        loss_f32, step_f32 = bench_step(v7_shared, smiles_bs, X_bs, amp=False)\n",
    "        loss_amp,  step_amp = bench_step(v7_shared, smiles_bs, X_bs, amp=True)\n",
    "\n",
    "        mem_alloc = torch.cuda.max_memory_allocated()/1024**3 if torch.cuda.is_available() else None\n",
    "        if torch.cuda.is_available(): torch.cuda.reset_peak_memory_stats()\n",
    "\n",
    "        results.append({\n",
    "            \"batch_size\": bs,\n",
    "            \"forward_fp32_s_per_batch\": round(t_f32, 4),\n",
    "            \"forward_amp_s_per_batch\":  round(t_amp, 4),\n",
    "            \"trainstep_fp32_s\": round(step_f32, 4),\n",
    "            \"trainstep_amp_s\":  round(step_amp, 4),\n",
    "            \"approx_samples_per_s_fp32\": round(bs / t_f32, 2),\n",
    "            \"approx_samples_per_s_amp\":  round(bs / t_amp, 2),\n",
    "            \"peak_mem_gb\": round(mem_alloc, 2) if mem_alloc is not None else None,\n",
    "        })\n",
    "        print(f\"BS={bs} | fwd fp32 {t_f32:.4f}s | fwd amp {t_amp:.4f}s | step fp32 {step_f32:.4f}s | step amp {step_amp:.4f}s | peak {results[-1]['peak_mem_gb']} GB\")\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e).lower():\n",
    "            print(f\"OOM at batch size {bs}.\")\n",
    "            results.append({\"batch_size\": bs, \"error\": \"OOM\"})\n",
    "            if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Save again\n",
    "df = pd.DataFrame(results)\n",
    "df.to_csv(RES_DIR / \"throughput_probe.csv\", index=False)\n",
    "(Path(RES_DIR / \"throughput_probe.json\")).write_text(json.dumps(results, indent=2))\n",
    "\n",
    "valid = [r for r in results if r.get(\"error\") is None]\n",
    "if valid:\n",
    "    succ_amp = [r for r in valid if r.get(\"forward_amp_s_per_batch\") is not None]\n",
    "    if succ_amp:\n",
    "        rec_bs = max(succ_amp, key=lambda r: r[\"batch_size\"])[\"batch_size\"]\n",
    "        rec_prec = \"amp\"\n",
    "    else:\n",
    "        rec_bs = max(valid, key=lambda r: r[\"batch_size\"])[\"batch_size\"]\n",
    "        rec_prec = \"fp32\"\n",
    "else:\n",
    "    rec_bs, rec_prec = 8, \"amp\"\n",
    "\n",
    "summary = {\n",
    "    \"hw\": hw,\n",
    "    \"max_nodes_seen\": int(max_nodes_seen),\n",
    "    \"results\": results,\n",
    "    \"recommendation\": {\n",
    "        \"batch_size\": int(rec_bs),\n",
    "        \"precision\": rec_prec,\n",
    "        \"note\": \"Use grad accumulation to reach higher effective batch if needed.\"\n",
    "    }\n",
    "}\n",
    "(Path(RES_DIR / \"hw_probe_summary.json\")).write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"\\n=== Probe Summary ===\")\n",
    "print(json.dumps(summary[\"recommendation\"], indent=2))\n",
    "print(f\"Results saved to: {RES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03be824b",
   "metadata": {},
   "source": [
    "### 1: Shared-Head Training Loop\n",
    "\n",
    "This cell trains the **shared multi-label V7 fusion model** with strong settings:\n",
    "\n",
    "- Windows-safe DataLoaders (`num_workers=0`)\n",
    "- Batch size 32, **grad accumulation 4** (effective 128)\n",
    "- **ASL** (gamma_neg=5.0, gamma_pos=1.0) + **class-balanced per-label weights**\n",
    "- **AMP** mixed precision, **EMA**, cosine LR with warmup\n",
    "- **Stage A** (8 epochs): ChemBERTa backbone **frozen**\n",
    "- **Stage B** (20 epochs): unfreeze **last 2** transformer blocks\n",
    "- Early stopping on **val macro PR-AUC**\n",
    "- Checkpoints:\n",
    "  - best → `v7/model/checkpoints/shared/best.pt`\n",
    "  - per-epoch → `v7/model/checkpoints/shared/epoch_{stage}{epoch:02d}.pt`\n",
    "- Logs:\n",
    "  - training rows → `v7/results/logs/train_log.jsonl`\n",
    "  - val summaries → `v7/results/artifacts/val_metrics.jsonl`\n",
    "  - run config → `v7/results/meta/train_run_config.json`\n",
    "\n",
    "> Run this cell to start training. We’ll add plots + specialist heads next.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "beae783c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Train batches: 195, Val batches: 25\n",
      "Train label prevalence: [0.047 0.038 0.128 0.051 0.135 0.055 0.027 0.165 0.037 0.056 0.167 0.061]\n",
      "Class-balanced alpha: [1.077 1.385 0.53  1.232 0.53  0.985 1.995 0.48  1.367 1.022 0.479 0.919]\n",
      "\n",
      "=== Stage A: Warm-up (text backbone frozen) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 1, 'train_loss': 0.035231313520135026, 'train_macro_pr_auc': 0.15394772991859337, 'train_macro_roc_auc': 0.6320955267390821, 'val_loss': 0.16247534304857253, 'val_macro_pr_auc': 0.060688107343346155, 'val_macro_roc_auc': 0.4460254985837288, 'epoch_time_s': 5.02}\n",
      "  ✅ New best (Stage A) macro PR-AUC: 0.0607 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:39] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 2, 'train_loss': 0.03287650308547876, 'train_macro_pr_auc': 0.2207926681933469, 'train_macro_roc_auc': 0.7010044653688422, 'val_loss': 0.1518526303768158, 'val_macro_pr_auc': 0.07123903216471082, 'val_macro_roc_auc': 0.48648823084936604, 'epoch_time_s': 4.77}\n",
      "  ✅ New best (Stage A) macro PR-AUC: 0.0712 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:47] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 3, 'train_loss': 0.03286819191506276, 'train_macro_pr_auc': 0.2186634655024724, 'train_macro_roc_auc': 0.7048009936671273, 'val_loss': 0.14330618798732758, 'val_macro_pr_auc': 0.0828460714633456, 'val_macro_roc_auc': 0.5262641855361049, 'epoch_time_s': 4.74}\n",
      "  ✅ New best (Stage A) macro PR-AUC: 0.0828 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:49] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 4, 'train_loss': 0.0327842725870701, 'train_macro_pr_auc': 0.21544627600788266, 'train_macro_roc_auc': 0.7084898348195994, 'val_loss': 0.13847267180681228, 'val_macro_pr_auc': 0.10282287714227774, 'val_macro_roc_auc': 0.5629917021669618, 'epoch_time_s': 4.75}\n",
      "  ✅ New best (Stage A) macro PR-AUC: 0.1028 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:42:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 5, 'train_loss': 0.032721771142230585, 'train_macro_pr_auc': 0.217635243463428, 'train_macro_roc_auc': 0.7083933660695082, 'val_loss': 0.13518619030714035, 'val_macro_pr_auc': 0.12165443303340044, 'val_macro_roc_auc': 0.5874946897864975, 'epoch_time_s': 4.8}\n",
      "  ✅ New best (Stage A) macro PR-AUC: 0.1217 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:00] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 6, 'train_loss': 0.03289635310379358, 'train_macro_pr_auc': 0.20782836636565902, 'train_macro_roc_auc': 0.7021579445824658, 'val_loss': 0.13288636565208434, 'val_macro_pr_auc': 0.13197449463464273, 'val_macro_roc_auc': 0.6008043797614259, 'epoch_time_s': 4.82}\n",
      "  ✅ New best (Stage A) macro PR-AUC: 0.1320 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:05] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 7, 'train_loss': 0.033106887522034154, 'train_macro_pr_auc': 0.20516994149759463, 'train_macro_roc_auc': 0.6974444627266895, 'val_loss': 0.13136810392141343, 'val_macro_pr_auc': 0.13953489453248116, 'val_macro_roc_auc': 0.6104861465660406, 'epoch_time_s': 4.73}\n",
      "  ✅ New best (Stage A) macro PR-AUC: 0.1395 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:11] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'A', 'epoch': 8, 'train_loss': 0.033446282559098345, 'train_macro_pr_auc': 0.19800331001972962, 'train_macro_roc_auc': 0.6793394336970406, 'val_loss': 0.13041243970394134, 'val_macro_pr_auc': 0.14403884369407427, 'val_macro_roc_auc': 0.6171125914864409, 'epoch_time_s': 4.75}\n",
      "  ✅ New best (Stage A) macro PR-AUC: 0.1440 → checkpoint saved.\n",
      "\n",
      "=== Stage B: Finetune (unfreeze last 2 ChemBERTa layers) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:19] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 1, 'train_loss': 0.033351336682263096, 'train_macro_pr_auc': 0.20295440301231638, 'train_macro_roc_auc': 0.6865177545804902, 'val_loss': 0.12948915004730224, 'val_macro_pr_auc': 0.14860819775385747, 'val_macro_roc_auc': 0.6244653691925603, 'epoch_time_s': 5.99}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.1486 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:25] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 2, 'train_loss': 0.0320538672976769, 'train_macro_pr_auc': 0.2409792810558551, 'train_macro_roc_auc': 0.7315953583704417, 'val_loss': 0.12786235362291337, 'val_macro_pr_auc': 0.15892553431027442, 'val_macro_roc_auc': 0.638336387427298, 'epoch_time_s': 5.89}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.1589 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:27] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 3, 'train_loss': 0.031573516235519676, 'train_macro_pr_auc': 0.25877013362413176, 'train_macro_roc_auc': 0.7455078833992309, 'val_loss': 0.1264248350262642, 'val_macro_pr_auc': 0.16992076406025358, 'val_macro_roc_auc': 0.6545090711061684, 'epoch_time_s': 5.95}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.1699 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:33] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 4, 'train_loss': 0.03169219841559728, 'train_macro_pr_auc': 0.2535870353864961, 'train_macro_roc_auc': 0.7403325204304396, 'val_loss': 0.12525496780872344, 'val_macro_pr_auc': 0.17670751058022716, 'val_macro_roc_auc': 0.664954602299176, 'epoch_time_s': 5.94}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.1767 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:44] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 5, 'train_loss': 0.0309824329872544, 'train_macro_pr_auc': 0.2783762235540493, 'train_macro_roc_auc': 0.7612776006180176, 'val_loss': 0.12415830075740814, 'val_macro_pr_auc': 0.18558657990269578, 'val_macro_roc_auc': 0.6738228522983359, 'epoch_time_s': 6.02}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.1856 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 6, 'train_loss': 0.031206133656012706, 'train_macro_pr_auc': 0.27829100621751796, 'train_macro_roc_auc': 0.755299053293554, 'val_loss': 0.1231931608915329, 'val_macro_pr_auc': 0.19293831224054891, 'val_macro_roc_auc': 0.6810440461639281, 'epoch_time_s': 5.92}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.1929 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:43:54] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 7, 'train_loss': 0.031034588914078017, 'train_macro_pr_auc': 0.2810264936485967, 'train_macro_roc_auc': 0.7623333892543859, 'val_loss': 0.1224514576792717, 'val_macro_pr_auc': 0.1996031092407037, 'val_macro_roc_auc': 0.6881710678940766, 'epoch_time_s': 5.98}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.1996 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:03] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 8, 'train_loss': 0.030679168958121384, 'train_macro_pr_auc': 0.29096576640307903, 'train_macro_roc_auc': 0.7672944482343568, 'val_loss': 0.12180023938417435, 'val_macro_pr_auc': 0.20525084567553417, 'val_macro_roc_auc': 0.6943129844375328, 'epoch_time_s': 5.98}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2053 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:03] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 9, 'train_loss': 0.030749009086344488, 'train_macro_pr_auc': 0.286702373022353, 'train_macro_roc_auc': 0.769002218430392, 'val_loss': 0.12127423912286758, 'val_macro_pr_auc': 0.21246464164262072, 'val_macro_roc_auc': 0.6987706221166011, 'epoch_time_s': 5.92}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2125 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:13] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 10, 'train_loss': 0.03075902950591766, 'train_macro_pr_auc': 0.2865881383643247, 'train_macro_roc_auc': 0.7706060849873775, 'val_loss': 0.12079172015190125, 'val_macro_pr_auc': 0.21125299214080603, 'val_macro_roc_auc': 0.7026318061256759, 'epoch_time_s': 6.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:17] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 11, 'train_loss': 0.03074433825050409, 'train_macro_pr_auc': 0.2866102209753219, 'train_macro_roc_auc': 0.767944986693523, 'val_loss': 0.12046253174543381, 'val_macro_pr_auc': 0.2160076280215174, 'val_macro_roc_auc': 0.7052656884511049, 'epoch_time_s': 5.99}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2160 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:24] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 12, 'train_loss': 0.03064479235177621, 'train_macro_pr_auc': 0.29587938734080016, 'train_macro_roc_auc': 0.7747545667274607, 'val_loss': 0.1201474142074585, 'val_macro_pr_auc': 0.21815972659022878, 'val_macro_roc_auc': 0.7084575461077836, 'epoch_time_s': 6.23}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2182 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:29] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 13, 'train_loss': 0.03051889530645731, 'train_macro_pr_auc': 0.3014124548450356, 'train_macro_roc_auc': 0.7754224122799486, 'val_loss': 0.11987470299005508, 'val_macro_pr_auc': 0.22021944101515215, 'val_macro_roc_auc': 0.7103294667602594, 'epoch_time_s': 6.24}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2202 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:37] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 14, 'train_loss': 0.03055528616771484, 'train_macro_pr_auc': 0.30036792174015225, 'train_macro_roc_auc': 0.7770674850742396, 'val_loss': 0.11969551861286164, 'val_macro_pr_auc': 0.2204572735624223, 'val_macro_roc_auc': 0.7117377024199903, 'epoch_time_s': 6.02}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2205 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:43] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 15, 'train_loss': 0.030464942605258564, 'train_macro_pr_auc': 0.3004765438036446, 'train_macro_roc_auc': 0.777889729862784, 'val_loss': 0.11952996402978897, 'val_macro_pr_auc': 0.22635373887372232, 'val_macro_roc_auc': 0.7136480190734761, 'epoch_time_s': 5.93}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2264 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:48] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 16, 'train_loss': 0.030600831323327162, 'train_macro_pr_auc': 0.2948815446342145, 'train_macro_roc_auc': 0.7766768345697753, 'val_loss': 0.11940244868397713, 'val_macro_pr_auc': 0.22757343295334945, 'val_macro_roc_auc': 0.7147992546108844, 'epoch_time_s': 5.95}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2276 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:44:57] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 17, 'train_loss': 0.0306711228707662, 'train_macro_pr_auc': 0.29773487707514557, 'train_macro_roc_auc': 0.7771101181372019, 'val_loss': 0.11931319043040275, 'val_macro_pr_auc': 0.2284273391126127, 'val_macro_roc_auc': 0.7155680955816607, 'epoch_time_s': 5.92}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2284 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:45:01] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 18, 'train_loss': 0.030714609569463973, 'train_macro_pr_auc': 0.29627989863976345, 'train_macro_roc_auc': 0.7806201270680816, 'val_loss': 0.11927412450313568, 'val_macro_pr_auc': 0.23048184628064763, 'val_macro_roc_auc': 0.7158172353803324, 'epoch_time_s': 5.94}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2305 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:45:10] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 19, 'train_loss': 0.030844925281902153, 'train_macro_pr_auc': 0.29332838681471207, 'train_macro_roc_auc': 0.7770113536906883, 'val_loss': 0.11922193139791488, 'val_macro_pr_auc': 0.23035495448753893, 'val_macro_roc_auc': 0.7166446565031673, 'epoch_time_s': 5.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:45:16] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'B', 'epoch': 20, 'train_loss': 0.030895793051100694, 'train_macro_pr_auc': 0.28553649395747516, 'train_macro_roc_auc': 0.7788041242291293, 'val_loss': 0.11917937502264976, 'val_macro_pr_auc': 0.2308804674395504, 'val_macro_roc_auc': 0.7172780654968326, 'epoch_time_s': 6.15}\n",
      "  ✅ New best (Stage B) macro PR-AUC: 0.2309 → checkpoint saved.\n",
      "\n",
      "🎯 Training complete. Best macro PR-AUC: 0.2309 | Best ckpt: v7\\model\\checkpoints\\shared\\best.pt\n",
      "Logs → v7\\results\\logs\\train_log.jsonl\n",
      "Val metrics → v7\\results\\artifacts\\val_metrics.jsonl\n",
      "Checkpoints → v7\\model\\checkpoints\\shared\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time, random, platform\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "\n",
    "# ---------------------------\n",
    "# Env & reproducibility\n",
    "# ---------------------------\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # quieter HF tokenizer\n",
    "def seed_everything(seed=42):\n",
    "    import numpy as _np, random as _r, torch as _t\n",
    "    _r.seed(seed); _np.random.seed(seed)\n",
    "    _t.manual_seed(seed); _t.cuda.manual_seed_all(seed)\n",
    "    _t.backends.cudnn.deterministic = False\n",
    "    _t.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# ---------------------------\n",
    "# Paths & setup\n",
    "# ---------------------------\n",
    "BASE_DIR   = Path(\"v7\")\n",
    "DATA_PREP  = BASE_DIR / \"data\" / \"prepared\"\n",
    "RESULTS_DIR= BASE_DIR / \"results\"\n",
    "LOGS_DIR   = RESULTS_DIR / \"logs\"\n",
    "ARTIF_DIR  = RESULTS_DIR / \"artifacts\"\n",
    "META_DIR   = RESULTS_DIR / \"meta\"\n",
    "CKPT_DIR   = BASE_DIR / \"model\" / \"checkpoints\" / \"shared\"\n",
    "for d in [RESULTS_DIR, LOGS_DIR, ARTIF_DIR, META_DIR, CKPT_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(DATA_PREP / \"dataset_manifest.json\") as f:\n",
    "    ds_manifest = json.load(f)\n",
    "LABEL_NAMES  = ds_manifest[\"labels\"]\n",
    "N_LABELS     = len(LABEL_NAMES)\n",
    "DESC_IN_DIM  = ds_manifest[\"n_features\"]\n",
    "\n",
    "# Expect v7_shared in memory (from Phase 2 — Cell 4)\n",
    "try:\n",
    "    v7_shared\n",
    "except NameError:\n",
    "    raise RuntimeError(\"v7_shared model not found. Please re-run Phase 2 (Cells 1–4) in this kernel.\")\n",
    "\n",
    "# ---------------------------\n",
    "# Data: Dataset & Loader (Windows-safe)\n",
    "# ---------------------------\n",
    "class Tox21NPZDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, npz_path: Path):\n",
    "        b = np.load(npz_path, allow_pickle=True)\n",
    "        self.X = b[\"X\"].astype(np.float32)             # (N, F)\n",
    "        self.Y = b[\"Y\"].astype(np.float32)             # (N, 12), may contain NaN\n",
    "        self.mask_y = b[\"y_missing_mask\"].astype(bool) # True where NaN\n",
    "        self.smiles = b[\"smiles\"].tolist()\n",
    "        self.mol_ids= b[\"mol_id\"].tolist()\n",
    "        assert self.X.shape[0] == self.Y.shape[0] == len(self.smiles)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "            \"smiles\": self.smiles[i],\n",
    "            \"desc\": torch.from_numpy(self.X[i]),\n",
    "            \"y\": torch.from_numpy(self.Y[i]),\n",
    "            \"y_mask\": torch.from_numpy(self.mask_y[i]),\n",
    "            \"mol_id\": self.mol_ids[i],\n",
    "        }\n",
    "\n",
    "def collate(batch):\n",
    "    smiles = [b[\"smiles\"] for b in batch]\n",
    "    desc   = torch.stack([b[\"desc\"] for b in batch], dim=0)\n",
    "    y      = torch.stack([b[\"y\"] for b in batch], dim=0)\n",
    "    y_mask = torch.stack([b[\"y_mask\"] for b in batch], dim=0)\n",
    "    mol_id = [b[\"mol_id\"] for b in batch]\n",
    "    return {\"smiles\": smiles, \"desc\": desc, \"y\": y, \"y_mask\": y_mask, \"mol_id\": mol_id}\n",
    "\n",
    "train_ds = Tox21NPZDataset(DATA_PREP / \"train.npz\")\n",
    "val_ds   = Tox21NPZDataset(DATA_PREP / \"val.npz\")\n",
    "\n",
    "IS_WINDOWS = platform.system() == \"Windows\"\n",
    "NUM_WORKERS = 0 if IS_WINDOWS else max(0, min(4, (os.cpu_count() or 2)//2))\n",
    "PERSISTENT = False\n",
    "\n",
    "# Batch & precision (from your probe)\n",
    "BATCH_SIZE  = 32\n",
    "GRAD_ACCUM  = 4      # effective batch 128\n",
    "USE_AMP     = True\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
    "    num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"),\n",
    "    collate_fn=collate, drop_last=True,\n",
    "    persistent_workers=(PERSISTENT if NUM_WORKERS>0 else False),\n",
    ")\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "    num_workers=NUM_WORKERS, pin_memory=(device.type==\"cuda\"),\n",
    "    collate_fn=collate,\n",
    "    persistent_workers=(PERSISTENT if NUM_WORKERS>0 else False),\n",
    ")\n",
    "\n",
    "print(f\"Train batches: {len(train_loader)}, Val batches: {len(val_loader)}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Label stats & weights\n",
    "# ---------------------------\n",
    "def compute_label_stats(y: np.ndarray, mask_missing: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    valid = ~mask_missing\n",
    "    pos = np.nansum((y == 1) & valid, axis=0)\n",
    "    neg = np.nansum((y == 0) & valid, axis=0)\n",
    "    total = pos + neg\n",
    "    prevalence = np.divide(pos, np.maximum(total, 1))\n",
    "    return {\"pos\": pos.astype(int), \"neg\": neg.astype(int), \"total\": total.astype(int), \"prevalence\": prevalence}\n",
    "\n",
    "train_blob = np.load(DATA_PREP / \"train.npz\", allow_pickle=True)\n",
    "train_stats = compute_label_stats(train_blob[\"Y\"], train_blob[\"y_missing_mask\"])\n",
    "print(\"Train label prevalence:\", np.round(train_stats[\"prevalence\"], 3))\n",
    "\n",
    "def effective_number_weights(pos_counts: np.ndarray, beta: float = 0.999) -> np.ndarray:\n",
    "    eps = 1e-8\n",
    "    eff_num = (1 - np.power(beta, pos_counts + eps)) / (1 - beta)\n",
    "    alpha = 1.0 / np.maximum(eff_num, eps)\n",
    "    alpha = alpha / (np.mean(alpha) + eps)\n",
    "    return alpha.astype(np.float32)\n",
    "\n",
    "alpha_cb = effective_number_weights(train_stats[\"pos\"])\n",
    "print(\"Class-balanced alpha:\", np.round(alpha_cb, 3))\n",
    "alpha_cb_t = torch.tensor(alpha_cb, device=device)\n",
    "\n",
    "# ---------------------------\n",
    "# Loss: Asymmetric Loss (stronger)\n",
    "# ---------------------------\n",
    "class AsymmetricLossCB(nn.Module):\n",
    "    def __init__(self, gamma_neg=5.0, gamma_pos=1.0, clip=0.05, alpha: Optional[torch.Tensor]=None):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.alpha = alpha  # (n_labels,) on device\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor, missing_mask: torch.Tensor):\n",
    "        # mask out missing labels\n",
    "        valid = ~missing_mask\n",
    "        if not valid.any():\n",
    "            return logits.new_tensor(0.0)\n",
    "\n",
    "        logits_v  = logits[valid]\n",
    "        targets_v = targets[valid]\n",
    "\n",
    "        pred = torch.sigmoid(logits_v)\n",
    "        if self.clip:\n",
    "            pred = torch.clamp(pred, self.clip, 1 - self.clip)\n",
    "\n",
    "        anti_targets = 1 - targets_v\n",
    "        pt = pred * targets_v + (1 - pred) * anti_targets\n",
    "        one_sided_gamma = self.gamma_pos * targets_v + self.gamma_neg * anti_targets\n",
    "        focal_weight = torch.pow(1 - pt, one_sided_gamma)\n",
    "\n",
    "        loss = - (targets_v * torch.log(pred) + (1 - targets_v) * torch.log(1 - pred))\n",
    "        loss = loss * focal_weight\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            n_labels = logits.size(1)\n",
    "            label_idx_full = torch.arange(n_labels, device=logits.device).unsqueeze(0).expand_as(missing_mask)\n",
    "            label_idx_valid = label_idx_full[valid]\n",
    "            alpha_vec = self.alpha[label_idx_valid]\n",
    "            loss = loss * alpha_vec\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "criterion = AsymmetricLossCB(gamma_neg=5.0, gamma_pos=1.0, clip=0.05, alpha=alpha_cb_t)\n",
    "\n",
    "# ---------------------------\n",
    "# Optimizer, EMA, Scheduler\n",
    "# ---------------------------\n",
    "def build_optimizer(model, lr=3e-4, wd=1e-2):\n",
    "    return optim.AdamW((p for p in model.parameters() if p.requires_grad), lr=lr, weight_decay=wd)\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, model: nn.Module, decay=0.999):\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                self.shadow[n] = p.detach().clone()\n",
    "    @torch.no_grad()\n",
    "    def update(self, model: nn.Module):\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                self.shadow[n].mul_(self.decay).add_(p.detach(), alpha=1 - self.decay)\n",
    "    @torch.no_grad()\n",
    "    def copy_to(self, model: nn.Module):\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad and n in self.shadow:\n",
    "                p.data.copy_(self.shadow[n])\n",
    "\n",
    "def build_warmup_cosine(total_steps, warmup_ratio=0.1, min_lr_scale=0.1):\n",
    "    def lr_lambda(step):\n",
    "        warm = int(total_steps * warmup_ratio)\n",
    "        if step < warm:\n",
    "            return float(step) / max(1, warm)\n",
    "        progress = (step - warm) / max(1, total_steps - warm)\n",
    "        cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return min_lr_scale + (1 - min_lr_scale) * cosine\n",
    "    return lr_lambda\n",
    "\n",
    "# ---------------------------\n",
    "# Metrics (macro PR-AUC primary)\n",
    "# ---------------------------\n",
    "def eval_metrics(y_true: np.ndarray, y_prob: np.ndarray) -> Dict[str, float]:\n",
    "    pr_aucs, roc_aucs = [], []\n",
    "    for j in range(y_true.shape[1]):\n",
    "        mask = ~np.isnan(y_true[:, j])\n",
    "        if mask.sum() < 2 or len(np.unique(y_true[mask, j])) < 2:\n",
    "            pr_aucs.append(np.nan); roc_aucs.append(np.nan); continue\n",
    "        try:\n",
    "            pr_aucs.append(average_precision_score(y_true[mask, j], y_prob[mask, j]))\n",
    "        except Exception:\n",
    "            pr_aucs.append(np.nan)\n",
    "        try:\n",
    "            roc_aucs.append(roc_auc_score(y_true[mask, j], y_prob[mask, j]))\n",
    "        except Exception:\n",
    "            roc_aucs.append(np.nan)\n",
    "    return {\"macro_pr_auc\": float(np.nanmean(pr_aucs)),\n",
    "            \"macro_roc_auc\": float(np.nanmean(roc_aucs))}\n",
    "\n",
    "# ---------------------------\n",
    "# Stage settings (stronger)\n",
    "# ---------------------------\n",
    "MAX_EPOCHS_STAGE_A = 8\n",
    "MAX_EPOCHS_STAGE_B = 20\n",
    "UNFREEZE_LAST_N    = 2\n",
    "EARLY_STOP_PATIENCE= 5\n",
    "EMA_DECAY          = 0.999\n",
    "LR                 = 3e-4\n",
    "WEIGHT_DECAY       = 1e-2\n",
    "WARMUP_RATIO       = 0.1\n",
    "CLIP_NORM          = 1.0\n",
    "\n",
    "# Freeze text backbone for Stage A\n",
    "v7_shared.freeze_text_backbone(n_unfrozen_layers=0)\n",
    "v7_shared.freeze_graph(freeze=False)  # keep graph trainable\n",
    "\n",
    "# Save run config for reproducibility\n",
    "run_cfg = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"grad_accum\": GRAD_ACCUM,\n",
    "    \"use_amp\": USE_AMP,\n",
    "    \"max_epochs\": {\"stage_a\": MAX_EPOCHS_STAGE_A, \"stage_b\": MAX_EPOCHS_STAGE_B},\n",
    "    \"optimizer\": {\"lr\": LR, \"weight_decay\": WEIGHT_DECAY},\n",
    "    \"scheduler\": {\"type\": \"warmup_cosine\", \"warmup_ratio\": WARMUP_RATIO},\n",
    "    \"asl\": {\"gamma_neg\": 5.0, \"gamma_pos\": 1.0, \"clip\": 0.05},\n",
    "    \"ema_decay\": EMA_DECAY,\n",
    "    \"unfreeze_last_n\": UNFREEZE_LAST_N,\n",
    "    \"early_stop_patience\": EARLY_STOP_PATIENCE,\n",
    "    \"clip_norm\": CLIP_NORM,\n",
    "    \"num_workers\": NUM_WORKERS,\n",
    "}\n",
    "(META_DIR / \"train_run_config.json\").write_text(json.dumps(run_cfg, indent=2))\n",
    "\n",
    "# ---------------------------\n",
    "# AMP scaler\n",
    "# ---------------------------\n",
    "scaler = torch.amp.GradScaler(\"cuda\", enabled=(USE_AMP and device.type==\"cuda\"))\n",
    "\n",
    "# ---------------------------\n",
    "# One epoch\n",
    "# ---------------------------\n",
    "def run_epoch(model: nn.Module, loader, optimizer, scheduler, ema: Optional[EMA], stage_name=\"train\"):\n",
    "    is_train = optimizer is not None\n",
    "    model.train(mode=is_train)\n",
    "\n",
    "    running_loss = 0.0\n",
    "    steps = 0\n",
    "    all_probs, all_true, all_mask = [], [], []\n",
    "\n",
    "    for step, batch in enumerate(loader):\n",
    "        smiles = batch[\"smiles\"]\n",
    "        desc   = batch[\"desc\"].to(device, non_blocking=True)\n",
    "        y      = batch[\"y\"].to(device, non_blocking=True)\n",
    "        y_mask = batch[\"y_mask\"].to(device, non_blocking=True)\n",
    "\n",
    "        ctx = torch.amp.autocast(\"cuda\", dtype=torch.float16) if (USE_AMP and device.type==\"cuda\") else torch.autocast(device_type=\"cpu\", enabled=False)\n",
    "        with ctx:\n",
    "            logits, _ = v7_shared(smiles, desc, return_intermediates=False)  # (B,12)\n",
    "            loss = criterion(logits, y, y_mask)\n",
    "\n",
    "        if is_train:\n",
    "            loss = loss / GRAD_ACCUM\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % GRAD_ACCUM == 0:\n",
    "                # gradient clipping\n",
    "                if CLIP_NORM is not None:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(v7_shared.parameters(), CLIP_NORM)\n",
    "\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "                if ema is not None:\n",
    "                    ema.update(model)\n",
    "\n",
    "        running_loss += float(loss.item()) * GRAD_ACCUM\n",
    "        steps += 1\n",
    "\n",
    "        with torch.no_grad():\n",
    "            probs = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "            y_np  = y.detach().cpu().numpy()\n",
    "            m_np  = y_mask.detach().cpu().numpy()\n",
    "            all_probs.append(probs); all_true.append(y_np); all_mask.append(m_np)\n",
    "\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    all_true  = np.concatenate(all_true, axis=0)\n",
    "    all_mask  = np.concatenate(all_mask, axis=0)\n",
    "    all_true  = np.where(all_mask, np.nan, all_true)  # ignore missing labels\n",
    "\n",
    "    metrics = eval_metrics(all_true, all_probs)\n",
    "    return running_loss / max(1, steps), metrics\n",
    "\n",
    "# ---------------------------\n",
    "# Orchestrate both stages with early stopping\n",
    "# ---------------------------\n",
    "def save_epoch_ckpt(stage_tag: str, epoch: int, ema: EMA, path: Path):\n",
    "    torch.save({\n",
    "        \"model\": v7_shared.state_dict(),\n",
    "        \"ema\": ema.shadow,\n",
    "        \"config\": {\"stage\": stage_tag, \"epoch\": epoch},\n",
    "    }, path)\n",
    "\n",
    "def train_shared_model():\n",
    "    best_metric = -1.0\n",
    "    best_path   = CKPT_DIR / \"best.pt\"\n",
    "    log_path    = LOGS_DIR / \"train_log.jsonl\"\n",
    "    val_log_path= ARTIF_DIR / \"val_metrics.jsonl\"\n",
    "    for p in [log_path, val_log_path]:\n",
    "        if p.exists(): p.unlink()\n",
    "\n",
    "    # ===== Stage A: warm-up (text frozen) =====\n",
    "    print(\"\\n=== Stage A: Warm-up (text backbone frozen) ===\")\n",
    "    opt = build_optimizer(v7_shared, lr=LR, wd=WEIGHT_DECAY)\n",
    "    total_steps = math.ceil(len(train_loader) / GRAD_ACCUM) * MAX_EPOCHS_STAGE_A\n",
    "    scheduler = LambdaLR(opt, build_warmup_cosine(total_steps, warmup_ratio=WARMUP_RATIO))\n",
    "    ema = EMA(v7_shared, decay=EMA_DECAY)\n",
    "    patience = EARLY_STOP_PATIENCE\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS_STAGE_A + 1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_metrics = run_epoch(v7_shared, train_loader, opt, scheduler, ema, stage_name=\"train\")\n",
    "        with torch.no_grad():\n",
    "            ema.copy_to(v7_shared)\n",
    "            va_loss, va_metrics = run_epoch(v7_shared, val_loader, optimizer=None, scheduler=None, ema=None, stage_name=\"val\")\n",
    "\n",
    "        row = {\n",
    "            \"stage\": \"A\", \"epoch\": epoch,\n",
    "            \"train_loss\": tr_loss, **{f\"train_{k}\": v for k,v in tr_metrics.items()},\n",
    "            \"val_loss\": va_loss, **{f\"val_{k}\": v for k,v in va_metrics.items()},\n",
    "            \"epoch_time_s\": round(time.time()-t0, 2),\n",
    "        }\n",
    "        with open(log_path, \"a\") as f: f.write(json.dumps(row) + \"\\n\")\n",
    "        with open(val_log_path, \"a\") as f: f.write(json.dumps({\"stage\":\"A\", \"epoch\": epoch, **va_metrics}) + \"\\n\")\n",
    "        print(row)\n",
    "\n",
    "        # Save per-epoch checkpoint\n",
    "        save_epoch_ckpt(\"A\", epoch, ema, CKPT_DIR / f\"epoch_A{epoch:02d}.pt\")\n",
    "\n",
    "        score = va_metrics[\"macro_pr_auc\"]\n",
    "        if score > best_metric:\n",
    "            best_metric = score\n",
    "            torch.save({\"model\": v7_shared.state_dict(), \"ema\": ema.shadow, \"config\": {\"stage\":\"A\"}}, best_path)\n",
    "            patience = EARLY_STOP_PATIENCE\n",
    "            print(f\"  ✅ New best (Stage A) macro PR-AUC: {best_metric:.4f} → checkpoint saved.\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"  ⏹ Early stop in Stage A.\")\n",
    "                break\n",
    "\n",
    "    # ===== Stage B: finetune (unfreeze last N) =====\n",
    "    print(\"\\n=== Stage B: Finetune (unfreeze last 2 ChemBERTa layers) ===\")\n",
    "    v7_shared.freeze_text_backbone(n_unfrozen_layers=UNFREEZE_LAST_N)\n",
    "\n",
    "    opt = build_optimizer(v7_shared, lr=LR * 0.5, wd=WEIGHT_DECAY)\n",
    "    total_steps = math.ceil(len(train_loader) / GRAD_ACCUM) * MAX_EPOCHS_STAGE_B\n",
    "    scheduler = LambdaLR(opt, build_warmup_cosine(total_steps, warmup_ratio=WARMUP_RATIO))\n",
    "    ema = EMA(v7_shared, decay=EMA_DECAY)\n",
    "    patience = EARLY_STOP_PATIENCE\n",
    "\n",
    "    for epoch in range(1, MAX_EPOCHS_STAGE_B + 1):\n",
    "        t0 = time.time()\n",
    "        tr_loss, tr_metrics = run_epoch(v7_shared, train_loader, opt, scheduler, ema, stage_name=\"train\")\n",
    "        with torch.no_grad():\n",
    "            ema.copy_to(v7_shared)\n",
    "            va_loss, va_metrics = run_epoch(v7_shared, val_loader, optimizer=None, scheduler=None, ema=None, stage_name=\"val\")\n",
    "\n",
    "        row = {\n",
    "            \"stage\": \"B\", \"epoch\": epoch,\n",
    "            \"train_loss\": tr_loss, **{f\"train_{k}\": v for k,v in tr_metrics.items()},\n",
    "            \"val_loss\": va_loss, **{f\"val_{k}\": v for k,v in va_metrics.items()},\n",
    "            \"epoch_time_s\": round(time.time()-t0, 2),\n",
    "        }\n",
    "        with open(log_path, \"a\") as f: f.write(json.dumps(row) + \"\\n\")\n",
    "        with open(val_log_path, \"a\") as f: f.write(json.dumps({\"stage\":\"B\", \"epoch\": epoch, **va_metrics}) + \"\\n\")\n",
    "        print(row)\n",
    "\n",
    "        # Save per-epoch checkpoint\n",
    "        save_epoch_ckpt(\"B\", epoch, ema, CKPT_DIR / f\"epoch_B{epoch:02d}.pt\")\n",
    "\n",
    "        score = va_metrics[\"macro_pr_auc\"]\n",
    "        if score > best_metric:\n",
    "            best_metric = score\n",
    "            torch.save({\"model\": v7_shared.state_dict(), \"ema\": ema.shadow, \"config\": {\"stage\":\"B\"}}, best_path)\n",
    "            patience = EARLY_STOP_PATIENCE\n",
    "            print(f\"  ✅ New best (Stage B) macro PR-AUC: {best_metric:.4f} → checkpoint saved.\")\n",
    "        else:\n",
    "            patience -= 1\n",
    "            if patience <= 0:\n",
    "                print(\"  ⏹ Early stop in Stage B.\")\n",
    "                break\n",
    "\n",
    "    print(f\"\\n🎯 Training complete. Best macro PR-AUC: {best_metric:.4f} | Best ckpt: {best_path}\")\n",
    "    print(f\"Logs → {LOGS_DIR/'train_log.jsonl'}\")\n",
    "    print(f\"Val metrics → {ARTIF_DIR/'val_metrics.jsonl'}\")\n",
    "    print(f\"Checkpoints → {CKPT_DIR}\")\n",
    "\n",
    "# ---------------------------\n",
    "# Kick it off\n",
    "# ---------------------------\n",
    "train_shared_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc37bab1",
   "metadata": {},
   "source": [
    "#### 1b) fine-tune the shared model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58a2be0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Stage C: Extended finetune (unfreeze last 4 ChemBERTa layers) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:20] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 1, 'train_loss': 0.03059412927295153, 'train_macro_pr_auc': 0.2940897781197137, 'train_macro_roc_auc': 0.7738731249518018, 'val_loss': 0.11911626175045967, 'val_macro_pr_auc': 0.2313494757438764, 'val_macro_roc_auc': 0.7179151724089748, 'epoch_time_s': 7.7}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2313 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:23] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 2, 'train_loss': 0.02976563608703705, 'train_macro_pr_auc': 0.32309141373025596, 'train_macro_roc_auc': 0.7923732547730861, 'val_loss': 0.11889535903930665, 'val_macro_pr_auc': 0.23306384630623586, 'val_macro_roc_auc': 0.7202172791739457, 'epoch_time_s': 7.43}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2331 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:35] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 3, 'train_loss': 0.02977958160619705, 'train_macro_pr_auc': 0.3266218849580093, 'train_macro_roc_auc': 0.7896876136921706, 'val_loss': 0.11872626304626464, 'val_macro_pr_auc': 0.2347075514972754, 'val_macro_roc_auc': 0.7224862467323508, 'epoch_time_s': 7.44}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2347 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:41] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 4, 'train_loss': 0.02964736360292404, 'train_macro_pr_auc': 0.33171891965683115, 'train_macro_roc_auc': 0.7927982693500946, 'val_loss': 0.11857076600193978, 'val_macro_pr_auc': 0.2361485101560743, 'val_macro_roc_auc': 0.7242838603441172, 'epoch_time_s': 7.34}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2361 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:51] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 5, 'train_loss': 0.02982027809111736, 'train_macro_pr_auc': 0.31763875566657007, 'train_macro_roc_auc': 0.7874365553179355, 'val_loss': 0.1184364566206932, 'val_macro_pr_auc': 0.23835799454299564, 'val_macro_roc_auc': 0.7257010995233294, 'epoch_time_s': 7.28}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2384 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:49:53] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 6, 'train_loss': 0.02947875121369576, 'train_macro_pr_auc': 0.3321220359256137, 'train_macro_roc_auc': 0.8006438802397594, 'val_loss': 0.11827784195542336, 'val_macro_pr_auc': 0.23832505697959694, 'val_macro_roc_auc': 0.727440428255865, 'epoch_time_s': 7.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:00] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 7, 'train_loss': 0.02935470045090486, 'train_macro_pr_auc': 0.3434656936237351, 'train_macro_roc_auc': 0.8023428187366073, 'val_loss': 0.11815975934267044, 'val_macro_pr_auc': 0.2393710062395936, 'val_macro_roc_auc': 0.7287112230799639, 'epoch_time_s': 7.25}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2394 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:10] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 8, 'train_loss': 0.029187886216319524, 'train_macro_pr_auc': 0.3496728510187987, 'train_macro_roc_auc': 0.8075722167410296, 'val_loss': 0.11802344962954521, 'val_macro_pr_auc': 0.240041162238516, 'val_macro_roc_auc': 0.7303042554528408, 'epoch_time_s': 7.22}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2400 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:19] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 9, 'train_loss': 0.029373395767731545, 'train_macro_pr_auc': 0.3405598536923287, 'train_macro_roc_auc': 0.805858050747465, 'val_loss': 0.11795216917991638, 'val_macro_pr_auc': 0.24077963991117687, 'val_macro_roc_auc': 0.7314910443527664, 'epoch_time_s': 7.28}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2408 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:26] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 10, 'train_loss': 0.02929941861388775, 'train_macro_pr_auc': 0.34471098714367926, 'train_macro_roc_auc': 0.8097664124196781, 'val_loss': 0.11790973782539367, 'val_macro_pr_auc': 0.24119206327093037, 'val_macro_roc_auc': 0.7322239918733962, 'epoch_time_s': 7.28}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2412 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:32] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 11, 'train_loss': 0.02930923376041345, 'train_macro_pr_auc': 0.34093325952352727, 'train_macro_roc_auc': 0.8092587990161633, 'val_loss': 0.1178943282365799, 'val_macro_pr_auc': 0.24154978012502534, 'val_macro_roc_auc': 0.7326831399686627, 'epoch_time_s': 7.2}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2415 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:38] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 12, 'train_loss': 0.029186946392441408, 'train_macro_pr_auc': 0.3597753005981579, 'train_macro_roc_auc': 0.8138671743287641, 'val_loss': 0.1178495578467846, 'val_macro_pr_auc': 0.2416392331610726, 'val_macro_roc_auc': 0.7333118541081923, 'epoch_time_s': 7.23}\n",
      "  ✅ New best (Stage C) macro PR-AUC: 0.2416 → checkpoint saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:45] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 13, 'train_loss': 0.029342801329226065, 'train_macro_pr_auc': 0.3456173786396817, 'train_macro_roc_auc': 0.8124665169572586, 'val_loss': 0.1178448085486889, 'val_macro_pr_auc': 0.24113489174898617, 'val_macro_roc_auc': 0.7337164190361283, 'epoch_time_s': 7.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:50:56] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 14, 'train_loss': 0.029542968245461966, 'train_macro_pr_auc': 0.34503395319943714, 'train_macro_roc_auc': 0.8082895448255402, 'val_loss': 0.11786730423569679, 'val_macro_pr_auc': 0.24131952642285667, 'val_macro_roc_auc': 0.733945058303203, 'epoch_time_s': 7.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:51:00] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'stage': 'C', 'epoch': 15, 'train_loss': 0.029530828427045772, 'train_macro_pr_auc': 0.34638721395507566, 'train_macro_roc_auc': 0.8117984862586809, 'val_loss': 0.11786027416586876, 'val_macro_pr_auc': 0.2411037459323543, 'val_macro_roc_auc': 0.7342894549870769, 'epoch_time_s': 7.17}\n",
      "\n",
      "🎯 Stage C done. Best macro PR-AUC now: 0.2416 | Best ckpt: v7\\model\\checkpoints\\shared\\best.pt\n"
     ]
    }
   ],
   "source": [
    "# === Phase 3 — Cell 1c: Optional extended finetune (Stage C) ===\n",
    "import json, math, time\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import numpy as np\n",
    "\n",
    "BASE_DIR   = Path(\"v7\")\n",
    "DATA_PREP  = BASE_DIR / \"data\" / \"prepared\"\n",
    "RESULTS_DIR= BASE_DIR / \"results\"\n",
    "LOGS_DIR   = RESULTS_DIR / \"logs\"\n",
    "ARTIF_DIR  = RESULTS_DIR / \"artifacts\"\n",
    "CKPT_DIR   = BASE_DIR / \"model\" / \"checkpoints\" / \"shared\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Reuse objects from previous cell:\n",
    "# - v7_shared, train_loader, val_loader, criterion, scaler, GRAD_ACCUM, USE_AMP,\n",
    "#   build_optimizer, EMA, build_warmup_cosine, eval_metrics, run_epoch (already defined)\n",
    "\n",
    "BEST_PATH = CKPT_DIR / \"best.pt\"\n",
    "assert BEST_PATH.exists(), \"Best checkpoint not found from Stage B.\"\n",
    "\n",
    "# ---- Load best weights (EMA weights baked in via copy_to flow) ----\n",
    "ckpt = torch.load(BEST_PATH, map_location=device)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "\n",
    "# ---- Stage C config (slightly smaller LR, unfreeze more) ----\n",
    "MAX_EPOCHS_STAGE_C   = 15\n",
    "UNFREEZE_LAST_N_MORE = 4   # total unfreezing depth for Stage C\n",
    "LR_C                 = 1e-4\n",
    "WEIGHT_DECAY         = 1e-2\n",
    "WARMUP_RATIO         = 0.1\n",
    "EARLY_STOP_PATIENCE  = 5\n",
    "EMA_DECAY            = 0.999\n",
    "CLIP_NORM            = 1.0\n",
    "\n",
    "print(\"\\n=== Stage C: Extended finetune (unfreeze last 4 ChemBERTa layers) ===\")\n",
    "v7_shared.freeze_text_backbone(n_unfrozen_layers=UNFREEZE_LAST_N_MORE)\n",
    "\n",
    "opt = build_optimizer(v7_shared, lr=LR_C, wd=WEIGHT_DECAY)\n",
    "total_steps = (len(train_loader) // max(1, GRAD_ACCUM)) * MAX_EPOCHS_STAGE_C\n",
    "scheduler = LambdaLR(opt, build_warmup_cosine(total_steps, warmup_ratio=WARMUP_RATIO))\n",
    "ema = EMA(v7_shared, decay=EMA_DECAY)\n",
    "\n",
    "# Retrieve current best to compare\n",
    "best_metric = -1.0\n",
    "try:\n",
    "    with open(ARTIF_DIR / \"val_metrics.jsonl\", \"r\") as f:\n",
    "        for line in f:\n",
    "            m = json.loads(line)\n",
    "            best_metric = max(best_metric, m.get(\"macro_pr_auc\", -1.0))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "patience = EARLY_STOP_PATIENCE\n",
    "for epoch in range(1, MAX_EPOCHS_STAGE_C + 1):\n",
    "    t0 = time.time()\n",
    "    tr_loss, tr_metrics = run_epoch(v7_shared, train_loader, opt, scheduler, ema, stage_name=\"train\")\n",
    "    with torch.no_grad():\n",
    "        ema.copy_to(v7_shared)\n",
    "        va_loss, va_metrics = run_epoch(v7_shared, val_loader, optimizer=None, scheduler=None, ema=None, stage_name=\"val\")\n",
    "    row = {\n",
    "        \"stage\": \"C\", \"epoch\": epoch,\n",
    "        \"train_loss\": tr_loss, **{f\"train_{k}\": v for k,v in tr_metrics.items()},\n",
    "        \"val_loss\": va_loss, **{f\"val_{k}\": v for k,v in va_metrics.items()},\n",
    "        \"epoch_time_s\": round(time.time()-t0, 2),\n",
    "    }\n",
    "    with open(LOGS_DIR / \"train_log.jsonl\", \"a\") as f: f.write(json.dumps(row) + \"\\n\")\n",
    "    with open(ARTIF_DIR / \"val_metrics.jsonl\", \"a\") as f: f.write(json.dumps({\"stage\":\"C\",\"epoch\":epoch, **va_metrics}) + \"\\n\")\n",
    "    print(row)\n",
    "\n",
    "    score = va_metrics[\"macro_pr_auc\"]\n",
    "    if score > best_metric:\n",
    "        best_metric = score\n",
    "        torch.save({\"model\": v7_shared.state_dict(), \"ema\": ema.shadow, \"config\": {\"stage\":\"C\"}}, BEST_PATH)\n",
    "        patience = EARLY_STOP_PATIENCE\n",
    "        print(f\"  ✅ New best (Stage C) macro PR-AUC: {best_metric:.4f} → checkpoint saved.\")\n",
    "    else:\n",
    "        patience -= 1\n",
    "        if patience <= 0:\n",
    "            print(\"  ⏹ Early stop in Stage C.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\n🎯 Stage C done. Best macro PR-AUC now: {best_metric:.4f} | Best ckpt: {BEST_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca248483",
   "metadata": {},
   "source": [
    "### 2 (boosted): Label-Specialist Heads (extended + multi-seed)\n",
    "\n",
    "We train one-vs-rest heads for each of the 12 Tox21 labels:\n",
    "\n",
    "- Cached fused features (from the shared model): `v7/data/fused/`\n",
    "- Head MLP: 768 → 512 → 256 → 128 → 1 (GELU + LayerNorm + Dropout 0.30, residual)\n",
    "- Loss: **Binary ASL** (γ⁻=5.0, γ⁺=1.0) with **class-balanced α**\n",
    "- Sampler: **class-balanced WeightedRandomSampler** to ensure positives per batch\n",
    "- Optimiser & schedule: AdamW (lr=3e-3) + **cosine warmup** (10% warmup)\n",
    "- Regularisation: **EMA**, AMP, grad-clip (L2-norm=1.0), WD=1e-2\n",
    "- Early stopping: patience=10 on **val AP**\n",
    "- **5 seeds** per label (can change via `SEEDS`)\n",
    "\n",
    "Artifacts:\n",
    "- `v7/model/ensembles/<LABEL>/seedXX/best.pt`\n",
    "- `v7/model/ensembles/<LABEL>/seedXX/val_preds.npz` (for calibration)\n",
    "- `v7/model/ensembles/ensemble_summary.json`\n",
    "\n",
    "> Expect ~30–50 mins on a 4070 Ti (depends on your desktop load). You can lower `SEEDS` or `EPOCHS_MAX` if needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2ca8f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[23:55:07] WARNING: not removing hydrogen atom without neighbors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cached train: fused (6265, 768)\n",
      "Cached val: fused (783, 768)\n",
      "Fused shapes → train: (6265, 768) | val: (783, 768)\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-AR (label 0)\n",
      "==============================\n",
      "[NR-AR | seed 13] ep 01  loss 0.0007  val AP 0.0165\n",
      "  ✅ New best AP: 0.0165 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 02  loss 0.0005  val AP 0.0169\n",
      "  ✅ New best AP: 0.0169 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 03  loss 0.0005  val AP 0.0176\n",
      "  ✅ New best AP: 0.0176 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 04  loss 0.0005  val AP 0.0187\n",
      "  ✅ New best AP: 0.0187 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 05  loss 0.0006  val AP 0.0224\n",
      "  ✅ New best AP: 0.0224 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 06  loss 0.0005  val AP 0.0935\n",
      "  ✅ New best AP: 0.0935 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 07  loss 0.0005  val AP 0.1298\n",
      "  ✅ New best AP: 0.1298 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 08  loss 0.0005  val AP 0.1305\n",
      "  ✅ New best AP: 0.1305 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 09  loss 0.0004  val AP 0.1311\n",
      "  ✅ New best AP: 0.1311 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 10  loss 0.0006  val AP 0.1541\n",
      "  ✅ New best AP: 0.1541 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 11  loss 0.0005  val AP 0.1550\n",
      "  ✅ New best AP: 0.1550 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 12  loss 0.0004  val AP 0.1558\n",
      "  ✅ New best AP: 0.1558 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 13  loss 0.0004  val AP 0.1564\n",
      "  ✅ New best AP: 0.1564 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 14  loss 0.0004  val AP 0.1568\n",
      "  ✅ New best AP: 0.1568 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 15  loss 0.0004  val AP 0.1572\n",
      "  ✅ New best AP: 0.1572 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 16  loss 0.0006  val AP 0.1575\n",
      "  ✅ New best AP: 0.1575 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 17  loss 0.0004  val AP 0.1583\n",
      "  ✅ New best AP: 0.1583 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 18  loss 0.0007  val AP 0.1587\n",
      "  ✅ New best AP: 0.1587 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 19  loss 0.0005  val AP 0.1594\n",
      "  ✅ New best AP: 0.1594 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 20  loss 0.0005  val AP 0.1599\n",
      "  ✅ New best AP: 0.1599 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 21  loss 0.0004  val AP 0.1605\n",
      "  ✅ New best AP: 0.1605 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 22  loss 0.0005  val AP 0.1608\n",
      "  ✅ New best AP: 0.1608 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 23  loss 0.0004  val AP 0.1612\n",
      "  ✅ New best AP: 0.1612 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 24  loss 0.0004  val AP 0.1614\n",
      "  ✅ New best AP: 0.1614 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 25  loss 0.0004  val AP 0.1618\n",
      "  ✅ New best AP: 0.1618 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 26  loss 0.0004  val AP 0.1620\n",
      "  ✅ New best AP: 0.1620 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 27  loss 0.0004  val AP 0.1621\n",
      "  ✅ New best AP: 0.1621 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 28  loss 0.0004  val AP 0.1623\n",
      "  ✅ New best AP: 0.1623 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 29  loss 0.0004  val AP 0.1625\n",
      "  ✅ New best AP: 0.1625 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 30  loss 0.0004  val AP 0.1625\n",
      "  ✅ New best AP: 0.1625 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 31  loss 0.0004  val AP 0.1626\n",
      "  ✅ New best AP: 0.1626 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 32  loss 0.0004  val AP 0.1626\n",
      "  ✅ New best AP: 0.1626 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 33  loss 0.0004  val AP 0.1626\n",
      "  ✅ New best AP: 0.1626 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 34  loss 0.0004  val AP 0.1629\n",
      "  ✅ New best AP: 0.1629 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 35  loss 0.0004  val AP 0.1631\n",
      "  ✅ New best AP: 0.1631 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 36  loss 0.0004  val AP 0.1633\n",
      "  ✅ New best AP: 0.1633 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 37  loss 0.0004  val AP 0.1634\n",
      "  ✅ New best AP: 0.1634 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 38  loss 0.0004  val AP 0.1635\n",
      "  ✅ New best AP: 0.1635 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 39  loss 0.0004  val AP 0.1637\n",
      "  ✅ New best AP: 0.1637 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 40  loss 0.0004  val AP 0.1638\n",
      "  ✅ New best AP: 0.1638 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 41  loss 0.0004  val AP 0.1639\n",
      "  ✅ New best AP: 0.1639 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 42  loss 0.0004  val AP 0.1640\n",
      "  ✅ New best AP: 0.1640 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 43  loss 0.0004  val AP 0.1640\n",
      "  ✅ New best AP: 0.1640 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 44  loss 0.0004  val AP 0.1640\n",
      "  ✅ New best AP: 0.1640 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 45  loss 0.0004  val AP 0.1640\n",
      "[NR-AR | seed 13] ep 46  loss 0.0004  val AP 0.1641\n",
      "  ✅ New best AP: 0.1641 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 47  loss 0.0004  val AP 0.1641\n",
      "[NR-AR | seed 13] ep 48  loss 0.0004  val AP 0.1642\n",
      "  ✅ New best AP: 0.1642 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 49  loss 0.0004  val AP 0.1643\n",
      "  ✅ New best AP: 0.1643 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 13] ep 50  loss 0.0004  val AP 0.1644\n",
      "  ✅ New best AP: 0.1644 → v7\\model\\ensembles\\NR-AR\\seed13\\best.pt\n",
      "[NR-AR | seed 29] ep 01  loss 0.0008  val AP 0.0563\n",
      "  ✅ New best AP: 0.0563 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 02  loss 0.0006  val AP 0.0698\n",
      "  ✅ New best AP: 0.0698 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 03  loss 0.0006  val AP 0.0826\n",
      "  ✅ New best AP: 0.0826 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 04  loss 0.0006  val AP 0.0975\n",
      "  ✅ New best AP: 0.0975 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 05  loss 0.0006  val AP 0.1016\n",
      "  ✅ New best AP: 0.1016 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 06  loss 0.0006  val AP 0.1452\n",
      "  ✅ New best AP: 0.1452 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 07  loss 0.0006  val AP 0.1668\n",
      "  ✅ New best AP: 0.1668 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 08  loss 0.0006  val AP 0.1669\n",
      "  ✅ New best AP: 0.1669 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 09  loss 0.0007  val AP 0.1676\n",
      "  ✅ New best AP: 0.1676 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 10  loss 0.0005  val AP 0.1675\n",
      "[NR-AR | seed 29] ep 11  loss 0.0004  val AP 0.1677\n",
      "  ✅ New best AP: 0.1677 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 12  loss 0.0005  val AP 0.1682\n",
      "  ✅ New best AP: 0.1682 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 13  loss 0.0005  val AP 0.1683\n",
      "  ✅ New best AP: 0.1683 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 14  loss 0.0006  val AP 0.1687\n",
      "  ✅ New best AP: 0.1687 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 15  loss 0.0005  val AP 0.1689\n",
      "  ✅ New best AP: 0.1689 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 16  loss 0.0005  val AP 0.1688\n",
      "[NR-AR | seed 29] ep 17  loss 0.0004  val AP 0.1689\n",
      "  ✅ New best AP: 0.1689 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 18  loss 0.0005  val AP 0.1689\n",
      "  ✅ New best AP: 0.1689 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 19  loss 0.0004  val AP 0.1689\n",
      "[NR-AR | seed 29] ep 20  loss 0.0004  val AP 0.1691\n",
      "  ✅ New best AP: 0.1691 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 21  loss 0.0005  val AP 0.1690\n",
      "[NR-AR | seed 29] ep 22  loss 0.0004  val AP 0.1690\n",
      "[NR-AR | seed 29] ep 23  loss 0.0005  val AP 0.1695\n",
      "  ✅ New best AP: 0.1695 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 24  loss 0.0004  val AP 0.1695\n",
      "  ✅ New best AP: 0.1695 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 25  loss 0.0005  val AP 0.1695\n",
      "[NR-AR | seed 29] ep 26  loss 0.0005  val AP 0.1696\n",
      "  ✅ New best AP: 0.1696 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 27  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 28  loss 0.0004  val AP 0.1696\n",
      "  ✅ New best AP: 0.1696 → v7\\model\\ensembles\\NR-AR\\seed29\\best.pt\n",
      "[NR-AR | seed 29] ep 29  loss 0.0004  val AP 0.1695\n",
      "[NR-AR | seed 29] ep 30  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 31  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 32  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 33  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 34  loss 0.0004  val AP 0.1695\n",
      "[NR-AR | seed 29] ep 35  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 36  loss 0.0004  val AP 0.1694\n",
      "[NR-AR | seed 29] ep 37  loss 0.0004  val AP 0.1695\n",
      "[NR-AR | seed 29] ep 38  loss 0.0004  val AP 0.1694\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.1696\n",
      "[NR-AR | seed 47] ep 01  loss 0.0009  val AP 0.0319\n",
      "  ✅ New best AP: 0.0319 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 02  loss 0.0006  val AP 0.0329\n",
      "  ✅ New best AP: 0.0329 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 03  loss 0.0006  val AP 0.0335\n",
      "  ✅ New best AP: 0.0335 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 04  loss 0.0007  val AP 0.0352\n",
      "  ✅ New best AP: 0.0352 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 05  loss 0.0006  val AP 0.0398\n",
      "  ✅ New best AP: 0.0398 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 06  loss 0.0006  val AP 0.0448\n",
      "  ✅ New best AP: 0.0448 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 07  loss 0.0007  val AP 0.0540\n",
      "  ✅ New best AP: 0.0540 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 08  loss 0.0007  val AP 0.0697\n",
      "  ✅ New best AP: 0.0697 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 09  loss 0.0006  val AP 0.0921\n",
      "  ✅ New best AP: 0.0921 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 10  loss 0.0005  val AP 0.1209\n",
      "  ✅ New best AP: 0.1209 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 11  loss 0.0005  val AP 0.1536\n",
      "  ✅ New best AP: 0.1536 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 12  loss 0.0005  val AP 0.1770\n",
      "  ✅ New best AP: 0.1770 → v7\\model\\ensembles\\NR-AR\\seed47\\best.pt\n",
      "[NR-AR | seed 47] ep 13  loss 0.0006  val AP 0.1768\n",
      "[NR-AR | seed 47] ep 14  loss 0.0005  val AP 0.1759\n",
      "[NR-AR | seed 47] ep 15  loss 0.0005  val AP 0.1753\n",
      "[NR-AR | seed 47] ep 16  loss 0.0004  val AP 0.1752\n",
      "[NR-AR | seed 47] ep 17  loss 0.0004  val AP 0.1750\n",
      "[NR-AR | seed 47] ep 18  loss 0.0005  val AP 0.1750\n",
      "[NR-AR | seed 47] ep 19  loss 0.0004  val AP 0.1745\n",
      "[NR-AR | seed 47] ep 20  loss 0.0004  val AP 0.1745\n",
      "[NR-AR | seed 47] ep 21  loss 0.0005  val AP 0.1745\n",
      "[NR-AR | seed 47] ep 22  loss 0.0004  val AP 0.1747\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.1770\n",
      "[NR-AR | seed 61] ep 01  loss 0.0010  val AP 0.0196\n",
      "  ✅ New best AP: 0.0196 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 02  loss 0.0007  val AP 0.0200\n",
      "  ✅ New best AP: 0.0200 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 03  loss 0.0007  val AP 0.0206\n",
      "  ✅ New best AP: 0.0206 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 04  loss 0.0007  val AP 0.0216\n",
      "  ✅ New best AP: 0.0216 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 05  loss 0.0006  val AP 0.0241\n",
      "  ✅ New best AP: 0.0241 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 06  loss 0.0006  val AP 0.0293\n",
      "  ✅ New best AP: 0.0293 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 07  loss 0.0009  val AP 0.0376\n",
      "  ✅ New best AP: 0.0376 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 08  loss 0.0006  val AP 0.0471\n",
      "  ✅ New best AP: 0.0471 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 09  loss 0.0007  val AP 0.0527\n",
      "  ✅ New best AP: 0.0527 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 10  loss 0.0005  val AP 0.0731\n",
      "  ✅ New best AP: 0.0731 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 11  loss 0.0005  val AP 0.0784\n",
      "  ✅ New best AP: 0.0784 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 12  loss 0.0005  val AP 0.1304\n",
      "  ✅ New best AP: 0.1304 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 13  loss 0.0006  val AP 0.1646\n",
      "  ✅ New best AP: 0.1646 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 14  loss 0.0005  val AP 0.1651\n",
      "  ✅ New best AP: 0.1651 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 15  loss 0.0005  val AP 0.1657\n",
      "  ✅ New best AP: 0.1657 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 16  loss 0.0005  val AP 0.1662\n",
      "  ✅ New best AP: 0.1662 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 17  loss 0.0004  val AP 0.1669\n",
      "  ✅ New best AP: 0.1669 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 18  loss 0.0005  val AP 0.1675\n",
      "  ✅ New best AP: 0.1675 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 19  loss 0.0005  val AP 0.1679\n",
      "  ✅ New best AP: 0.1679 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 20  loss 0.0005  val AP 0.1683\n",
      "  ✅ New best AP: 0.1683 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 21  loss 0.0005  val AP 0.1685\n",
      "  ✅ New best AP: 0.1685 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 22  loss 0.0005  val AP 0.1688\n",
      "  ✅ New best AP: 0.1688 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 23  loss 0.0004  val AP 0.1689\n",
      "  ✅ New best AP: 0.1689 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 24  loss 0.0005  val AP 0.1690\n",
      "  ✅ New best AP: 0.1690 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 25  loss 0.0004  val AP 0.1691\n",
      "  ✅ New best AP: 0.1691 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 26  loss 0.0004  val AP 0.1692\n",
      "  ✅ New best AP: 0.1692 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 27  loss 0.0005  val AP 0.1694\n",
      "  ✅ New best AP: 0.1694 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 28  loss 0.0005  val AP 0.1694\n",
      "[NR-AR | seed 61] ep 29  loss 0.0004  val AP 0.1693\n",
      "[NR-AR | seed 61] ep 30  loss 0.0004  val AP 0.1695\n",
      "  ✅ New best AP: 0.1695 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 31  loss 0.0004  val AP 0.1696\n",
      "  ✅ New best AP: 0.1696 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 32  loss 0.0004  val AP 0.1697\n",
      "  ✅ New best AP: 0.1697 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 33  loss 0.0004  val AP 0.1698\n",
      "  ✅ New best AP: 0.1698 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 34  loss 0.0004  val AP 0.1697\n",
      "[NR-AR | seed 61] ep 35  loss 0.0004  val AP 0.1696\n",
      "[NR-AR | seed 61] ep 36  loss 0.0004  val AP 0.1696\n",
      "[NR-AR | seed 61] ep 37  loss 0.0004  val AP 0.1696\n",
      "[NR-AR | seed 61] ep 38  loss 0.0004  val AP 0.1697\n",
      "[NR-AR | seed 61] ep 39  loss 0.0004  val AP 0.1697\n",
      "[NR-AR | seed 61] ep 40  loss 0.0004  val AP 0.1698\n",
      "  ✅ New best AP: 0.1698 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 41  loss 0.0004  val AP 0.1699\n",
      "  ✅ New best AP: 0.1699 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 42  loss 0.0004  val AP 0.1700\n",
      "  ✅ New best AP: 0.1700 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 43  loss 0.0004  val AP 0.1700\n",
      "  ✅ New best AP: 0.1700 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 44  loss 0.0005  val AP 0.1701\n",
      "  ✅ New best AP: 0.1701 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 45  loss 0.0004  val AP 0.1703\n",
      "  ✅ New best AP: 0.1703 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 46  loss 0.0005  val AP 0.1703\n",
      "  ✅ New best AP: 0.1703 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 47  loss 0.0004  val AP 0.1703\n",
      "  ✅ New best AP: 0.1703 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 48  loss 0.0004  val AP 0.1704\n",
      "  ✅ New best AP: 0.1704 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 49  loss 0.0005  val AP 0.1705\n",
      "  ✅ New best AP: 0.1705 → v7\\model\\ensembles\\NR-AR\\seed61\\best.pt\n",
      "[NR-AR | seed 61] ep 50  loss 0.0004  val AP 0.1705\n",
      "[NR-AR | seed 83] ep 01  loss 0.0008  val AP 0.0249\n",
      "  ✅ New best AP: 0.0249 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 02  loss 0.0006  val AP 0.0251\n",
      "  ✅ New best AP: 0.0251 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 03  loss 0.0006  val AP 0.0260\n",
      "  ✅ New best AP: 0.0260 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 04  loss 0.0006  val AP 0.0278\n",
      "  ✅ New best AP: 0.0278 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 05  loss 0.0005  val AP 0.0306\n",
      "  ✅ New best AP: 0.0306 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 06  loss 0.0007  val AP 0.0358\n",
      "  ✅ New best AP: 0.0358 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 07  loss 0.0006  val AP 0.0459\n",
      "  ✅ New best AP: 0.0459 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 08  loss 0.0007  val AP 0.0533\n",
      "  ✅ New best AP: 0.0533 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 09  loss 0.0005  val AP 0.0653\n",
      "  ✅ New best AP: 0.0653 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 10  loss 0.0007  val AP 0.0798\n",
      "  ✅ New best AP: 0.0798 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 11  loss 0.0005  val AP 0.0897\n",
      "  ✅ New best AP: 0.0897 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 12  loss 0.0006  val AP 0.1293\n",
      "  ✅ New best AP: 0.1293 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 13  loss 0.0004  val AP 0.1294\n",
      "  ✅ New best AP: 0.1294 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 14  loss 0.0005  val AP 0.1292\n",
      "[NR-AR | seed 83] ep 15  loss 0.0004  val AP 0.1294\n",
      "[NR-AR | seed 83] ep 16  loss 0.0005  val AP 0.1294\n",
      "[NR-AR | seed 83] ep 17  loss 0.0005  val AP 0.1294\n",
      "  ✅ New best AP: 0.1294 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 18  loss 0.0004  val AP 0.1294\n",
      "[NR-AR | seed 83] ep 19  loss 0.0005  val AP 0.1294\n",
      "[NR-AR | seed 83] ep 20  loss 0.0004  val AP 0.1406\n",
      "  ✅ New best AP: 0.1406 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 21  loss 0.0005  val AP 0.1405\n",
      "[NR-AR | seed 83] ep 22  loss 0.0004  val AP 0.1405\n",
      "[NR-AR | seed 83] ep 23  loss 0.0005  val AP 0.1405\n",
      "[NR-AR | seed 83] ep 24  loss 0.0004  val AP 0.1406\n",
      "[NR-AR | seed 83] ep 25  loss 0.0004  val AP 0.1408\n",
      "  ✅ New best AP: 0.1408 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 26  loss 0.0004  val AP 0.1408\n",
      "[NR-AR | seed 83] ep 27  loss 0.0004  val AP 0.1409\n",
      "  ✅ New best AP: 0.1409 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 28  loss 0.0005  val AP 0.1410\n",
      "  ✅ New best AP: 0.1410 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 29  loss 0.0004  val AP 0.1409\n",
      "[NR-AR | seed 83] ep 30  loss 0.0005  val AP 0.1409\n",
      "[NR-AR | seed 83] ep 31  loss 0.0004  val AP 0.1409\n",
      "[NR-AR | seed 83] ep 32  loss 0.0004  val AP 0.1409\n",
      "[NR-AR | seed 83] ep 33  loss 0.0004  val AP 0.1410\n",
      "[NR-AR | seed 83] ep 34  loss 0.0004  val AP 0.1412\n",
      "  ✅ New best AP: 0.1412 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 35  loss 0.0004  val AP 0.1413\n",
      "  ✅ New best AP: 0.1413 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 36  loss 0.0004  val AP 0.1413\n",
      "  ✅ New best AP: 0.1413 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 37  loss 0.0004  val AP 0.1414\n",
      "  ✅ New best AP: 0.1414 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 38  loss 0.0004  val AP 0.1415\n",
      "  ✅ New best AP: 0.1415 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 39  loss 0.0004  val AP 0.1416\n",
      "  ✅ New best AP: 0.1416 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 40  loss 0.0004  val AP 0.1416\n",
      "  ✅ New best AP: 0.1416 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 41  loss 0.0005  val AP 0.1417\n",
      "  ✅ New best AP: 0.1417 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 42  loss 0.0004  val AP 0.1417\n",
      "  ✅ New best AP: 0.1417 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 43  loss 0.0004  val AP 0.1418\n",
      "  ✅ New best AP: 0.1418 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 44  loss 0.0004  val AP 0.1419\n",
      "  ✅ New best AP: 0.1419 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 45  loss 0.0004  val AP 0.1418\n",
      "[NR-AR | seed 83] ep 46  loss 0.0004  val AP 0.1418\n",
      "[NR-AR | seed 83] ep 47  loss 0.0004  val AP 0.1419\n",
      "[NR-AR | seed 83] ep 48  loss 0.0005  val AP 0.1420\n",
      "  ✅ New best AP: 0.1420 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "[NR-AR | seed 83] ep 49  loss 0.0004  val AP 0.1419\n",
      "[NR-AR | seed 83] ep 50  loss 0.0004  val AP 0.1420\n",
      "  ✅ New best AP: 0.1420 → v7\\model\\ensembles\\NR-AR\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-AR-LBD (label 1)\n",
      "==============================\n",
      "[NR-AR-LBD | seed 13] ep 01  loss 0.0009  val AP 0.0153\n",
      "  ✅ New best AP: 0.0153 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 02  loss 0.0007  val AP 0.0164\n",
      "  ✅ New best AP: 0.0164 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 03  loss 0.0006  val AP 0.0184\n",
      "  ✅ New best AP: 0.0184 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 04  loss 0.0006  val AP 0.0223\n",
      "  ✅ New best AP: 0.0223 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 05  loss 0.0006  val AP 0.0315\n",
      "  ✅ New best AP: 0.0315 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 06  loss 0.0006  val AP 0.1510\n",
      "  ✅ New best AP: 0.1510 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 07  loss 0.0005  val AP 0.2017\n",
      "  ✅ New best AP: 0.2017 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 08  loss 0.0006  val AP 0.2167\n",
      "  ✅ New best AP: 0.2167 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 09  loss 0.0005  val AP 0.2384\n",
      "  ✅ New best AP: 0.2384 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 10  loss 0.0006  val AP 0.2389\n",
      "  ✅ New best AP: 0.2389 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 11  loss 0.0005  val AP 0.2435\n",
      "  ✅ New best AP: 0.2435 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 12  loss 0.0005  val AP 0.2439\n",
      "  ✅ New best AP: 0.2439 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 13  loss 0.0005  val AP 0.2448\n",
      "  ✅ New best AP: 0.2448 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 14  loss 0.0005  val AP 0.2455\n",
      "  ✅ New best AP: 0.2455 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 15  loss 0.0005  val AP 0.2462\n",
      "  ✅ New best AP: 0.2462 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 16  loss 0.0005  val AP 0.2466\n",
      "  ✅ New best AP: 0.2466 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 17  loss 0.0005  val AP 0.2473\n",
      "  ✅ New best AP: 0.2473 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 18  loss 0.0005  val AP 0.2482\n",
      "  ✅ New best AP: 0.2482 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 19  loss 0.0006  val AP 0.2485\n",
      "  ✅ New best AP: 0.2485 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 20  loss 0.0005  val AP 0.2487\n",
      "  ✅ New best AP: 0.2487 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 21  loss 0.0005  val AP 0.2862\n",
      "  ✅ New best AP: 0.2862 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 22  loss 0.0005  val AP 0.2868\n",
      "  ✅ New best AP: 0.2868 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 23  loss 0.0005  val AP 0.2876\n",
      "  ✅ New best AP: 0.2876 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 24  loss 0.0005  val AP 0.2878\n",
      "  ✅ New best AP: 0.2878 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 25  loss 0.0006  val AP 0.2880\n",
      "  ✅ New best AP: 0.2880 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 26  loss 0.0005  val AP 0.2884\n",
      "  ✅ New best AP: 0.2884 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 27  loss 0.0005  val AP 0.2889\n",
      "  ✅ New best AP: 0.2889 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 28  loss 0.0005  val AP 0.2891\n",
      "  ✅ New best AP: 0.2891 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 29  loss 0.0005  val AP 0.2895\n",
      "  ✅ New best AP: 0.2895 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 30  loss 0.0005  val AP 0.2896\n",
      "  ✅ New best AP: 0.2896 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 31  loss 0.0005  val AP 0.2899\n",
      "  ✅ New best AP: 0.2899 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 32  loss 0.0004  val AP 0.2900\n",
      "  ✅ New best AP: 0.2900 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 33  loss 0.0004  val AP 0.2901\n",
      "  ✅ New best AP: 0.2901 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 34  loss 0.0005  val AP 0.2905\n",
      "  ✅ New best AP: 0.2905 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 35  loss 0.0005  val AP 0.2905\n",
      "  ✅ New best AP: 0.2905 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 36  loss 0.0005  val AP 0.2906\n",
      "  ✅ New best AP: 0.2906 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 37  loss 0.0005  val AP 0.2907\n",
      "  ✅ New best AP: 0.2907 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 38  loss 0.0005  val AP 0.2910\n",
      "  ✅ New best AP: 0.2910 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 39  loss 0.0004  val AP 0.2910\n",
      "[NR-AR-LBD | seed 13] ep 40  loss 0.0004  val AP 0.2911\n",
      "  ✅ New best AP: 0.2911 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 41  loss 0.0005  val AP 0.2914\n",
      "  ✅ New best AP: 0.2914 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 42  loss 0.0005  val AP 0.2917\n",
      "  ✅ New best AP: 0.2917 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 43  loss 0.0005  val AP 0.2919\n",
      "  ✅ New best AP: 0.2919 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 44  loss 0.0005  val AP 0.2920\n",
      "  ✅ New best AP: 0.2920 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 45  loss 0.0005  val AP 0.2920\n",
      "[NR-AR-LBD | seed 13] ep 46  loss 0.0005  val AP 0.2920\n",
      "[NR-AR-LBD | seed 13] ep 47  loss 0.0005  val AP 0.2920\n",
      "[NR-AR-LBD | seed 13] ep 48  loss 0.0005  val AP 0.2921\n",
      "  ✅ New best AP: 0.2921 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 49  loss 0.0005  val AP 0.2922\n",
      "  ✅ New best AP: 0.2922 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 13] ep 50  loss 0.0005  val AP 0.2922\n",
      "  ✅ New best AP: 0.2922 → v7\\model\\ensembles\\NR-AR-LBD\\seed13\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 01  loss 0.0010  val AP 0.0610\n",
      "  ✅ New best AP: 0.0610 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 02  loss 0.0007  val AP 0.0902\n",
      "  ✅ New best AP: 0.0902 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 03  loss 0.0008  val AP 0.1255\n",
      "  ✅ New best AP: 0.1255 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 04  loss 0.0006  val AP 0.1442\n",
      "  ✅ New best AP: 0.1442 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 05  loss 0.0006  val AP 0.2254\n",
      "  ✅ New best AP: 0.2254 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 06  loss 0.0007  val AP 0.2677\n",
      "  ✅ New best AP: 0.2677 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 07  loss 0.0007  val AP 0.2721\n",
      "  ✅ New best AP: 0.2721 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 08  loss 0.0009  val AP 0.2701\n",
      "[NR-AR-LBD | seed 29] ep 09  loss 0.0006  val AP 0.2705\n",
      "[NR-AR-LBD | seed 29] ep 10  loss 0.0005  val AP 0.2727\n",
      "  ✅ New best AP: 0.2727 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 11  loss 0.0005  val AP 0.2747\n",
      "  ✅ New best AP: 0.2747 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 12  loss 0.0005  val AP 0.2740\n",
      "[NR-AR-LBD | seed 29] ep 13  loss 0.0006  val AP 0.2746\n",
      "[NR-AR-LBD | seed 29] ep 14  loss 0.0005  val AP 0.2750\n",
      "  ✅ New best AP: 0.2750 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 15  loss 0.0006  val AP 0.2772\n",
      "  ✅ New best AP: 0.2772 → v7\\model\\ensembles\\NR-AR-LBD\\seed29\\best.pt\n",
      "[NR-AR-LBD | seed 29] ep 16  loss 0.0006  val AP 0.2397\n",
      "[NR-AR-LBD | seed 29] ep 17  loss 0.0005  val AP 0.2400\n",
      "[NR-AR-LBD | seed 29] ep 18  loss 0.0005  val AP 0.2433\n",
      "[NR-AR-LBD | seed 29] ep 19  loss 0.0005  val AP 0.2476\n",
      "[NR-AR-LBD | seed 29] ep 20  loss 0.0005  val AP 0.2479\n",
      "[NR-AR-LBD | seed 29] ep 21  loss 0.0005  val AP 0.2480\n",
      "[NR-AR-LBD | seed 29] ep 22  loss 0.0005  val AP 0.2480\n",
      "[NR-AR-LBD | seed 29] ep 23  loss 0.0005  val AP 0.2484\n",
      "[NR-AR-LBD | seed 29] ep 24  loss 0.0006  val AP 0.2484\n",
      "[NR-AR-LBD | seed 29] ep 25  loss 0.0005  val AP 0.2488\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.2772\n",
      "[NR-AR-LBD | seed 47] ep 01  loss 0.0011  val AP 0.0221\n",
      "  ✅ New best AP: 0.0221 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 02  loss 0.0007  val AP 0.0236\n",
      "  ✅ New best AP: 0.0236 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 03  loss 0.0007  val AP 0.0243\n",
      "  ✅ New best AP: 0.0243 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 04  loss 0.0008  val AP 0.0263\n",
      "  ✅ New best AP: 0.0263 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 05  loss 0.0008  val AP 0.0300\n",
      "  ✅ New best AP: 0.0300 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 06  loss 0.0006  val AP 0.0420\n",
      "  ✅ New best AP: 0.0420 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 07  loss 0.0008  val AP 0.0560\n",
      "  ✅ New best AP: 0.0560 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 08  loss 0.0007  val AP 0.1045\n",
      "  ✅ New best AP: 0.1045 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 09  loss 0.0006  val AP 0.1293\n",
      "  ✅ New best AP: 0.1293 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 10  loss 0.0005  val AP 0.1429\n",
      "  ✅ New best AP: 0.1429 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 11  loss 0.0006  val AP 0.1634\n",
      "  ✅ New best AP: 0.1634 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 12  loss 0.0007  val AP 0.1644\n",
      "  ✅ New best AP: 0.1644 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 13  loss 0.0007  val AP 0.1660\n",
      "  ✅ New best AP: 0.1660 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 14  loss 0.0005  val AP 0.1689\n",
      "  ✅ New best AP: 0.1689 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 15  loss 0.0005  val AP 0.1698\n",
      "  ✅ New best AP: 0.1698 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 16  loss 0.0005  val AP 0.1698\n",
      "[NR-AR-LBD | seed 47] ep 17  loss 0.0005  val AP 0.1707\n",
      "  ✅ New best AP: 0.1707 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 18  loss 0.0005  val AP 0.1728\n",
      "  ✅ New best AP: 0.1728 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 19  loss 0.0005  val AP 0.1755\n",
      "  ✅ New best AP: 0.1755 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 20  loss 0.0005  val AP 0.1768\n",
      "  ✅ New best AP: 0.1768 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 21  loss 0.0005  val AP 0.1772\n",
      "  ✅ New best AP: 0.1772 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 22  loss 0.0005  val AP 0.1774\n",
      "  ✅ New best AP: 0.1774 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 23  loss 0.0006  val AP 0.1772\n",
      "[NR-AR-LBD | seed 47] ep 24  loss 0.0005  val AP 0.1791\n",
      "  ✅ New best AP: 0.1791 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 25  loss 0.0005  val AP 0.1792\n",
      "  ✅ New best AP: 0.1792 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 26  loss 0.0005  val AP 0.1795\n",
      "  ✅ New best AP: 0.1795 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 27  loss 0.0005  val AP 0.1795\n",
      "[NR-AR-LBD | seed 47] ep 28  loss 0.0005  val AP 0.1797\n",
      "  ✅ New best AP: 0.1797 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 29  loss 0.0005  val AP 0.1799\n",
      "  ✅ New best AP: 0.1799 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 30  loss 0.0005  val AP 0.1801\n",
      "  ✅ New best AP: 0.1801 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 31  loss 0.0005  val AP 0.1801\n",
      "  ✅ New best AP: 0.1801 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 32  loss 0.0005  val AP 0.1801\n",
      "  ✅ New best AP: 0.1801 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 33  loss 0.0005  val AP 0.1803\n",
      "  ✅ New best AP: 0.1803 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 34  loss 0.0005  val AP 0.1804\n",
      "  ✅ New best AP: 0.1804 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 35  loss 0.0005  val AP 0.1805\n",
      "  ✅ New best AP: 0.1805 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 36  loss 0.0005  val AP 0.1806\n",
      "  ✅ New best AP: 0.1806 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 37  loss 0.0005  val AP 0.1806\n",
      "  ✅ New best AP: 0.1806 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 38  loss 0.0005  val AP 0.1827\n",
      "  ✅ New best AP: 0.1827 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 39  loss 0.0004  val AP 0.1830\n",
      "  ✅ New best AP: 0.1830 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 40  loss 0.0005  val AP 0.1831\n",
      "  ✅ New best AP: 0.1831 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 41  loss 0.0005  val AP 0.1834\n",
      "  ✅ New best AP: 0.1834 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 42  loss 0.0005  val AP 0.1835\n",
      "  ✅ New best AP: 0.1835 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 43  loss 0.0005  val AP 0.1836\n",
      "  ✅ New best AP: 0.1836 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 44  loss 0.0005  val AP 0.1837\n",
      "  ✅ New best AP: 0.1837 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 45  loss 0.0005  val AP 0.1838\n",
      "  ✅ New best AP: 0.1838 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 46  loss 0.0005  val AP 0.1839\n",
      "  ✅ New best AP: 0.1839 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 47  loss 0.0005  val AP 0.1840\n",
      "  ✅ New best AP: 0.1840 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 48  loss 0.0005  val AP 0.1841\n",
      "  ✅ New best AP: 0.1841 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 49  loss 0.0005  val AP 0.1842\n",
      "  ✅ New best AP: 0.1842 → v7\\model\\ensembles\\NR-AR-LBD\\seed47\\best.pt\n",
      "[NR-AR-LBD | seed 47] ep 50  loss 0.0005  val AP 0.1842\n",
      "[NR-AR-LBD | seed 61] ep 01  loss 0.0013  val AP 0.0202\n",
      "  ✅ New best AP: 0.0202 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 02  loss 0.0008  val AP 0.0220\n",
      "  ✅ New best AP: 0.0220 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 03  loss 0.0008  val AP 0.0245\n",
      "  ✅ New best AP: 0.0245 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 04  loss 0.0009  val AP 0.0287\n",
      "  ✅ New best AP: 0.0287 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 05  loss 0.0007  val AP 0.0400\n",
      "  ✅ New best AP: 0.0400 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 06  loss 0.0007  val AP 0.0539\n",
      "  ✅ New best AP: 0.0539 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 07  loss 0.0008  val AP 0.0670\n",
      "  ✅ New best AP: 0.0670 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 08  loss 0.0008  val AP 0.0791\n",
      "  ✅ New best AP: 0.0791 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 09  loss 0.0007  val AP 0.0904\n",
      "  ✅ New best AP: 0.0904 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 10  loss 0.0006  val AP 0.1446\n",
      "  ✅ New best AP: 0.1446 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 11  loss 0.0005  val AP 0.2057\n",
      "  ✅ New best AP: 0.2057 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 12  loss 0.0007  val AP 0.2361\n",
      "  ✅ New best AP: 0.2361 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 13  loss 0.0006  val AP 0.2370\n",
      "  ✅ New best AP: 0.2370 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 14  loss 0.0007  val AP 0.2383\n",
      "  ✅ New best AP: 0.2383 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 15  loss 0.0005  val AP 0.2398\n",
      "  ✅ New best AP: 0.2398 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 16  loss 0.0005  val AP 0.2411\n",
      "  ✅ New best AP: 0.2411 → v7\\model\\ensembles\\NR-AR-LBD\\seed61\\best.pt\n",
      "[NR-AR-LBD | seed 61] ep 17  loss 0.0005  val AP 0.1860\n",
      "[NR-AR-LBD | seed 61] ep 18  loss 0.0005  val AP 0.1913\n",
      "[NR-AR-LBD | seed 61] ep 19  loss 0.0005  val AP 0.1914\n",
      "[NR-AR-LBD | seed 61] ep 20  loss 0.0005  val AP 0.1946\n",
      "[NR-AR-LBD | seed 61] ep 21  loss 0.0005  val AP 0.1955\n",
      "[NR-AR-LBD | seed 61] ep 22  loss 0.0005  val AP 0.1957\n",
      "[NR-AR-LBD | seed 61] ep 23  loss 0.0005  val AP 0.1961\n",
      "[NR-AR-LBD | seed 61] ep 24  loss 0.0005  val AP 0.1965\n",
      "[NR-AR-LBD | seed 61] ep 25  loss 0.0005  val AP 0.1967\n",
      "[NR-AR-LBD | seed 61] ep 26  loss 0.0005  val AP 0.1971\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.2411\n",
      "[NR-AR-LBD | seed 83] ep 01  loss 0.0010  val AP 0.0176\n",
      "  ✅ New best AP: 0.0176 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 02  loss 0.0007  val AP 0.0189\n",
      "  ✅ New best AP: 0.0189 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 03  loss 0.0008  val AP 0.0215\n",
      "  ✅ New best AP: 0.0215 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 04  loss 0.0006  val AP 0.0264\n",
      "  ✅ New best AP: 0.0264 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 05  loss 0.0006  val AP 0.0352\n",
      "  ✅ New best AP: 0.0352 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 06  loss 0.0006  val AP 0.0524\n",
      "  ✅ New best AP: 0.0524 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 07  loss 0.0007  val AP 0.0859\n",
      "  ✅ New best AP: 0.0859 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 08  loss 0.0006  val AP 0.1295\n",
      "  ✅ New best AP: 0.1295 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 09  loss 0.0007  val AP 0.1399\n",
      "  ✅ New best AP: 0.1399 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 10  loss 0.0006  val AP 0.1574\n",
      "  ✅ New best AP: 0.1574 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 11  loss 0.0007  val AP 0.2139\n",
      "  ✅ New best AP: 0.2139 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 12  loss 0.0005  val AP 0.2250\n",
      "  ✅ New best AP: 0.2250 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 13  loss 0.0006  val AP 0.2252\n",
      "  ✅ New best AP: 0.2252 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 14  loss 0.0005  val AP 0.2364\n",
      "  ✅ New best AP: 0.2364 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 15  loss 0.0007  val AP 0.2368\n",
      "  ✅ New best AP: 0.2368 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 16  loss 0.0005  val AP 0.2369\n",
      "  ✅ New best AP: 0.2369 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 17  loss 0.0006  val AP 0.2370\n",
      "  ✅ New best AP: 0.2370 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 18  loss 0.0005  val AP 0.2559\n",
      "  ✅ New best AP: 0.2559 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 19  loss 0.0005  val AP 0.2562\n",
      "  ✅ New best AP: 0.2562 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 20  loss 0.0005  val AP 0.2563\n",
      "  ✅ New best AP: 0.2563 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 21  loss 0.0005  val AP 0.2566\n",
      "  ✅ New best AP: 0.2566 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 22  loss 0.0005  val AP 0.2569\n",
      "  ✅ New best AP: 0.2569 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 23  loss 0.0005  val AP 0.2571\n",
      "  ✅ New best AP: 0.2571 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 24  loss 0.0006  val AP 0.2576\n",
      "  ✅ New best AP: 0.2576 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 25  loss 0.0005  val AP 0.2580\n",
      "  ✅ New best AP: 0.2580 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 26  loss 0.0004  val AP 0.2586\n",
      "  ✅ New best AP: 0.2586 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 27  loss 0.0005  val AP 0.2593\n",
      "  ✅ New best AP: 0.2593 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 28  loss 0.0005  val AP 0.2597\n",
      "  ✅ New best AP: 0.2597 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 29  loss 0.0004  val AP 0.2599\n",
      "  ✅ New best AP: 0.2599 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 30  loss 0.0005  val AP 0.2600\n",
      "  ✅ New best AP: 0.2600 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 31  loss 0.0005  val AP 0.2601\n",
      "  ✅ New best AP: 0.2601 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 32  loss 0.0005  val AP 0.2605\n",
      "  ✅ New best AP: 0.2605 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 33  loss 0.0005  val AP 0.2606\n",
      "  ✅ New best AP: 0.2606 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 34  loss 0.0005  val AP 0.2611\n",
      "  ✅ New best AP: 0.2611 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 35  loss 0.0005  val AP 0.2615\n",
      "  ✅ New best AP: 0.2615 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 36  loss 0.0005  val AP 0.2620\n",
      "  ✅ New best AP: 0.2620 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 37  loss 0.0005  val AP 0.2625\n",
      "  ✅ New best AP: 0.2625 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 38  loss 0.0005  val AP 0.2626\n",
      "  ✅ New best AP: 0.2626 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 39  loss 0.0005  val AP 0.2626\n",
      "  ✅ New best AP: 0.2626 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 40  loss 0.0005  val AP 0.2627\n",
      "  ✅ New best AP: 0.2627 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 41  loss 0.0005  val AP 0.2628\n",
      "  ✅ New best AP: 0.2628 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 42  loss 0.0005  val AP 0.2629\n",
      "  ✅ New best AP: 0.2629 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 43  loss 0.0005  val AP 0.2628\n",
      "[NR-AR-LBD | seed 83] ep 44  loss 0.0005  val AP 0.2629\n",
      "  ✅ New best AP: 0.2629 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 45  loss 0.0005  val AP 0.2999\n",
      "  ✅ New best AP: 0.2999 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 46  loss 0.0005  val AP 0.3000\n",
      "  ✅ New best AP: 0.3000 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 47  loss 0.0005  val AP 0.3006\n",
      "  ✅ New best AP: 0.3006 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 48  loss 0.0005  val AP 0.3006\n",
      "  ✅ New best AP: 0.3006 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 49  loss 0.0005  val AP 0.3007\n",
      "  ✅ New best AP: 0.3007 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "[NR-AR-LBD | seed 83] ep 50  loss 0.0005  val AP 0.3007\n",
      "  ✅ New best AP: 0.3007 → v7\\model\\ensembles\\NR-AR-LBD\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-AhR (label 2)\n",
      "==============================\n",
      "[NR-AhR | seed 13] ep 01  loss 0.0004  val AP 0.0435\n",
      "  ✅ New best AP: 0.0435 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 02  loss 0.0003  val AP 0.0448\n",
      "  ✅ New best AP: 0.0448 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 03  loss 0.0003  val AP 0.0472\n",
      "  ✅ New best AP: 0.0472 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 04  loss 0.0003  val AP 0.0517\n",
      "  ✅ New best AP: 0.0517 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 05  loss 0.0003  val AP 0.0601\n",
      "  ✅ New best AP: 0.0601 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 06  loss 0.0004  val AP 0.0773\n",
      "  ✅ New best AP: 0.0773 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 07  loss 0.0003  val AP 0.1098\n",
      "  ✅ New best AP: 0.1098 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 08  loss 0.0004  val AP 0.1619\n",
      "  ✅ New best AP: 0.1619 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 09  loss 0.0003  val AP 0.2332\n",
      "  ✅ New best AP: 0.2332 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 10  loss 0.0003  val AP 0.3089\n",
      "  ✅ New best AP: 0.3089 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 11  loss 0.0003  val AP 0.3432\n",
      "  ✅ New best AP: 0.3432 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 12  loss 0.0003  val AP 0.3746\n",
      "  ✅ New best AP: 0.3746 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 13  loss 0.0002  val AP 0.3983\n",
      "  ✅ New best AP: 0.3983 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 14  loss 0.0003  val AP 0.4104\n",
      "  ✅ New best AP: 0.4104 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 15  loss 0.0002  val AP 0.4211\n",
      "  ✅ New best AP: 0.4211 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 16  loss 0.0003  val AP 0.4387\n",
      "  ✅ New best AP: 0.4387 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 17  loss 0.0002  val AP 0.4445\n",
      "  ✅ New best AP: 0.4445 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 18  loss 0.0003  val AP 0.4519\n",
      "  ✅ New best AP: 0.4519 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 19  loss 0.0002  val AP 0.4542\n",
      "  ✅ New best AP: 0.4542 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 20  loss 0.0003  val AP 0.4576\n",
      "  ✅ New best AP: 0.4576 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 21  loss 0.0002  val AP 0.4630\n",
      "  ✅ New best AP: 0.4630 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 22  loss 0.0003  val AP 0.4649\n",
      "  ✅ New best AP: 0.4649 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 23  loss 0.0002  val AP 0.4676\n",
      "  ✅ New best AP: 0.4676 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 24  loss 0.0003  val AP 0.4713\n",
      "  ✅ New best AP: 0.4713 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 25  loss 0.0003  val AP 0.4715\n",
      "  ✅ New best AP: 0.4715 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 26  loss 0.0003  val AP 0.4741\n",
      "  ✅ New best AP: 0.4741 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 27  loss 0.0002  val AP 0.4781\n",
      "  ✅ New best AP: 0.4781 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 28  loss 0.0002  val AP 0.4785\n",
      "  ✅ New best AP: 0.4785 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 29  loss 0.0002  val AP 0.4798\n",
      "  ✅ New best AP: 0.4798 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 30  loss 0.0003  val AP 0.4800\n",
      "  ✅ New best AP: 0.4800 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 31  loss 0.0002  val AP 0.4797\n",
      "[NR-AhR | seed 13] ep 32  loss 0.0002  val AP 0.4802\n",
      "  ✅ New best AP: 0.4802 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 33  loss 0.0002  val AP 0.4819\n",
      "  ✅ New best AP: 0.4819 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 34  loss 0.0002  val AP 0.4820\n",
      "  ✅ New best AP: 0.4820 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 35  loss 0.0002  val AP 0.4841\n",
      "  ✅ New best AP: 0.4841 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 36  loss 0.0002  val AP 0.4858\n",
      "  ✅ New best AP: 0.4858 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 37  loss 0.0002  val AP 0.4878\n",
      "  ✅ New best AP: 0.4878 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 38  loss 0.0002  val AP 0.4871\n",
      "[NR-AhR | seed 13] ep 39  loss 0.0003  val AP 0.4882\n",
      "  ✅ New best AP: 0.4882 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 40  loss 0.0002  val AP 0.4885\n",
      "  ✅ New best AP: 0.4885 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 41  loss 0.0002  val AP 0.4890\n",
      "  ✅ New best AP: 0.4890 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 42  loss 0.0002  val AP 0.4887\n",
      "[NR-AhR | seed 13] ep 43  loss 0.0002  val AP 0.4886\n",
      "[NR-AhR | seed 13] ep 44  loss 0.0002  val AP 0.4898\n",
      "  ✅ New best AP: 0.4898 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 45  loss 0.0002  val AP 0.4891\n",
      "[NR-AhR | seed 13] ep 46  loss 0.0002  val AP 0.4892\n",
      "[NR-AhR | seed 13] ep 47  loss 0.0002  val AP 0.4909\n",
      "  ✅ New best AP: 0.4909 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 48  loss 0.0002  val AP 0.4965\n",
      "  ✅ New best AP: 0.4965 → v7\\model\\ensembles\\NR-AhR\\seed13\\best.pt\n",
      "[NR-AhR | seed 13] ep 49  loss 0.0003  val AP 0.4964\n",
      "[NR-AhR | seed 13] ep 50  loss 0.0002  val AP 0.4962\n",
      "[NR-AhR | seed 29] ep 01  loss 0.0005  val AP 0.1463\n",
      "  ✅ New best AP: 0.1463 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 02  loss 0.0003  val AP 0.1783\n",
      "  ✅ New best AP: 0.1783 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 03  loss 0.0004  val AP 0.2228\n",
      "  ✅ New best AP: 0.2228 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 04  loss 0.0004  val AP 0.2970\n",
      "  ✅ New best AP: 0.2970 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 05  loss 0.0003  val AP 0.3429\n",
      "  ✅ New best AP: 0.3429 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 06  loss 0.0003  val AP 0.3760\n",
      "  ✅ New best AP: 0.3760 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 07  loss 0.0003  val AP 0.3963\n",
      "  ✅ New best AP: 0.3963 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 08  loss 0.0004  val AP 0.4069\n",
      "  ✅ New best AP: 0.4069 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 09  loss 0.0003  val AP 0.4157\n",
      "  ✅ New best AP: 0.4157 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 10  loss 0.0003  val AP 0.4281\n",
      "  ✅ New best AP: 0.4281 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 11  loss 0.0003  val AP 0.4382\n",
      "  ✅ New best AP: 0.4382 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 12  loss 0.0002  val AP 0.4465\n",
      "  ✅ New best AP: 0.4465 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 13  loss 0.0003  val AP 0.4537\n",
      "  ✅ New best AP: 0.4537 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 14  loss 0.0003  val AP 0.4622\n",
      "  ✅ New best AP: 0.4622 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 15  loss 0.0003  val AP 0.4742\n",
      "  ✅ New best AP: 0.4742 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 16  loss 0.0002  val AP 0.4782\n",
      "  ✅ New best AP: 0.4782 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 17  loss 0.0003  val AP 0.4809\n",
      "  ✅ New best AP: 0.4809 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 18  loss 0.0002  val AP 0.4916\n",
      "  ✅ New best AP: 0.4916 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 19  loss 0.0003  val AP 0.4930\n",
      "  ✅ New best AP: 0.4930 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 20  loss 0.0003  val AP 0.4928\n",
      "[NR-AhR | seed 29] ep 21  loss 0.0002  val AP 0.4987\n",
      "  ✅ New best AP: 0.4987 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 22  loss 0.0002  val AP 0.5025\n",
      "  ✅ New best AP: 0.5025 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 23  loss 0.0002  val AP 0.5063\n",
      "  ✅ New best AP: 0.5063 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 24  loss 0.0002  val AP 0.5064\n",
      "  ✅ New best AP: 0.5064 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 25  loss 0.0002  val AP 0.5152\n",
      "  ✅ New best AP: 0.5152 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 26  loss 0.0002  val AP 0.5150\n",
      "[NR-AhR | seed 29] ep 27  loss 0.0002  val AP 0.5174\n",
      "  ✅ New best AP: 0.5174 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 28  loss 0.0003  val AP 0.5227\n",
      "  ✅ New best AP: 0.5227 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 29  loss 0.0002  val AP 0.5232\n",
      "  ✅ New best AP: 0.5232 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 30  loss 0.0003  val AP 0.5251\n",
      "  ✅ New best AP: 0.5251 → v7\\model\\ensembles\\NR-AhR\\seed29\\best.pt\n",
      "[NR-AhR | seed 29] ep 31  loss 0.0002  val AP 0.5234\n",
      "[NR-AhR | seed 29] ep 32  loss 0.0003  val AP 0.5232\n",
      "[NR-AhR | seed 29] ep 33  loss 0.0002  val AP 0.5205\n",
      "[NR-AhR | seed 29] ep 34  loss 0.0002  val AP 0.5217\n",
      "[NR-AhR | seed 29] ep 35  loss 0.0002  val AP 0.5216\n",
      "[NR-AhR | seed 29] ep 36  loss 0.0003  val AP 0.5214\n",
      "[NR-AhR | seed 29] ep 37  loss 0.0002  val AP 0.5221\n",
      "[NR-AhR | seed 29] ep 38  loss 0.0002  val AP 0.5236\n",
      "[NR-AhR | seed 29] ep 39  loss 0.0002  val AP 0.5220\n",
      "[NR-AhR | seed 29] ep 40  loss 0.0002  val AP 0.5222\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.5251\n",
      "[NR-AhR | seed 47] ep 01  loss 0.0004  val AP 0.2954\n",
      "  ✅ New best AP: 0.2954 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 02  loss 0.0003  val AP 0.3084\n",
      "  ✅ New best AP: 0.3084 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 03  loss 0.0004  val AP 0.3248\n",
      "  ✅ New best AP: 0.3248 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 04  loss 0.0004  val AP 0.3428\n",
      "  ✅ New best AP: 0.3428 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 05  loss 0.0003  val AP 0.3663\n",
      "  ✅ New best AP: 0.3663 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 06  loss 0.0003  val AP 0.3803\n",
      "  ✅ New best AP: 0.3803 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 07  loss 0.0003  val AP 0.3906\n",
      "  ✅ New best AP: 0.3906 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 08  loss 0.0003  val AP 0.4022\n",
      "  ✅ New best AP: 0.4022 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 09  loss 0.0004  val AP 0.4048\n",
      "  ✅ New best AP: 0.4048 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 10  loss 0.0003  val AP 0.4110\n",
      "  ✅ New best AP: 0.4110 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 11  loss 0.0003  val AP 0.4125\n",
      "  ✅ New best AP: 0.4125 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 12  loss 0.0002  val AP 0.4136\n",
      "  ✅ New best AP: 0.4136 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 13  loss 0.0003  val AP 0.4150\n",
      "  ✅ New best AP: 0.4150 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 14  loss 0.0002  val AP 0.4197\n",
      "  ✅ New best AP: 0.4197 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 15  loss 0.0003  val AP 0.4206\n",
      "  ✅ New best AP: 0.4206 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 16  loss 0.0003  val AP 0.4206\n",
      "  ✅ New best AP: 0.4206 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 17  loss 0.0002  val AP 0.4217\n",
      "  ✅ New best AP: 0.4217 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 18  loss 0.0003  val AP 0.4271\n",
      "  ✅ New best AP: 0.4271 → v7\\model\\ensembles\\NR-AhR\\seed47\\best.pt\n",
      "[NR-AhR | seed 47] ep 19  loss 0.0002  val AP 0.4268\n",
      "[NR-AhR | seed 47] ep 20  loss 0.0002  val AP 0.4267\n",
      "[NR-AhR | seed 47] ep 21  loss 0.0002  val AP 0.4253\n",
      "[NR-AhR | seed 47] ep 22  loss 0.0003  val AP 0.4235\n",
      "[NR-AhR | seed 47] ep 23  loss 0.0002  val AP 0.4243\n",
      "[NR-AhR | seed 47] ep 24  loss 0.0003  val AP 0.4236\n",
      "[NR-AhR | seed 47] ep 25  loss 0.0002  val AP 0.4240\n",
      "[NR-AhR | seed 47] ep 26  loss 0.0002  val AP 0.4245\n",
      "[NR-AhR | seed 47] ep 27  loss 0.0002  val AP 0.4240\n",
      "[NR-AhR | seed 47] ep 28  loss 0.0002  val AP 0.4254\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.4271\n",
      "[NR-AhR | seed 61] ep 01  loss 0.0006  val AP 0.1098\n",
      "  ✅ New best AP: 0.1098 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 02  loss 0.0003  val AP 0.1275\n",
      "  ✅ New best AP: 0.1275 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 03  loss 0.0004  val AP 0.1531\n",
      "  ✅ New best AP: 0.1531 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 04  loss 0.0004  val AP 0.1907\n",
      "  ✅ New best AP: 0.1907 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 05  loss 0.0003  val AP 0.2298\n",
      "  ✅ New best AP: 0.2298 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 06  loss 0.0003  val AP 0.2699\n",
      "  ✅ New best AP: 0.2699 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 07  loss 0.0003  val AP 0.3004\n",
      "  ✅ New best AP: 0.3004 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 08  loss 0.0004  val AP 0.3174\n",
      "  ✅ New best AP: 0.3174 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 09  loss 0.0003  val AP 0.3381\n",
      "  ✅ New best AP: 0.3381 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 10  loss 0.0003  val AP 0.3478\n",
      "  ✅ New best AP: 0.3478 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 11  loss 0.0003  val AP 0.3587\n",
      "  ✅ New best AP: 0.3587 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 12  loss 0.0003  val AP 0.3694\n",
      "  ✅ New best AP: 0.3694 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 13  loss 0.0003  val AP 0.3844\n",
      "  ✅ New best AP: 0.3844 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 14  loss 0.0003  val AP 0.3907\n",
      "  ✅ New best AP: 0.3907 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 15  loss 0.0003  val AP 0.4004\n",
      "  ✅ New best AP: 0.4004 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 16  loss 0.0003  val AP 0.4105\n",
      "  ✅ New best AP: 0.4105 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 17  loss 0.0003  val AP 0.4138\n",
      "  ✅ New best AP: 0.4138 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 18  loss 0.0002  val AP 0.4254\n",
      "  ✅ New best AP: 0.4254 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 19  loss 0.0003  val AP 0.4280\n",
      "  ✅ New best AP: 0.4280 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 20  loss 0.0002  val AP 0.4323\n",
      "  ✅ New best AP: 0.4323 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 21  loss 0.0002  val AP 0.4386\n",
      "  ✅ New best AP: 0.4386 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 22  loss 0.0003  val AP 0.4399\n",
      "  ✅ New best AP: 0.4399 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 23  loss 0.0002  val AP 0.4434\n",
      "  ✅ New best AP: 0.4434 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 24  loss 0.0003  val AP 0.4448\n",
      "  ✅ New best AP: 0.4448 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 25  loss 0.0002  val AP 0.4476\n",
      "  ✅ New best AP: 0.4476 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 26  loss 0.0002  val AP 0.4512\n",
      "  ✅ New best AP: 0.4512 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 27  loss 0.0002  val AP 0.4532\n",
      "  ✅ New best AP: 0.4532 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 28  loss 0.0002  val AP 0.4552\n",
      "  ✅ New best AP: 0.4552 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 29  loss 0.0003  val AP 0.4500\n",
      "[NR-AhR | seed 61] ep 30  loss 0.0002  val AP 0.4512\n",
      "[NR-AhR | seed 61] ep 31  loss 0.0003  val AP 0.4541\n",
      "[NR-AhR | seed 61] ep 32  loss 0.0002  val AP 0.4565\n",
      "  ✅ New best AP: 0.4565 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 33  loss 0.0002  val AP 0.4514\n",
      "[NR-AhR | seed 61] ep 34  loss 0.0002  val AP 0.4541\n",
      "[NR-AhR | seed 61] ep 35  loss 0.0002  val AP 0.4578\n",
      "  ✅ New best AP: 0.4578 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 36  loss 0.0002  val AP 0.4577\n",
      "[NR-AhR | seed 61] ep 37  loss 0.0002  val AP 0.4585\n",
      "  ✅ New best AP: 0.4585 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 38  loss 0.0002  val AP 0.4592\n",
      "  ✅ New best AP: 0.4592 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 39  loss 0.0002  val AP 0.4604\n",
      "  ✅ New best AP: 0.4604 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 40  loss 0.0002  val AP 0.4621\n",
      "  ✅ New best AP: 0.4621 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 41  loss 0.0002  val AP 0.4633\n",
      "  ✅ New best AP: 0.4633 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 42  loss 0.0002  val AP 0.4641\n",
      "  ✅ New best AP: 0.4641 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 43  loss 0.0002  val AP 0.4641\n",
      "[NR-AhR | seed 61] ep 44  loss 0.0003  val AP 0.4648\n",
      "  ✅ New best AP: 0.4648 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 45  loss 0.0002  val AP 0.4660\n",
      "  ✅ New best AP: 0.4660 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 46  loss 0.0002  val AP 0.4677\n",
      "  ✅ New best AP: 0.4677 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 47  loss 0.0002  val AP 0.4681\n",
      "  ✅ New best AP: 0.4681 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 48  loss 0.0003  val AP 0.4683\n",
      "  ✅ New best AP: 0.4683 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 49  loss 0.0002  val AP 0.4691\n",
      "  ✅ New best AP: 0.4691 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 61] ep 50  loss 0.0002  val AP 0.4693\n",
      "  ✅ New best AP: 0.4693 → v7\\model\\ensembles\\NR-AhR\\seed61\\best.pt\n",
      "[NR-AhR | seed 83] ep 01  loss 0.0004  val AP 0.0667\n",
      "  ✅ New best AP: 0.0667 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 02  loss 0.0003  val AP 0.0718\n",
      "  ✅ New best AP: 0.0718 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 03  loss 0.0004  val AP 0.0813\n",
      "  ✅ New best AP: 0.0813 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 04  loss 0.0004  val AP 0.0978\n",
      "  ✅ New best AP: 0.0978 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 05  loss 0.0003  val AP 0.1263\n",
      "  ✅ New best AP: 0.1263 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 06  loss 0.0003  val AP 0.1662\n",
      "  ✅ New best AP: 0.1662 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 07  loss 0.0004  val AP 0.2027\n",
      "  ✅ New best AP: 0.2027 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 08  loss 0.0003  val AP 0.2321\n",
      "  ✅ New best AP: 0.2321 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 09  loss 0.0003  val AP 0.2562\n",
      "  ✅ New best AP: 0.2562 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 10  loss 0.0003  val AP 0.2742\n",
      "  ✅ New best AP: 0.2742 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 11  loss 0.0003  val AP 0.3033\n",
      "  ✅ New best AP: 0.3033 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 12  loss 0.0003  val AP 0.3276\n",
      "  ✅ New best AP: 0.3276 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 13  loss 0.0002  val AP 0.3417\n",
      "  ✅ New best AP: 0.3417 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 14  loss 0.0002  val AP 0.3503\n",
      "  ✅ New best AP: 0.3503 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 15  loss 0.0003  val AP 0.3610\n",
      "  ✅ New best AP: 0.3610 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 16  loss 0.0003  val AP 0.3679\n",
      "  ✅ New best AP: 0.3679 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 17  loss 0.0003  val AP 0.3736\n",
      "  ✅ New best AP: 0.3736 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 18  loss 0.0003  val AP 0.3780\n",
      "  ✅ New best AP: 0.3780 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 19  loss 0.0002  val AP 0.3829\n",
      "  ✅ New best AP: 0.3829 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 20  loss 0.0003  val AP 0.3999\n",
      "  ✅ New best AP: 0.3999 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 21  loss 0.0002  val AP 0.4033\n",
      "  ✅ New best AP: 0.4033 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 22  loss 0.0003  val AP 0.4074\n",
      "  ✅ New best AP: 0.4074 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 23  loss 0.0002  val AP 0.4140\n",
      "  ✅ New best AP: 0.4140 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 24  loss 0.0003  val AP 0.4192\n",
      "  ✅ New best AP: 0.4192 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 25  loss 0.0002  val AP 0.4215\n",
      "  ✅ New best AP: 0.4215 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 26  loss 0.0003  val AP 0.4297\n",
      "  ✅ New best AP: 0.4297 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 27  loss 0.0002  val AP 0.4314\n",
      "  ✅ New best AP: 0.4314 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 28  loss 0.0003  val AP 0.4342\n",
      "  ✅ New best AP: 0.4342 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 29  loss 0.0002  val AP 0.4348\n",
      "  ✅ New best AP: 0.4348 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 30  loss 0.0002  val AP 0.4344\n",
      "[NR-AhR | seed 83] ep 31  loss 0.0002  val AP 0.4348\n",
      "[NR-AhR | seed 83] ep 32  loss 0.0002  val AP 0.4371\n",
      "  ✅ New best AP: 0.4371 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 33  loss 0.0002  val AP 0.4374\n",
      "  ✅ New best AP: 0.4374 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 34  loss 0.0002  val AP 0.4384\n",
      "  ✅ New best AP: 0.4384 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 35  loss 0.0002  val AP 0.4390\n",
      "  ✅ New best AP: 0.4390 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 36  loss 0.0002  val AP 0.4408\n",
      "  ✅ New best AP: 0.4408 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 37  loss 0.0002  val AP 0.4433\n",
      "  ✅ New best AP: 0.4433 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 38  loss 0.0002  val AP 0.4462\n",
      "  ✅ New best AP: 0.4462 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 39  loss 0.0002  val AP 0.4475\n",
      "  ✅ New best AP: 0.4475 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 40  loss 0.0002  val AP 0.4477\n",
      "  ✅ New best AP: 0.4477 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 41  loss 0.0003  val AP 0.4482\n",
      "  ✅ New best AP: 0.4482 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 42  loss 0.0003  val AP 0.4490\n",
      "  ✅ New best AP: 0.4490 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 43  loss 0.0003  val AP 0.4502\n",
      "  ✅ New best AP: 0.4502 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 44  loss 0.0003  val AP 0.4508\n",
      "  ✅ New best AP: 0.4508 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 45  loss 0.0002  val AP 0.4514\n",
      "  ✅ New best AP: 0.4514 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 46  loss 0.0003  val AP 0.4525\n",
      "  ✅ New best AP: 0.4525 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 47  loss 0.0002  val AP 0.4536\n",
      "  ✅ New best AP: 0.4536 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 48  loss 0.0002  val AP 0.4538\n",
      "  ✅ New best AP: 0.4538 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "[NR-AhR | seed 83] ep 49  loss 0.0002  val AP 0.4528\n",
      "[NR-AhR | seed 83] ep 50  loss 0.0002  val AP 0.4540\n",
      "  ✅ New best AP: 0.4540 → v7\\model\\ensembles\\NR-AhR\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-Aromatase (label 3)\n",
      "==============================\n",
      "[NR-Aromatase | seed 13] ep 01  loss 0.0009  val AP 0.0446\n",
      "  ✅ New best AP: 0.0446 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 02  loss 0.0007  val AP 0.0452\n",
      "  ✅ New best AP: 0.0452 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 03  loss 0.0007  val AP 0.0467\n",
      "  ✅ New best AP: 0.0467 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 04  loss 0.0009  val AP 0.0495\n",
      "  ✅ New best AP: 0.0495 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 05  loss 0.0007  val AP 0.0517\n",
      "  ✅ New best AP: 0.0517 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 06  loss 0.0006  val AP 0.0553\n",
      "  ✅ New best AP: 0.0553 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 07  loss 0.0006  val AP 0.0599\n",
      "  ✅ New best AP: 0.0599 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 08  loss 0.0007  val AP 0.0668\n",
      "  ✅ New best AP: 0.0668 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 09  loss 0.0007  val AP 0.0740\n",
      "  ✅ New best AP: 0.0740 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 10  loss 0.0008  val AP 0.0822\n",
      "  ✅ New best AP: 0.0822 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 11  loss 0.0007  val AP 0.0899\n",
      "  ✅ New best AP: 0.0899 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 12  loss 0.0006  val AP 0.0982\n",
      "  ✅ New best AP: 0.0982 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 13  loss 0.0006  val AP 0.1073\n",
      "  ✅ New best AP: 0.1073 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 14  loss 0.0006  val AP 0.1133\n",
      "  ✅ New best AP: 0.1133 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 15  loss 0.0006  val AP 0.1202\n",
      "  ✅ New best AP: 0.1202 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 16  loss 0.0008  val AP 0.1273\n",
      "  ✅ New best AP: 0.1273 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 17  loss 0.0006  val AP 0.1329\n",
      "  ✅ New best AP: 0.1329 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 18  loss 0.0006  val AP 0.1386\n",
      "  ✅ New best AP: 0.1386 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 19  loss 0.0006  val AP 0.1428\n",
      "  ✅ New best AP: 0.1428 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 20  loss 0.0006  val AP 0.1479\n",
      "  ✅ New best AP: 0.1479 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 21  loss 0.0006  val AP 0.1567\n",
      "  ✅ New best AP: 0.1567 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 22  loss 0.0006  val AP 0.1596\n",
      "  ✅ New best AP: 0.1596 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 23  loss 0.0005  val AP 0.1677\n",
      "  ✅ New best AP: 0.1677 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 24  loss 0.0006  val AP 0.1708\n",
      "  ✅ New best AP: 0.1708 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 25  loss 0.0006  val AP 0.1736\n",
      "  ✅ New best AP: 0.1736 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 26  loss 0.0006  val AP 0.1757\n",
      "  ✅ New best AP: 0.1757 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 27  loss 0.0006  val AP 0.1923\n",
      "  ✅ New best AP: 0.1923 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 28  loss 0.0005  val AP 0.1957\n",
      "  ✅ New best AP: 0.1957 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 29  loss 0.0006  val AP 0.1981\n",
      "  ✅ New best AP: 0.1981 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 30  loss 0.0005  val AP 0.2031\n",
      "  ✅ New best AP: 0.2031 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 31  loss 0.0005  val AP 0.2062\n",
      "  ✅ New best AP: 0.2062 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 32  loss 0.0006  val AP 0.2084\n",
      "  ✅ New best AP: 0.2084 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 33  loss 0.0005  val AP 0.2093\n",
      "  ✅ New best AP: 0.2093 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 34  loss 0.0006  val AP 0.2107\n",
      "  ✅ New best AP: 0.2107 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 35  loss 0.0005  val AP 0.2117\n",
      "  ✅ New best AP: 0.2117 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 36  loss 0.0006  val AP 0.2126\n",
      "  ✅ New best AP: 0.2126 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 37  loss 0.0006  val AP 0.2144\n",
      "  ✅ New best AP: 0.2144 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 38  loss 0.0006  val AP 0.2152\n",
      "  ✅ New best AP: 0.2152 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 39  loss 0.0006  val AP 0.2157\n",
      "  ✅ New best AP: 0.2157 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 40  loss 0.0006  val AP 0.2209\n",
      "  ✅ New best AP: 0.2209 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 41  loss 0.0005  val AP 0.2211\n",
      "  ✅ New best AP: 0.2211 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 42  loss 0.0006  val AP 0.2239\n",
      "  ✅ New best AP: 0.2239 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 43  loss 0.0006  val AP 0.2242\n",
      "  ✅ New best AP: 0.2242 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 44  loss 0.0006  val AP 0.2254\n",
      "  ✅ New best AP: 0.2254 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 45  loss 0.0006  val AP 0.2256\n",
      "  ✅ New best AP: 0.2256 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 46  loss 0.0006  val AP 0.2268\n",
      "  ✅ New best AP: 0.2268 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 47  loss 0.0006  val AP 0.2306\n",
      "  ✅ New best AP: 0.2306 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 48  loss 0.0006  val AP 0.2317\n",
      "  ✅ New best AP: 0.2317 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 49  loss 0.0006  val AP 0.2337\n",
      "  ✅ New best AP: 0.2337 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 13] ep 50  loss 0.0006  val AP 0.2439\n",
      "  ✅ New best AP: 0.2439 → v7\\model\\ensembles\\NR-Aromatase\\seed13\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 01  loss 0.0012  val AP 0.0793\n",
      "  ✅ New best AP: 0.0793 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 02  loss 0.0008  val AP 0.0833\n",
      "  ✅ New best AP: 0.0833 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 03  loss 0.0010  val AP 0.0887\n",
      "  ✅ New best AP: 0.0887 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 04  loss 0.0010  val AP 0.0992\n",
      "  ✅ New best AP: 0.0992 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 05  loss 0.0007  val AP 0.1043\n",
      "  ✅ New best AP: 0.1043 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 06  loss 0.0007  val AP 0.1083\n",
      "  ✅ New best AP: 0.1083 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 07  loss 0.0008  val AP 0.1123\n",
      "  ✅ New best AP: 0.1123 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 08  loss 0.0007  val AP 0.1165\n",
      "  ✅ New best AP: 0.1165 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 09  loss 0.0007  val AP 0.1222\n",
      "  ✅ New best AP: 0.1222 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 10  loss 0.0007  val AP 0.1243\n",
      "  ✅ New best AP: 0.1243 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 11  loss 0.0006  val AP 0.1276\n",
      "  ✅ New best AP: 0.1276 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 12  loss 0.0006  val AP 0.1310\n",
      "  ✅ New best AP: 0.1310 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 13  loss 0.0007  val AP 0.1356\n",
      "  ✅ New best AP: 0.1356 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 14  loss 0.0006  val AP 0.1368\n",
      "  ✅ New best AP: 0.1368 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 15  loss 0.0006  val AP 0.1387\n",
      "  ✅ New best AP: 0.1387 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 16  loss 0.0006  val AP 0.1413\n",
      "  ✅ New best AP: 0.1413 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 17  loss 0.0006  val AP 0.1452\n",
      "  ✅ New best AP: 0.1452 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 18  loss 0.0006  val AP 0.1484\n",
      "  ✅ New best AP: 0.1484 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 19  loss 0.0006  val AP 0.1503\n",
      "  ✅ New best AP: 0.1503 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 20  loss 0.0006  val AP 0.1525\n",
      "  ✅ New best AP: 0.1525 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 21  loss 0.0006  val AP 0.1547\n",
      "  ✅ New best AP: 0.1547 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 22  loss 0.0005  val AP 0.1558\n",
      "  ✅ New best AP: 0.1558 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 23  loss 0.0006  val AP 0.1576\n",
      "  ✅ New best AP: 0.1576 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 24  loss 0.0006  val AP 0.1595\n",
      "  ✅ New best AP: 0.1595 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 25  loss 0.0006  val AP 0.1620\n",
      "  ✅ New best AP: 0.1620 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 26  loss 0.0006  val AP 0.1638\n",
      "  ✅ New best AP: 0.1638 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 27  loss 0.0006  val AP 0.1679\n",
      "  ✅ New best AP: 0.1679 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 28  loss 0.0006  val AP 0.1704\n",
      "  ✅ New best AP: 0.1704 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 29  loss 0.0006  val AP 0.1754\n",
      "  ✅ New best AP: 0.1754 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 30  loss 0.0006  val AP 0.1782\n",
      "  ✅ New best AP: 0.1782 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 31  loss 0.0006  val AP 0.1786\n",
      "  ✅ New best AP: 0.1786 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 32  loss 0.0006  val AP 0.1821\n",
      "  ✅ New best AP: 0.1821 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 33  loss 0.0006  val AP 0.1859\n",
      "  ✅ New best AP: 0.1859 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 34  loss 0.0006  val AP 0.1873\n",
      "  ✅ New best AP: 0.1873 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 35  loss 0.0006  val AP 0.1881\n",
      "  ✅ New best AP: 0.1881 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 36  loss 0.0006  val AP 0.1889\n",
      "  ✅ New best AP: 0.1889 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 37  loss 0.0006  val AP 0.1903\n",
      "  ✅ New best AP: 0.1903 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 38  loss 0.0006  val AP 0.1906\n",
      "  ✅ New best AP: 0.1906 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 39  loss 0.0006  val AP 0.1908\n",
      "  ✅ New best AP: 0.1908 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 40  loss 0.0006  val AP 0.1906\n",
      "[NR-Aromatase | seed 29] ep 41  loss 0.0006  val AP 0.1917\n",
      "  ✅ New best AP: 0.1917 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 42  loss 0.0006  val AP 0.1929\n",
      "  ✅ New best AP: 0.1929 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 43  loss 0.0006  val AP 0.1940\n",
      "  ✅ New best AP: 0.1940 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 44  loss 0.0006  val AP 0.1960\n",
      "  ✅ New best AP: 0.1960 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 45  loss 0.0006  val AP 0.1955\n",
      "[NR-Aromatase | seed 29] ep 46  loss 0.0006  val AP 0.1961\n",
      "  ✅ New best AP: 0.1961 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 47  loss 0.0006  val AP 0.1971\n",
      "  ✅ New best AP: 0.1971 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 48  loss 0.0006  val AP 0.1983\n",
      "  ✅ New best AP: 0.1983 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 49  loss 0.0006  val AP 0.1988\n",
      "  ✅ New best AP: 0.1988 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 29] ep 50  loss 0.0006  val AP 0.2004\n",
      "  ✅ New best AP: 0.2004 → v7\\model\\ensembles\\NR-Aromatase\\seed29\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 01  loss 0.0011  val AP 0.1492\n",
      "  ✅ New best AP: 0.1492 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 02  loss 0.0008  val AP 0.1587\n",
      "  ✅ New best AP: 0.1587 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 03  loss 0.0009  val AP 0.1618\n",
      "  ✅ New best AP: 0.1618 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 04  loss 0.0011  val AP 0.1690\n",
      "  ✅ New best AP: 0.1690 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 05  loss 0.0007  val AP 0.1778\n",
      "  ✅ New best AP: 0.1778 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 06  loss 0.0007  val AP 0.1831\n",
      "  ✅ New best AP: 0.1831 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 07  loss 0.0007  val AP 0.1870\n",
      "  ✅ New best AP: 0.1870 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 08  loss 0.0007  val AP 0.1889\n",
      "  ✅ New best AP: 0.1889 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 09  loss 0.0006  val AP 0.1921\n",
      "  ✅ New best AP: 0.1921 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 10  loss 0.0007  val AP 0.1959\n",
      "  ✅ New best AP: 0.1959 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 11  loss 0.0006  val AP 0.1975\n",
      "  ✅ New best AP: 0.1975 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 12  loss 0.0008  val AP 0.1987\n",
      "  ✅ New best AP: 0.1987 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 13  loss 0.0006  val AP 0.2003\n",
      "  ✅ New best AP: 0.2003 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 14  loss 0.0006  val AP 0.2022\n",
      "  ✅ New best AP: 0.2022 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 15  loss 0.0006  val AP 0.2035\n",
      "  ✅ New best AP: 0.2035 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 16  loss 0.0006  val AP 0.2050\n",
      "  ✅ New best AP: 0.2050 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 17  loss 0.0006  val AP 0.2066\n",
      "  ✅ New best AP: 0.2066 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 18  loss 0.0006  val AP 0.2090\n",
      "  ✅ New best AP: 0.2090 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 19  loss 0.0006  val AP 0.2098\n",
      "  ✅ New best AP: 0.2098 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 20  loss 0.0006  val AP 0.2103\n",
      "  ✅ New best AP: 0.2103 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 21  loss 0.0006  val AP 0.2128\n",
      "  ✅ New best AP: 0.2128 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 22  loss 0.0005  val AP 0.2151\n",
      "  ✅ New best AP: 0.2151 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 23  loss 0.0006  val AP 0.2163\n",
      "  ✅ New best AP: 0.2163 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 24  loss 0.0006  val AP 0.2178\n",
      "  ✅ New best AP: 0.2178 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 25  loss 0.0005  val AP 0.2183\n",
      "  ✅ New best AP: 0.2183 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 26  loss 0.0006  val AP 0.2208\n",
      "  ✅ New best AP: 0.2208 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 27  loss 0.0006  val AP 0.2251\n",
      "  ✅ New best AP: 0.2251 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 28  loss 0.0005  val AP 0.2259\n",
      "  ✅ New best AP: 0.2259 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 29  loss 0.0006  val AP 0.2259\n",
      "[NR-Aromatase | seed 47] ep 30  loss 0.0006  val AP 0.2263\n",
      "  ✅ New best AP: 0.2263 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 31  loss 0.0006  val AP 0.2284\n",
      "  ✅ New best AP: 0.2284 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 32  loss 0.0006  val AP 0.2322\n",
      "  ✅ New best AP: 0.2322 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 33  loss 0.0006  val AP 0.2354\n",
      "  ✅ New best AP: 0.2354 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 34  loss 0.0006  val AP 0.2359\n",
      "  ✅ New best AP: 0.2359 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 35  loss 0.0006  val AP 0.2365\n",
      "  ✅ New best AP: 0.2365 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 36  loss 0.0006  val AP 0.2365\n",
      "[NR-Aromatase | seed 47] ep 37  loss 0.0006  val AP 0.2370\n",
      "  ✅ New best AP: 0.2370 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 38  loss 0.0006  val AP 0.2364\n",
      "[NR-Aromatase | seed 47] ep 39  loss 0.0006  val AP 0.2384\n",
      "  ✅ New best AP: 0.2384 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 40  loss 0.0006  val AP 0.2389\n",
      "  ✅ New best AP: 0.2389 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 41  loss 0.0006  val AP 0.2388\n",
      "[NR-Aromatase | seed 47] ep 42  loss 0.0006  val AP 0.2389\n",
      "[NR-Aromatase | seed 47] ep 43  loss 0.0006  val AP 0.2390\n",
      "  ✅ New best AP: 0.2390 → v7\\model\\ensembles\\NR-Aromatase\\seed47\\best.pt\n",
      "[NR-Aromatase | seed 47] ep 44  loss 0.0006  val AP 0.2377\n",
      "[NR-Aromatase | seed 47] ep 45  loss 0.0006  val AP 0.2378\n",
      "[NR-Aromatase | seed 47] ep 46  loss 0.0006  val AP 0.2380\n",
      "[NR-Aromatase | seed 47] ep 47  loss 0.0006  val AP 0.2379\n",
      "[NR-Aromatase | seed 47] ep 48  loss 0.0006  val AP 0.2378\n",
      "[NR-Aromatase | seed 47] ep 49  loss 0.0006  val AP 0.2388\n",
      "[NR-Aromatase | seed 47] ep 50  loss 0.0006  val AP 0.2383\n",
      "[NR-Aromatase | seed 61] ep 01  loss 0.0014  val AP 0.0926\n",
      "  ✅ New best AP: 0.0926 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 02  loss 0.0009  val AP 0.1002\n",
      "  ✅ New best AP: 0.1002 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 03  loss 0.0009  val AP 0.1122\n",
      "  ✅ New best AP: 0.1122 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 04  loss 0.0010  val AP 0.1212\n",
      "  ✅ New best AP: 0.1212 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 05  loss 0.0008  val AP 0.1403\n",
      "  ✅ New best AP: 0.1403 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 06  loss 0.0008  val AP 0.1597\n",
      "  ✅ New best AP: 0.1597 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 07  loss 0.0008  val AP 0.1740\n",
      "  ✅ New best AP: 0.1740 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 08  loss 0.0007  val AP 0.1971\n",
      "  ✅ New best AP: 0.1971 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 09  loss 0.0007  val AP 0.2087\n",
      "  ✅ New best AP: 0.2087 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 10  loss 0.0007  val AP 0.2221\n",
      "  ✅ New best AP: 0.2221 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 11  loss 0.0007  val AP 0.2123\n",
      "[NR-Aromatase | seed 61] ep 12  loss 0.0006  val AP 0.2184\n",
      "[NR-Aromatase | seed 61] ep 13  loss 0.0007  val AP 0.2258\n",
      "  ✅ New best AP: 0.2258 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 14  loss 0.0007  val AP 0.2325\n",
      "  ✅ New best AP: 0.2325 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 15  loss 0.0006  val AP 0.2326\n",
      "  ✅ New best AP: 0.2326 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 16  loss 0.0006  val AP 0.2372\n",
      "  ✅ New best AP: 0.2372 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 17  loss 0.0006  val AP 0.2414\n",
      "  ✅ New best AP: 0.2414 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 18  loss 0.0006  val AP 0.2539\n",
      "  ✅ New best AP: 0.2539 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 19  loss 0.0006  val AP 0.2572\n",
      "  ✅ New best AP: 0.2572 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 20  loss 0.0006  val AP 0.2594\n",
      "  ✅ New best AP: 0.2594 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 21  loss 0.0005  val AP 0.2606\n",
      "  ✅ New best AP: 0.2606 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 22  loss 0.0006  val AP 0.2622\n",
      "  ✅ New best AP: 0.2622 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 23  loss 0.0006  val AP 0.2639\n",
      "  ✅ New best AP: 0.2639 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 24  loss 0.0006  val AP 0.2643\n",
      "  ✅ New best AP: 0.2643 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 25  loss 0.0006  val AP 0.2626\n",
      "[NR-Aromatase | seed 61] ep 26  loss 0.0006  val AP 0.2632\n",
      "[NR-Aromatase | seed 61] ep 27  loss 0.0006  val AP 0.2638\n",
      "[NR-Aromatase | seed 61] ep 28  loss 0.0005  val AP 0.2645\n",
      "  ✅ New best AP: 0.2645 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 29  loss 0.0006  val AP 0.2650\n",
      "  ✅ New best AP: 0.2650 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 30  loss 0.0006  val AP 0.2650\n",
      "  ✅ New best AP: 0.2650 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 31  loss 0.0006  val AP 0.2646\n",
      "[NR-Aromatase | seed 61] ep 32  loss 0.0006  val AP 0.2693\n",
      "  ✅ New best AP: 0.2693 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 33  loss 0.0006  val AP 0.2693\n",
      "[NR-Aromatase | seed 61] ep 34  loss 0.0006  val AP 0.2681\n",
      "[NR-Aromatase | seed 61] ep 35  loss 0.0006  val AP 0.2683\n",
      "[NR-Aromatase | seed 61] ep 36  loss 0.0006  val AP 0.2689\n",
      "[NR-Aromatase | seed 61] ep 37  loss 0.0006  val AP 0.2697\n",
      "  ✅ New best AP: 0.2697 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 38  loss 0.0006  val AP 0.2699\n",
      "  ✅ New best AP: 0.2699 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 39  loss 0.0006  val AP 0.2701\n",
      "  ✅ New best AP: 0.2701 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 40  loss 0.0006  val AP 0.2705\n",
      "  ✅ New best AP: 0.2705 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 41  loss 0.0006  val AP 0.2706\n",
      "  ✅ New best AP: 0.2706 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 42  loss 0.0006  val AP 0.2704\n",
      "[NR-Aromatase | seed 61] ep 43  loss 0.0006  val AP 0.2708\n",
      "  ✅ New best AP: 0.2708 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 44  loss 0.0006  val AP 0.2708\n",
      "  ✅ New best AP: 0.2708 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 45  loss 0.0006  val AP 0.2708\n",
      "[NR-Aromatase | seed 61] ep 46  loss 0.0006  val AP 0.2709\n",
      "  ✅ New best AP: 0.2709 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 47  loss 0.0006  val AP 0.2708\n",
      "[NR-Aromatase | seed 61] ep 48  loss 0.0006  val AP 0.2707\n",
      "[NR-Aromatase | seed 61] ep 49  loss 0.0006  val AP 0.2753\n",
      "  ✅ New best AP: 0.2753 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 61] ep 50  loss 0.0006  val AP 0.2754\n",
      "  ✅ New best AP: 0.2754 → v7\\model\\ensembles\\NR-Aromatase\\seed61\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 01  loss 0.0010  val AP 0.0987\n",
      "  ✅ New best AP: 0.0987 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 02  loss 0.0007  val AP 0.1008\n",
      "  ✅ New best AP: 0.1008 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 03  loss 0.0009  val AP 0.1092\n",
      "  ✅ New best AP: 0.1092 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 04  loss 0.0011  val AP 0.1199\n",
      "  ✅ New best AP: 0.1199 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 05  loss 0.0007  val AP 0.1344\n",
      "  ✅ New best AP: 0.1344 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 06  loss 0.0007  val AP 0.1469\n",
      "  ✅ New best AP: 0.1469 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 07  loss 0.0008  val AP 0.1571\n",
      "  ✅ New best AP: 0.1571 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 08  loss 0.0007  val AP 0.1656\n",
      "  ✅ New best AP: 0.1656 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 09  loss 0.0006  val AP 0.1702\n",
      "  ✅ New best AP: 0.1702 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 10  loss 0.0006  val AP 0.1776\n",
      "  ✅ New best AP: 0.1776 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 11  loss 0.0006  val AP 0.1831\n",
      "  ✅ New best AP: 0.1831 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 12  loss 0.0007  val AP 0.2032\n",
      "  ✅ New best AP: 0.2032 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 13  loss 0.0007  val AP 0.2105\n",
      "  ✅ New best AP: 0.2105 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 14  loss 0.0006  val AP 0.2166\n",
      "  ✅ New best AP: 0.2166 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 15  loss 0.0006  val AP 0.2151\n",
      "[NR-Aromatase | seed 83] ep 16  loss 0.0006  val AP 0.2189\n",
      "  ✅ New best AP: 0.2189 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 17  loss 0.0006  val AP 0.2430\n",
      "  ✅ New best AP: 0.2430 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 18  loss 0.0006  val AP 0.2504\n",
      "  ✅ New best AP: 0.2504 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 19  loss 0.0005  val AP 0.2510\n",
      "  ✅ New best AP: 0.2510 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 20  loss 0.0007  val AP 0.2533\n",
      "  ✅ New best AP: 0.2533 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 21  loss 0.0006  val AP 0.2532\n",
      "[NR-Aromatase | seed 83] ep 22  loss 0.0006  val AP 0.2548\n",
      "  ✅ New best AP: 0.2548 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 23  loss 0.0005  val AP 0.2553\n",
      "  ✅ New best AP: 0.2553 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 24  loss 0.0005  val AP 0.2554\n",
      "  ✅ New best AP: 0.2554 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 25  loss 0.0006  val AP 0.2563\n",
      "  ✅ New best AP: 0.2563 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 26  loss 0.0005  val AP 0.2566\n",
      "  ✅ New best AP: 0.2566 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 27  loss 0.0005  val AP 0.2574\n",
      "  ✅ New best AP: 0.2574 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 28  loss 0.0005  val AP 0.2580\n",
      "  ✅ New best AP: 0.2580 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 29  loss 0.0006  val AP 0.2578\n",
      "[NR-Aromatase | seed 83] ep 30  loss 0.0006  val AP 0.2606\n",
      "  ✅ New best AP: 0.2606 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 31  loss 0.0005  val AP 0.2617\n",
      "  ✅ New best AP: 0.2617 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 32  loss 0.0006  val AP 0.2612\n",
      "[NR-Aromatase | seed 83] ep 33  loss 0.0006  val AP 0.2619\n",
      "  ✅ New best AP: 0.2619 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 34  loss 0.0006  val AP 0.2628\n",
      "  ✅ New best AP: 0.2628 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 35  loss 0.0006  val AP 0.2637\n",
      "  ✅ New best AP: 0.2637 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 36  loss 0.0006  val AP 0.2634\n",
      "[NR-Aromatase | seed 83] ep 37  loss 0.0006  val AP 0.2646\n",
      "  ✅ New best AP: 0.2646 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 38  loss 0.0006  val AP 0.2663\n",
      "  ✅ New best AP: 0.2663 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 39  loss 0.0006  val AP 0.2672\n",
      "  ✅ New best AP: 0.2672 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 40  loss 0.0006  val AP 0.2669\n",
      "[NR-Aromatase | seed 83] ep 41  loss 0.0006  val AP 0.2671\n",
      "[NR-Aromatase | seed 83] ep 42  loss 0.0006  val AP 0.2672\n",
      "  ✅ New best AP: 0.2672 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 43  loss 0.0006  val AP 0.2672\n",
      "[NR-Aromatase | seed 83] ep 44  loss 0.0006  val AP 0.2682\n",
      "  ✅ New best AP: 0.2682 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 45  loss 0.0006  val AP 0.2684\n",
      "  ✅ New best AP: 0.2684 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 46  loss 0.0006  val AP 0.2697\n",
      "  ✅ New best AP: 0.2697 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 47  loss 0.0006  val AP 0.2711\n",
      "  ✅ New best AP: 0.2711 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 48  loss 0.0006  val AP 0.2716\n",
      "  ✅ New best AP: 0.2716 → v7\\model\\ensembles\\NR-Aromatase\\seed83\\best.pt\n",
      "[NR-Aromatase | seed 83] ep 49  loss 0.0006  val AP 0.2713\n",
      "[NR-Aromatase | seed 83] ep 50  loss 0.0006  val AP 0.2714\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-ER (label 4)\n",
      "==============================\n",
      "[NR-ER | seed 13] ep 01  loss 0.0004  val AP 0.0694\n",
      "  ✅ New best AP: 0.0694 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 02  loss 0.0004  val AP 0.0697\n",
      "  ✅ New best AP: 0.0697 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 03  loss 0.0003  val AP 0.0702\n",
      "  ✅ New best AP: 0.0702 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 04  loss 0.0003  val AP 0.0710\n",
      "  ✅ New best AP: 0.0710 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 05  loss 0.0003  val AP 0.0725\n",
      "  ✅ New best AP: 0.0725 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 06  loss 0.0003  val AP 0.0748\n",
      "  ✅ New best AP: 0.0748 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 07  loss 0.0005  val AP 0.0772\n",
      "  ✅ New best AP: 0.0772 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 08  loss 0.0003  val AP 0.0804\n",
      "  ✅ New best AP: 0.0804 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 09  loss 0.0004  val AP 0.0850\n",
      "  ✅ New best AP: 0.0850 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 10  loss 0.0003  val AP 0.0909\n",
      "  ✅ New best AP: 0.0909 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 11  loss 0.0003  val AP 0.0968\n",
      "  ✅ New best AP: 0.0968 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 12  loss 0.0003  val AP 0.1045\n",
      "  ✅ New best AP: 0.1045 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 13  loss 0.0003  val AP 0.1154\n",
      "  ✅ New best AP: 0.1154 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 14  loss 0.0004  val AP 0.1272\n",
      "  ✅ New best AP: 0.1272 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 15  loss 0.0003  val AP 0.1412\n",
      "  ✅ New best AP: 0.1412 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 16  loss 0.0003  val AP 0.1549\n",
      "  ✅ New best AP: 0.1549 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 17  loss 0.0003  val AP 0.1620\n",
      "  ✅ New best AP: 0.1620 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 18  loss 0.0003  val AP 0.1687\n",
      "  ✅ New best AP: 0.1687 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 19  loss 0.0003  val AP 0.1929\n",
      "  ✅ New best AP: 0.1929 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 20  loss 0.0003  val AP 0.2026\n",
      "  ✅ New best AP: 0.2026 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 21  loss 0.0003  val AP 0.2052\n",
      "  ✅ New best AP: 0.2052 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 22  loss 0.0004  val AP 0.2062\n",
      "  ✅ New best AP: 0.2062 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 23  loss 0.0003  val AP 0.2057\n",
      "[NR-ER | seed 13] ep 24  loss 0.0003  val AP 0.2121\n",
      "  ✅ New best AP: 0.2121 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 25  loss 0.0003  val AP 0.2131\n",
      "  ✅ New best AP: 0.2131 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 26  loss 0.0003  val AP 0.2116\n",
      "[NR-ER | seed 13] ep 27  loss 0.0003  val AP 0.2109\n",
      "[NR-ER | seed 13] ep 28  loss 0.0003  val AP 0.2186\n",
      "  ✅ New best AP: 0.2186 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 29  loss 0.0003  val AP 0.2187\n",
      "  ✅ New best AP: 0.2187 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 30  loss 0.0003  val AP 0.2207\n",
      "  ✅ New best AP: 0.2207 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 31  loss 0.0003  val AP 0.2195\n",
      "[NR-ER | seed 13] ep 32  loss 0.0003  val AP 0.2195\n",
      "[NR-ER | seed 13] ep 33  loss 0.0003  val AP 0.2191\n",
      "[NR-ER | seed 13] ep 34  loss 0.0003  val AP 0.2193\n",
      "[NR-ER | seed 13] ep 35  loss 0.0003  val AP 0.2222\n",
      "  ✅ New best AP: 0.2222 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 36  loss 0.0003  val AP 0.2223\n",
      "  ✅ New best AP: 0.2223 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 37  loss 0.0003  val AP 0.2245\n",
      "  ✅ New best AP: 0.2245 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 38  loss 0.0003  val AP 0.2252\n",
      "  ✅ New best AP: 0.2252 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 39  loss 0.0003  val AP 0.2251\n",
      "[NR-ER | seed 13] ep 40  loss 0.0003  val AP 0.2240\n",
      "[NR-ER | seed 13] ep 41  loss 0.0003  val AP 0.2261\n",
      "  ✅ New best AP: 0.2261 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 42  loss 0.0003  val AP 0.2261\n",
      "  ✅ New best AP: 0.2261 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 43  loss 0.0003  val AP 0.2261\n",
      "  ✅ New best AP: 0.2261 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 44  loss 0.0003  val AP 0.2264\n",
      "  ✅ New best AP: 0.2264 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 45  loss 0.0003  val AP 0.2304\n",
      "  ✅ New best AP: 0.2304 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 46  loss 0.0003  val AP 0.2305\n",
      "  ✅ New best AP: 0.2305 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 47  loss 0.0003  val AP 0.2306\n",
      "  ✅ New best AP: 0.2306 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 48  loss 0.0003  val AP 0.2308\n",
      "  ✅ New best AP: 0.2308 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 49  loss 0.0003  val AP 0.2309\n",
      "  ✅ New best AP: 0.2309 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 13] ep 50  loss 0.0003  val AP 0.2309\n",
      "  ✅ New best AP: 0.2309 → v7\\model\\ensembles\\NR-ER\\seed13\\best.pt\n",
      "[NR-ER | seed 29] ep 01  loss 0.0005  val AP 0.1210\n",
      "  ✅ New best AP: 0.1210 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 02  loss 0.0004  val AP 0.1232\n",
      "  ✅ New best AP: 0.1232 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 03  loss 0.0004  val AP 0.1245\n",
      "  ✅ New best AP: 0.1245 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 04  loss 0.0005  val AP 0.1275\n",
      "  ✅ New best AP: 0.1275 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 05  loss 0.0004  val AP 0.1290\n",
      "  ✅ New best AP: 0.1290 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 06  loss 0.0004  val AP 0.1331\n",
      "  ✅ New best AP: 0.1331 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 07  loss 0.0004  val AP 0.1347\n",
      "  ✅ New best AP: 0.1347 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 08  loss 0.0004  val AP 0.1378\n",
      "  ✅ New best AP: 0.1378 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 09  loss 0.0004  val AP 0.1392\n",
      "  ✅ New best AP: 0.1392 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 10  loss 0.0003  val AP 0.1409\n",
      "  ✅ New best AP: 0.1409 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 11  loss 0.0003  val AP 0.1496\n",
      "  ✅ New best AP: 0.1496 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 12  loss 0.0004  val AP 0.1501\n",
      "  ✅ New best AP: 0.1501 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 13  loss 0.0004  val AP 0.1536\n",
      "  ✅ New best AP: 0.1536 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 14  loss 0.0003  val AP 0.1554\n",
      "  ✅ New best AP: 0.1554 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 15  loss 0.0003  val AP 0.1566\n",
      "  ✅ New best AP: 0.1566 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 16  loss 0.0003  val AP 0.1569\n",
      "  ✅ New best AP: 0.1569 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 17  loss 0.0003  val AP 0.1581\n",
      "  ✅ New best AP: 0.1581 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 18  loss 0.0003  val AP 0.1588\n",
      "  ✅ New best AP: 0.1588 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 19  loss 0.0003  val AP 0.1609\n",
      "  ✅ New best AP: 0.1609 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 20  loss 0.0003  val AP 0.1622\n",
      "  ✅ New best AP: 0.1622 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 21  loss 0.0003  val AP 0.1627\n",
      "  ✅ New best AP: 0.1627 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 22  loss 0.0003  val AP 0.1640\n",
      "  ✅ New best AP: 0.1640 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 23  loss 0.0003  val AP 0.1662\n",
      "  ✅ New best AP: 0.1662 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 24  loss 0.0003  val AP 0.1675\n",
      "  ✅ New best AP: 0.1675 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 25  loss 0.0003  val AP 0.1694\n",
      "  ✅ New best AP: 0.1694 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 26  loss 0.0003  val AP 0.1706\n",
      "  ✅ New best AP: 0.1706 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 27  loss 0.0003  val AP 0.1715\n",
      "  ✅ New best AP: 0.1715 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 28  loss 0.0003  val AP 0.1715\n",
      "  ✅ New best AP: 0.1715 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 29  loss 0.0003  val AP 0.1722\n",
      "  ✅ New best AP: 0.1722 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 30  loss 0.0003  val AP 0.1734\n",
      "  ✅ New best AP: 0.1734 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 31  loss 0.0003  val AP 0.1737\n",
      "  ✅ New best AP: 0.1737 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 32  loss 0.0003  val AP 0.1740\n",
      "  ✅ New best AP: 0.1740 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 33  loss 0.0003  val AP 0.1750\n",
      "  ✅ New best AP: 0.1750 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 34  loss 0.0003  val AP 0.1761\n",
      "  ✅ New best AP: 0.1761 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 35  loss 0.0003  val AP 0.1768\n",
      "  ✅ New best AP: 0.1768 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 36  loss 0.0003  val AP 0.1775\n",
      "  ✅ New best AP: 0.1775 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 37  loss 0.0003  val AP 0.1785\n",
      "  ✅ New best AP: 0.1785 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 38  loss 0.0003  val AP 0.1795\n",
      "  ✅ New best AP: 0.1795 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 39  loss 0.0003  val AP 0.1808\n",
      "  ✅ New best AP: 0.1808 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 40  loss 0.0003  val AP 0.1812\n",
      "  ✅ New best AP: 0.1812 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 41  loss 0.0003  val AP 0.1819\n",
      "  ✅ New best AP: 0.1819 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 42  loss 0.0003  val AP 0.1830\n",
      "  ✅ New best AP: 0.1830 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 43  loss 0.0003  val AP 0.1835\n",
      "  ✅ New best AP: 0.1835 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 44  loss 0.0003  val AP 0.1835\n",
      "  ✅ New best AP: 0.1835 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 45  loss 0.0003  val AP 0.1837\n",
      "  ✅ New best AP: 0.1837 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 46  loss 0.0003  val AP 0.1846\n",
      "  ✅ New best AP: 0.1846 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 47  loss 0.0003  val AP 0.1853\n",
      "  ✅ New best AP: 0.1853 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 48  loss 0.0003  val AP 0.1856\n",
      "  ✅ New best AP: 0.1856 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 29] ep 49  loss 0.0003  val AP 0.1855\n",
      "[NR-ER | seed 29] ep 50  loss 0.0003  val AP 0.1859\n",
      "  ✅ New best AP: 0.1859 → v7\\model\\ensembles\\NR-ER\\seed29\\best.pt\n",
      "[NR-ER | seed 47] ep 01  loss 0.0005  val AP 0.1717\n",
      "  ✅ New best AP: 0.1717 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 02  loss 0.0004  val AP 0.1729\n",
      "  ✅ New best AP: 0.1729 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 03  loss 0.0004  val AP 0.1747\n",
      "  ✅ New best AP: 0.1747 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 04  loss 0.0005  val AP 0.1739\n",
      "[NR-ER | seed 47] ep 05  loss 0.0004  val AP 0.1760\n",
      "  ✅ New best AP: 0.1760 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 06  loss 0.0004  val AP 0.1793\n",
      "  ✅ New best AP: 0.1793 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 07  loss 0.0003  val AP 0.1831\n",
      "  ✅ New best AP: 0.1831 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 08  loss 0.0004  val AP 0.1855\n",
      "  ✅ New best AP: 0.1855 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 09  loss 0.0003  val AP 0.1885\n",
      "  ✅ New best AP: 0.1885 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 10  loss 0.0003  val AP 0.1893\n",
      "  ✅ New best AP: 0.1893 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 11  loss 0.0003  val AP 0.1902\n",
      "  ✅ New best AP: 0.1902 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 12  loss 0.0003  val AP 0.1916\n",
      "  ✅ New best AP: 0.1916 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 13  loss 0.0003  val AP 0.1926\n",
      "  ✅ New best AP: 0.1926 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 14  loss 0.0003  val AP 0.1931\n",
      "  ✅ New best AP: 0.1931 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 15  loss 0.0003  val AP 0.1933\n",
      "  ✅ New best AP: 0.1933 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 16  loss 0.0003  val AP 0.1947\n",
      "  ✅ New best AP: 0.1947 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 17  loss 0.0003  val AP 0.1984\n",
      "  ✅ New best AP: 0.1984 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 18  loss 0.0004  val AP 0.1997\n",
      "  ✅ New best AP: 0.1997 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 19  loss 0.0003  val AP 0.2015\n",
      "  ✅ New best AP: 0.2015 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 20  loss 0.0003  val AP 0.2029\n",
      "  ✅ New best AP: 0.2029 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 21  loss 0.0003  val AP 0.2040\n",
      "  ✅ New best AP: 0.2040 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 22  loss 0.0003  val AP 0.2049\n",
      "  ✅ New best AP: 0.2049 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 23  loss 0.0003  val AP 0.2064\n",
      "  ✅ New best AP: 0.2064 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 24  loss 0.0003  val AP 0.2078\n",
      "  ✅ New best AP: 0.2078 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 25  loss 0.0003  val AP 0.2094\n",
      "  ✅ New best AP: 0.2094 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 26  loss 0.0003  val AP 0.2106\n",
      "  ✅ New best AP: 0.2106 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 27  loss 0.0003  val AP 0.2115\n",
      "  ✅ New best AP: 0.2115 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 28  loss 0.0003  val AP 0.2119\n",
      "  ✅ New best AP: 0.2119 → v7\\model\\ensembles\\NR-ER\\seed47\\best.pt\n",
      "[NR-ER | seed 47] ep 29  loss 0.0003  val AP 0.2117\n",
      "[NR-ER | seed 47] ep 30  loss 0.0003  val AP 0.2117\n",
      "[NR-ER | seed 47] ep 31  loss 0.0003  val AP 0.2119\n",
      "[NR-ER | seed 47] ep 32  loss 0.0003  val AP 0.2117\n",
      "[NR-ER | seed 47] ep 33  loss 0.0003  val AP 0.2112\n",
      "[NR-ER | seed 47] ep 34  loss 0.0003  val AP 0.2114\n",
      "[NR-ER | seed 47] ep 35  loss 0.0003  val AP 0.2116\n",
      "[NR-ER | seed 47] ep 36  loss 0.0003  val AP 0.2115\n",
      "[NR-ER | seed 47] ep 37  loss 0.0003  val AP 0.2118\n",
      "[NR-ER | seed 47] ep 38  loss 0.0003  val AP 0.2119\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.2119\n",
      "[NR-ER | seed 61] ep 01  loss 0.0006  val AP 0.1161\n",
      "  ✅ New best AP: 0.1161 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 02  loss 0.0004  val AP 0.1168\n",
      "  ✅ New best AP: 0.1168 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 03  loss 0.0004  val AP 0.1203\n",
      "  ✅ New best AP: 0.1203 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 04  loss 0.0005  val AP 0.1248\n",
      "  ✅ New best AP: 0.1248 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 05  loss 0.0004  val AP 0.1305\n",
      "  ✅ New best AP: 0.1305 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 06  loss 0.0004  val AP 0.1412\n",
      "  ✅ New best AP: 0.1412 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 07  loss 0.0004  val AP 0.1486\n",
      "  ✅ New best AP: 0.1486 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 08  loss 0.0004  val AP 0.1512\n",
      "  ✅ New best AP: 0.1512 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 09  loss 0.0003  val AP 0.1561\n",
      "  ✅ New best AP: 0.1561 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 10  loss 0.0003  val AP 0.1591\n",
      "  ✅ New best AP: 0.1591 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 11  loss 0.0003  val AP 0.1649\n",
      "  ✅ New best AP: 0.1649 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 12  loss 0.0004  val AP 0.1733\n",
      "  ✅ New best AP: 0.1733 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 13  loss 0.0003  val AP 0.1911\n",
      "  ✅ New best AP: 0.1911 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 14  loss 0.0003  val AP 0.1981\n",
      "  ✅ New best AP: 0.1981 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 15  loss 0.0003  val AP 0.1991\n",
      "  ✅ New best AP: 0.1991 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 16  loss 0.0003  val AP 0.1996\n",
      "  ✅ New best AP: 0.1996 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 17  loss 0.0003  val AP 0.2000\n",
      "  ✅ New best AP: 0.2000 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 18  loss 0.0003  val AP 0.2057\n",
      "  ✅ New best AP: 0.2057 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 19  loss 0.0003  val AP 0.2059\n",
      "  ✅ New best AP: 0.2059 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 20  loss 0.0003  val AP 0.2050\n",
      "[NR-ER | seed 61] ep 21  loss 0.0003  val AP 0.2059\n",
      "[NR-ER | seed 61] ep 22  loss 0.0003  val AP 0.2062\n",
      "  ✅ New best AP: 0.2062 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 23  loss 0.0003  val AP 0.2066\n",
      "  ✅ New best AP: 0.2066 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 24  loss 0.0003  val AP 0.2075\n",
      "  ✅ New best AP: 0.2075 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 25  loss 0.0003  val AP 0.2077\n",
      "  ✅ New best AP: 0.2077 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 26  loss 0.0003  val AP 0.2079\n",
      "  ✅ New best AP: 0.2079 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 27  loss 0.0003  val AP 0.2098\n",
      "  ✅ New best AP: 0.2098 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 28  loss 0.0003  val AP 0.2110\n",
      "  ✅ New best AP: 0.2110 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 29  loss 0.0003  val AP 0.2117\n",
      "  ✅ New best AP: 0.2117 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 30  loss 0.0003  val AP 0.2120\n",
      "  ✅ New best AP: 0.2120 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 31  loss 0.0003  val AP 0.2125\n",
      "  ✅ New best AP: 0.2125 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 32  loss 0.0003  val AP 0.2133\n",
      "  ✅ New best AP: 0.2133 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 33  loss 0.0003  val AP 0.2128\n",
      "[NR-ER | seed 61] ep 34  loss 0.0003  val AP 0.2131\n",
      "[NR-ER | seed 61] ep 35  loss 0.0003  val AP 0.2144\n",
      "  ✅ New best AP: 0.2144 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 36  loss 0.0003  val AP 0.2148\n",
      "  ✅ New best AP: 0.2148 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 37  loss 0.0003  val AP 0.2154\n",
      "  ✅ New best AP: 0.2154 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 38  loss 0.0003  val AP 0.2177\n",
      "  ✅ New best AP: 0.2177 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 39  loss 0.0003  val AP 0.2182\n",
      "  ✅ New best AP: 0.2182 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 40  loss 0.0003  val AP 0.2183\n",
      "  ✅ New best AP: 0.2183 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 41  loss 0.0003  val AP 0.2185\n",
      "  ✅ New best AP: 0.2185 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 42  loss 0.0003  val AP 0.2184\n",
      "[NR-ER | seed 61] ep 43  loss 0.0003  val AP 0.2191\n",
      "  ✅ New best AP: 0.2191 → v7\\model\\ensembles\\NR-ER\\seed61\\best.pt\n",
      "[NR-ER | seed 61] ep 44  loss 0.0003  val AP 0.2190\n",
      "[NR-ER | seed 61] ep 45  loss 0.0003  val AP 0.2190\n",
      "[NR-ER | seed 61] ep 46  loss 0.0003  val AP 0.2188\n",
      "[NR-ER | seed 61] ep 47  loss 0.0003  val AP 0.2189\n",
      "[NR-ER | seed 61] ep 48  loss 0.0003  val AP 0.2184\n",
      "[NR-ER | seed 61] ep 49  loss 0.0003  val AP 0.2187\n",
      "[NR-ER | seed 61] ep 50  loss 0.0003  val AP 0.2189\n",
      "[NR-ER | seed 83] ep 01  loss 0.0005  val AP 0.0816\n",
      "  ✅ New best AP: 0.0816 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 02  loss 0.0004  val AP 0.0825\n",
      "  ✅ New best AP: 0.0825 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 03  loss 0.0004  val AP 0.0849\n",
      "  ✅ New best AP: 0.0849 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 04  loss 0.0005  val AP 0.0879\n",
      "  ✅ New best AP: 0.0879 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 05  loss 0.0004  val AP 0.0924\n",
      "  ✅ New best AP: 0.0924 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 06  loss 0.0003  val AP 0.0979\n",
      "  ✅ New best AP: 0.0979 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 07  loss 0.0004  val AP 0.1044\n",
      "  ✅ New best AP: 0.1044 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 08  loss 0.0004  val AP 0.1106\n",
      "  ✅ New best AP: 0.1106 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 09  loss 0.0004  val AP 0.1201\n",
      "  ✅ New best AP: 0.1201 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 10  loss 0.0003  val AP 0.1280\n",
      "  ✅ New best AP: 0.1280 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 11  loss 0.0003  val AP 0.1379\n",
      "  ✅ New best AP: 0.1379 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 12  loss 0.0003  val AP 0.1397\n",
      "  ✅ New best AP: 0.1397 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 13  loss 0.0004  val AP 0.1446\n",
      "  ✅ New best AP: 0.1446 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 14  loss 0.0003  val AP 0.1485\n",
      "  ✅ New best AP: 0.1485 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 15  loss 0.0003  val AP 0.1541\n",
      "  ✅ New best AP: 0.1541 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 16  loss 0.0003  val AP 0.1578\n",
      "  ✅ New best AP: 0.1578 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 17  loss 0.0003  val AP 0.1606\n",
      "  ✅ New best AP: 0.1606 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 18  loss 0.0003  val AP 0.1656\n",
      "  ✅ New best AP: 0.1656 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 19  loss 0.0003  val AP 0.1696\n",
      "  ✅ New best AP: 0.1696 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 20  loss 0.0003  val AP 0.1772\n",
      "  ✅ New best AP: 0.1772 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 21  loss 0.0003  val AP 0.1827\n",
      "  ✅ New best AP: 0.1827 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 22  loss 0.0003  val AP 0.1864\n",
      "  ✅ New best AP: 0.1864 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 23  loss 0.0003  val AP 0.1873\n",
      "  ✅ New best AP: 0.1873 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 24  loss 0.0003  val AP 0.1978\n",
      "  ✅ New best AP: 0.1978 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 25  loss 0.0003  val AP 0.1989\n",
      "  ✅ New best AP: 0.1989 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 26  loss 0.0003  val AP 0.1995\n",
      "  ✅ New best AP: 0.1995 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 27  loss 0.0003  val AP 0.2022\n",
      "  ✅ New best AP: 0.2022 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 28  loss 0.0003  val AP 0.2023\n",
      "  ✅ New best AP: 0.2023 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 29  loss 0.0003  val AP 0.2032\n",
      "  ✅ New best AP: 0.2032 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 30  loss 0.0003  val AP 0.2048\n",
      "  ✅ New best AP: 0.2048 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 31  loss 0.0003  val AP 0.2152\n",
      "  ✅ New best AP: 0.2152 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 32  loss 0.0003  val AP 0.2142\n",
      "[NR-ER | seed 83] ep 33  loss 0.0003  val AP 0.2153\n",
      "  ✅ New best AP: 0.2153 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 34  loss 0.0003  val AP 0.2159\n",
      "  ✅ New best AP: 0.2159 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 35  loss 0.0003  val AP 0.2173\n",
      "  ✅ New best AP: 0.2173 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 36  loss 0.0003  val AP 0.2173\n",
      "[NR-ER | seed 83] ep 37  loss 0.0003  val AP 0.2220\n",
      "  ✅ New best AP: 0.2220 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 38  loss 0.0003  val AP 0.2229\n",
      "  ✅ New best AP: 0.2229 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 39  loss 0.0003  val AP 0.2229\n",
      "  ✅ New best AP: 0.2229 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 40  loss 0.0003  val AP 0.2230\n",
      "  ✅ New best AP: 0.2230 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 41  loss 0.0003  val AP 0.2232\n",
      "  ✅ New best AP: 0.2232 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 42  loss 0.0003  val AP 0.2252\n",
      "  ✅ New best AP: 0.2252 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 43  loss 0.0003  val AP 0.2255\n",
      "  ✅ New best AP: 0.2255 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 44  loss 0.0003  val AP 0.2266\n",
      "  ✅ New best AP: 0.2266 → v7\\model\\ensembles\\NR-ER\\seed83\\best.pt\n",
      "[NR-ER | seed 83] ep 45  loss 0.0003  val AP 0.2261\n",
      "[NR-ER | seed 83] ep 46  loss 0.0003  val AP 0.2262\n",
      "[NR-ER | seed 83] ep 47  loss 0.0003  val AP 0.2262\n",
      "[NR-ER | seed 83] ep 48  loss 0.0003  val AP 0.2257\n",
      "[NR-ER | seed 83] ep 49  loss 0.0003  val AP 0.2257\n",
      "[NR-ER | seed 83] ep 50  loss 0.0003  val AP 0.2257\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-ER-LBD (label 5)\n",
      "==============================\n",
      "[NR-ER-LBD | seed 13] ep 01  loss 0.0008  val AP 0.0263\n",
      "  ✅ New best AP: 0.0263 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 02  loss 0.0006  val AP 0.0267\n",
      "  ✅ New best AP: 0.0267 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 03  loss 0.0005  val AP 0.0273\n",
      "  ✅ New best AP: 0.0273 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 04  loss 0.0005  val AP 0.0287\n",
      "  ✅ New best AP: 0.0287 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 05  loss 0.0007  val AP 0.0306\n",
      "  ✅ New best AP: 0.0306 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 06  loss 0.0005  val AP 0.0332\n",
      "  ✅ New best AP: 0.0332 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 07  loss 0.0007  val AP 0.0367\n",
      "  ✅ New best AP: 0.0367 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 08  loss 0.0005  val AP 0.0411\n",
      "  ✅ New best AP: 0.0411 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 09  loss 0.0005  val AP 0.0461\n",
      "  ✅ New best AP: 0.0461 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 10  loss 0.0005  val AP 0.0559\n",
      "  ✅ New best AP: 0.0559 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 11  loss 0.0007  val AP 0.0639\n",
      "  ✅ New best AP: 0.0639 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 12  loss 0.0005  val AP 0.0741\n",
      "  ✅ New best AP: 0.0741 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 13  loss 0.0005  val AP 0.0870\n",
      "  ✅ New best AP: 0.0870 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 14  loss 0.0005  val AP 0.0921\n",
      "  ✅ New best AP: 0.0921 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 15  loss 0.0005  val AP 0.0925\n",
      "  ✅ New best AP: 0.0925 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 16  loss 0.0005  val AP 0.0942\n",
      "  ✅ New best AP: 0.0942 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 17  loss 0.0005  val AP 0.0955\n",
      "  ✅ New best AP: 0.0955 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 18  loss 0.0005  val AP 0.1248\n",
      "  ✅ New best AP: 0.1248 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 19  loss 0.0005  val AP 0.1294\n",
      "  ✅ New best AP: 0.1294 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 20  loss 0.0005  val AP 0.1291\n",
      "[NR-ER-LBD | seed 13] ep 21  loss 0.0005  val AP 0.1294\n",
      "[NR-ER-LBD | seed 13] ep 22  loss 0.0005  val AP 0.1261\n",
      "[NR-ER-LBD | seed 13] ep 23  loss 0.0004  val AP 0.1269\n",
      "[NR-ER-LBD | seed 13] ep 24  loss 0.0005  val AP 0.1272\n",
      "[NR-ER-LBD | seed 13] ep 25  loss 0.0005  val AP 0.1284\n",
      "[NR-ER-LBD | seed 13] ep 26  loss 0.0004  val AP 0.1283\n",
      "[NR-ER-LBD | seed 13] ep 27  loss 0.0004  val AP 0.1335\n",
      "  ✅ New best AP: 0.1335 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 28  loss 0.0004  val AP 0.1335\n",
      "  ✅ New best AP: 0.1335 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 29  loss 0.0004  val AP 0.1336\n",
      "  ✅ New best AP: 0.1336 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 30  loss 0.0004  val AP 0.1334\n",
      "[NR-ER-LBD | seed 13] ep 31  loss 0.0005  val AP 0.1384\n",
      "  ✅ New best AP: 0.1384 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 32  loss 0.0005  val AP 0.1333\n",
      "[NR-ER-LBD | seed 13] ep 33  loss 0.0005  val AP 0.1329\n",
      "[NR-ER-LBD | seed 13] ep 34  loss 0.0005  val AP 0.1384\n",
      "[NR-ER-LBD | seed 13] ep 35  loss 0.0005  val AP 0.1380\n",
      "[NR-ER-LBD | seed 13] ep 36  loss 0.0004  val AP 0.1380\n",
      "[NR-ER-LBD | seed 13] ep 37  loss 0.0005  val AP 0.1472\n",
      "  ✅ New best AP: 0.1472 → v7\\model\\ensembles\\NR-ER-LBD\\seed13\\best.pt\n",
      "[NR-ER-LBD | seed 13] ep 38  loss 0.0004  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 39  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 40  loss 0.0005  val AP 0.1457\n",
      "[NR-ER-LBD | seed 13] ep 41  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 42  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 43  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 44  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 45  loss 0.0005  val AP 0.1456\n",
      "[NR-ER-LBD | seed 13] ep 46  loss 0.0005  val AP 0.1457\n",
      "[NR-ER-LBD | seed 13] ep 47  loss 0.0005  val AP 0.1457\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.1472\n",
      "[NR-ER-LBD | seed 29] ep 01  loss 0.0009  val AP 0.0403\n",
      "  ✅ New best AP: 0.0403 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 02  loss 0.0006  val AP 0.0481\n",
      "  ✅ New best AP: 0.0481 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 03  loss 0.0007  val AP 0.0491\n",
      "  ✅ New best AP: 0.0491 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 04  loss 0.0008  val AP 0.0502\n",
      "  ✅ New best AP: 0.0502 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 05  loss 0.0007  val AP 0.0617\n",
      "  ✅ New best AP: 0.0617 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 06  loss 0.0006  val AP 0.0635\n",
      "  ✅ New best AP: 0.0635 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 07  loss 0.0006  val AP 0.0645\n",
      "  ✅ New best AP: 0.0645 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 08  loss 0.0007  val AP 0.0940\n",
      "  ✅ New best AP: 0.0940 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 09  loss 0.0006  val AP 0.0956\n",
      "  ✅ New best AP: 0.0956 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 10  loss 0.0006  val AP 0.0988\n",
      "  ✅ New best AP: 0.0988 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 11  loss 0.0005  val AP 0.1014\n",
      "  ✅ New best AP: 0.1014 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 12  loss 0.0005  val AP 0.1031\n",
      "  ✅ New best AP: 0.1031 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 13  loss 0.0005  val AP 0.1047\n",
      "  ✅ New best AP: 0.1047 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 14  loss 0.0006  val AP 0.1059\n",
      "  ✅ New best AP: 0.1059 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 15  loss 0.0005  val AP 0.1078\n",
      "  ✅ New best AP: 0.1078 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 16  loss 0.0005  val AP 0.1090\n",
      "  ✅ New best AP: 0.1090 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 17  loss 0.0005  val AP 0.1098\n",
      "  ✅ New best AP: 0.1098 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 18  loss 0.0006  val AP 0.1094\n",
      "[NR-ER-LBD | seed 29] ep 19  loss 0.0005  val AP 0.1093\n",
      "[NR-ER-LBD | seed 29] ep 20  loss 0.0006  val AP 0.1105\n",
      "  ✅ New best AP: 0.1105 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 21  loss 0.0005  val AP 0.1129\n",
      "  ✅ New best AP: 0.1129 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 22  loss 0.0006  val AP 0.1155\n",
      "  ✅ New best AP: 0.1155 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 23  loss 0.0005  val AP 0.1154\n",
      "[NR-ER-LBD | seed 29] ep 24  loss 0.0005  val AP 0.1158\n",
      "  ✅ New best AP: 0.1158 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 25  loss 0.0005  val AP 0.1165\n",
      "  ✅ New best AP: 0.1165 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 26  loss 0.0005  val AP 0.1170\n",
      "  ✅ New best AP: 0.1170 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 27  loss 0.0005  val AP 0.1171\n",
      "  ✅ New best AP: 0.1171 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 28  loss 0.0005  val AP 0.1173\n",
      "  ✅ New best AP: 0.1173 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 29  loss 0.0005  val AP 0.1187\n",
      "  ✅ New best AP: 0.1187 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 30  loss 0.0005  val AP 0.1197\n",
      "  ✅ New best AP: 0.1197 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 31  loss 0.0004  val AP 0.1197\n",
      "  ✅ New best AP: 0.1197 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 32  loss 0.0005  val AP 0.1219\n",
      "  ✅ New best AP: 0.1219 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 33  loss 0.0005  val AP 0.1220\n",
      "  ✅ New best AP: 0.1220 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 34  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 35  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 36  loss 0.0005  val AP 0.1218\n",
      "[NR-ER-LBD | seed 29] ep 37  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 38  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 39  loss 0.0005  val AP 0.1219\n",
      "[NR-ER-LBD | seed 29] ep 40  loss 0.0005  val AP 0.1217\n",
      "[NR-ER-LBD | seed 29] ep 41  loss 0.0005  val AP 0.1216\n",
      "[NR-ER-LBD | seed 29] ep 42  loss 0.0005  val AP 0.1217\n",
      "[NR-ER-LBD | seed 29] ep 43  loss 0.0005  val AP 0.1232\n",
      "  ✅ New best AP: 0.1232 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 44  loss 0.0005  val AP 0.1233\n",
      "  ✅ New best AP: 0.1233 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 45  loss 0.0005  val AP 0.1233\n",
      "  ✅ New best AP: 0.1233 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 46  loss 0.0005  val AP 0.1233\n",
      "[NR-ER-LBD | seed 29] ep 47  loss 0.0005  val AP 0.1233\n",
      "[NR-ER-LBD | seed 29] ep 48  loss 0.0005  val AP 0.1233\n",
      "[NR-ER-LBD | seed 29] ep 49  loss 0.0005  val AP 0.1252\n",
      "  ✅ New best AP: 0.1252 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 29] ep 50  loss 0.0005  val AP 0.1254\n",
      "  ✅ New best AP: 0.1254 → v7\\model\\ensembles\\NR-ER-LBD\\seed29\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 01  loss 0.0009  val AP 0.0319\n",
      "  ✅ New best AP: 0.0319 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 02  loss 0.0006  val AP 0.0322\n",
      "  ✅ New best AP: 0.0322 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 03  loss 0.0007  val AP 0.0331\n",
      "  ✅ New best AP: 0.0331 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 04  loss 0.0007  val AP 0.0337\n",
      "  ✅ New best AP: 0.0337 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 05  loss 0.0007  val AP 0.0345\n",
      "  ✅ New best AP: 0.0345 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 06  loss 0.0006  val AP 0.0356\n",
      "  ✅ New best AP: 0.0356 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 07  loss 0.0006  val AP 0.0371\n",
      "  ✅ New best AP: 0.0371 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 08  loss 0.0006  val AP 0.0383\n",
      "  ✅ New best AP: 0.0383 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 09  loss 0.0007  val AP 0.0401\n",
      "  ✅ New best AP: 0.0401 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 10  loss 0.0006  val AP 0.0412\n",
      "  ✅ New best AP: 0.0412 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 11  loss 0.0006  val AP 0.0437\n",
      "  ✅ New best AP: 0.0437 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 12  loss 0.0005  val AP 0.0441\n",
      "  ✅ New best AP: 0.0441 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 13  loss 0.0006  val AP 0.0448\n",
      "  ✅ New best AP: 0.0448 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 14  loss 0.0005  val AP 0.0456\n",
      "  ✅ New best AP: 0.0456 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 15  loss 0.0005  val AP 0.0465\n",
      "  ✅ New best AP: 0.0465 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 16  loss 0.0005  val AP 0.0485\n",
      "  ✅ New best AP: 0.0485 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 17  loss 0.0004  val AP 0.0488\n",
      "  ✅ New best AP: 0.0488 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 18  loss 0.0005  val AP 0.0501\n",
      "  ✅ New best AP: 0.0501 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 19  loss 0.0004  val AP 0.0536\n",
      "  ✅ New best AP: 0.0536 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 20  loss 0.0005  val AP 0.0536\n",
      "  ✅ New best AP: 0.0536 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 21  loss 0.0004  val AP 0.0546\n",
      "  ✅ New best AP: 0.0546 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 22  loss 0.0005  val AP 0.0548\n",
      "  ✅ New best AP: 0.0548 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 23  loss 0.0005  val AP 0.0576\n",
      "  ✅ New best AP: 0.0576 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 24  loss 0.0005  val AP 0.0576\n",
      "[NR-ER-LBD | seed 47] ep 25  loss 0.0004  val AP 0.0623\n",
      "  ✅ New best AP: 0.0623 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 26  loss 0.0004  val AP 0.0636\n",
      "  ✅ New best AP: 0.0636 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 27  loss 0.0005  val AP 0.0636\n",
      "  ✅ New best AP: 0.0636 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 28  loss 0.0005  val AP 0.0637\n",
      "  ✅ New best AP: 0.0637 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 29  loss 0.0004  val AP 0.0652\n",
      "  ✅ New best AP: 0.0652 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 30  loss 0.0004  val AP 0.0672\n",
      "  ✅ New best AP: 0.0672 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 31  loss 0.0005  val AP 0.0672\n",
      "[NR-ER-LBD | seed 47] ep 32  loss 0.0005  val AP 0.0671\n",
      "[NR-ER-LBD | seed 47] ep 33  loss 0.0004  val AP 0.0672\n",
      "[NR-ER-LBD | seed 47] ep 34  loss 0.0004  val AP 0.0699\n",
      "  ✅ New best AP: 0.0699 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 35  loss 0.0004  val AP 0.0737\n",
      "  ✅ New best AP: 0.0737 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 36  loss 0.0005  val AP 0.0737\n",
      "  ✅ New best AP: 0.0737 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 37  loss 0.0005  val AP 0.0737\n",
      "  ✅ New best AP: 0.0737 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 38  loss 0.0004  val AP 0.0738\n",
      "  ✅ New best AP: 0.0738 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 39  loss 0.0005  val AP 0.0737\n",
      "[NR-ER-LBD | seed 47] ep 40  loss 0.0005  val AP 0.0737\n",
      "[NR-ER-LBD | seed 47] ep 41  loss 0.0005  val AP 0.0793\n",
      "  ✅ New best AP: 0.0793 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 42  loss 0.0005  val AP 0.0793\n",
      "[NR-ER-LBD | seed 47] ep 43  loss 0.0005  val AP 0.0793\n",
      "[NR-ER-LBD | seed 47] ep 44  loss 0.0005  val AP 0.0793\n",
      "[NR-ER-LBD | seed 47] ep 45  loss 0.0005  val AP 0.0793\n",
      "  ✅ New best AP: 0.0793 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 46  loss 0.0005  val AP 0.0794\n",
      "  ✅ New best AP: 0.0794 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 47  loss 0.0005  val AP 0.0795\n",
      "  ✅ New best AP: 0.0795 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 48  loss 0.0005  val AP 0.0796\n",
      "  ✅ New best AP: 0.0796 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 49  loss 0.0005  val AP 0.0798\n",
      "  ✅ New best AP: 0.0798 → v7\\model\\ensembles\\NR-ER-LBD\\seed47\\best.pt\n",
      "[NR-ER-LBD | seed 47] ep 50  loss 0.0005  val AP 0.0798\n",
      "[NR-ER-LBD | seed 61] ep 01  loss 0.0011  val AP 0.0256\n",
      "  ✅ New best AP: 0.0256 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 02  loss 0.0007  val AP 0.0263\n",
      "  ✅ New best AP: 0.0263 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 03  loss 0.0007  val AP 0.0294\n",
      "  ✅ New best AP: 0.0294 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 04  loss 0.0008  val AP 0.0314\n",
      "  ✅ New best AP: 0.0314 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 05  loss 0.0007  val AP 0.0344\n",
      "  ✅ New best AP: 0.0344 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 06  loss 0.0006  val AP 0.0392\n",
      "  ✅ New best AP: 0.0392 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 07  loss 0.0006  val AP 0.0409\n",
      "  ✅ New best AP: 0.0409 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 08  loss 0.0008  val AP 0.0469\n",
      "  ✅ New best AP: 0.0469 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 09  loss 0.0006  val AP 0.0475\n",
      "  ✅ New best AP: 0.0475 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 10  loss 0.0006  val AP 0.0484\n",
      "  ✅ New best AP: 0.0484 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 11  loss 0.0006  val AP 0.0871\n",
      "  ✅ New best AP: 0.0871 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 12  loss 0.0005  val AP 0.0879\n",
      "  ✅ New best AP: 0.0879 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 13  loss 0.0005  val AP 0.0613\n",
      "[NR-ER-LBD | seed 61] ep 14  loss 0.0005  val AP 0.0633\n",
      "[NR-ER-LBD | seed 61] ep 15  loss 0.0005  val AP 0.0661\n",
      "[NR-ER-LBD | seed 61] ep 16  loss 0.0005  val AP 0.0687\n",
      "[NR-ER-LBD | seed 61] ep 17  loss 0.0005  val AP 0.0711\n",
      "[NR-ER-LBD | seed 61] ep 18  loss 0.0005  val AP 0.0765\n",
      "[NR-ER-LBD | seed 61] ep 19  loss 0.0005  val AP 0.0799\n",
      "[NR-ER-LBD | seed 61] ep 20  loss 0.0005  val AP 0.0845\n",
      "[NR-ER-LBD | seed 61] ep 21  loss 0.0005  val AP 0.0905\n",
      "  ✅ New best AP: 0.0905 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 22  loss 0.0005  val AP 0.0916\n",
      "  ✅ New best AP: 0.0916 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 23  loss 0.0005  val AP 0.1287\n",
      "  ✅ New best AP: 0.1287 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 24  loss 0.0005  val AP 0.1289\n",
      "  ✅ New best AP: 0.1289 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 25  loss 0.0005  val AP 0.1299\n",
      "  ✅ New best AP: 0.1299 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 26  loss 0.0005  val AP 0.1300\n",
      "  ✅ New best AP: 0.1300 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 27  loss 0.0005  val AP 0.1301\n",
      "  ✅ New best AP: 0.1301 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 28  loss 0.0005  val AP 0.1302\n",
      "  ✅ New best AP: 0.1302 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 29  loss 0.0005  val AP 0.1309\n",
      "  ✅ New best AP: 0.1309 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 30  loss 0.0005  val AP 0.1316\n",
      "  ✅ New best AP: 0.1316 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 31  loss 0.0005  val AP 0.1324\n",
      "  ✅ New best AP: 0.1324 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 32  loss 0.0005  val AP 0.1333\n",
      "  ✅ New best AP: 0.1333 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 33  loss 0.0005  val AP 0.1333\n",
      "  ✅ New best AP: 0.1333 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 34  loss 0.0005  val AP 0.1518\n",
      "  ✅ New best AP: 0.1518 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 35  loss 0.0005  val AP 0.1519\n",
      "  ✅ New best AP: 0.1519 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 36  loss 0.0005  val AP 0.1520\n",
      "  ✅ New best AP: 0.1520 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 37  loss 0.0005  val AP 0.1519\n",
      "[NR-ER-LBD | seed 61] ep 38  loss 0.0005  val AP 0.1519\n",
      "[NR-ER-LBD | seed 61] ep 39  loss 0.0005  val AP 0.1519\n",
      "[NR-ER-LBD | seed 61] ep 40  loss 0.0005  val AP 0.1522\n",
      "  ✅ New best AP: 0.1522 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 41  loss 0.0005  val AP 0.1523\n",
      "  ✅ New best AP: 0.1523 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 42  loss 0.0005  val AP 0.1526\n",
      "  ✅ New best AP: 0.1526 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 43  loss 0.0005  val AP 0.1527\n",
      "  ✅ New best AP: 0.1527 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 44  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 45  loss 0.0005  val AP 0.1527\n",
      "  ✅ New best AP: 0.1527 → v7\\model\\ensembles\\NR-ER-LBD\\seed61\\best.pt\n",
      "[NR-ER-LBD | seed 61] ep 46  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 47  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 48  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 49  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 61] ep 50  loss 0.0005  val AP 0.1526\n",
      "[NR-ER-LBD | seed 83] ep 01  loss 0.0008  val AP 0.0249\n",
      "  ✅ New best AP: 0.0249 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 02  loss 0.0006  val AP 0.0253\n",
      "  ✅ New best AP: 0.0253 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 03  loss 0.0007  val AP 0.0262\n",
      "  ✅ New best AP: 0.0262 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 04  loss 0.0006  val AP 0.0270\n",
      "  ✅ New best AP: 0.0270 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 05  loss 0.0007  val AP 0.0285\n",
      "  ✅ New best AP: 0.0285 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 06  loss 0.0006  val AP 0.0302\n",
      "  ✅ New best AP: 0.0302 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 07  loss 0.0007  val AP 0.0328\n",
      "  ✅ New best AP: 0.0328 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 08  loss 0.0005  val AP 0.0346\n",
      "  ✅ New best AP: 0.0346 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 09  loss 0.0007  val AP 0.0365\n",
      "  ✅ New best AP: 0.0365 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 10  loss 0.0005  val AP 0.0395\n",
      "  ✅ New best AP: 0.0395 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 11  loss 0.0006  val AP 0.0431\n",
      "  ✅ New best AP: 0.0431 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 12  loss 0.0005  val AP 0.0489\n",
      "  ✅ New best AP: 0.0489 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 13  loss 0.0006  val AP 0.0510\n",
      "  ✅ New best AP: 0.0510 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 14  loss 0.0004  val AP 0.0588\n",
      "  ✅ New best AP: 0.0588 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 15  loss 0.0005  val AP 0.0654\n",
      "  ✅ New best AP: 0.0654 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 16  loss 0.0005  val AP 0.0760\n",
      "  ✅ New best AP: 0.0760 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 17  loss 0.0005  val AP 0.0759\n",
      "[NR-ER-LBD | seed 83] ep 18  loss 0.0005  val AP 0.1050\n",
      "  ✅ New best AP: 0.1050 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 19  loss 0.0005  val AP 0.1045\n",
      "[NR-ER-LBD | seed 83] ep 20  loss 0.0005  val AP 0.1081\n",
      "  ✅ New best AP: 0.1081 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 21  loss 0.0004  val AP 0.1084\n",
      "  ✅ New best AP: 0.1084 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 22  loss 0.0005  val AP 0.1110\n",
      "  ✅ New best AP: 0.1110 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 23  loss 0.0004  val AP 0.1111\n",
      "  ✅ New best AP: 0.1111 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 24  loss 0.0005  val AP 0.1118\n",
      "  ✅ New best AP: 0.1118 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 25  loss 0.0005  val AP 0.1139\n",
      "  ✅ New best AP: 0.1139 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 26  loss 0.0004  val AP 0.1139\n",
      "  ✅ New best AP: 0.1139 → v7\\model\\ensembles\\NR-ER-LBD\\seed83\\best.pt\n",
      "[NR-ER-LBD | seed 83] ep 27  loss 0.0004  val AP 0.1134\n",
      "[NR-ER-LBD | seed 83] ep 28  loss 0.0005  val AP 0.1131\n",
      "[NR-ER-LBD | seed 83] ep 29  loss 0.0004  val AP 0.1131\n",
      "[NR-ER-LBD | seed 83] ep 30  loss 0.0005  val AP 0.1134\n",
      "[NR-ER-LBD | seed 83] ep 31  loss 0.0005  val AP 0.1134\n",
      "[NR-ER-LBD | seed 83] ep 32  loss 0.0005  val AP 0.1134\n",
      "[NR-ER-LBD | seed 83] ep 33  loss 0.0004  val AP 0.1132\n",
      "[NR-ER-LBD | seed 83] ep 34  loss 0.0004  val AP 0.1132\n",
      "[NR-ER-LBD | seed 83] ep 35  loss 0.0004  val AP 0.1136\n",
      "[NR-ER-LBD | seed 83] ep 36  loss 0.0005  val AP 0.1136\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.1139\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: NR-PPAR-gamma (label 6)\n",
      "==============================\n",
      "[NR-PPAR-gamma | seed 13] ep 01  loss 0.0016  val AP 0.0408\n",
      "  ✅ New best AP: 0.0408 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 02  loss 0.0012  val AP 0.0413\n",
      "  ✅ New best AP: 0.0413 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 03  loss 0.0010  val AP 0.0418\n",
      "  ✅ New best AP: 0.0418 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 04  loss 0.0014  val AP 0.0425\n",
      "  ✅ New best AP: 0.0425 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 05  loss 0.0010  val AP 0.0436\n",
      "  ✅ New best AP: 0.0436 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 06  loss 0.0015  val AP 0.0447\n",
      "  ✅ New best AP: 0.0447 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 07  loss 0.0010  val AP 0.0459\n",
      "  ✅ New best AP: 0.0459 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 08  loss 0.0013  val AP 0.0471\n",
      "  ✅ New best AP: 0.0471 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 09  loss 0.0010  val AP 0.0482\n",
      "  ✅ New best AP: 0.0482 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 10  loss 0.0012  val AP 0.0494\n",
      "  ✅ New best AP: 0.0494 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 11  loss 0.0009  val AP 0.0503\n",
      "  ✅ New best AP: 0.0503 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 12  loss 0.0009  val AP 0.0513\n",
      "  ✅ New best AP: 0.0513 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 13  loss 0.0011  val AP 0.0524\n",
      "  ✅ New best AP: 0.0524 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 14  loss 0.0009  val AP 0.0535\n",
      "  ✅ New best AP: 0.0535 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 15  loss 0.0017  val AP 0.0542\n",
      "  ✅ New best AP: 0.0542 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 16  loss 0.0010  val AP 0.0552\n",
      "  ✅ New best AP: 0.0552 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 17  loss 0.0009  val AP 0.0559\n",
      "  ✅ New best AP: 0.0559 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 18  loss 0.0009  val AP 0.0565\n",
      "  ✅ New best AP: 0.0565 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 19  loss 0.0009  val AP 0.0571\n",
      "  ✅ New best AP: 0.0571 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 20  loss 0.0013  val AP 0.0574\n",
      "  ✅ New best AP: 0.0574 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 21  loss 0.0009  val AP 0.0580\n",
      "  ✅ New best AP: 0.0580 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 22  loss 0.0011  val AP 0.0589\n",
      "  ✅ New best AP: 0.0589 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 23  loss 0.0008  val AP 0.0595\n",
      "  ✅ New best AP: 0.0595 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 24  loss 0.0009  val AP 0.0602\n",
      "  ✅ New best AP: 0.0602 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 25  loss 0.0008  val AP 0.0611\n",
      "  ✅ New best AP: 0.0611 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 26  loss 0.0010  val AP 0.0616\n",
      "  ✅ New best AP: 0.0616 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 27  loss 0.0009  val AP 0.0624\n",
      "  ✅ New best AP: 0.0624 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 28  loss 0.0009  val AP 0.0630\n",
      "  ✅ New best AP: 0.0630 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 29  loss 0.0009  val AP 0.0639\n",
      "  ✅ New best AP: 0.0639 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 30  loss 0.0009  val AP 0.0643\n",
      "  ✅ New best AP: 0.0643 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 31  loss 0.0009  val AP 0.0650\n",
      "  ✅ New best AP: 0.0650 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 32  loss 0.0009  val AP 0.0655\n",
      "  ✅ New best AP: 0.0655 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 33  loss 0.0010  val AP 0.0661\n",
      "  ✅ New best AP: 0.0661 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 34  loss 0.0009  val AP 0.0669\n",
      "  ✅ New best AP: 0.0669 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 35  loss 0.0009  val AP 0.0676\n",
      "  ✅ New best AP: 0.0676 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 36  loss 0.0009  val AP 0.0682\n",
      "  ✅ New best AP: 0.0682 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 37  loss 0.0009  val AP 0.0685\n",
      "  ✅ New best AP: 0.0685 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 38  loss 0.0009  val AP 0.0687\n",
      "  ✅ New best AP: 0.0687 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 39  loss 0.0009  val AP 0.0692\n",
      "  ✅ New best AP: 0.0692 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 40  loss 0.0009  val AP 0.0695\n",
      "  ✅ New best AP: 0.0695 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 41  loss 0.0009  val AP 0.0700\n",
      "  ✅ New best AP: 0.0700 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 42  loss 0.0009  val AP 0.0703\n",
      "  ✅ New best AP: 0.0703 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 43  loss 0.0009  val AP 0.0706\n",
      "  ✅ New best AP: 0.0706 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 44  loss 0.0009  val AP 0.0708\n",
      "  ✅ New best AP: 0.0708 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 45  loss 0.0009  val AP 0.0712\n",
      "  ✅ New best AP: 0.0712 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 46  loss 0.0009  val AP 0.0714\n",
      "  ✅ New best AP: 0.0714 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 47  loss 0.0009  val AP 0.0715\n",
      "  ✅ New best AP: 0.0715 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 48  loss 0.0009  val AP 0.0717\n",
      "  ✅ New best AP: 0.0717 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 49  loss 0.0009  val AP 0.0718\n",
      "  ✅ New best AP: 0.0718 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 13] ep 50  loss 0.0009  val AP 0.0721\n",
      "  ✅ New best AP: 0.0721 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed13\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 01  loss 0.0019  val AP 0.0565\n",
      "  ✅ New best AP: 0.0565 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 02  loss 0.0013  val AP 0.0572\n",
      "  ✅ New best AP: 0.0572 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 03  loss 0.0014  val AP 0.0591\n",
      "  ✅ New best AP: 0.0591 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 04  loss 0.0016  val AP 0.0613\n",
      "  ✅ New best AP: 0.0613 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 05  loss 0.0013  val AP 0.0702\n",
      "  ✅ New best AP: 0.0702 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 06  loss 0.0011  val AP 0.0873\n",
      "  ✅ New best AP: 0.0873 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 07  loss 0.0012  val AP 0.0876\n",
      "  ✅ New best AP: 0.0876 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 08  loss 0.0013  val AP 0.0881\n",
      "  ✅ New best AP: 0.0881 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 09  loss 0.0010  val AP 0.0884\n",
      "  ✅ New best AP: 0.0884 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 10  loss 0.0015  val AP 0.0889\n",
      "  ✅ New best AP: 0.0889 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 11  loss 0.0009  val AP 0.0892\n",
      "  ✅ New best AP: 0.0892 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 12  loss 0.0012  val AP 0.0898\n",
      "  ✅ New best AP: 0.0898 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 13  loss 0.0010  val AP 0.0907\n",
      "  ✅ New best AP: 0.0907 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 14  loss 0.0009  val AP 0.0918\n",
      "  ✅ New best AP: 0.0918 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed29\\best.pt\n",
      "[NR-PPAR-gamma | seed 29] ep 15  loss 0.0011  val AP 0.0690\n",
      "[NR-PPAR-gamma | seed 29] ep 16  loss 0.0009  val AP 0.0668\n",
      "[NR-PPAR-gamma | seed 29] ep 17  loss 0.0010  val AP 0.0658\n",
      "[NR-PPAR-gamma | seed 29] ep 18  loss 0.0011  val AP 0.0656\n",
      "[NR-PPAR-gamma | seed 29] ep 19  loss 0.0010  val AP 0.0645\n",
      "[NR-PPAR-gamma | seed 29] ep 20  loss 0.0009  val AP 0.0645\n",
      "[NR-PPAR-gamma | seed 29] ep 21  loss 0.0009  val AP 0.0645\n",
      "[NR-PPAR-gamma | seed 29] ep 22  loss 0.0011  val AP 0.0655\n",
      "[NR-PPAR-gamma | seed 29] ep 23  loss 0.0010  val AP 0.0666\n",
      "[NR-PPAR-gamma | seed 29] ep 24  loss 0.0009  val AP 0.0672\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.0918\n",
      "[NR-PPAR-gamma | seed 47] ep 01  loss 0.0018  val AP 0.0351\n",
      "  ✅ New best AP: 0.0351 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 02  loss 0.0012  val AP 0.0354\n",
      "  ✅ New best AP: 0.0354 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 03  loss 0.0013  val AP 0.0360\n",
      "  ✅ New best AP: 0.0360 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 04  loss 0.0015  val AP 0.0369\n",
      "  ✅ New best AP: 0.0369 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 05  loss 0.0012  val AP 0.0382\n",
      "  ✅ New best AP: 0.0382 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 06  loss 0.0012  val AP 0.0399\n",
      "  ✅ New best AP: 0.0399 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 07  loss 0.0011  val AP 0.0414\n",
      "  ✅ New best AP: 0.0414 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 08  loss 0.0014  val AP 0.0428\n",
      "  ✅ New best AP: 0.0428 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 09  loss 0.0011  val AP 0.0441\n",
      "  ✅ New best AP: 0.0441 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 10  loss 0.0013  val AP 0.0453\n",
      "  ✅ New best AP: 0.0453 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 11  loss 0.0010  val AP 0.0462\n",
      "  ✅ New best AP: 0.0462 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 12  loss 0.0009  val AP 0.0472\n",
      "  ✅ New best AP: 0.0472 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 13  loss 0.0010  val AP 0.0480\n",
      "  ✅ New best AP: 0.0480 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 14  loss 0.0011  val AP 0.0488\n",
      "  ✅ New best AP: 0.0488 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 15  loss 0.0010  val AP 0.0496\n",
      "  ✅ New best AP: 0.0496 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 16  loss 0.0010  val AP 0.0505\n",
      "  ✅ New best AP: 0.0505 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 17  loss 0.0009  val AP 0.0511\n",
      "  ✅ New best AP: 0.0511 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 18  loss 0.0009  val AP 0.0519\n",
      "  ✅ New best AP: 0.0519 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 19  loss 0.0008  val AP 0.0524\n",
      "  ✅ New best AP: 0.0524 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 20  loss 0.0011  val AP 0.0530\n",
      "  ✅ New best AP: 0.0530 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 21  loss 0.0010  val AP 0.0535\n",
      "  ✅ New best AP: 0.0535 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 22  loss 0.0010  val AP 0.0541\n",
      "  ✅ New best AP: 0.0541 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 23  loss 0.0009  val AP 0.0546\n",
      "  ✅ New best AP: 0.0546 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 24  loss 0.0009  val AP 0.0550\n",
      "  ✅ New best AP: 0.0550 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 25  loss 0.0009  val AP 0.0556\n",
      "  ✅ New best AP: 0.0556 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 26  loss 0.0009  val AP 0.0560\n",
      "  ✅ New best AP: 0.0560 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 27  loss 0.0009  val AP 0.0564\n",
      "  ✅ New best AP: 0.0564 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 28  loss 0.0009  val AP 0.0570\n",
      "  ✅ New best AP: 0.0570 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 29  loss 0.0008  val AP 0.0577\n",
      "  ✅ New best AP: 0.0577 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 30  loss 0.0010  val AP 0.0582\n",
      "  ✅ New best AP: 0.0582 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 31  loss 0.0008  val AP 0.0587\n",
      "  ✅ New best AP: 0.0587 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 32  loss 0.0009  val AP 0.0590\n",
      "  ✅ New best AP: 0.0590 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 33  loss 0.0009  val AP 0.0597\n",
      "  ✅ New best AP: 0.0597 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 34  loss 0.0009  val AP 0.0601\n",
      "  ✅ New best AP: 0.0601 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 35  loss 0.0009  val AP 0.0604\n",
      "  ✅ New best AP: 0.0604 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 36  loss 0.0008  val AP 0.0609\n",
      "  ✅ New best AP: 0.0609 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 37  loss 0.0009  val AP 0.0612\n",
      "  ✅ New best AP: 0.0612 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 38  loss 0.0009  val AP 0.0616\n",
      "  ✅ New best AP: 0.0616 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 39  loss 0.0009  val AP 0.0620\n",
      "  ✅ New best AP: 0.0620 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 40  loss 0.0009  val AP 0.0622\n",
      "  ✅ New best AP: 0.0622 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 41  loss 0.0010  val AP 0.0626\n",
      "  ✅ New best AP: 0.0626 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 42  loss 0.0009  val AP 0.0630\n",
      "  ✅ New best AP: 0.0630 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 43  loss 0.0009  val AP 0.0632\n",
      "  ✅ New best AP: 0.0632 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 44  loss 0.0010  val AP 0.0634\n",
      "  ✅ New best AP: 0.0634 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 45  loss 0.0009  val AP 0.0637\n",
      "  ✅ New best AP: 0.0637 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 46  loss 0.0010  val AP 0.0640\n",
      "  ✅ New best AP: 0.0640 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 47  loss 0.0010  val AP 0.0643\n",
      "  ✅ New best AP: 0.0643 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 48  loss 0.0009  val AP 0.0643\n",
      "  ✅ New best AP: 0.0643 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 49  loss 0.0010  val AP 0.0645\n",
      "  ✅ New best AP: 0.0645 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 47] ep 50  loss 0.0009  val AP 0.0648\n",
      "  ✅ New best AP: 0.0648 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed47\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 01  loss 0.0022  val AP 0.0447\n",
      "  ✅ New best AP: 0.0447 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 02  loss 0.0013  val AP 0.0457\n",
      "  ✅ New best AP: 0.0457 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 03  loss 0.0014  val AP 0.0449\n",
      "[NR-PPAR-gamma | seed 61] ep 04  loss 0.0016  val AP 0.0443\n",
      "[NR-PPAR-gamma | seed 61] ep 05  loss 0.0013  val AP 0.0443\n",
      "[NR-PPAR-gamma | seed 61] ep 06  loss 0.0013  val AP 0.0450\n",
      "[NR-PPAR-gamma | seed 61] ep 07  loss 0.0013  val AP 0.0467\n",
      "  ✅ New best AP: 0.0467 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 08  loss 0.0014  val AP 0.0477\n",
      "  ✅ New best AP: 0.0477 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 09  loss 0.0015  val AP 0.0490\n",
      "  ✅ New best AP: 0.0490 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 10  loss 0.0011  val AP 0.0505\n",
      "  ✅ New best AP: 0.0505 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 11  loss 0.0011  val AP 0.0515\n",
      "  ✅ New best AP: 0.0515 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 12  loss 0.0010  val AP 0.0522\n",
      "  ✅ New best AP: 0.0522 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 13  loss 0.0011  val AP 0.0527\n",
      "  ✅ New best AP: 0.0527 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 14  loss 0.0012  val AP 0.0535\n",
      "  ✅ New best AP: 0.0535 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 15  loss 0.0009  val AP 0.0546\n",
      "  ✅ New best AP: 0.0546 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 16  loss 0.0014  val AP 0.0549\n",
      "  ✅ New best AP: 0.0549 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 17  loss 0.0009  val AP 0.0555\n",
      "  ✅ New best AP: 0.0555 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 18  loss 0.0010  val AP 0.0564\n",
      "  ✅ New best AP: 0.0564 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 19  loss 0.0009  val AP 0.0568\n",
      "  ✅ New best AP: 0.0568 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 20  loss 0.0009  val AP 0.0573\n",
      "  ✅ New best AP: 0.0573 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 21  loss 0.0010  val AP 0.0579\n",
      "  ✅ New best AP: 0.0579 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 22  loss 0.0009  val AP 0.0587\n",
      "  ✅ New best AP: 0.0587 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 23  loss 0.0010  val AP 0.0593\n",
      "  ✅ New best AP: 0.0593 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 24  loss 0.0009  val AP 0.0597\n",
      "  ✅ New best AP: 0.0597 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 25  loss 0.0010  val AP 0.0601\n",
      "  ✅ New best AP: 0.0601 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 26  loss 0.0009  val AP 0.0610\n",
      "  ✅ New best AP: 0.0610 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 27  loss 0.0010  val AP 0.0616\n",
      "  ✅ New best AP: 0.0616 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 28  loss 0.0009  val AP 0.0622\n",
      "  ✅ New best AP: 0.0622 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 29  loss 0.0010  val AP 0.0629\n",
      "  ✅ New best AP: 0.0629 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 30  loss 0.0009  val AP 0.0634\n",
      "  ✅ New best AP: 0.0634 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 31  loss 0.0009  val AP 0.0639\n",
      "  ✅ New best AP: 0.0639 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 32  loss 0.0009  val AP 0.0643\n",
      "  ✅ New best AP: 0.0643 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 33  loss 0.0009  val AP 0.0647\n",
      "  ✅ New best AP: 0.0647 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 34  loss 0.0009  val AP 0.0650\n",
      "  ✅ New best AP: 0.0650 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 35  loss 0.0009  val AP 0.0652\n",
      "  ✅ New best AP: 0.0652 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 36  loss 0.0009  val AP 0.0658\n",
      "  ✅ New best AP: 0.0658 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 37  loss 0.0009  val AP 0.0661\n",
      "  ✅ New best AP: 0.0661 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 38  loss 0.0009  val AP 0.0665\n",
      "  ✅ New best AP: 0.0665 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 39  loss 0.0009  val AP 0.0669\n",
      "  ✅ New best AP: 0.0669 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 40  loss 0.0009  val AP 0.0671\n",
      "  ✅ New best AP: 0.0671 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 41  loss 0.0009  val AP 0.0675\n",
      "  ✅ New best AP: 0.0675 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 42  loss 0.0009  val AP 0.0677\n",
      "  ✅ New best AP: 0.0677 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 43  loss 0.0009  val AP 0.0679\n",
      "  ✅ New best AP: 0.0679 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 44  loss 0.0009  val AP 0.0682\n",
      "  ✅ New best AP: 0.0682 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 45  loss 0.0009  val AP 0.0683\n",
      "  ✅ New best AP: 0.0683 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 46  loss 0.0009  val AP 0.0686\n",
      "  ✅ New best AP: 0.0686 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 47  loss 0.0010  val AP 0.0688\n",
      "  ✅ New best AP: 0.0688 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 48  loss 0.0009  val AP 0.0688\n",
      "  ✅ New best AP: 0.0688 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 49  loss 0.0009  val AP 0.0689\n",
      "  ✅ New best AP: 0.0689 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 61] ep 50  loss 0.0009  val AP 0.0691\n",
      "  ✅ New best AP: 0.0691 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed61\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 01  loss 0.0018  val AP 0.0411\n",
      "  ✅ New best AP: 0.0411 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 02  loss 0.0012  val AP 0.0416\n",
      "  ✅ New best AP: 0.0416 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 03  loss 0.0013  val AP 0.0421\n",
      "  ✅ New best AP: 0.0421 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 04  loss 0.0014  val AP 0.0426\n",
      "  ✅ New best AP: 0.0426 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 05  loss 0.0014  val AP 0.0433\n",
      "  ✅ New best AP: 0.0433 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 06  loss 0.0011  val AP 0.0444\n",
      "  ✅ New best AP: 0.0444 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 07  loss 0.0014  val AP 0.0451\n",
      "  ✅ New best AP: 0.0451 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 08  loss 0.0010  val AP 0.0458\n",
      "  ✅ New best AP: 0.0458 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 09  loss 0.0016  val AP 0.0463\n",
      "  ✅ New best AP: 0.0463 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 10  loss 0.0010  val AP 0.0467\n",
      "  ✅ New best AP: 0.0467 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 11  loss 0.0012  val AP 0.0469\n",
      "  ✅ New best AP: 0.0469 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 12  loss 0.0009  val AP 0.0472\n",
      "  ✅ New best AP: 0.0472 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 13  loss 0.0010  val AP 0.0476\n",
      "  ✅ New best AP: 0.0476 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 14  loss 0.0009  val AP 0.0481\n",
      "  ✅ New best AP: 0.0481 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 15  loss 0.0010  val AP 0.0485\n",
      "  ✅ New best AP: 0.0485 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 16  loss 0.0009  val AP 0.0491\n",
      "  ✅ New best AP: 0.0491 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 17  loss 0.0012  val AP 0.0497\n",
      "  ✅ New best AP: 0.0497 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 18  loss 0.0009  val AP 0.0504\n",
      "  ✅ New best AP: 0.0504 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 19  loss 0.0012  val AP 0.0509\n",
      "  ✅ New best AP: 0.0509 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 20  loss 0.0009  val AP 0.0515\n",
      "  ✅ New best AP: 0.0515 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 21  loss 0.0009  val AP 0.0522\n",
      "  ✅ New best AP: 0.0522 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 22  loss 0.0010  val AP 0.0528\n",
      "  ✅ New best AP: 0.0528 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 23  loss 0.0009  val AP 0.0535\n",
      "  ✅ New best AP: 0.0535 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 24  loss 0.0009  val AP 0.0542\n",
      "  ✅ New best AP: 0.0542 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 25  loss 0.0009  val AP 0.0552\n",
      "  ✅ New best AP: 0.0552 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 26  loss 0.0009  val AP 0.0561\n",
      "  ✅ New best AP: 0.0561 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 27  loss 0.0011  val AP 0.0565\n",
      "  ✅ New best AP: 0.0565 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 28  loss 0.0009  val AP 0.0573\n",
      "  ✅ New best AP: 0.0573 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 29  loss 0.0009  val AP 0.0581\n",
      "  ✅ New best AP: 0.0581 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 30  loss 0.0009  val AP 0.0584\n",
      "  ✅ New best AP: 0.0584 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 31  loss 0.0009  val AP 0.0590\n",
      "  ✅ New best AP: 0.0590 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 32  loss 0.0009  val AP 0.0594\n",
      "  ✅ New best AP: 0.0594 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 33  loss 0.0009  val AP 0.0600\n",
      "  ✅ New best AP: 0.0600 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 34  loss 0.0010  val AP 0.0606\n",
      "  ✅ New best AP: 0.0606 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 35  loss 0.0009  val AP 0.0609\n",
      "  ✅ New best AP: 0.0609 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 36  loss 0.0009  val AP 0.0612\n",
      "  ✅ New best AP: 0.0612 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 37  loss 0.0009  val AP 0.0618\n",
      "  ✅ New best AP: 0.0618 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 38  loss 0.0009  val AP 0.0623\n",
      "  ✅ New best AP: 0.0623 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 39  loss 0.0009  val AP 0.0627\n",
      "  ✅ New best AP: 0.0627 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 40  loss 0.0009  val AP 0.0630\n",
      "  ✅ New best AP: 0.0630 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 41  loss 0.0009  val AP 0.0634\n",
      "  ✅ New best AP: 0.0634 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 42  loss 0.0009  val AP 0.0637\n",
      "  ✅ New best AP: 0.0637 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 43  loss 0.0009  val AP 0.0641\n",
      "  ✅ New best AP: 0.0641 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 44  loss 0.0009  val AP 0.0645\n",
      "  ✅ New best AP: 0.0645 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 45  loss 0.0009  val AP 0.0650\n",
      "  ✅ New best AP: 0.0650 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 46  loss 0.0009  val AP 0.0656\n",
      "  ✅ New best AP: 0.0656 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 47  loss 0.0010  val AP 0.0661\n",
      "  ✅ New best AP: 0.0661 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 48  loss 0.0009  val AP 0.0664\n",
      "  ✅ New best AP: 0.0664 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 49  loss 0.0009  val AP 0.0667\n",
      "  ✅ New best AP: 0.0667 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "[NR-PPAR-gamma | seed 83] ep 50  loss 0.0009  val AP 0.0671\n",
      "  ✅ New best AP: 0.0671 → v7\\model\\ensembles\\NR-PPAR-gamma\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-ARE (label 7)\n",
      "==============================\n",
      "[SR-ARE | seed 13] ep 01  loss 0.0004  val AP 0.1361\n",
      "  ✅ New best AP: 0.1361 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 02  loss 0.0003  val AP 0.1379\n",
      "  ✅ New best AP: 0.1379 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 03  loss 0.0003  val AP 0.1412\n",
      "  ✅ New best AP: 0.1412 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 04  loss 0.0004  val AP 0.1457\n",
      "  ✅ New best AP: 0.1457 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 05  loss 0.0003  val AP 0.1518\n",
      "  ✅ New best AP: 0.1518 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 06  loss 0.0003  val AP 0.1622\n",
      "  ✅ New best AP: 0.1622 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 07  loss 0.0003  val AP 0.1751\n",
      "  ✅ New best AP: 0.1751 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 08  loss 0.0003  val AP 0.1888\n",
      "  ✅ New best AP: 0.1888 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 09  loss 0.0003  val AP 0.2044\n",
      "  ✅ New best AP: 0.2044 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 10  loss 0.0003  val AP 0.2173\n",
      "  ✅ New best AP: 0.2173 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 11  loss 0.0003  val AP 0.2305\n",
      "  ✅ New best AP: 0.2305 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 12  loss 0.0003  val AP 0.2426\n",
      "  ✅ New best AP: 0.2426 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 13  loss 0.0003  val AP 0.2596\n",
      "  ✅ New best AP: 0.2596 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 14  loss 0.0003  val AP 0.2707\n",
      "  ✅ New best AP: 0.2707 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 15  loss 0.0003  val AP 0.2792\n",
      "  ✅ New best AP: 0.2792 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 16  loss 0.0003  val AP 0.2868\n",
      "  ✅ New best AP: 0.2868 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 17  loss 0.0003  val AP 0.2921\n",
      "  ✅ New best AP: 0.2921 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 18  loss 0.0003  val AP 0.2985\n",
      "  ✅ New best AP: 0.2985 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 19  loss 0.0002  val AP 0.3026\n",
      "  ✅ New best AP: 0.3026 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 20  loss 0.0002  val AP 0.3044\n",
      "  ✅ New best AP: 0.3044 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 21  loss 0.0003  val AP 0.3095\n",
      "  ✅ New best AP: 0.3095 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 22  loss 0.0003  val AP 0.3127\n",
      "  ✅ New best AP: 0.3127 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 23  loss 0.0002  val AP 0.3144\n",
      "  ✅ New best AP: 0.3144 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 24  loss 0.0003  val AP 0.3162\n",
      "  ✅ New best AP: 0.3162 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 25  loss 0.0002  val AP 0.3172\n",
      "  ✅ New best AP: 0.3172 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 26  loss 0.0003  val AP 0.3176\n",
      "  ✅ New best AP: 0.3176 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 27  loss 0.0002  val AP 0.3187\n",
      "  ✅ New best AP: 0.3187 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 28  loss 0.0003  val AP 0.3217\n",
      "  ✅ New best AP: 0.3217 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 29  loss 0.0002  val AP 0.3228\n",
      "  ✅ New best AP: 0.3228 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 30  loss 0.0003  val AP 0.3237\n",
      "  ✅ New best AP: 0.3237 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 31  loss 0.0003  val AP 0.3238\n",
      "  ✅ New best AP: 0.3238 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 32  loss 0.0003  val AP 0.3245\n",
      "  ✅ New best AP: 0.3245 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 33  loss 0.0002  val AP 0.3269\n",
      "  ✅ New best AP: 0.3269 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 34  loss 0.0002  val AP 0.3270\n",
      "  ✅ New best AP: 0.3270 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 35  loss 0.0002  val AP 0.3279\n",
      "  ✅ New best AP: 0.3279 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 36  loss 0.0003  val AP 0.3287\n",
      "  ✅ New best AP: 0.3287 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 37  loss 0.0003  val AP 0.3308\n",
      "  ✅ New best AP: 0.3308 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 38  loss 0.0003  val AP 0.3316\n",
      "  ✅ New best AP: 0.3316 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 39  loss 0.0002  val AP 0.3327\n",
      "  ✅ New best AP: 0.3327 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 40  loss 0.0003  val AP 0.3326\n",
      "[SR-ARE | seed 13] ep 41  loss 0.0003  val AP 0.3330\n",
      "  ✅ New best AP: 0.3330 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 42  loss 0.0003  val AP 0.3333\n",
      "  ✅ New best AP: 0.3333 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 43  loss 0.0002  val AP 0.3336\n",
      "  ✅ New best AP: 0.3336 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 44  loss 0.0003  val AP 0.3338\n",
      "  ✅ New best AP: 0.3338 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 45  loss 0.0003  val AP 0.3346\n",
      "  ✅ New best AP: 0.3346 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 46  loss 0.0003  val AP 0.3345\n",
      "[SR-ARE | seed 13] ep 47  loss 0.0003  val AP 0.3346\n",
      "  ✅ New best AP: 0.3346 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 48  loss 0.0003  val AP 0.3347\n",
      "  ✅ New best AP: 0.3347 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 49  loss 0.0003  val AP 0.3350\n",
      "  ✅ New best AP: 0.3350 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 13] ep 50  loss 0.0003  val AP 0.3360\n",
      "  ✅ New best AP: 0.3360 → v7\\model\\ensembles\\SR-ARE\\seed13\\best.pt\n",
      "[SR-ARE | seed 29] ep 01  loss 0.0005  val AP 0.1791\n",
      "  ✅ New best AP: 0.1791 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 02  loss 0.0003  val AP 0.1833\n",
      "  ✅ New best AP: 0.1833 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 03  loss 0.0004  val AP 0.1912\n",
      "  ✅ New best AP: 0.1912 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 04  loss 0.0004  val AP 0.2007\n",
      "  ✅ New best AP: 0.2007 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 05  loss 0.0003  val AP 0.2102\n",
      "  ✅ New best AP: 0.2102 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 06  loss 0.0003  val AP 0.2156\n",
      "  ✅ New best AP: 0.2156 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 07  loss 0.0004  val AP 0.2281\n",
      "  ✅ New best AP: 0.2281 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 08  loss 0.0003  val AP 0.2338\n",
      "  ✅ New best AP: 0.2338 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 09  loss 0.0003  val AP 0.2398\n",
      "  ✅ New best AP: 0.2398 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 10  loss 0.0003  val AP 0.2422\n",
      "  ✅ New best AP: 0.2422 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 11  loss 0.0003  val AP 0.2450\n",
      "  ✅ New best AP: 0.2450 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 12  loss 0.0003  val AP 0.2471\n",
      "  ✅ New best AP: 0.2471 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 13  loss 0.0003  val AP 0.2514\n",
      "  ✅ New best AP: 0.2514 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 14  loss 0.0003  val AP 0.2528\n",
      "  ✅ New best AP: 0.2528 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 15  loss 0.0003  val AP 0.2540\n",
      "  ✅ New best AP: 0.2540 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 16  loss 0.0003  val AP 0.2561\n",
      "  ✅ New best AP: 0.2561 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 17  loss 0.0003  val AP 0.2593\n",
      "  ✅ New best AP: 0.2593 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 18  loss 0.0003  val AP 0.2617\n",
      "  ✅ New best AP: 0.2617 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 19  loss 0.0003  val AP 0.2629\n",
      "  ✅ New best AP: 0.2629 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 20  loss 0.0003  val AP 0.2640\n",
      "  ✅ New best AP: 0.2640 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 21  loss 0.0003  val AP 0.2652\n",
      "  ✅ New best AP: 0.2652 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 22  loss 0.0003  val AP 0.2671\n",
      "  ✅ New best AP: 0.2671 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 23  loss 0.0003  val AP 0.2685\n",
      "  ✅ New best AP: 0.2685 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 24  loss 0.0003  val AP 0.2698\n",
      "  ✅ New best AP: 0.2698 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 25  loss 0.0002  val AP 0.2713\n",
      "  ✅ New best AP: 0.2713 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 26  loss 0.0003  val AP 0.2730\n",
      "  ✅ New best AP: 0.2730 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 27  loss 0.0003  val AP 0.2749\n",
      "  ✅ New best AP: 0.2749 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 28  loss 0.0003  val AP 0.2763\n",
      "  ✅ New best AP: 0.2763 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 29  loss 0.0003  val AP 0.2788\n",
      "  ✅ New best AP: 0.2788 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 30  loss 0.0003  val AP 0.2802\n",
      "  ✅ New best AP: 0.2802 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 31  loss 0.0003  val AP 0.2815\n",
      "  ✅ New best AP: 0.2815 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 32  loss 0.0003  val AP 0.2835\n",
      "  ✅ New best AP: 0.2835 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 33  loss 0.0003  val AP 0.2867\n",
      "  ✅ New best AP: 0.2867 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 34  loss 0.0003  val AP 0.2880\n",
      "  ✅ New best AP: 0.2880 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 35  loss 0.0003  val AP 0.2894\n",
      "  ✅ New best AP: 0.2894 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 36  loss 0.0003  val AP 0.2916\n",
      "  ✅ New best AP: 0.2916 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 37  loss 0.0003  val AP 0.2929\n",
      "  ✅ New best AP: 0.2929 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 38  loss 0.0003  val AP 0.2948\n",
      "  ✅ New best AP: 0.2948 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 39  loss 0.0003  val AP 0.2958\n",
      "  ✅ New best AP: 0.2958 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 40  loss 0.0003  val AP 0.2966\n",
      "  ✅ New best AP: 0.2966 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 41  loss 0.0003  val AP 0.2986\n",
      "  ✅ New best AP: 0.2986 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 42  loss 0.0003  val AP 0.2990\n",
      "  ✅ New best AP: 0.2990 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 43  loss 0.0003  val AP 0.3004\n",
      "  ✅ New best AP: 0.3004 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 44  loss 0.0003  val AP 0.3005\n",
      "  ✅ New best AP: 0.3005 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 45  loss 0.0003  val AP 0.3007\n",
      "  ✅ New best AP: 0.3007 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 46  loss 0.0003  val AP 0.3011\n",
      "  ✅ New best AP: 0.3011 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 47  loss 0.0003  val AP 0.3019\n",
      "  ✅ New best AP: 0.3019 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 48  loss 0.0003  val AP 0.3021\n",
      "  ✅ New best AP: 0.3021 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 49  loss 0.0003  val AP 0.3036\n",
      "  ✅ New best AP: 0.3036 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 29] ep 50  loss 0.0003  val AP 0.3038\n",
      "  ✅ New best AP: 0.3038 → v7\\model\\ensembles\\SR-ARE\\seed29\\best.pt\n",
      "[SR-ARE | seed 47] ep 01  loss 0.0004  val AP 0.1807\n",
      "  ✅ New best AP: 0.1807 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 02  loss 0.0003  val AP 0.1847\n",
      "  ✅ New best AP: 0.1847 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 03  loss 0.0004  val AP 0.1903\n",
      "  ✅ New best AP: 0.1903 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 04  loss 0.0005  val AP 0.1985\n",
      "  ✅ New best AP: 0.1985 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 05  loss 0.0003  val AP 0.2040\n",
      "  ✅ New best AP: 0.2040 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 06  loss 0.0003  val AP 0.2097\n",
      "  ✅ New best AP: 0.2097 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 07  loss 0.0003  val AP 0.2173\n",
      "  ✅ New best AP: 0.2173 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 08  loss 0.0003  val AP 0.2214\n",
      "  ✅ New best AP: 0.2214 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 09  loss 0.0003  val AP 0.2259\n",
      "  ✅ New best AP: 0.2259 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 10  loss 0.0003  val AP 0.2283\n",
      "  ✅ New best AP: 0.2283 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 11  loss 0.0003  val AP 0.2320\n",
      "  ✅ New best AP: 0.2320 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 12  loss 0.0003  val AP 0.2345\n",
      "  ✅ New best AP: 0.2345 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 13  loss 0.0003  val AP 0.2359\n",
      "  ✅ New best AP: 0.2359 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 14  loss 0.0003  val AP 0.2376\n",
      "  ✅ New best AP: 0.2376 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 15  loss 0.0003  val AP 0.2394\n",
      "  ✅ New best AP: 0.2394 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 16  loss 0.0003  val AP 0.2413\n",
      "  ✅ New best AP: 0.2413 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 17  loss 0.0003  val AP 0.2433\n",
      "  ✅ New best AP: 0.2433 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 18  loss 0.0003  val AP 0.2466\n",
      "  ✅ New best AP: 0.2466 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 19  loss 0.0003  val AP 0.2490\n",
      "  ✅ New best AP: 0.2490 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 20  loss 0.0003  val AP 0.2506\n",
      "  ✅ New best AP: 0.2506 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 21  loss 0.0003  val AP 0.2525\n",
      "  ✅ New best AP: 0.2525 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 22  loss 0.0003  val AP 0.2535\n",
      "  ✅ New best AP: 0.2535 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 23  loss 0.0002  val AP 0.2549\n",
      "  ✅ New best AP: 0.2549 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 24  loss 0.0003  val AP 0.2563\n",
      "  ✅ New best AP: 0.2563 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 25  loss 0.0002  val AP 0.2589\n",
      "  ✅ New best AP: 0.2589 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 26  loss 0.0002  val AP 0.2599\n",
      "  ✅ New best AP: 0.2599 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 27  loss 0.0003  val AP 0.2604\n",
      "  ✅ New best AP: 0.2604 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 28  loss 0.0002  val AP 0.2615\n",
      "  ✅ New best AP: 0.2615 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 29  loss 0.0002  val AP 0.2615\n",
      "  ✅ New best AP: 0.2615 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 30  loss 0.0003  val AP 0.2625\n",
      "  ✅ New best AP: 0.2625 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 31  loss 0.0003  val AP 0.2623\n",
      "[SR-ARE | seed 47] ep 32  loss 0.0002  val AP 0.2632\n",
      "  ✅ New best AP: 0.2632 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 33  loss 0.0002  val AP 0.2637\n",
      "  ✅ New best AP: 0.2637 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 34  loss 0.0002  val AP 0.2640\n",
      "  ✅ New best AP: 0.2640 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 35  loss 0.0003  val AP 0.2655\n",
      "  ✅ New best AP: 0.2655 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 36  loss 0.0003  val AP 0.2647\n",
      "[SR-ARE | seed 47] ep 37  loss 0.0003  val AP 0.2655\n",
      "  ✅ New best AP: 0.2655 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 38  loss 0.0002  val AP 0.2658\n",
      "  ✅ New best AP: 0.2658 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 39  loss 0.0003  val AP 0.2663\n",
      "  ✅ New best AP: 0.2663 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 40  loss 0.0003  val AP 0.2670\n",
      "  ✅ New best AP: 0.2670 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 41  loss 0.0003  val AP 0.2668\n",
      "[SR-ARE | seed 47] ep 42  loss 0.0003  val AP 0.2673\n",
      "  ✅ New best AP: 0.2673 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 43  loss 0.0002  val AP 0.2684\n",
      "  ✅ New best AP: 0.2684 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 44  loss 0.0003  val AP 0.2689\n",
      "  ✅ New best AP: 0.2689 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 45  loss 0.0003  val AP 0.2689\n",
      "  ✅ New best AP: 0.2689 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 46  loss 0.0003  val AP 0.2696\n",
      "  ✅ New best AP: 0.2696 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 47  loss 0.0003  val AP 0.2696\n",
      "  ✅ New best AP: 0.2696 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 48  loss 0.0003  val AP 0.2698\n",
      "  ✅ New best AP: 0.2698 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 49  loss 0.0003  val AP 0.2698\n",
      "  ✅ New best AP: 0.2698 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 47] ep 50  loss 0.0003  val AP 0.2708\n",
      "  ✅ New best AP: 0.2708 → v7\\model\\ensembles\\SR-ARE\\seed47\\best.pt\n",
      "[SR-ARE | seed 61] ep 01  loss 0.0006  val AP 0.1439\n",
      "  ✅ New best AP: 0.1439 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 02  loss 0.0004  val AP 0.1481\n",
      "  ✅ New best AP: 0.1481 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 03  loss 0.0004  val AP 0.1524\n",
      "  ✅ New best AP: 0.1524 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 04  loss 0.0004  val AP 0.1583\n",
      "  ✅ New best AP: 0.1583 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 05  loss 0.0003  val AP 0.1679\n",
      "  ✅ New best AP: 0.1679 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 06  loss 0.0004  val AP 0.1796\n",
      "  ✅ New best AP: 0.1796 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 07  loss 0.0004  val AP 0.1908\n",
      "  ✅ New best AP: 0.1908 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 08  loss 0.0003  val AP 0.2031\n",
      "  ✅ New best AP: 0.2031 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 09  loss 0.0003  val AP 0.2111\n",
      "  ✅ New best AP: 0.2111 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 10  loss 0.0003  val AP 0.2206\n",
      "  ✅ New best AP: 0.2206 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 11  loss 0.0003  val AP 0.2342\n",
      "  ✅ New best AP: 0.2342 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 12  loss 0.0003  val AP 0.2409\n",
      "  ✅ New best AP: 0.2409 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 13  loss 0.0003  val AP 0.2466\n",
      "  ✅ New best AP: 0.2466 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 14  loss 0.0003  val AP 0.2523\n",
      "  ✅ New best AP: 0.2523 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 15  loss 0.0003  val AP 0.2583\n",
      "  ✅ New best AP: 0.2583 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 16  loss 0.0003  val AP 0.2632\n",
      "  ✅ New best AP: 0.2632 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 17  loss 0.0003  val AP 0.2667\n",
      "  ✅ New best AP: 0.2667 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 18  loss 0.0003  val AP 0.2705\n",
      "  ✅ New best AP: 0.2705 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 19  loss 0.0003  val AP 0.2746\n",
      "  ✅ New best AP: 0.2746 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 20  loss 0.0003  val AP 0.2770\n",
      "  ✅ New best AP: 0.2770 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 21  loss 0.0003  val AP 0.2801\n",
      "  ✅ New best AP: 0.2801 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 22  loss 0.0003  val AP 0.2859\n",
      "  ✅ New best AP: 0.2859 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 23  loss 0.0003  val AP 0.2897\n",
      "  ✅ New best AP: 0.2897 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 24  loss 0.0003  val AP 0.2919\n",
      "  ✅ New best AP: 0.2919 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 25  loss 0.0003  val AP 0.2961\n",
      "  ✅ New best AP: 0.2961 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 26  loss 0.0003  val AP 0.2981\n",
      "  ✅ New best AP: 0.2981 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 27  loss 0.0003  val AP 0.3001\n",
      "  ✅ New best AP: 0.3001 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 28  loss 0.0003  val AP 0.3031\n",
      "  ✅ New best AP: 0.3031 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 29  loss 0.0003  val AP 0.3056\n",
      "  ✅ New best AP: 0.3056 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 30  loss 0.0003  val AP 0.3067\n",
      "  ✅ New best AP: 0.3067 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 31  loss 0.0003  val AP 0.3095\n",
      "  ✅ New best AP: 0.3095 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 32  loss 0.0003  val AP 0.3096\n",
      "  ✅ New best AP: 0.3096 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 33  loss 0.0003  val AP 0.3108\n",
      "  ✅ New best AP: 0.3108 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 34  loss 0.0003  val AP 0.3117\n",
      "  ✅ New best AP: 0.3117 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 35  loss 0.0003  val AP 0.3134\n",
      "  ✅ New best AP: 0.3134 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 36  loss 0.0003  val AP 0.3140\n",
      "  ✅ New best AP: 0.3140 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 37  loss 0.0002  val AP 0.3153\n",
      "  ✅ New best AP: 0.3153 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 38  loss 0.0003  val AP 0.3165\n",
      "  ✅ New best AP: 0.3165 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 39  loss 0.0003  val AP 0.3171\n",
      "  ✅ New best AP: 0.3171 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 40  loss 0.0003  val AP 0.3176\n",
      "  ✅ New best AP: 0.3176 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 41  loss 0.0003  val AP 0.3179\n",
      "  ✅ New best AP: 0.3179 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 42  loss 0.0003  val AP 0.3201\n",
      "  ✅ New best AP: 0.3201 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 43  loss 0.0003  val AP 0.3211\n",
      "  ✅ New best AP: 0.3211 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 44  loss 0.0003  val AP 0.3215\n",
      "  ✅ New best AP: 0.3215 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 45  loss 0.0003  val AP 0.3217\n",
      "  ✅ New best AP: 0.3217 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 46  loss 0.0003  val AP 0.3219\n",
      "  ✅ New best AP: 0.3219 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 47  loss 0.0003  val AP 0.3224\n",
      "  ✅ New best AP: 0.3224 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 48  loss 0.0003  val AP 0.3231\n",
      "  ✅ New best AP: 0.3231 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 49  loss 0.0003  val AP 0.3233\n",
      "  ✅ New best AP: 0.3233 → v7\\model\\ensembles\\SR-ARE\\seed61\\best.pt\n",
      "[SR-ARE | seed 61] ep 50  loss 0.0003  val AP 0.3232\n",
      "[SR-ARE | seed 83] ep 01  loss 0.0004  val AP 0.2069\n",
      "  ✅ New best AP: 0.2069 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 02  loss 0.0003  val AP 0.2111\n",
      "  ✅ New best AP: 0.2111 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 03  loss 0.0004  val AP 0.2195\n",
      "  ✅ New best AP: 0.2195 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 04  loss 0.0004  val AP 0.2254\n",
      "  ✅ New best AP: 0.2254 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 05  loss 0.0003  val AP 0.2365\n",
      "  ✅ New best AP: 0.2365 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 06  loss 0.0003  val AP 0.2425\n",
      "  ✅ New best AP: 0.2425 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 07  loss 0.0003  val AP 0.2514\n",
      "  ✅ New best AP: 0.2514 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 08  loss 0.0004  val AP 0.2573\n",
      "  ✅ New best AP: 0.2573 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 09  loss 0.0004  val AP 0.2673\n",
      "  ✅ New best AP: 0.2673 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 10  loss 0.0003  val AP 0.2760\n",
      "  ✅ New best AP: 0.2760 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 11  loss 0.0003  val AP 0.2776\n",
      "  ✅ New best AP: 0.2776 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 12  loss 0.0003  val AP 0.2797\n",
      "  ✅ New best AP: 0.2797 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 13  loss 0.0003  val AP 0.2804\n",
      "  ✅ New best AP: 0.2804 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 14  loss 0.0003  val AP 0.2823\n",
      "  ✅ New best AP: 0.2823 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 15  loss 0.0003  val AP 0.2862\n",
      "  ✅ New best AP: 0.2862 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 16  loss 0.0003  val AP 0.2873\n",
      "  ✅ New best AP: 0.2873 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 17  loss 0.0003  val AP 0.2879\n",
      "  ✅ New best AP: 0.2879 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 18  loss 0.0003  val AP 0.2892\n",
      "  ✅ New best AP: 0.2892 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 19  loss 0.0003  val AP 0.2916\n",
      "  ✅ New best AP: 0.2916 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 20  loss 0.0003  val AP 0.2932\n",
      "  ✅ New best AP: 0.2932 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 21  loss 0.0003  val AP 0.3034\n",
      "  ✅ New best AP: 0.3034 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 22  loss 0.0003  val AP 0.3055\n",
      "  ✅ New best AP: 0.3055 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 23  loss 0.0003  val AP 0.3068\n",
      "  ✅ New best AP: 0.3068 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 24  loss 0.0003  val AP 0.3083\n",
      "  ✅ New best AP: 0.3083 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 25  loss 0.0003  val AP 0.3079\n",
      "[SR-ARE | seed 83] ep 26  loss 0.0003  val AP 0.3091\n",
      "  ✅ New best AP: 0.3091 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 27  loss 0.0002  val AP 0.3100\n",
      "  ✅ New best AP: 0.3100 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 28  loss 0.0003  val AP 0.3095\n",
      "[SR-ARE | seed 83] ep 29  loss 0.0003  val AP 0.3101\n",
      "  ✅ New best AP: 0.3101 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 30  loss 0.0003  val AP 0.3105\n",
      "  ✅ New best AP: 0.3105 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 31  loss 0.0002  val AP 0.3108\n",
      "  ✅ New best AP: 0.3108 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 32  loss 0.0003  val AP 0.3117\n",
      "  ✅ New best AP: 0.3117 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 33  loss 0.0003  val AP 0.3124\n",
      "  ✅ New best AP: 0.3124 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 34  loss 0.0003  val AP 0.3120\n",
      "[SR-ARE | seed 83] ep 35  loss 0.0003  val AP 0.3128\n",
      "  ✅ New best AP: 0.3128 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 36  loss 0.0003  val AP 0.3132\n",
      "  ✅ New best AP: 0.3132 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 37  loss 0.0003  val AP 0.3137\n",
      "  ✅ New best AP: 0.3137 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 38  loss 0.0003  val AP 0.3138\n",
      "  ✅ New best AP: 0.3138 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 39  loss 0.0003  val AP 0.3139\n",
      "  ✅ New best AP: 0.3139 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 40  loss 0.0003  val AP 0.3146\n",
      "  ✅ New best AP: 0.3146 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 41  loss 0.0003  val AP 0.3145\n",
      "[SR-ARE | seed 83] ep 42  loss 0.0003  val AP 0.3140\n",
      "[SR-ARE | seed 83] ep 43  loss 0.0003  val AP 0.3145\n",
      "[SR-ARE | seed 83] ep 44  loss 0.0003  val AP 0.3146\n",
      "[SR-ARE | seed 83] ep 45  loss 0.0003  val AP 0.3167\n",
      "  ✅ New best AP: 0.3167 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 46  loss 0.0003  val AP 0.3174\n",
      "  ✅ New best AP: 0.3174 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 47  loss 0.0003  val AP 0.3194\n",
      "  ✅ New best AP: 0.3194 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 48  loss 0.0003  val AP 0.3205\n",
      "  ✅ New best AP: 0.3205 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "[SR-ARE | seed 83] ep 49  loss 0.0003  val AP 0.3204\n",
      "[SR-ARE | seed 83] ep 50  loss 0.0003  val AP 0.3232\n",
      "  ✅ New best AP: 0.3232 → v7\\model\\ensembles\\SR-ARE\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-ATAD5 (label 8)\n",
      "==============================\n",
      "[SR-ATAD5 | seed 13] ep 01  loss 0.0011  val AP 0.0402\n",
      "  ✅ New best AP: 0.0402 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 02  loss 0.0008  val AP 0.0409\n",
      "  ✅ New best AP: 0.0409 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 03  loss 0.0008  val AP 0.0421\n",
      "  ✅ New best AP: 0.0421 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 04  loss 0.0009  val AP 0.0439\n",
      "  ✅ New best AP: 0.0439 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 05  loss 0.0007  val AP 0.0464\n",
      "  ✅ New best AP: 0.0464 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 06  loss 0.0010  val AP 0.0504\n",
      "  ✅ New best AP: 0.0504 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 07  loss 0.0007  val AP 0.0554\n",
      "  ✅ New best AP: 0.0554 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 08  loss 0.0012  val AP 0.0608\n",
      "  ✅ New best AP: 0.0608 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 09  loss 0.0007  val AP 0.0687\n",
      "  ✅ New best AP: 0.0687 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 10  loss 0.0007  val AP 0.0777\n",
      "  ✅ New best AP: 0.0777 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 11  loss 0.0006  val AP 0.0851\n",
      "  ✅ New best AP: 0.0851 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 12  loss 0.0007  val AP 0.0915\n",
      "  ✅ New best AP: 0.0915 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 13  loss 0.0007  val AP 0.0944\n",
      "  ✅ New best AP: 0.0944 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 14  loss 0.0008  val AP 0.1017\n",
      "  ✅ New best AP: 0.1017 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 15  loss 0.0006  val AP 0.1048\n",
      "  ✅ New best AP: 0.1048 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 16  loss 0.0010  val AP 0.1077\n",
      "  ✅ New best AP: 0.1077 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 17  loss 0.0006  val AP 0.1116\n",
      "  ✅ New best AP: 0.1116 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 18  loss 0.0007  val AP 0.1206\n",
      "  ✅ New best AP: 0.1206 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 19  loss 0.0006  val AP 0.1244\n",
      "  ✅ New best AP: 0.1244 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 20  loss 0.0008  val AP 0.1282\n",
      "  ✅ New best AP: 0.1282 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 21  loss 0.0007  val AP 0.1295\n",
      "  ✅ New best AP: 0.1295 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 22  loss 0.0008  val AP 0.1384\n",
      "  ✅ New best AP: 0.1384 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 23  loss 0.0006  val AP 0.1408\n",
      "  ✅ New best AP: 0.1408 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 24  loss 0.0006  val AP 0.1369\n",
      "[SR-ATAD5 | seed 13] ep 25  loss 0.0006  val AP 0.1369\n",
      "[SR-ATAD5 | seed 13] ep 26  loss 0.0006  val AP 0.1374\n",
      "[SR-ATAD5 | seed 13] ep 27  loss 0.0007  val AP 0.1380\n",
      "[SR-ATAD5 | seed 13] ep 28  loss 0.0006  val AP 0.1385\n",
      "[SR-ATAD5 | seed 13] ep 29  loss 0.0006  val AP 0.1390\n",
      "[SR-ATAD5 | seed 13] ep 30  loss 0.0006  val AP 0.1390\n",
      "[SR-ATAD5 | seed 13] ep 31  loss 0.0006  val AP 0.1393\n",
      "[SR-ATAD5 | seed 13] ep 32  loss 0.0006  val AP 0.1395\n",
      "[SR-ATAD5 | seed 13] ep 33  loss 0.0006  val AP 0.1576\n",
      "  ✅ New best AP: 0.1576 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 34  loss 0.0007  val AP 0.1578\n",
      "  ✅ New best AP: 0.1578 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 35  loss 0.0006  val AP 0.1580\n",
      "  ✅ New best AP: 0.1580 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 36  loss 0.0006  val AP 0.1579\n",
      "[SR-ATAD5 | seed 13] ep 37  loss 0.0006  val AP 0.1579\n",
      "[SR-ATAD5 | seed 13] ep 38  loss 0.0006  val AP 0.1576\n",
      "[SR-ATAD5 | seed 13] ep 39  loss 0.0006  val AP 0.1578\n",
      "[SR-ATAD5 | seed 13] ep 40  loss 0.0006  val AP 0.1580\n",
      "  ✅ New best AP: 0.1580 → v7\\model\\ensembles\\SR-ATAD5\\seed13\\best.pt\n",
      "[SR-ATAD5 | seed 13] ep 41  loss 0.0006  val AP 0.1568\n",
      "[SR-ATAD5 | seed 13] ep 42  loss 0.0006  val AP 0.1569\n",
      "[SR-ATAD5 | seed 13] ep 43  loss 0.0006  val AP 0.1570\n",
      "[SR-ATAD5 | seed 13] ep 44  loss 0.0006  val AP 0.1572\n",
      "[SR-ATAD5 | seed 13] ep 45  loss 0.0006  val AP 0.1574\n",
      "[SR-ATAD5 | seed 13] ep 46  loss 0.0006  val AP 0.1574\n",
      "[SR-ATAD5 | seed 13] ep 47  loss 0.0007  val AP 0.1577\n",
      "[SR-ATAD5 | seed 13] ep 48  loss 0.0006  val AP 0.1577\n",
      "[SR-ATAD5 | seed 13] ep 49  loss 0.0006  val AP 0.1577\n",
      "[SR-ATAD5 | seed 13] ep 50  loss 0.0006  val AP 0.1578\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.1580\n",
      "[SR-ATAD5 | seed 29] ep 01  loss 0.0013  val AP 0.0665\n",
      "  ✅ New best AP: 0.0665 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 02  loss 0.0008  val AP 0.0766\n",
      "  ✅ New best AP: 0.0766 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 03  loss 0.0010  val AP 0.0906\n",
      "  ✅ New best AP: 0.0906 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 04  loss 0.0012  val AP 0.1090\n",
      "  ✅ New best AP: 0.1090 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 05  loss 0.0009  val AP 0.1694\n",
      "  ✅ New best AP: 0.1694 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 06  loss 0.0008  val AP 0.1777\n",
      "  ✅ New best AP: 0.1777 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 07  loss 0.0008  val AP 0.1951\n",
      "  ✅ New best AP: 0.1951 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 08  loss 0.0010  val AP 0.1996\n",
      "  ✅ New best AP: 0.1996 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 09  loss 0.0009  val AP 0.2009\n",
      "  ✅ New best AP: 0.2009 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 10  loss 0.0009  val AP 0.2019\n",
      "  ✅ New best AP: 0.2019 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 11  loss 0.0007  val AP 0.2013\n",
      "[SR-ATAD5 | seed 29] ep 12  loss 0.0007  val AP 0.2020\n",
      "  ✅ New best AP: 0.2020 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 13  loss 0.0007  val AP 0.2077\n",
      "  ✅ New best AP: 0.2077 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 14  loss 0.0009  val AP 0.2091\n",
      "  ✅ New best AP: 0.2091 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 15  loss 0.0007  val AP 0.2119\n",
      "  ✅ New best AP: 0.2119 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 16  loss 0.0007  val AP 0.2112\n",
      "[SR-ATAD5 | seed 29] ep 17  loss 0.0007  val AP 0.2114\n",
      "[SR-ATAD5 | seed 29] ep 18  loss 0.0007  val AP 0.2110\n",
      "[SR-ATAD5 | seed 29] ep 19  loss 0.0006  val AP 0.2115\n",
      "[SR-ATAD5 | seed 29] ep 20  loss 0.0007  val AP 0.2145\n",
      "  ✅ New best AP: 0.2145 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 21  loss 0.0006  val AP 0.2143\n",
      "[SR-ATAD5 | seed 29] ep 22  loss 0.0008  val AP 0.2166\n",
      "  ✅ New best AP: 0.2166 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 23  loss 0.0006  val AP 0.2233\n",
      "  ✅ New best AP: 0.2233 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 24  loss 0.0007  val AP 0.2149\n",
      "[SR-ATAD5 | seed 29] ep 25  loss 0.0006  val AP 0.2236\n",
      "  ✅ New best AP: 0.2236 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 26  loss 0.0007  val AP 0.2235\n",
      "[SR-ATAD5 | seed 29] ep 27  loss 0.0006  val AP 0.2263\n",
      "  ✅ New best AP: 0.2263 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 28  loss 0.0007  val AP 0.2265\n",
      "  ✅ New best AP: 0.2265 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 29  loss 0.0006  val AP 0.2270\n",
      "  ✅ New best AP: 0.2270 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 30  loss 0.0007  val AP 0.2272\n",
      "  ✅ New best AP: 0.2272 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 31  loss 0.0006  val AP 0.2277\n",
      "  ✅ New best AP: 0.2277 → v7\\model\\ensembles\\SR-ATAD5\\seed29\\best.pt\n",
      "[SR-ATAD5 | seed 29] ep 32  loss 0.0006  val AP 0.2236\n",
      "[SR-ATAD5 | seed 29] ep 33  loss 0.0006  val AP 0.2238\n",
      "[SR-ATAD5 | seed 29] ep 34  loss 0.0006  val AP 0.2242\n",
      "[SR-ATAD5 | seed 29] ep 35  loss 0.0006  val AP 0.2190\n",
      "[SR-ATAD5 | seed 29] ep 36  loss 0.0006  val AP 0.2165\n",
      "[SR-ATAD5 | seed 29] ep 37  loss 0.0006  val AP 0.2166\n",
      "[SR-ATAD5 | seed 29] ep 38  loss 0.0006  val AP 0.2126\n",
      "[SR-ATAD5 | seed 29] ep 39  loss 0.0006  val AP 0.2104\n",
      "[SR-ATAD5 | seed 29] ep 40  loss 0.0006  val AP 0.2091\n",
      "[SR-ATAD5 | seed 29] ep 41  loss 0.0007  val AP 0.2096\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.2277\n",
      "[SR-ATAD5 | seed 47] ep 01  loss 0.0011  val AP 0.1028\n",
      "  ✅ New best AP: 0.1028 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 02  loss 0.0008  val AP 0.1051\n",
      "  ✅ New best AP: 0.1051 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 03  loss 0.0010  val AP 0.1103\n",
      "  ✅ New best AP: 0.1103 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 04  loss 0.0011  val AP 0.1143\n",
      "  ✅ New best AP: 0.1143 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 05  loss 0.0009  val AP 0.1204\n",
      "  ✅ New best AP: 0.1204 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 06  loss 0.0007  val AP 0.1306\n",
      "  ✅ New best AP: 0.1306 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 07  loss 0.0009  val AP 0.1327\n",
      "  ✅ New best AP: 0.1327 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 08  loss 0.0010  val AP 0.1342\n",
      "  ✅ New best AP: 0.1342 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 09  loss 0.0008  val AP 0.1365\n",
      "  ✅ New best AP: 0.1365 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 10  loss 0.0007  val AP 0.1397\n",
      "  ✅ New best AP: 0.1397 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 11  loss 0.0007  val AP 0.1381\n",
      "[SR-ATAD5 | seed 47] ep 12  loss 0.0007  val AP 0.1410\n",
      "  ✅ New best AP: 0.1410 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 13  loss 0.0007  val AP 0.1409\n",
      "[SR-ATAD5 | seed 47] ep 14  loss 0.0007  val AP 0.1416\n",
      "  ✅ New best AP: 0.1416 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 15  loss 0.0007  val AP 0.1447\n",
      "  ✅ New best AP: 0.1447 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 16  loss 0.0007  val AP 0.1428\n",
      "[SR-ATAD5 | seed 47] ep 17  loss 0.0006  val AP 0.1429\n",
      "[SR-ATAD5 | seed 47] ep 18  loss 0.0008  val AP 0.1495\n",
      "  ✅ New best AP: 0.1495 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 19  loss 0.0006  val AP 0.1524\n",
      "  ✅ New best AP: 0.1524 → v7\\model\\ensembles\\SR-ATAD5\\seed47\\best.pt\n",
      "[SR-ATAD5 | seed 47] ep 20  loss 0.0006  val AP 0.1474\n",
      "[SR-ATAD5 | seed 47] ep 21  loss 0.0006  val AP 0.1422\n",
      "[SR-ATAD5 | seed 47] ep 22  loss 0.0006  val AP 0.1424\n",
      "[SR-ATAD5 | seed 47] ep 23  loss 0.0006  val AP 0.1426\n",
      "[SR-ATAD5 | seed 47] ep 24  loss 0.0006  val AP 0.1436\n",
      "[SR-ATAD5 | seed 47] ep 25  loss 0.0006  val AP 0.1435\n",
      "[SR-ATAD5 | seed 47] ep 26  loss 0.0006  val AP 0.1437\n",
      "[SR-ATAD5 | seed 47] ep 27  loss 0.0006  val AP 0.1440\n",
      "[SR-ATAD5 | seed 47] ep 28  loss 0.0006  val AP 0.1473\n",
      "[SR-ATAD5 | seed 47] ep 29  loss 0.0006  val AP 0.1486\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.1524\n",
      "[SR-ATAD5 | seed 61] ep 01  loss 0.0015  val AP 0.0491\n",
      "  ✅ New best AP: 0.0491 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 02  loss 0.0009  val AP 0.0525\n",
      "  ✅ New best AP: 0.0525 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 03  loss 0.0010  val AP 0.0575\n",
      "  ✅ New best AP: 0.0575 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 04  loss 0.0011  val AP 0.0638\n",
      "  ✅ New best AP: 0.0638 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 05  loss 0.0009  val AP 0.0728\n",
      "  ✅ New best AP: 0.0728 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 06  loss 0.0008  val AP 0.0801\n",
      "  ✅ New best AP: 0.0801 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 07  loss 0.0010  val AP 0.0825\n",
      "  ✅ New best AP: 0.0825 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 08  loss 0.0009  val AP 0.0856\n",
      "  ✅ New best AP: 0.0856 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 09  loss 0.0010  val AP 0.0861\n",
      "  ✅ New best AP: 0.0861 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 10  loss 0.0009  val AP 0.0913\n",
      "  ✅ New best AP: 0.0913 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 11  loss 0.0008  val AP 0.0955\n",
      "  ✅ New best AP: 0.0955 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 12  loss 0.0007  val AP 0.0977\n",
      "  ✅ New best AP: 0.0977 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 13  loss 0.0008  val AP 0.0994\n",
      "  ✅ New best AP: 0.0994 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 14  loss 0.0007  val AP 0.1020\n",
      "  ✅ New best AP: 0.1020 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 15  loss 0.0008  val AP 0.1094\n",
      "  ✅ New best AP: 0.1094 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 16  loss 0.0007  val AP 0.1085\n",
      "[SR-ATAD5 | seed 61] ep 17  loss 0.0007  val AP 0.1110\n",
      "  ✅ New best AP: 0.1110 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 18  loss 0.0006  val AP 0.1121\n",
      "  ✅ New best AP: 0.1121 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 19  loss 0.0007  val AP 0.1129\n",
      "  ✅ New best AP: 0.1129 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 20  loss 0.0006  val AP 0.1131\n",
      "  ✅ New best AP: 0.1131 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 21  loss 0.0007  val AP 0.1208\n",
      "  ✅ New best AP: 0.1208 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 22  loss 0.0006  val AP 0.1216\n",
      "  ✅ New best AP: 0.1216 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 23  loss 0.0006  val AP 0.1226\n",
      "  ✅ New best AP: 0.1226 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 24  loss 0.0006  val AP 0.1241\n",
      "  ✅ New best AP: 0.1241 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 25  loss 0.0006  val AP 0.1251\n",
      "  ✅ New best AP: 0.1251 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 26  loss 0.0006  val AP 0.1244\n",
      "[SR-ATAD5 | seed 61] ep 27  loss 0.0006  val AP 0.1264\n",
      "  ✅ New best AP: 0.1264 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 28  loss 0.0006  val AP 0.1273\n",
      "  ✅ New best AP: 0.1273 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 29  loss 0.0007  val AP 0.1288\n",
      "  ✅ New best AP: 0.1288 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 30  loss 0.0006  val AP 0.1292\n",
      "  ✅ New best AP: 0.1292 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 31  loss 0.0007  val AP 0.1313\n",
      "  ✅ New best AP: 0.1313 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 32  loss 0.0006  val AP 0.1315\n",
      "  ✅ New best AP: 0.1315 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 33  loss 0.0006  val AP 0.1318\n",
      "  ✅ New best AP: 0.1318 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 34  loss 0.0007  val AP 0.1339\n",
      "  ✅ New best AP: 0.1339 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 35  loss 0.0006  val AP 0.1333\n",
      "[SR-ATAD5 | seed 61] ep 36  loss 0.0006  val AP 0.1339\n",
      "  ✅ New best AP: 0.1339 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 37  loss 0.0007  val AP 0.1341\n",
      "  ✅ New best AP: 0.1341 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 38  loss 0.0006  val AP 0.1340\n",
      "[SR-ATAD5 | seed 61] ep 39  loss 0.0007  val AP 0.1338\n",
      "[SR-ATAD5 | seed 61] ep 40  loss 0.0007  val AP 0.1338\n",
      "[SR-ATAD5 | seed 61] ep 41  loss 0.0006  val AP 0.1339\n",
      "[SR-ATAD5 | seed 61] ep 42  loss 0.0006  val AP 0.1341\n",
      "[SR-ATAD5 | seed 61] ep 43  loss 0.0006  val AP 0.1343\n",
      "  ✅ New best AP: 0.1343 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 44  loss 0.0006  val AP 0.1361\n",
      "  ✅ New best AP: 0.1361 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 45  loss 0.0007  val AP 0.1362\n",
      "  ✅ New best AP: 0.1362 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 46  loss 0.0006  val AP 0.1362\n",
      "  ✅ New best AP: 0.1362 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 47  loss 0.0007  val AP 0.1364\n",
      "  ✅ New best AP: 0.1364 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 48  loss 0.0007  val AP 0.1383\n",
      "  ✅ New best AP: 0.1383 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 49  loss 0.0006  val AP 0.1384\n",
      "  ✅ New best AP: 0.1384 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 61] ep 50  loss 0.0007  val AP 0.1385\n",
      "  ✅ New best AP: 0.1385 → v7\\model\\ensembles\\SR-ATAD5\\seed61\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 01  loss 0.0011  val AP 0.0418\n",
      "  ✅ New best AP: 0.0418 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 02  loss 0.0008  val AP 0.0428\n",
      "  ✅ New best AP: 0.0428 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 03  loss 0.0009  val AP 0.0442\n",
      "  ✅ New best AP: 0.0442 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 04  loss 0.0009  val AP 0.0465\n",
      "  ✅ New best AP: 0.0465 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 05  loss 0.0007  val AP 0.0499\n",
      "  ✅ New best AP: 0.0499 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 06  loss 0.0008  val AP 0.0542\n",
      "  ✅ New best AP: 0.0542 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 07  loss 0.0008  val AP 0.0582\n",
      "  ✅ New best AP: 0.0582 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 08  loss 0.0009  val AP 0.0637\n",
      "  ✅ New best AP: 0.0637 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 09  loss 0.0007  val AP 0.0690\n",
      "  ✅ New best AP: 0.0690 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 10  loss 0.0008  val AP 0.0764\n",
      "  ✅ New best AP: 0.0764 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 11  loss 0.0008  val AP 0.0830\n",
      "  ✅ New best AP: 0.0830 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 12  loss 0.0007  val AP 0.0877\n",
      "  ✅ New best AP: 0.0877 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 13  loss 0.0007  val AP 0.0956\n",
      "  ✅ New best AP: 0.0956 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 14  loss 0.0006  val AP 0.0983\n",
      "  ✅ New best AP: 0.0983 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 15  loss 0.0007  val AP 0.1032\n",
      "  ✅ New best AP: 0.1032 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 16  loss 0.0007  val AP 0.1273\n",
      "  ✅ New best AP: 0.1273 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 17  loss 0.0007  val AP 0.1366\n",
      "  ✅ New best AP: 0.1366 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 18  loss 0.0006  val AP 0.1581\n",
      "  ✅ New best AP: 0.1581 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 19  loss 0.0008  val AP 0.1594\n",
      "  ✅ New best AP: 0.1594 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 20  loss 0.0006  val AP 0.1672\n",
      "  ✅ New best AP: 0.1672 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 21  loss 0.0007  val AP 0.1693\n",
      "  ✅ New best AP: 0.1693 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 22  loss 0.0006  val AP 0.1699\n",
      "  ✅ New best AP: 0.1699 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 23  loss 0.0008  val AP 0.1708\n",
      "  ✅ New best AP: 0.1708 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 24  loss 0.0007  val AP 0.1716\n",
      "  ✅ New best AP: 0.1716 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 25  loss 0.0007  val AP 0.1728\n",
      "  ✅ New best AP: 0.1728 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 26  loss 0.0006  val AP 0.1736\n",
      "  ✅ New best AP: 0.1736 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 27  loss 0.0006  val AP 0.1737\n",
      "  ✅ New best AP: 0.1737 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 28  loss 0.0006  val AP 0.1752\n",
      "  ✅ New best AP: 0.1752 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 29  loss 0.0006  val AP 0.1752\n",
      "  ✅ New best AP: 0.1752 → v7\\model\\ensembles\\SR-ATAD5\\seed83\\best.pt\n",
      "[SR-ATAD5 | seed 83] ep 30  loss 0.0006  val AP 0.1644\n",
      "[SR-ATAD5 | seed 83] ep 31  loss 0.0006  val AP 0.1650\n",
      "[SR-ATAD5 | seed 83] ep 32  loss 0.0006  val AP 0.1651\n",
      "[SR-ATAD5 | seed 83] ep 33  loss 0.0006  val AP 0.1653\n",
      "[SR-ATAD5 | seed 83] ep 34  loss 0.0006  val AP 0.1653\n",
      "[SR-ATAD5 | seed 83] ep 35  loss 0.0006  val AP 0.1658\n",
      "[SR-ATAD5 | seed 83] ep 36  loss 0.0006  val AP 0.1660\n",
      "[SR-ATAD5 | seed 83] ep 37  loss 0.0006  val AP 0.1616\n",
      "[SR-ATAD5 | seed 83] ep 38  loss 0.0006  val AP 0.1617\n",
      "[SR-ATAD5 | seed 83] ep 39  loss 0.0006  val AP 0.1629\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.1752\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-HSE (label 9)\n",
      "==============================\n",
      "[SR-HSE | seed 13] ep 01  loss 0.0008  val AP 0.0507\n",
      "  ✅ New best AP: 0.0507 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 02  loss 0.0006  val AP 0.0518\n",
      "  ✅ New best AP: 0.0518 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 03  loss 0.0006  val AP 0.0529\n",
      "  ✅ New best AP: 0.0529 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 04  loss 0.0006  val AP 0.0549\n",
      "  ✅ New best AP: 0.0549 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 05  loss 0.0006  val AP 0.0579\n",
      "  ✅ New best AP: 0.0579 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 06  loss 0.0010  val AP 0.0619\n",
      "  ✅ New best AP: 0.0619 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 07  loss 0.0006  val AP 0.0650\n",
      "  ✅ New best AP: 0.0650 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 08  loss 0.0008  val AP 0.0690\n",
      "  ✅ New best AP: 0.0690 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 09  loss 0.0006  val AP 0.0741\n",
      "  ✅ New best AP: 0.0741 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 10  loss 0.0006  val AP 0.0791\n",
      "  ✅ New best AP: 0.0791 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 11  loss 0.0006  val AP 0.0852\n",
      "  ✅ New best AP: 0.0852 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 12  loss 0.0006  val AP 0.0914\n",
      "  ✅ New best AP: 0.0914 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 13  loss 0.0006  val AP 0.0974\n",
      "  ✅ New best AP: 0.0974 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 14  loss 0.0007  val AP 0.1035\n",
      "  ✅ New best AP: 0.1035 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 15  loss 0.0005  val AP 0.1087\n",
      "  ✅ New best AP: 0.1087 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 16  loss 0.0006  val AP 0.1158\n",
      "  ✅ New best AP: 0.1158 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 17  loss 0.0006  val AP 0.1215\n",
      "  ✅ New best AP: 0.1215 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 18  loss 0.0008  val AP 0.1250\n",
      "  ✅ New best AP: 0.1250 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 19  loss 0.0006  val AP 0.1331\n",
      "  ✅ New best AP: 0.1331 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 20  loss 0.0006  val AP 0.1375\n",
      "  ✅ New best AP: 0.1375 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 21  loss 0.0005  val AP 0.1427\n",
      "  ✅ New best AP: 0.1427 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 22  loss 0.0006  val AP 0.1446\n",
      "  ✅ New best AP: 0.1446 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 23  loss 0.0005  val AP 0.1486\n",
      "  ✅ New best AP: 0.1486 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 24  loss 0.0006  val AP 0.1504\n",
      "  ✅ New best AP: 0.1504 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 25  loss 0.0005  val AP 0.1525\n",
      "  ✅ New best AP: 0.1525 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 26  loss 0.0005  val AP 0.1533\n",
      "  ✅ New best AP: 0.1533 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 27  loss 0.0005  val AP 0.1565\n",
      "  ✅ New best AP: 0.1565 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 28  loss 0.0005  val AP 0.1607\n",
      "  ✅ New best AP: 0.1607 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 29  loss 0.0005  val AP 0.1632\n",
      "  ✅ New best AP: 0.1632 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 30  loss 0.0006  val AP 0.1640\n",
      "  ✅ New best AP: 0.1640 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 31  loss 0.0005  val AP 0.1657\n",
      "  ✅ New best AP: 0.1657 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 32  loss 0.0005  val AP 0.1677\n",
      "  ✅ New best AP: 0.1677 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 33  loss 0.0005  val AP 0.1682\n",
      "  ✅ New best AP: 0.1682 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 34  loss 0.0005  val AP 0.1700\n",
      "  ✅ New best AP: 0.1700 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 35  loss 0.0005  val AP 0.1712\n",
      "  ✅ New best AP: 0.1712 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 36  loss 0.0005  val AP 0.1731\n",
      "  ✅ New best AP: 0.1731 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 37  loss 0.0005  val AP 0.1738\n",
      "  ✅ New best AP: 0.1738 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 38  loss 0.0005  val AP 0.1752\n",
      "  ✅ New best AP: 0.1752 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 39  loss 0.0005  val AP 0.1760\n",
      "  ✅ New best AP: 0.1760 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 40  loss 0.0005  val AP 0.1770\n",
      "  ✅ New best AP: 0.1770 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 41  loss 0.0005  val AP 0.1784\n",
      "  ✅ New best AP: 0.1784 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 42  loss 0.0005  val AP 0.1783\n",
      "[SR-HSE | seed 13] ep 43  loss 0.0005  val AP 0.1787\n",
      "  ✅ New best AP: 0.1787 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 44  loss 0.0005  val AP 0.1792\n",
      "  ✅ New best AP: 0.1792 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 45  loss 0.0005  val AP 0.1797\n",
      "  ✅ New best AP: 0.1797 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 46  loss 0.0006  val AP 0.1804\n",
      "  ✅ New best AP: 0.1804 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 47  loss 0.0005  val AP 0.1807\n",
      "  ✅ New best AP: 0.1807 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 48  loss 0.0005  val AP 0.1814\n",
      "  ✅ New best AP: 0.1814 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 49  loss 0.0006  val AP 0.1815\n",
      "  ✅ New best AP: 0.1815 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 13] ep 50  loss 0.0005  val AP 0.1816\n",
      "  ✅ New best AP: 0.1816 → v7\\model\\ensembles\\SR-HSE\\seed13\\best.pt\n",
      "[SR-HSE | seed 29] ep 01  loss 0.0010  val AP 0.0864\n",
      "  ✅ New best AP: 0.0864 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 02  loss 0.0007  val AP 0.0904\n",
      "  ✅ New best AP: 0.0904 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 03  loss 0.0009  val AP 0.0977\n",
      "  ✅ New best AP: 0.0977 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 04  loss 0.0009  val AP 0.1021\n",
      "  ✅ New best AP: 0.1021 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 05  loss 0.0007  val AP 0.1087\n",
      "  ✅ New best AP: 0.1087 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 06  loss 0.0007  val AP 0.1197\n",
      "  ✅ New best AP: 0.1197 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 07  loss 0.0009  val AP 0.1202\n",
      "  ✅ New best AP: 0.1202 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 08  loss 0.0008  val AP 0.1366\n",
      "  ✅ New best AP: 0.1366 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 09  loss 0.0007  val AP 0.1490\n",
      "  ✅ New best AP: 0.1490 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 10  loss 0.0006  val AP 0.1488\n",
      "[SR-HSE | seed 29] ep 11  loss 0.0006  val AP 0.1486\n",
      "[SR-HSE | seed 29] ep 12  loss 0.0006  val AP 0.1516\n",
      "  ✅ New best AP: 0.1516 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 13  loss 0.0006  val AP 0.1548\n",
      "  ✅ New best AP: 0.1548 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 14  loss 0.0006  val AP 0.1568\n",
      "  ✅ New best AP: 0.1568 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 15  loss 0.0007  val AP 0.1608\n",
      "  ✅ New best AP: 0.1608 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 16  loss 0.0006  val AP 0.1547\n",
      "[SR-HSE | seed 29] ep 17  loss 0.0006  val AP 0.1444\n",
      "[SR-HSE | seed 29] ep 18  loss 0.0006  val AP 0.1472\n",
      "[SR-HSE | seed 29] ep 19  loss 0.0005  val AP 0.1443\n",
      "[SR-HSE | seed 29] ep 20  loss 0.0006  val AP 0.1432\n",
      "[SR-HSE | seed 29] ep 21  loss 0.0005  val AP 0.1473\n",
      "[SR-HSE | seed 29] ep 22  loss 0.0006  val AP 0.1520\n",
      "[SR-HSE | seed 29] ep 23  loss 0.0006  val AP 0.1551\n",
      "[SR-HSE | seed 29] ep 24  loss 0.0006  val AP 0.1586\n",
      "[SR-HSE | seed 29] ep 25  loss 0.0005  val AP 0.1652\n",
      "  ✅ New best AP: 0.1652 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 26  loss 0.0006  val AP 0.1659\n",
      "  ✅ New best AP: 0.1659 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 27  loss 0.0005  val AP 0.1728\n",
      "  ✅ New best AP: 0.1728 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 28  loss 0.0005  val AP 0.1804\n",
      "  ✅ New best AP: 0.1804 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 29  loss 0.0006  val AP 0.1860\n",
      "  ✅ New best AP: 0.1860 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 30  loss 0.0006  val AP 0.1886\n",
      "  ✅ New best AP: 0.1886 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 31  loss 0.0005  val AP 0.1943\n",
      "  ✅ New best AP: 0.1943 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 32  loss 0.0006  val AP 0.1960\n",
      "  ✅ New best AP: 0.1960 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 33  loss 0.0005  val AP 0.1985\n",
      "  ✅ New best AP: 0.1985 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 34  loss 0.0005  val AP 0.2018\n",
      "  ✅ New best AP: 0.2018 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 35  loss 0.0006  val AP 0.2028\n",
      "  ✅ New best AP: 0.2028 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 36  loss 0.0005  val AP 0.2049\n",
      "  ✅ New best AP: 0.2049 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 37  loss 0.0006  val AP 0.2067\n",
      "  ✅ New best AP: 0.2067 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 38  loss 0.0006  val AP 0.2116\n",
      "  ✅ New best AP: 0.2116 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 39  loss 0.0006  val AP 0.2122\n",
      "  ✅ New best AP: 0.2122 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 40  loss 0.0006  val AP 0.2157\n",
      "  ✅ New best AP: 0.2157 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 41  loss 0.0006  val AP 0.2166\n",
      "  ✅ New best AP: 0.2166 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 42  loss 0.0006  val AP 0.2174\n",
      "  ✅ New best AP: 0.2174 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 43  loss 0.0006  val AP 0.2197\n",
      "  ✅ New best AP: 0.2197 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 44  loss 0.0005  val AP 0.2203\n",
      "  ✅ New best AP: 0.2203 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 45  loss 0.0006  val AP 0.2233\n",
      "  ✅ New best AP: 0.2233 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 46  loss 0.0006  val AP 0.2283\n",
      "  ✅ New best AP: 0.2283 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 47  loss 0.0006  val AP 0.2280\n",
      "[SR-HSE | seed 29] ep 48  loss 0.0006  val AP 0.2308\n",
      "  ✅ New best AP: 0.2308 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 49  loss 0.0006  val AP 0.2310\n",
      "  ✅ New best AP: 0.2310 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 29] ep 50  loss 0.0006  val AP 0.2323\n",
      "  ✅ New best AP: 0.2323 → v7\\model\\ensembles\\SR-HSE\\seed29\\best.pt\n",
      "[SR-HSE | seed 47] ep 01  loss 0.0010  val AP 0.0650\n",
      "  ✅ New best AP: 0.0650 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 02  loss 0.0007  val AP 0.0654\n",
      "  ✅ New best AP: 0.0654 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 03  loss 0.0008  val AP 0.0663\n",
      "  ✅ New best AP: 0.0663 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 04  loss 0.0009  val AP 0.0677\n",
      "  ✅ New best AP: 0.0677 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 05  loss 0.0007  val AP 0.0697\n",
      "  ✅ New best AP: 0.0697 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 06  loss 0.0006  val AP 0.0715\n",
      "  ✅ New best AP: 0.0715 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 07  loss 0.0007  val AP 0.0729\n",
      "  ✅ New best AP: 0.0729 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 08  loss 0.0007  val AP 0.0734\n",
      "  ✅ New best AP: 0.0734 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 09  loss 0.0009  val AP 0.0745\n",
      "  ✅ New best AP: 0.0745 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 10  loss 0.0006  val AP 0.0765\n",
      "  ✅ New best AP: 0.0765 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 11  loss 0.0006  val AP 0.0784\n",
      "  ✅ New best AP: 0.0784 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 12  loss 0.0005  val AP 0.0809\n",
      "  ✅ New best AP: 0.0809 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 13  loss 0.0007  val AP 0.0830\n",
      "  ✅ New best AP: 0.0830 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 14  loss 0.0006  val AP 0.0845\n",
      "  ✅ New best AP: 0.0845 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 15  loss 0.0006  val AP 0.0870\n",
      "  ✅ New best AP: 0.0870 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 16  loss 0.0006  val AP 0.0889\n",
      "  ✅ New best AP: 0.0889 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 17  loss 0.0006  val AP 0.0902\n",
      "  ✅ New best AP: 0.0902 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 18  loss 0.0005  val AP 0.0920\n",
      "  ✅ New best AP: 0.0920 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 19  loss 0.0007  val AP 0.0944\n",
      "  ✅ New best AP: 0.0944 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 20  loss 0.0005  val AP 0.0967\n",
      "  ✅ New best AP: 0.0967 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 21  loss 0.0005  val AP 0.0974\n",
      "  ✅ New best AP: 0.0974 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 22  loss 0.0006  val AP 0.0984\n",
      "  ✅ New best AP: 0.0984 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 23  loss 0.0006  val AP 0.1020\n",
      "  ✅ New best AP: 0.1020 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 24  loss 0.0006  val AP 0.1030\n",
      "  ✅ New best AP: 0.1030 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 25  loss 0.0005  val AP 0.1082\n",
      "  ✅ New best AP: 0.1082 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 26  loss 0.0006  val AP 0.1092\n",
      "  ✅ New best AP: 0.1092 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 27  loss 0.0005  val AP 0.1115\n",
      "  ✅ New best AP: 0.1115 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 28  loss 0.0005  val AP 0.1125\n",
      "  ✅ New best AP: 0.1125 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 29  loss 0.0005  val AP 0.1134\n",
      "  ✅ New best AP: 0.1134 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 30  loss 0.0005  val AP 0.1144\n",
      "  ✅ New best AP: 0.1144 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 31  loss 0.0005  val AP 0.1150\n",
      "  ✅ New best AP: 0.1150 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 32  loss 0.0005  val AP 0.1171\n",
      "  ✅ New best AP: 0.1171 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 33  loss 0.0005  val AP 0.1183\n",
      "  ✅ New best AP: 0.1183 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 34  loss 0.0005  val AP 0.1201\n",
      "  ✅ New best AP: 0.1201 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 35  loss 0.0005  val AP 0.1209\n",
      "  ✅ New best AP: 0.1209 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 36  loss 0.0005  val AP 0.1215\n",
      "  ✅ New best AP: 0.1215 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 37  loss 0.0005  val AP 0.1223\n",
      "  ✅ New best AP: 0.1223 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 38  loss 0.0005  val AP 0.1233\n",
      "  ✅ New best AP: 0.1233 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 39  loss 0.0005  val AP 0.1245\n",
      "  ✅ New best AP: 0.1245 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 40  loss 0.0005  val AP 0.1252\n",
      "  ✅ New best AP: 0.1252 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 41  loss 0.0005  val AP 0.1274\n",
      "  ✅ New best AP: 0.1274 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 42  loss 0.0005  val AP 0.1283\n",
      "  ✅ New best AP: 0.1283 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 43  loss 0.0005  val AP 0.1290\n",
      "  ✅ New best AP: 0.1290 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 44  loss 0.0005  val AP 0.1302\n",
      "  ✅ New best AP: 0.1302 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 45  loss 0.0005  val AP 0.1302\n",
      "[SR-HSE | seed 47] ep 46  loss 0.0005  val AP 0.1312\n",
      "  ✅ New best AP: 0.1312 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 47  loss 0.0005  val AP 0.1316\n",
      "  ✅ New best AP: 0.1316 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 48  loss 0.0005  val AP 0.1319\n",
      "  ✅ New best AP: 0.1319 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 49  loss 0.0006  val AP 0.1324\n",
      "  ✅ New best AP: 0.1324 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 47] ep 50  loss 0.0005  val AP 0.1327\n",
      "  ✅ New best AP: 0.1327 → v7\\model\\ensembles\\SR-HSE\\seed47\\best.pt\n",
      "[SR-HSE | seed 61] ep 01  loss 0.0011  val AP 0.0753\n",
      "  ✅ New best AP: 0.0753 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 02  loss 0.0008  val AP 0.0790\n",
      "  ✅ New best AP: 0.0790 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 03  loss 0.0008  val AP 0.0841\n",
      "  ✅ New best AP: 0.0841 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 04  loss 0.0009  val AP 0.0876\n",
      "  ✅ New best AP: 0.0876 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 05  loss 0.0008  val AP 0.0934\n",
      "  ✅ New best AP: 0.0934 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 06  loss 0.0007  val AP 0.0996\n",
      "  ✅ New best AP: 0.0996 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 07  loss 0.0007  val AP 0.1034\n",
      "  ✅ New best AP: 0.1034 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 08  loss 0.0008  val AP 0.1064\n",
      "  ✅ New best AP: 0.1064 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 09  loss 0.0007  val AP 0.1132\n",
      "  ✅ New best AP: 0.1132 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 10  loss 0.0007  val AP 0.1279\n",
      "  ✅ New best AP: 0.1279 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 11  loss 0.0007  val AP 0.1328\n",
      "  ✅ New best AP: 0.1328 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 12  loss 0.0006  val AP 0.1351\n",
      "  ✅ New best AP: 0.1351 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 13  loss 0.0006  val AP 0.1417\n",
      "  ✅ New best AP: 0.1417 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 14  loss 0.0007  val AP 0.1431\n",
      "  ✅ New best AP: 0.1431 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 15  loss 0.0006  val AP 0.1443\n",
      "  ✅ New best AP: 0.1443 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 16  loss 0.0007  val AP 0.1476\n",
      "  ✅ New best AP: 0.1476 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 17  loss 0.0006  val AP 0.1493\n",
      "  ✅ New best AP: 0.1493 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 18  loss 0.0006  val AP 0.1383\n",
      "[SR-HSE | seed 61] ep 19  loss 0.0006  val AP 0.1395\n",
      "[SR-HSE | seed 61] ep 20  loss 0.0006  val AP 0.1406\n",
      "[SR-HSE | seed 61] ep 21  loss 0.0006  val AP 0.1418\n",
      "[SR-HSE | seed 61] ep 22  loss 0.0006  val AP 0.1429\n",
      "[SR-HSE | seed 61] ep 23  loss 0.0006  val AP 0.1445\n",
      "[SR-HSE | seed 61] ep 24  loss 0.0006  val AP 0.1470\n",
      "[SR-HSE | seed 61] ep 25  loss 0.0005  val AP 0.1482\n",
      "[SR-HSE | seed 61] ep 26  loss 0.0005  val AP 0.1497\n",
      "  ✅ New best AP: 0.1497 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 27  loss 0.0005  val AP 0.1516\n",
      "  ✅ New best AP: 0.1516 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 28  loss 0.0006  val AP 0.1523\n",
      "  ✅ New best AP: 0.1523 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 29  loss 0.0005  val AP 0.1537\n",
      "  ✅ New best AP: 0.1537 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 30  loss 0.0005  val AP 0.1547\n",
      "  ✅ New best AP: 0.1547 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 31  loss 0.0005  val AP 0.1632\n",
      "  ✅ New best AP: 0.1632 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 32  loss 0.0005  val AP 0.1646\n",
      "  ✅ New best AP: 0.1646 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 33  loss 0.0005  val AP 0.1658\n",
      "  ✅ New best AP: 0.1658 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 34  loss 0.0005  val AP 0.1666\n",
      "  ✅ New best AP: 0.1666 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 35  loss 0.0005  val AP 0.1673\n",
      "  ✅ New best AP: 0.1673 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 36  loss 0.0005  val AP 0.1676\n",
      "  ✅ New best AP: 0.1676 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 37  loss 0.0006  val AP 0.1683\n",
      "  ✅ New best AP: 0.1683 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 38  loss 0.0006  val AP 0.1683\n",
      "  ✅ New best AP: 0.1683 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 39  loss 0.0005  val AP 0.1684\n",
      "  ✅ New best AP: 0.1684 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 40  loss 0.0006  val AP 0.1691\n",
      "  ✅ New best AP: 0.1691 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 41  loss 0.0006  val AP 0.1693\n",
      "  ✅ New best AP: 0.1693 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 42  loss 0.0005  val AP 0.1694\n",
      "  ✅ New best AP: 0.1694 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 43  loss 0.0006  val AP 0.1704\n",
      "  ✅ New best AP: 0.1704 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 44  loss 0.0006  val AP 0.1709\n",
      "  ✅ New best AP: 0.1709 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 45  loss 0.0006  val AP 0.1713\n",
      "  ✅ New best AP: 0.1713 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 46  loss 0.0005  val AP 0.1720\n",
      "  ✅ New best AP: 0.1720 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 47  loss 0.0005  val AP 0.1728\n",
      "  ✅ New best AP: 0.1728 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 48  loss 0.0006  val AP 0.1731\n",
      "  ✅ New best AP: 0.1731 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 49  loss 0.0005  val AP 0.1735\n",
      "  ✅ New best AP: 0.1735 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 61] ep 50  loss 0.0005  val AP 0.1736\n",
      "  ✅ New best AP: 0.1736 → v7\\model\\ensembles\\SR-HSE\\seed61\\best.pt\n",
      "[SR-HSE | seed 83] ep 01  loss 0.0009  val AP 0.0853\n",
      "  ✅ New best AP: 0.0853 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 02  loss 0.0007  val AP 0.0881\n",
      "  ✅ New best AP: 0.0881 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 03  loss 0.0008  val AP 0.0902\n",
      "  ✅ New best AP: 0.0902 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 04  loss 0.0008  val AP 0.0906\n",
      "  ✅ New best AP: 0.0906 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 05  loss 0.0006  val AP 0.0887\n",
      "[SR-HSE | seed 83] ep 06  loss 0.0007  val AP 0.0919\n",
      "  ✅ New best AP: 0.0919 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 07  loss 0.0007  val AP 0.0958\n",
      "  ✅ New best AP: 0.0958 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 08  loss 0.0007  val AP 0.0974\n",
      "  ✅ New best AP: 0.0974 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 09  loss 0.0006  val AP 0.1003\n",
      "  ✅ New best AP: 0.1003 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 10  loss 0.0007  val AP 0.1014\n",
      "  ✅ New best AP: 0.1014 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 11  loss 0.0006  val AP 0.1021\n",
      "  ✅ New best AP: 0.1021 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 12  loss 0.0006  val AP 0.1045\n",
      "  ✅ New best AP: 0.1045 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 13  loss 0.0006  val AP 0.1080\n",
      "  ✅ New best AP: 0.1080 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 14  loss 0.0006  val AP 0.1111\n",
      "  ✅ New best AP: 0.1111 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 15  loss 0.0007  val AP 0.1127\n",
      "  ✅ New best AP: 0.1127 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 16  loss 0.0005  val AP 0.1146\n",
      "  ✅ New best AP: 0.1146 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 17  loss 0.0006  val AP 0.1166\n",
      "  ✅ New best AP: 0.1166 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 18  loss 0.0005  val AP 0.1188\n",
      "  ✅ New best AP: 0.1188 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 19  loss 0.0007  val AP 0.1214\n",
      "  ✅ New best AP: 0.1214 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 20  loss 0.0005  val AP 0.1224\n",
      "  ✅ New best AP: 0.1224 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 21  loss 0.0005  val AP 0.1249\n",
      "  ✅ New best AP: 0.1249 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 22  loss 0.0006  val AP 0.1261\n",
      "  ✅ New best AP: 0.1261 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 23  loss 0.0005  val AP 0.1263\n",
      "  ✅ New best AP: 0.1263 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 24  loss 0.0005  val AP 0.1277\n",
      "  ✅ New best AP: 0.1277 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 25  loss 0.0006  val AP 0.1300\n",
      "  ✅ New best AP: 0.1300 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 26  loss 0.0005  val AP 0.1324\n",
      "  ✅ New best AP: 0.1324 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 27  loss 0.0006  val AP 0.1343\n",
      "  ✅ New best AP: 0.1343 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 28  loss 0.0005  val AP 0.1360\n",
      "  ✅ New best AP: 0.1360 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 29  loss 0.0005  val AP 0.1373\n",
      "  ✅ New best AP: 0.1373 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 30  loss 0.0005  val AP 0.1383\n",
      "  ✅ New best AP: 0.1383 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 31  loss 0.0006  val AP 0.1398\n",
      "  ✅ New best AP: 0.1398 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 32  loss 0.0005  val AP 0.1409\n",
      "  ✅ New best AP: 0.1409 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 33  loss 0.0005  val AP 0.1423\n",
      "  ✅ New best AP: 0.1423 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 34  loss 0.0005  val AP 0.1441\n",
      "  ✅ New best AP: 0.1441 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 35  loss 0.0005  val AP 0.1453\n",
      "  ✅ New best AP: 0.1453 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 36  loss 0.0005  val AP 0.1463\n",
      "  ✅ New best AP: 0.1463 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 37  loss 0.0006  val AP 0.1494\n",
      "  ✅ New best AP: 0.1494 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 38  loss 0.0005  val AP 0.1506\n",
      "  ✅ New best AP: 0.1506 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 39  loss 0.0005  val AP 0.1536\n",
      "  ✅ New best AP: 0.1536 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 40  loss 0.0005  val AP 0.1542\n",
      "  ✅ New best AP: 0.1542 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 41  loss 0.0005  val AP 0.1558\n",
      "  ✅ New best AP: 0.1558 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 42  loss 0.0005  val AP 0.1561\n",
      "  ✅ New best AP: 0.1561 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 43  loss 0.0005  val AP 0.1574\n",
      "  ✅ New best AP: 0.1574 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 44  loss 0.0005  val AP 0.1585\n",
      "  ✅ New best AP: 0.1585 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 45  loss 0.0006  val AP 0.1594\n",
      "  ✅ New best AP: 0.1594 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 46  loss 0.0006  val AP 0.1597\n",
      "  ✅ New best AP: 0.1597 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 47  loss 0.0005  val AP 0.1600\n",
      "  ✅ New best AP: 0.1600 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 48  loss 0.0005  val AP 0.1605\n",
      "  ✅ New best AP: 0.1605 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 49  loss 0.0005  val AP 0.1620\n",
      "  ✅ New best AP: 0.1620 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "[SR-HSE | seed 83] ep 50  loss 0.0006  val AP 0.1629\n",
      "  ✅ New best AP: 0.1629 → v7\\model\\ensembles\\SR-HSE\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-MMP (label 10)\n",
      "==============================\n",
      "[SR-MMP | seed 13] ep 01  loss 0.0004  val AP 0.0771\n",
      "  ✅ New best AP: 0.0771 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 02  loss 0.0003  val AP 0.0781\n",
      "  ✅ New best AP: 0.0781 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 03  loss 0.0003  val AP 0.0798\n",
      "  ✅ New best AP: 0.0798 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 04  loss 0.0003  val AP 0.0827\n",
      "  ✅ New best AP: 0.0827 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 05  loss 0.0003  val AP 0.0873\n",
      "  ✅ New best AP: 0.0873 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 06  loss 0.0003  val AP 0.0956\n",
      "  ✅ New best AP: 0.0956 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 07  loss 0.0003  val AP 0.1068\n",
      "  ✅ New best AP: 0.1068 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 08  loss 0.0003  val AP 0.1330\n",
      "  ✅ New best AP: 0.1330 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 09  loss 0.0004  val AP 0.1506\n",
      "  ✅ New best AP: 0.1506 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 10  loss 0.0003  val AP 0.1721\n",
      "  ✅ New best AP: 0.1721 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 11  loss 0.0002  val AP 0.1967\n",
      "  ✅ New best AP: 0.1967 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 12  loss 0.0002  val AP 0.2263\n",
      "  ✅ New best AP: 0.2263 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 13  loss 0.0002  val AP 0.2425\n",
      "  ✅ New best AP: 0.2425 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 14  loss 0.0003  val AP 0.2567\n",
      "  ✅ New best AP: 0.2567 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 15  loss 0.0002  val AP 0.2710\n",
      "  ✅ New best AP: 0.2710 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 16  loss 0.0003  val AP 0.2826\n",
      "  ✅ New best AP: 0.2826 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 17  loss 0.0002  val AP 0.2932\n",
      "  ✅ New best AP: 0.2932 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 18  loss 0.0002  val AP 0.3044\n",
      "  ✅ New best AP: 0.3044 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 19  loss 0.0003  val AP 0.3156\n",
      "  ✅ New best AP: 0.3156 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 20  loss 0.0003  val AP 0.3208\n",
      "  ✅ New best AP: 0.3208 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 21  loss 0.0002  val AP 0.3208\n",
      "  ✅ New best AP: 0.3208 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 22  loss 0.0002  val AP 0.3268\n",
      "  ✅ New best AP: 0.3268 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 23  loss 0.0003  val AP 0.3301\n",
      "  ✅ New best AP: 0.3301 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 24  loss 0.0002  val AP 0.3330\n",
      "  ✅ New best AP: 0.3330 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 25  loss 0.0002  val AP 0.3383\n",
      "  ✅ New best AP: 0.3383 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 26  loss 0.0002  val AP 0.3414\n",
      "  ✅ New best AP: 0.3414 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 27  loss 0.0002  val AP 0.3461\n",
      "  ✅ New best AP: 0.3461 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 28  loss 0.0002  val AP 0.3517\n",
      "  ✅ New best AP: 0.3517 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 29  loss 0.0002  val AP 0.3536\n",
      "  ✅ New best AP: 0.3536 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 30  loss 0.0002  val AP 0.3562\n",
      "  ✅ New best AP: 0.3562 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 31  loss 0.0002  val AP 0.3568\n",
      "  ✅ New best AP: 0.3568 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 32  loss 0.0002  val AP 0.3575\n",
      "  ✅ New best AP: 0.3575 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 33  loss 0.0002  val AP 0.3590\n",
      "  ✅ New best AP: 0.3590 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 34  loss 0.0002  val AP 0.3661\n",
      "  ✅ New best AP: 0.3661 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 35  loss 0.0002  val AP 0.3676\n",
      "  ✅ New best AP: 0.3676 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 36  loss 0.0002  val AP 0.3703\n",
      "  ✅ New best AP: 0.3703 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 37  loss 0.0002  val AP 0.3712\n",
      "  ✅ New best AP: 0.3712 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 38  loss 0.0002  val AP 0.3719\n",
      "  ✅ New best AP: 0.3719 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 39  loss 0.0002  val AP 0.3733\n",
      "  ✅ New best AP: 0.3733 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 40  loss 0.0002  val AP 0.3736\n",
      "  ✅ New best AP: 0.3736 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 41  loss 0.0002  val AP 0.3747\n",
      "  ✅ New best AP: 0.3747 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 42  loss 0.0002  val AP 0.3748\n",
      "  ✅ New best AP: 0.3748 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 43  loss 0.0002  val AP 0.3761\n",
      "  ✅ New best AP: 0.3761 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 44  loss 0.0002  val AP 0.3772\n",
      "  ✅ New best AP: 0.3772 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 45  loss 0.0002  val AP 0.3777\n",
      "  ✅ New best AP: 0.3777 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 46  loss 0.0002  val AP 0.3789\n",
      "  ✅ New best AP: 0.3789 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 47  loss 0.0002  val AP 0.3796\n",
      "  ✅ New best AP: 0.3796 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 48  loss 0.0002  val AP 0.3803\n",
      "  ✅ New best AP: 0.3803 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 49  loss 0.0002  val AP 0.3808\n",
      "  ✅ New best AP: 0.3808 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 13] ep 50  loss 0.0002  val AP 0.3811\n",
      "  ✅ New best AP: 0.3811 → v7\\model\\ensembles\\SR-MMP\\seed13\\best.pt\n",
      "[SR-MMP | seed 29] ep 01  loss 0.0005  val AP 0.1965\n",
      "  ✅ New best AP: 0.1965 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 02  loss 0.0003  val AP 0.2079\n",
      "  ✅ New best AP: 0.2079 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 03  loss 0.0004  val AP 0.2252\n",
      "  ✅ New best AP: 0.2252 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 04  loss 0.0004  val AP 0.2431\n",
      "  ✅ New best AP: 0.2431 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 05  loss 0.0003  val AP 0.2608\n",
      "  ✅ New best AP: 0.2608 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 06  loss 0.0003  val AP 0.2841\n",
      "  ✅ New best AP: 0.2841 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 07  loss 0.0003  val AP 0.2860\n",
      "  ✅ New best AP: 0.2860 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 08  loss 0.0003  val AP 0.2921\n",
      "  ✅ New best AP: 0.2921 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 09  loss 0.0003  val AP 0.2989\n",
      "  ✅ New best AP: 0.2989 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 10  loss 0.0003  val AP 0.3087\n",
      "  ✅ New best AP: 0.3087 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 11  loss 0.0002  val AP 0.3118\n",
      "  ✅ New best AP: 0.3118 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 12  loss 0.0003  val AP 0.3212\n",
      "  ✅ New best AP: 0.3212 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 13  loss 0.0002  val AP 0.3277\n",
      "  ✅ New best AP: 0.3277 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 14  loss 0.0002  val AP 0.3300\n",
      "  ✅ New best AP: 0.3300 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 15  loss 0.0002  val AP 0.3406\n",
      "  ✅ New best AP: 0.3406 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 16  loss 0.0003  val AP 0.3466\n",
      "  ✅ New best AP: 0.3466 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 17  loss 0.0003  val AP 0.3545\n",
      "  ✅ New best AP: 0.3545 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 18  loss 0.0002  val AP 0.3606\n",
      "  ✅ New best AP: 0.3606 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 19  loss 0.0002  val AP 0.3724\n",
      "  ✅ New best AP: 0.3724 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 20  loss 0.0002  val AP 0.3785\n",
      "  ✅ New best AP: 0.3785 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 21  loss 0.0002  val AP 0.3851\n",
      "  ✅ New best AP: 0.3851 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 22  loss 0.0002  val AP 0.3929\n",
      "  ✅ New best AP: 0.3929 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 23  loss 0.0002  val AP 0.3964\n",
      "  ✅ New best AP: 0.3964 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 24  loss 0.0002  val AP 0.4028\n",
      "  ✅ New best AP: 0.4028 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 25  loss 0.0002  val AP 0.4069\n",
      "  ✅ New best AP: 0.4069 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 26  loss 0.0002  val AP 0.4092\n",
      "  ✅ New best AP: 0.4092 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 27  loss 0.0002  val AP 0.4117\n",
      "  ✅ New best AP: 0.4117 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 28  loss 0.0002  val AP 0.4174\n",
      "  ✅ New best AP: 0.4174 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 29  loss 0.0002  val AP 0.4187\n",
      "  ✅ New best AP: 0.4187 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 30  loss 0.0002  val AP 0.4263\n",
      "  ✅ New best AP: 0.4263 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 31  loss 0.0002  val AP 0.4275\n",
      "  ✅ New best AP: 0.4275 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 32  loss 0.0002  val AP 0.4286\n",
      "  ✅ New best AP: 0.4286 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 33  loss 0.0002  val AP 0.4317\n",
      "  ✅ New best AP: 0.4317 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 34  loss 0.0002  val AP 0.4338\n",
      "  ✅ New best AP: 0.4338 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 35  loss 0.0002  val AP 0.4360\n",
      "  ✅ New best AP: 0.4360 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 36  loss 0.0002  val AP 0.4371\n",
      "  ✅ New best AP: 0.4371 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 37  loss 0.0002  val AP 0.4377\n",
      "  ✅ New best AP: 0.4377 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 38  loss 0.0002  val AP 0.4387\n",
      "  ✅ New best AP: 0.4387 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 39  loss 0.0002  val AP 0.4391\n",
      "  ✅ New best AP: 0.4391 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 40  loss 0.0002  val AP 0.4401\n",
      "  ✅ New best AP: 0.4401 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 41  loss 0.0002  val AP 0.4410\n",
      "  ✅ New best AP: 0.4410 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 42  loss 0.0002  val AP 0.4418\n",
      "  ✅ New best AP: 0.4418 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 43  loss 0.0002  val AP 0.4423\n",
      "  ✅ New best AP: 0.4423 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 44  loss 0.0002  val AP 0.4430\n",
      "  ✅ New best AP: 0.4430 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 45  loss 0.0002  val AP 0.4431\n",
      "  ✅ New best AP: 0.4431 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 46  loss 0.0002  val AP 0.4439\n",
      "  ✅ New best AP: 0.4439 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 47  loss 0.0002  val AP 0.4445\n",
      "  ✅ New best AP: 0.4445 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 48  loss 0.0002  val AP 0.4452\n",
      "  ✅ New best AP: 0.4452 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 49  loss 0.0002  val AP 0.4457\n",
      "  ✅ New best AP: 0.4457 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 29] ep 50  loss 0.0002  val AP 0.4462\n",
      "  ✅ New best AP: 0.4462 → v7\\model\\ensembles\\SR-MMP\\seed29\\best.pt\n",
      "[SR-MMP | seed 47] ep 01  loss 0.0004  val AP 0.2174\n",
      "  ✅ New best AP: 0.2174 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 02  loss 0.0003  val AP 0.2276\n",
      "  ✅ New best AP: 0.2276 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 03  loss 0.0004  val AP 0.2362\n",
      "  ✅ New best AP: 0.2362 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 04  loss 0.0004  val AP 0.2469\n",
      "  ✅ New best AP: 0.2469 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 05  loss 0.0003  val AP 0.2631\n",
      "  ✅ New best AP: 0.2631 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 06  loss 0.0003  val AP 0.2799\n",
      "  ✅ New best AP: 0.2799 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 07  loss 0.0003  val AP 0.2909\n",
      "  ✅ New best AP: 0.2909 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 08  loss 0.0003  val AP 0.2941\n",
      "  ✅ New best AP: 0.2941 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 09  loss 0.0003  val AP 0.3013\n",
      "  ✅ New best AP: 0.3013 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 10  loss 0.0003  val AP 0.3061\n",
      "  ✅ New best AP: 0.3061 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 11  loss 0.0002  val AP 0.3118\n",
      "  ✅ New best AP: 0.3118 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 12  loss 0.0003  val AP 0.3154\n",
      "  ✅ New best AP: 0.3154 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 13  loss 0.0003  val AP 0.3190\n",
      "  ✅ New best AP: 0.3190 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 14  loss 0.0003  val AP 0.3249\n",
      "  ✅ New best AP: 0.3249 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 15  loss 0.0002  val AP 0.3289\n",
      "  ✅ New best AP: 0.3289 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 16  loss 0.0002  val AP 0.3323\n",
      "  ✅ New best AP: 0.3323 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 17  loss 0.0003  val AP 0.3346\n",
      "  ✅ New best AP: 0.3346 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 18  loss 0.0002  val AP 0.3377\n",
      "  ✅ New best AP: 0.3377 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 19  loss 0.0002  val AP 0.3396\n",
      "  ✅ New best AP: 0.3396 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 20  loss 0.0002  val AP 0.3437\n",
      "  ✅ New best AP: 0.3437 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 21  loss 0.0002  val AP 0.3443\n",
      "  ✅ New best AP: 0.3443 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 22  loss 0.0002  val AP 0.3494\n",
      "  ✅ New best AP: 0.3494 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 23  loss 0.0002  val AP 0.3506\n",
      "  ✅ New best AP: 0.3506 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 24  loss 0.0002  val AP 0.3518\n",
      "  ✅ New best AP: 0.3518 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 25  loss 0.0002  val AP 0.3531\n",
      "  ✅ New best AP: 0.3531 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 26  loss 0.0002  val AP 0.3560\n",
      "  ✅ New best AP: 0.3560 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 27  loss 0.0002  val AP 0.3567\n",
      "  ✅ New best AP: 0.3567 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 28  loss 0.0002  val AP 0.3582\n",
      "  ✅ New best AP: 0.3582 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 29  loss 0.0002  val AP 0.3596\n",
      "  ✅ New best AP: 0.3596 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 30  loss 0.0002  val AP 0.3617\n",
      "  ✅ New best AP: 0.3617 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 31  loss 0.0002  val AP 0.3629\n",
      "  ✅ New best AP: 0.3629 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 32  loss 0.0002  val AP 0.3632\n",
      "  ✅ New best AP: 0.3632 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 33  loss 0.0002  val AP 0.3642\n",
      "  ✅ New best AP: 0.3642 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 34  loss 0.0002  val AP 0.3645\n",
      "  ✅ New best AP: 0.3645 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 35  loss 0.0002  val AP 0.3653\n",
      "  ✅ New best AP: 0.3653 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 36  loss 0.0002  val AP 0.3661\n",
      "  ✅ New best AP: 0.3661 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 37  loss 0.0002  val AP 0.3664\n",
      "  ✅ New best AP: 0.3664 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 38  loss 0.0002  val AP 0.3672\n",
      "  ✅ New best AP: 0.3672 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 39  loss 0.0002  val AP 0.3685\n",
      "  ✅ New best AP: 0.3685 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 40  loss 0.0002  val AP 0.3692\n",
      "  ✅ New best AP: 0.3692 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 41  loss 0.0002  val AP 0.3698\n",
      "  ✅ New best AP: 0.3698 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 42  loss 0.0002  val AP 0.3704\n",
      "  ✅ New best AP: 0.3704 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 43  loss 0.0002  val AP 0.3706\n",
      "  ✅ New best AP: 0.3706 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 44  loss 0.0002  val AP 0.3713\n",
      "  ✅ New best AP: 0.3713 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 45  loss 0.0002  val AP 0.3721\n",
      "  ✅ New best AP: 0.3721 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 46  loss 0.0002  val AP 0.3723\n",
      "  ✅ New best AP: 0.3723 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 47  loss 0.0002  val AP 0.3726\n",
      "  ✅ New best AP: 0.3726 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 48  loss 0.0002  val AP 0.3732\n",
      "  ✅ New best AP: 0.3732 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 49  loss 0.0002  val AP 0.3737\n",
      "  ✅ New best AP: 0.3737 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 47] ep 50  loss 0.0002  val AP 0.3740\n",
      "  ✅ New best AP: 0.3740 → v7\\model\\ensembles\\SR-MMP\\seed47\\best.pt\n",
      "[SR-MMP | seed 61] ep 01  loss 0.0006  val AP 0.1793\n",
      "  ✅ New best AP: 0.1793 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 02  loss 0.0003  val AP 0.1921\n",
      "  ✅ New best AP: 0.1921 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 03  loss 0.0004  val AP 0.2100\n",
      "  ✅ New best AP: 0.2100 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 04  loss 0.0004  val AP 0.2293\n",
      "  ✅ New best AP: 0.2293 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 05  loss 0.0003  val AP 0.2488\n",
      "  ✅ New best AP: 0.2488 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 06  loss 0.0003  val AP 0.2724\n",
      "  ✅ New best AP: 0.2724 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 07  loss 0.0003  val AP 0.2906\n",
      "  ✅ New best AP: 0.2906 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 08  loss 0.0003  val AP 0.3073\n",
      "  ✅ New best AP: 0.3073 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 09  loss 0.0003  val AP 0.3330\n",
      "  ✅ New best AP: 0.3330 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 10  loss 0.0003  val AP 0.3384\n",
      "  ✅ New best AP: 0.3384 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 11  loss 0.0003  val AP 0.3480\n",
      "  ✅ New best AP: 0.3480 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 12  loss 0.0002  val AP 0.3567\n",
      "  ✅ New best AP: 0.3567 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 13  loss 0.0003  val AP 0.3629\n",
      "  ✅ New best AP: 0.3629 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 14  loss 0.0002  val AP 0.3674\n",
      "  ✅ New best AP: 0.3674 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 15  loss 0.0002  val AP 0.3726\n",
      "  ✅ New best AP: 0.3726 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 16  loss 0.0002  val AP 0.3784\n",
      "  ✅ New best AP: 0.3784 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 17  loss 0.0002  val AP 0.3804\n",
      "  ✅ New best AP: 0.3804 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 18  loss 0.0003  val AP 0.3825\n",
      "  ✅ New best AP: 0.3825 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 19  loss 0.0002  val AP 0.3854\n",
      "  ✅ New best AP: 0.3854 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 20  loss 0.0002  val AP 0.3869\n",
      "  ✅ New best AP: 0.3869 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 21  loss 0.0002  val AP 0.3902\n",
      "  ✅ New best AP: 0.3902 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 22  loss 0.0002  val AP 0.3925\n",
      "  ✅ New best AP: 0.3925 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 23  loss 0.0002  val AP 0.3960\n",
      "  ✅ New best AP: 0.3960 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 24  loss 0.0002  val AP 0.3985\n",
      "  ✅ New best AP: 0.3985 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 25  loss 0.0002  val AP 0.4009\n",
      "  ✅ New best AP: 0.4009 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 26  loss 0.0002  val AP 0.4026\n",
      "  ✅ New best AP: 0.4026 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 27  loss 0.0002  val AP 0.4049\n",
      "  ✅ New best AP: 0.4049 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 28  loss 0.0002  val AP 0.4061\n",
      "  ✅ New best AP: 0.4061 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 29  loss 0.0002  val AP 0.4073\n",
      "  ✅ New best AP: 0.4073 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 30  loss 0.0002  val AP 0.4104\n",
      "  ✅ New best AP: 0.4104 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 31  loss 0.0002  val AP 0.4109\n",
      "  ✅ New best AP: 0.4109 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 32  loss 0.0002  val AP 0.4124\n",
      "  ✅ New best AP: 0.4124 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 33  loss 0.0002  val AP 0.4144\n",
      "  ✅ New best AP: 0.4144 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 34  loss 0.0002  val AP 0.4157\n",
      "  ✅ New best AP: 0.4157 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 35  loss 0.0002  val AP 0.4164\n",
      "  ✅ New best AP: 0.4164 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 36  loss 0.0002  val AP 0.4161\n",
      "[SR-MMP | seed 61] ep 37  loss 0.0002  val AP 0.4169\n",
      "  ✅ New best AP: 0.4169 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 38  loss 0.0002  val AP 0.4193\n",
      "  ✅ New best AP: 0.4193 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 39  loss 0.0002  val AP 0.4178\n",
      "[SR-MMP | seed 61] ep 40  loss 0.0002  val AP 0.4184\n",
      "[SR-MMP | seed 61] ep 41  loss 0.0002  val AP 0.4195\n",
      "  ✅ New best AP: 0.4195 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 42  loss 0.0002  val AP 0.4174\n",
      "[SR-MMP | seed 61] ep 43  loss 0.0002  val AP 0.4182\n",
      "[SR-MMP | seed 61] ep 44  loss 0.0002  val AP 0.4190\n",
      "[SR-MMP | seed 61] ep 45  loss 0.0002  val AP 0.4198\n",
      "  ✅ New best AP: 0.4198 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 46  loss 0.0002  val AP 0.4206\n",
      "  ✅ New best AP: 0.4206 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 47  loss 0.0002  val AP 0.4217\n",
      "  ✅ New best AP: 0.4217 → v7\\model\\ensembles\\SR-MMP\\seed61\\best.pt\n",
      "[SR-MMP | seed 61] ep 48  loss 0.0002  val AP 0.4211\n",
      "[SR-MMP | seed 61] ep 49  loss 0.0002  val AP 0.4210\n",
      "[SR-MMP | seed 61] ep 50  loss 0.0002  val AP 0.4212\n",
      "[SR-MMP | seed 83] ep 01  loss 0.0004  val AP 0.1483\n",
      "  ✅ New best AP: 0.1483 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 02  loss 0.0003  val AP 0.1554\n",
      "  ✅ New best AP: 0.1554 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 03  loss 0.0003  val AP 0.1683\n",
      "  ✅ New best AP: 0.1683 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 04  loss 0.0004  val AP 0.1846\n",
      "  ✅ New best AP: 0.1846 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 05  loss 0.0003  val AP 0.2074\n",
      "  ✅ New best AP: 0.2074 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 06  loss 0.0003  val AP 0.2324\n",
      "  ✅ New best AP: 0.2324 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 07  loss 0.0003  val AP 0.2515\n",
      "  ✅ New best AP: 0.2515 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 08  loss 0.0003  val AP 0.2768\n",
      "  ✅ New best AP: 0.2768 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 09  loss 0.0003  val AP 0.2942\n",
      "  ✅ New best AP: 0.2942 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 10  loss 0.0002  val AP 0.3047\n",
      "  ✅ New best AP: 0.3047 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 11  loss 0.0002  val AP 0.3149\n",
      "  ✅ New best AP: 0.3149 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 12  loss 0.0003  val AP 0.3270\n",
      "  ✅ New best AP: 0.3270 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 13  loss 0.0003  val AP 0.3326\n",
      "  ✅ New best AP: 0.3326 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 14  loss 0.0003  val AP 0.3398\n",
      "  ✅ New best AP: 0.3398 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 15  loss 0.0002  val AP 0.3475\n",
      "  ✅ New best AP: 0.3475 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 16  loss 0.0002  val AP 0.3527\n",
      "  ✅ New best AP: 0.3527 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 17  loss 0.0003  val AP 0.3570\n",
      "  ✅ New best AP: 0.3570 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 18  loss 0.0002  val AP 0.3660\n",
      "  ✅ New best AP: 0.3660 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 19  loss 0.0002  val AP 0.3747\n",
      "  ✅ New best AP: 0.3747 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 20  loss 0.0002  val AP 0.3731\n",
      "[SR-MMP | seed 83] ep 21  loss 0.0002  val AP 0.3733\n",
      "[SR-MMP | seed 83] ep 22  loss 0.0003  val AP 0.3787\n",
      "  ✅ New best AP: 0.3787 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 23  loss 0.0002  val AP 0.3803\n",
      "  ✅ New best AP: 0.3803 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 24  loss 0.0002  val AP 0.3838\n",
      "  ✅ New best AP: 0.3838 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 25  loss 0.0002  val AP 0.3883\n",
      "  ✅ New best AP: 0.3883 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 26  loss 0.0002  val AP 0.3890\n",
      "  ✅ New best AP: 0.3890 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 27  loss 0.0002  val AP 0.3920\n",
      "  ✅ New best AP: 0.3920 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 28  loss 0.0002  val AP 0.3916\n",
      "[SR-MMP | seed 83] ep 29  loss 0.0002  val AP 0.3916\n",
      "[SR-MMP | seed 83] ep 30  loss 0.0002  val AP 0.3928\n",
      "  ✅ New best AP: 0.3928 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 31  loss 0.0002  val AP 0.3935\n",
      "  ✅ New best AP: 0.3935 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 32  loss 0.0002  val AP 0.3936\n",
      "  ✅ New best AP: 0.3936 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 33  loss 0.0002  val AP 0.3936\n",
      "  ✅ New best AP: 0.3936 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 34  loss 0.0002  val AP 0.3956\n",
      "  ✅ New best AP: 0.3956 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 35  loss 0.0002  val AP 0.3983\n",
      "  ✅ New best AP: 0.3983 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 36  loss 0.0002  val AP 0.3986\n",
      "  ✅ New best AP: 0.3986 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 37  loss 0.0002  val AP 0.3992\n",
      "  ✅ New best AP: 0.3992 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 38  loss 0.0002  val AP 0.4016\n",
      "  ✅ New best AP: 0.4016 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 39  loss 0.0002  val AP 0.4022\n",
      "  ✅ New best AP: 0.4022 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 40  loss 0.0002  val AP 0.4032\n",
      "  ✅ New best AP: 0.4032 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 41  loss 0.0002  val AP 0.4049\n",
      "  ✅ New best AP: 0.4049 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 42  loss 0.0002  val AP 0.4075\n",
      "  ✅ New best AP: 0.4075 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 43  loss 0.0002  val AP 0.4077\n",
      "  ✅ New best AP: 0.4077 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 44  loss 0.0002  val AP 0.4079\n",
      "  ✅ New best AP: 0.4079 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 45  loss 0.0002  val AP 0.4081\n",
      "  ✅ New best AP: 0.4081 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 46  loss 0.0002  val AP 0.4082\n",
      "  ✅ New best AP: 0.4082 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 47  loss 0.0002  val AP 0.4083\n",
      "  ✅ New best AP: 0.4083 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 48  loss 0.0002  val AP 0.4093\n",
      "  ✅ New best AP: 0.4093 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 49  loss 0.0002  val AP 0.4096\n",
      "  ✅ New best AP: 0.4096 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "[SR-MMP | seed 83] ep 50  loss 0.0002  val AP 0.4103\n",
      "  ✅ New best AP: 0.4103 → v7\\model\\ensembles\\SR-MMP\\seed83\\best.pt\n",
      "\n",
      "==============================\n",
      "Training specialist heads for: SR-p53 (label 11)\n",
      "==============================\n",
      "[SR-p53 | seed 13] ep 01  loss 0.0007  val AP 0.0549\n",
      "  ✅ New best AP: 0.0549 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 02  loss 0.0005  val AP 0.0562\n",
      "  ✅ New best AP: 0.0562 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 03  loss 0.0005  val AP 0.0586\n",
      "  ✅ New best AP: 0.0586 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 04  loss 0.0005  val AP 0.0628\n",
      "  ✅ New best AP: 0.0628 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 05  loss 0.0006  val AP 0.0703\n",
      "  ✅ New best AP: 0.0703 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 06  loss 0.0005  val AP 0.0833\n",
      "  ✅ New best AP: 0.0833 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 07  loss 0.0006  val AP 0.1088\n",
      "  ✅ New best AP: 0.1088 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 08  loss 0.0005  val AP 0.1189\n",
      "  ✅ New best AP: 0.1189 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 09  loss 0.0007  val AP 0.1316\n",
      "  ✅ New best AP: 0.1316 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 10  loss 0.0005  val AP 0.1348\n",
      "  ✅ New best AP: 0.1348 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 11  loss 0.0005  val AP 0.1495\n",
      "  ✅ New best AP: 0.1495 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 12  loss 0.0004  val AP 0.1540\n",
      "  ✅ New best AP: 0.1540 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 13  loss 0.0005  val AP 0.1629\n",
      "  ✅ New best AP: 0.1629 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 14  loss 0.0004  val AP 0.1873\n",
      "  ✅ New best AP: 0.1873 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 15  loss 0.0008  val AP 0.1936\n",
      "  ✅ New best AP: 0.1936 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 16  loss 0.0005  val AP 0.1974\n",
      "  ✅ New best AP: 0.1974 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 17  loss 0.0005  val AP 0.2032\n",
      "  ✅ New best AP: 0.2032 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 18  loss 0.0004  val AP 0.2091\n",
      "  ✅ New best AP: 0.2091 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 19  loss 0.0004  val AP 0.2098\n",
      "  ✅ New best AP: 0.2098 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 20  loss 0.0004  val AP 0.2093\n",
      "[SR-p53 | seed 13] ep 21  loss 0.0005  val AP 0.2107\n",
      "  ✅ New best AP: 0.2107 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 22  loss 0.0004  val AP 0.2117\n",
      "  ✅ New best AP: 0.2117 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 23  loss 0.0004  val AP 0.2102\n",
      "[SR-p53 | seed 13] ep 24  loss 0.0005  val AP 0.2108\n",
      "[SR-p53 | seed 13] ep 25  loss 0.0004  val AP 0.2111\n",
      "[SR-p53 | seed 13] ep 26  loss 0.0004  val AP 0.2122\n",
      "  ✅ New best AP: 0.2122 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 27  loss 0.0004  val AP 0.2129\n",
      "  ✅ New best AP: 0.2129 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 28  loss 0.0005  val AP 0.2083\n",
      "[SR-p53 | seed 13] ep 29  loss 0.0004  val AP 0.2108\n",
      "[SR-p53 | seed 13] ep 30  loss 0.0004  val AP 0.2110\n",
      "[SR-p53 | seed 13] ep 31  loss 0.0004  val AP 0.2110\n",
      "[SR-p53 | seed 13] ep 32  loss 0.0004  val AP 0.2114\n",
      "[SR-p53 | seed 13] ep 33  loss 0.0004  val AP 0.2122\n",
      "[SR-p53 | seed 13] ep 34  loss 0.0004  val AP 0.2122\n",
      "[SR-p53 | seed 13] ep 35  loss 0.0004  val AP 0.2131\n",
      "  ✅ New best AP: 0.2131 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 36  loss 0.0004  val AP 0.2147\n",
      "  ✅ New best AP: 0.2147 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 37  loss 0.0004  val AP 0.2151\n",
      "  ✅ New best AP: 0.2151 → v7\\model\\ensembles\\SR-p53\\seed13\\best.pt\n",
      "[SR-p53 | seed 13] ep 38  loss 0.0004  val AP 0.2138\n",
      "[SR-p53 | seed 13] ep 39  loss 0.0004  val AP 0.2138\n",
      "[SR-p53 | seed 13] ep 40  loss 0.0004  val AP 0.2132\n",
      "[SR-p53 | seed 13] ep 41  loss 0.0004  val AP 0.2127\n",
      "[SR-p53 | seed 13] ep 42  loss 0.0004  val AP 0.2128\n",
      "[SR-p53 | seed 13] ep 43  loss 0.0004  val AP 0.2130\n",
      "[SR-p53 | seed 13] ep 44  loss 0.0004  val AP 0.2129\n",
      "[SR-p53 | seed 13] ep 45  loss 0.0004  val AP 0.2127\n",
      "[SR-p53 | seed 13] ep 46  loss 0.0004  val AP 0.2138\n",
      "[SR-p53 | seed 13] ep 47  loss 0.0004  val AP 0.2134\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.2151\n",
      "[SR-p53 | seed 29] ep 01  loss 0.0008  val AP 0.1221\n",
      "  ✅ New best AP: 0.1221 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 02  loss 0.0006  val AP 0.1295\n",
      "  ✅ New best AP: 0.1295 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 03  loss 0.0007  val AP 0.1409\n",
      "  ✅ New best AP: 0.1409 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 04  loss 0.0007  val AP 0.1517\n",
      "  ✅ New best AP: 0.1517 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 05  loss 0.0006  val AP 0.1620\n",
      "  ✅ New best AP: 0.1620 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 06  loss 0.0005  val AP 0.1751\n",
      "  ✅ New best AP: 0.1751 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 07  loss 0.0005  val AP 0.1775\n",
      "  ✅ New best AP: 0.1775 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 08  loss 0.0006  val AP 0.1770\n",
      "[SR-p53 | seed 29] ep 09  loss 0.0005  val AP 0.1796\n",
      "  ✅ New best AP: 0.1796 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 10  loss 0.0006  val AP 0.1823\n",
      "  ✅ New best AP: 0.1823 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 11  loss 0.0005  val AP 0.1877\n",
      "  ✅ New best AP: 0.1877 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 12  loss 0.0005  val AP 0.1970\n",
      "  ✅ New best AP: 0.1970 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 13  loss 0.0005  val AP 0.1984\n",
      "  ✅ New best AP: 0.1984 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 14  loss 0.0005  val AP 0.2020\n",
      "  ✅ New best AP: 0.2020 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 15  loss 0.0005  val AP 0.2079\n",
      "  ✅ New best AP: 0.2079 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 16  loss 0.0004  val AP 0.2094\n",
      "  ✅ New best AP: 0.2094 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 17  loss 0.0005  val AP 0.2193\n",
      "  ✅ New best AP: 0.2193 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 18  loss 0.0005  val AP 0.2230\n",
      "  ✅ New best AP: 0.2230 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 19  loss 0.0005  val AP 0.2284\n",
      "  ✅ New best AP: 0.2284 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 20  loss 0.0004  val AP 0.2315\n",
      "  ✅ New best AP: 0.2315 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 21  loss 0.0005  val AP 0.2234\n",
      "[SR-p53 | seed 29] ep 22  loss 0.0005  val AP 0.2241\n",
      "[SR-p53 | seed 29] ep 23  loss 0.0004  val AP 0.2266\n",
      "[SR-p53 | seed 29] ep 24  loss 0.0004  val AP 0.2311\n",
      "[SR-p53 | seed 29] ep 25  loss 0.0005  val AP 0.2334\n",
      "  ✅ New best AP: 0.2334 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 26  loss 0.0004  val AP 0.2346\n",
      "  ✅ New best AP: 0.2346 → v7\\model\\ensembles\\SR-p53\\seed29\\best.pt\n",
      "[SR-p53 | seed 29] ep 27  loss 0.0005  val AP 0.2308\n",
      "[SR-p53 | seed 29] ep 28  loss 0.0004  val AP 0.2303\n",
      "[SR-p53 | seed 29] ep 29  loss 0.0004  val AP 0.2291\n",
      "[SR-p53 | seed 29] ep 30  loss 0.0004  val AP 0.2302\n",
      "[SR-p53 | seed 29] ep 31  loss 0.0004  val AP 0.2261\n",
      "[SR-p53 | seed 29] ep 32  loss 0.0004  val AP 0.2268\n",
      "[SR-p53 | seed 29] ep 33  loss 0.0004  val AP 0.2270\n",
      "[SR-p53 | seed 29] ep 34  loss 0.0004  val AP 0.2267\n",
      "[SR-p53 | seed 29] ep 35  loss 0.0005  val AP 0.2269\n",
      "[SR-p53 | seed 29] ep 36  loss 0.0004  val AP 0.2231\n",
      "  ⏹ Early stop (no improve for 10 epochs). Best AP: 0.2346\n",
      "[SR-p53 | seed 47] ep 01  loss 0.0008  val AP 0.0996\n",
      "  ✅ New best AP: 0.0996 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 02  loss 0.0005  val AP 0.1046\n",
      "  ✅ New best AP: 0.1046 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 03  loss 0.0007  val AP 0.1093\n",
      "  ✅ New best AP: 0.1093 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 04  loss 0.0007  val AP 0.1155\n",
      "  ✅ New best AP: 0.1155 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 05  loss 0.0006  val AP 0.1225\n",
      "  ✅ New best AP: 0.1225 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 06  loss 0.0005  val AP 0.1251\n",
      "  ✅ New best AP: 0.1251 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 07  loss 0.0006  val AP 0.1280\n",
      "  ✅ New best AP: 0.1280 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 08  loss 0.0006  val AP 0.1304\n",
      "  ✅ New best AP: 0.1304 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 09  loss 0.0006  val AP 0.1315\n",
      "  ✅ New best AP: 0.1315 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 10  loss 0.0005  val AP 0.1351\n",
      "  ✅ New best AP: 0.1351 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 11  loss 0.0005  val AP 0.1348\n",
      "[SR-p53 | seed 47] ep 12  loss 0.0004  val AP 0.1356\n",
      "  ✅ New best AP: 0.1356 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 13  loss 0.0006  val AP 0.1377\n",
      "  ✅ New best AP: 0.1377 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 14  loss 0.0005  val AP 0.1378\n",
      "  ✅ New best AP: 0.1378 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 15  loss 0.0005  val AP 0.1409\n",
      "  ✅ New best AP: 0.1409 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 16  loss 0.0004  val AP 0.1411\n",
      "  ✅ New best AP: 0.1411 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 17  loss 0.0004  val AP 0.1408\n",
      "[SR-p53 | seed 47] ep 18  loss 0.0004  val AP 0.1395\n",
      "[SR-p53 | seed 47] ep 19  loss 0.0006  val AP 0.1436\n",
      "  ✅ New best AP: 0.1436 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 20  loss 0.0004  val AP 0.1445\n",
      "  ✅ New best AP: 0.1445 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 21  loss 0.0004  val AP 0.1452\n",
      "  ✅ New best AP: 0.1452 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 22  loss 0.0004  val AP 0.1462\n",
      "  ✅ New best AP: 0.1462 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 23  loss 0.0004  val AP 0.1465\n",
      "  ✅ New best AP: 0.1465 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 24  loss 0.0005  val AP 0.1473\n",
      "  ✅ New best AP: 0.1473 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 25  loss 0.0004  val AP 0.1465\n",
      "[SR-p53 | seed 47] ep 26  loss 0.0004  val AP 0.1461\n",
      "[SR-p53 | seed 47] ep 27  loss 0.0004  val AP 0.1468\n",
      "[SR-p53 | seed 47] ep 28  loss 0.0004  val AP 0.1471\n",
      "[SR-p53 | seed 47] ep 29  loss 0.0004  val AP 0.1475\n",
      "  ✅ New best AP: 0.1475 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 30  loss 0.0004  val AP 0.1480\n",
      "  ✅ New best AP: 0.1480 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 31  loss 0.0004  val AP 0.1488\n",
      "  ✅ New best AP: 0.1488 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 32  loss 0.0004  val AP 0.1487\n",
      "[SR-p53 | seed 47] ep 33  loss 0.0004  val AP 0.1489\n",
      "  ✅ New best AP: 0.1489 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 34  loss 0.0004  val AP 0.1484\n",
      "[SR-p53 | seed 47] ep 35  loss 0.0004  val AP 0.1487\n",
      "[SR-p53 | seed 47] ep 36  loss 0.0004  val AP 0.1491\n",
      "  ✅ New best AP: 0.1491 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 37  loss 0.0004  val AP 0.1495\n",
      "  ✅ New best AP: 0.1495 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 38  loss 0.0004  val AP 0.1498\n",
      "  ✅ New best AP: 0.1498 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 39  loss 0.0004  val AP 0.1499\n",
      "  ✅ New best AP: 0.1499 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 40  loss 0.0004  val AP 0.1500\n",
      "  ✅ New best AP: 0.1500 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 41  loss 0.0004  val AP 0.1502\n",
      "  ✅ New best AP: 0.1502 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 42  loss 0.0004  val AP 0.1503\n",
      "  ✅ New best AP: 0.1503 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 43  loss 0.0004  val AP 0.1505\n",
      "  ✅ New best AP: 0.1505 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 44  loss 0.0004  val AP 0.1511\n",
      "  ✅ New best AP: 0.1511 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 45  loss 0.0004  val AP 0.1509\n",
      "[SR-p53 | seed 47] ep 46  loss 0.0004  val AP 0.1510\n",
      "[SR-p53 | seed 47] ep 47  loss 0.0005  val AP 0.1511\n",
      "  ✅ New best AP: 0.1511 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 48  loss 0.0004  val AP 0.1515\n",
      "  ✅ New best AP: 0.1515 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 49  loss 0.0005  val AP 0.1516\n",
      "  ✅ New best AP: 0.1516 → v7\\model\\ensembles\\SR-p53\\seed47\\best.pt\n",
      "[SR-p53 | seed 47] ep 50  loss 0.0004  val AP 0.1515\n",
      "[SR-p53 | seed 61] ep 01  loss 0.0010  val AP 0.0629\n",
      "  ✅ New best AP: 0.0629 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 02  loss 0.0006  val AP 0.0678\n",
      "  ✅ New best AP: 0.0678 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 03  loss 0.0007  val AP 0.0759\n",
      "  ✅ New best AP: 0.0759 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 04  loss 0.0007  val AP 0.0856\n",
      "  ✅ New best AP: 0.0856 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 05  loss 0.0006  val AP 0.0974\n",
      "  ✅ New best AP: 0.0974 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 06  loss 0.0006  val AP 0.1156\n",
      "  ✅ New best AP: 0.1156 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 07  loss 0.0006  val AP 0.1216\n",
      "  ✅ New best AP: 0.1216 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 08  loss 0.0007  val AP 0.1294\n",
      "  ✅ New best AP: 0.1294 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 09  loss 0.0006  val AP 0.1427\n",
      "  ✅ New best AP: 0.1427 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 10  loss 0.0006  val AP 0.1491\n",
      "  ✅ New best AP: 0.1491 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 11  loss 0.0005  val AP 0.1488\n",
      "[SR-p53 | seed 61] ep 12  loss 0.0005  val AP 0.1520\n",
      "  ✅ New best AP: 0.1520 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 13  loss 0.0005  val AP 0.1548\n",
      "  ✅ New best AP: 0.1548 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 14  loss 0.0005  val AP 0.1580\n",
      "  ✅ New best AP: 0.1580 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 15  loss 0.0005  val AP 0.1600\n",
      "  ✅ New best AP: 0.1600 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 16  loss 0.0005  val AP 0.1613\n",
      "  ✅ New best AP: 0.1613 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 17  loss 0.0005  val AP 0.1644\n",
      "  ✅ New best AP: 0.1644 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 18  loss 0.0005  val AP 0.1671\n",
      "  ✅ New best AP: 0.1671 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 19  loss 0.0005  val AP 0.1670\n",
      "[SR-p53 | seed 61] ep 20  loss 0.0005  val AP 0.1673\n",
      "  ✅ New best AP: 0.1673 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 21  loss 0.0005  val AP 0.1689\n",
      "  ✅ New best AP: 0.1689 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 22  loss 0.0004  val AP 0.1709\n",
      "  ✅ New best AP: 0.1709 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 23  loss 0.0004  val AP 0.1713\n",
      "  ✅ New best AP: 0.1713 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 24  loss 0.0004  val AP 0.1720\n",
      "  ✅ New best AP: 0.1720 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 25  loss 0.0004  val AP 0.1737\n",
      "  ✅ New best AP: 0.1737 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 26  loss 0.0004  val AP 0.1752\n",
      "  ✅ New best AP: 0.1752 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 27  loss 0.0004  val AP 0.1762\n",
      "  ✅ New best AP: 0.1762 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 28  loss 0.0004  val AP 0.1764\n",
      "  ✅ New best AP: 0.1764 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 29  loss 0.0005  val AP 0.1758\n",
      "[SR-p53 | seed 61] ep 30  loss 0.0004  val AP 0.1764\n",
      "[SR-p53 | seed 61] ep 31  loss 0.0004  val AP 0.1778\n",
      "  ✅ New best AP: 0.1778 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 32  loss 0.0004  val AP 0.1798\n",
      "  ✅ New best AP: 0.1798 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 33  loss 0.0004  val AP 0.1796\n",
      "[SR-p53 | seed 61] ep 34  loss 0.0004  val AP 0.1792\n",
      "[SR-p53 | seed 61] ep 35  loss 0.0004  val AP 0.1796\n",
      "[SR-p53 | seed 61] ep 36  loss 0.0004  val AP 0.1794\n",
      "[SR-p53 | seed 61] ep 37  loss 0.0004  val AP 0.1795\n",
      "[SR-p53 | seed 61] ep 38  loss 0.0005  val AP 0.1792\n",
      "[SR-p53 | seed 61] ep 39  loss 0.0004  val AP 0.1795\n",
      "[SR-p53 | seed 61] ep 40  loss 0.0004  val AP 0.1800\n",
      "  ✅ New best AP: 0.1800 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 41  loss 0.0004  val AP 0.1809\n",
      "  ✅ New best AP: 0.1809 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 42  loss 0.0004  val AP 0.1810\n",
      "  ✅ New best AP: 0.1810 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 43  loss 0.0004  val AP 0.1807\n",
      "[SR-p53 | seed 61] ep 44  loss 0.0004  val AP 0.1826\n",
      "  ✅ New best AP: 0.1826 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 45  loss 0.0004  val AP 0.1831\n",
      "  ✅ New best AP: 0.1831 → v7\\model\\ensembles\\SR-p53\\seed61\\best.pt\n",
      "[SR-p53 | seed 61] ep 46  loss 0.0004  val AP 0.1829\n",
      "[SR-p53 | seed 61] ep 47  loss 0.0004  val AP 0.1831\n",
      "[SR-p53 | seed 61] ep 48  loss 0.0005  val AP 0.1829\n",
      "[SR-p53 | seed 61] ep 49  loss 0.0004  val AP 0.1831\n",
      "[SR-p53 | seed 61] ep 50  loss 0.0004  val AP 0.1831\n",
      "[SR-p53 | seed 83] ep 01  loss 0.0008  val AP 0.0693\n",
      "  ✅ New best AP: 0.0693 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 02  loss 0.0005  val AP 0.0716\n",
      "  ✅ New best AP: 0.0716 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 03  loss 0.0006  val AP 0.0749\n",
      "  ✅ New best AP: 0.0749 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 04  loss 0.0006  val AP 0.0811\n",
      "  ✅ New best AP: 0.0811 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 05  loss 0.0006  val AP 0.0904\n",
      "  ✅ New best AP: 0.0904 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 06  loss 0.0005  val AP 0.1019\n",
      "  ✅ New best AP: 0.1019 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 07  loss 0.0006  val AP 0.1121\n",
      "  ✅ New best AP: 0.1121 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 08  loss 0.0005  val AP 0.1216\n",
      "  ✅ New best AP: 0.1216 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 09  loss 0.0006  val AP 0.1272\n",
      "  ✅ New best AP: 0.1272 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 10  loss 0.0005  val AP 0.1350\n",
      "  ✅ New best AP: 0.1350 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 11  loss 0.0005  val AP 0.1441\n",
      "  ✅ New best AP: 0.1441 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 12  loss 0.0004  val AP 0.1509\n",
      "  ✅ New best AP: 0.1509 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 13  loss 0.0005  val AP 0.1540\n",
      "  ✅ New best AP: 0.1540 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 14  loss 0.0004  val AP 0.1597\n",
      "  ✅ New best AP: 0.1597 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 15  loss 0.0005  val AP 0.1733\n",
      "  ✅ New best AP: 0.1733 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 16  loss 0.0004  val AP 0.1799\n",
      "  ✅ New best AP: 0.1799 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 17  loss 0.0005  val AP 0.1828\n",
      "  ✅ New best AP: 0.1828 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 18  loss 0.0004  val AP 0.1820\n",
      "[SR-p53 | seed 83] ep 19  loss 0.0005  val AP 0.1834\n",
      "  ✅ New best AP: 0.1834 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 20  loss 0.0005  val AP 0.1842\n",
      "  ✅ New best AP: 0.1842 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 21  loss 0.0005  val AP 0.1842\n",
      "  ✅ New best AP: 0.1842 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 22  loss 0.0004  val AP 0.1867\n",
      "  ✅ New best AP: 0.1867 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 23  loss 0.0004  val AP 0.1875\n",
      "  ✅ New best AP: 0.1875 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 24  loss 0.0004  val AP 0.1886\n",
      "  ✅ New best AP: 0.1886 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 25  loss 0.0005  val AP 0.1892\n",
      "  ✅ New best AP: 0.1892 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 26  loss 0.0004  val AP 0.1869\n",
      "[SR-p53 | seed 83] ep 27  loss 0.0004  val AP 0.1877\n",
      "[SR-p53 | seed 83] ep 28  loss 0.0004  val AP 0.1882\n",
      "[SR-p53 | seed 83] ep 29  loss 0.0004  val AP 0.1894\n",
      "  ✅ New best AP: 0.1894 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 30  loss 0.0004  val AP 0.1874\n",
      "[SR-p53 | seed 83] ep 31  loss 0.0004  val AP 0.1873\n",
      "[SR-p53 | seed 83] ep 32  loss 0.0004  val AP 0.1873\n",
      "[SR-p53 | seed 83] ep 33  loss 0.0004  val AP 0.1881\n",
      "[SR-p53 | seed 83] ep 34  loss 0.0004  val AP 0.1890\n",
      "[SR-p53 | seed 83] ep 35  loss 0.0004  val AP 0.1895\n",
      "  ✅ New best AP: 0.1895 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 36  loss 0.0004  val AP 0.1903\n",
      "  ✅ New best AP: 0.1903 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 37  loss 0.0004  val AP 0.1904\n",
      "  ✅ New best AP: 0.1904 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 38  loss 0.0004  val AP 0.1905\n",
      "  ✅ New best AP: 0.1905 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 39  loss 0.0004  val AP 0.1901\n",
      "[SR-p53 | seed 83] ep 40  loss 0.0004  val AP 0.1906\n",
      "  ✅ New best AP: 0.1906 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 41  loss 0.0004  val AP 0.1911\n",
      "  ✅ New best AP: 0.1911 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 42  loss 0.0004  val AP 0.1913\n",
      "  ✅ New best AP: 0.1913 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 43  loss 0.0004  val AP 0.1916\n",
      "  ✅ New best AP: 0.1916 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 44  loss 0.0004  val AP 0.1916\n",
      "  ✅ New best AP: 0.1916 → v7\\model\\ensembles\\SR-p53\\seed83\\best.pt\n",
      "[SR-p53 | seed 83] ep 45  loss 0.0004  val AP 0.1883\n",
      "[SR-p53 | seed 83] ep 46  loss 0.0004  val AP 0.1884\n",
      "[SR-p53 | seed 83] ep 47  loss 0.0004  val AP 0.1885\n",
      "[SR-p53 | seed 83] ep 48  loss 0.0004  val AP 0.1889\n",
      "[SR-p53 | seed 83] ep 49  loss 0.0004  val AP 0.1887\n",
      "[SR-p53 | seed 83] ep 50  loss 0.0005  val AP 0.1889\n",
      "\n",
      "✅ Ensemble training complete.\n",
      "{\n",
      "  \"best_per_label\": {\n",
      "    \"NR-AR\": {\n",
      "      \"best_ap\": 0.17695605929185818\n",
      "    },\n",
      "    \"NR-AR-LBD\": {\n",
      "      \"best_ap\": 0.30067178313078136\n",
      "    },\n",
      "    \"NR-AhR\": {\n",
      "      \"best_ap\": 0.5251213928312277\n",
      "    },\n",
      "    \"NR-Aromatase\": {\n",
      "      \"best_ap\": 0.27536501073172526\n",
      "    },\n",
      "    \"NR-ER\": {\n",
      "      \"best_ap\": 0.23092139236215176\n",
      "    },\n",
      "    \"NR-ER-LBD\": {\n",
      "      \"best_ap\": 0.15266075493878553\n",
      "    },\n",
      "    \"NR-PPAR-gamma\": {\n",
      "      \"best_ap\": 0.09182322445982688\n",
      "    },\n",
      "    \"SR-ARE\": {\n",
      "      \"best_ap\": 0.33600945789867853\n",
      "    },\n",
      "    \"SR-ATAD5\": {\n",
      "      \"best_ap\": 0.22774289601379685\n",
      "    },\n",
      "    \"SR-HSE\": {\n",
      "      \"best_ap\": 0.23226244074941663\n",
      "    },\n",
      "    \"SR-MMP\": {\n",
      "      \"best_ap\": 0.4462153762698644\n",
      "    },\n",
      "    \"SR-p53\": {\n",
      "      \"best_ap\": 0.23462777931161904\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time, random, platform\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# ------------------------------\n",
    "# Paths & globals\n",
    "# ------------------------------\n",
    "BASE_DIR     = Path(\"v7\")\n",
    "DATA_PREP    = BASE_DIR / \"data\" / \"prepared\"\n",
    "FUSED_DIR    = BASE_DIR / \"data\" / \"fused\"\n",
    "FUSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ENSEMBLE_DIR = BASE_DIR / \"model\" / \"ensembles\"\n",
    "CKPT_SHARED  = BASE_DIR / \"model\" / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Expect shared model & helpers from earlier cells:\n",
    "assert CKPT_SHARED.exists(), \"Shared best checkpoint not found. Run Phase 3 Cell 1 (and 1c optional) first.\"\n",
    "\n",
    "# ------------------------------\n",
    "# Utilities / reproducibility\n",
    "# ------------------------------\n",
    "def seed_everything(seed: int):\n",
    "    import numpy as _np, random as _r, torch as _t\n",
    "    _r.seed(seed); _np.random.seed(seed)\n",
    "    _t.manual_seed(seed); _t.cuda.manual_seed_all(seed)\n",
    "\n",
    "try:\n",
    "    LABEL_NAMES\n",
    "    v7_shared\n",
    "except NameError:\n",
    "    raise RuntimeError(\"Missing v7_shared or LABEL_NAMES in memory. Please re-run Phase 2 Cells 1–4 and Phase 3 Cell 1.\")\n",
    "\n",
    "# masked_mean fallback (if not in scope)\n",
    "try:\n",
    "    masked_mean\n",
    "except NameError:\n",
    "    def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "        mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "        denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "        return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "# ------------------------------\n",
    "# Restore best shared weights & set eval\n",
    "# ------------------------------\n",
    "ckpt = torch.load(CKPT_SHARED, map_location=DEVICE)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "v7_shared.eval()\n",
    "\n",
    "# ------------------------------\n",
    "# Fused cache helpers\n",
    "# ------------------------------\n",
    "@torch.no_grad()\n",
    "def compute_fused_batch(smiles_list: List[str], desc_feats: torch.Tensor) -> torch.Tensor:\n",
    "    tt, tm = v7_shared.text_encoder(smiles_list, max_length=256)\n",
    "    gn, gm = v7_shared.graph_encoder(smiles_list, max_nodes=128)\n",
    "    tt, tm = tt.to(DEVICE), tm.to(DEVICE)\n",
    "    gn, gm = gn.to(DEVICE), gm.to(DEVICE)\n",
    "    desc_feats = desc_feats.to(DEVICE)\n",
    "    tta = v7_shared.cross(tt, tm, gn, gm)\n",
    "    de  = v7_shared.desc_mlp(desc_feats)\n",
    "    text_pool  = masked_mean(tta, tm, dim=1)\n",
    "    graph_pool = masked_mean(gn,  gm, dim=1)\n",
    "    fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B, 768)\n",
    "    return fused\n",
    "\n",
    "@torch.no_grad()\n",
    "def cache_fused(npz_path: Path, out_prefix: str, batch_size: int = 256):\n",
    "    blob = np.load(npz_path, allow_pickle=True)\n",
    "    smiles = [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "    Xd    = torch.tensor(blob[\"X\"], dtype=torch.float32, device=DEVICE)\n",
    "    Y     = blob[\"Y\"].astype(np.float32)\n",
    "    M     = blob[\"y_missing_mask\"].astype(bool)\n",
    "    molid = blob[\"mol_id\"].tolist()\n",
    "\n",
    "    fused_list = []\n",
    "    for i in range(0, len(smiles), batch_size):\n",
    "        fu = compute_fused_batch(smiles[i:i+batch_size], Xd[i:i+batch_size])\n",
    "        fused_list.append(fu.cpu().numpy())\n",
    "    F = np.concatenate(fused_list, axis=0).astype(np.float32)\n",
    "\n",
    "    np.save(FUSED_DIR / f\"{out_prefix}_fused.npy\", F)\n",
    "    np.save(FUSED_DIR / f\"{out_prefix}_Y.npy\",    Y)\n",
    "    np.save(FUSED_DIR / f\"{out_prefix}_mask.npy\", M)\n",
    "    np.save(FUSED_DIR / f\"{out_prefix}_mol_id.npy\", np.array(molid, dtype=object))\n",
    "    print(f\"Cached {out_prefix}: fused {F.shape}\")\n",
    "\n",
    "# Build/refresh caches if missing\n",
    "if not (FUSED_DIR / \"train_fused.npy\").exists():\n",
    "    cache_fused(DATA_PREP / \"train.npz\", \"train\", batch_size=256)\n",
    "if not (FUSED_DIR / \"val_fused.npy\").exists():\n",
    "    cache_fused(DATA_PREP / \"val.npz\", \"val\", batch_size=256)\n",
    "\n",
    "# ------------------------------\n",
    "# Load caches into memory\n",
    "# ------------------------------\n",
    "Xtr = np.load(FUSED_DIR / \"train_fused.npy\")\n",
    "Ytr = np.load(FUSED_DIR / \"train_Y.npy\")\n",
    "Mtr = np.load(FUSED_DIR / \"train_mask.npy\")\n",
    "Xva = np.load(FUSED_DIR / \"val_fused.npy\")\n",
    "Yva = np.load(FUSED_DIR / \"val_Y.npy\")\n",
    "Mva = np.load(FUSED_DIR / \"val_mask.npy\")\n",
    "print(\"Fused shapes → train:\", Xtr.shape, \"| val:\", Xva.shape)\n",
    "\n",
    "# ------------------------------\n",
    "# Dataset & balanced sampler\n",
    "# ------------------------------\n",
    "class FusedLabelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X: np.ndarray, Y: np.ndarray, M: np.ndarray, j: int):\n",
    "        valid = ~M[:, j]\n",
    "        self.X = X[valid].astype(np.float32)\n",
    "        self.y = Y[valid, j].astype(np.float32)\n",
    "        assert self.X.shape[0] == self.y.shape[0]\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return torch.from_numpy(self.X[i]), torch.tensor(self.y[i])\n",
    "\n",
    "def make_balanced_sampler(y_np: np.ndarray):\n",
    "    # Pos/neg weights so expected sampling is ~balanced\n",
    "    pos = (y_np == 1).astype(np.float32)\n",
    "    neg = (y_np == 0).astype(np.float32)\n",
    "    n_pos = pos.sum(); n_neg = neg.sum()\n",
    "    # Avoid zero-division; if n_pos==0, fall back to uniform\n",
    "    if n_pos < 1:\n",
    "        w = np.ones_like(y_np, dtype=np.float32)\n",
    "    else:\n",
    "        w_pos = 0.5 / max(n_pos, 1.0)\n",
    "        w_neg = 0.5 / max(n_neg, 1.0)\n",
    "        w = pos * w_pos + neg * w_neg\n",
    "    return torch.DoubleTensor(w)\n",
    "\n",
    "def make_loaders_for_label(j: int, bs: int = 1024):\n",
    "    dtr = FusedLabelDataset(Xtr, Ytr, Mtr, j)\n",
    "    dva = FusedLabelDataset(Xva, Yva, Mva, j)\n",
    "    # Balanced sampler for training\n",
    "    w = make_balanced_sampler(dtr.y)\n",
    "    sampler = torch.utils.data.WeightedRandomSampler(weights=w, num_samples=len(dtr), replacement=True)\n",
    "    train_loader = torch.utils.data.DataLoader(dtr, batch_size=bs, sampler=sampler,\n",
    "                                               num_workers=0, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "    val_loader   = torch.utils.data.DataLoader(dva, batch_size=bs, shuffle=False,\n",
    "                                               num_workers=0, pin_memory=(DEVICE.type==\"cuda\"))\n",
    "    return train_loader, val_loader\n",
    "\n",
    "# ------------------------------\n",
    "# Loss (Binary ASL with per-label α via effective number)\n",
    "# ------------------------------\n",
    "def effective_alpha(pos_count: int, beta: float = 0.999) -> float:\n",
    "    # Class-Balanced factor α for positives; scale to mean ~1 across labels later if desired.\n",
    "    eff = (1 - (beta ** max(pos_count, 1))) / (1 - beta)\n",
    "    return float(1.0 / max(eff, 1e-8))\n",
    "\n",
    "class BinaryASL(nn.Module):\n",
    "    def __init__(self, gamma_neg=5.0, gamma_pos=1.0, clip=0.05, alpha: float = 1.0):\n",
    "        super().__init__()\n",
    "        self.gamma_neg = gamma_neg\n",
    "        self.gamma_pos = gamma_pos\n",
    "        self.clip = clip\n",
    "        self.alpha = float(alpha)\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        # logits/targets: (B,)\n",
    "        p = torch.sigmoid(logits)\n",
    "        if self.clip:\n",
    "            p = torch.clamp(p, self.clip, 1 - self.clip)\n",
    "        pos = targets\n",
    "        neg = 1 - targets\n",
    "        pt = p * pos + (1 - p) * neg\n",
    "        gamma = self.gamma_pos * pos + self.gamma_neg * neg\n",
    "        focal = torch.pow(1 - pt, gamma)\n",
    "        loss = - (pos * torch.log(p + 1e-8) + neg * torch.log(1 - p + 1e-8))\n",
    "        loss = loss * focal * self.alpha\n",
    "        return loss.mean()\n",
    "\n",
    "# ------------------------------\n",
    "# Head model (stronger MLP + residual)\n",
    "# ------------------------------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)  # residual path to help optimisation\n",
    "    def forward(self, x):  # x: (B,768)\n",
    "        z1 = self.block1(x)\n",
    "        z2 = self.block2(z1)\n",
    "        z3 = self.block3(z2)\n",
    "        # residual from input\n",
    "        z = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "# ------------------------------\n",
    "# Scheduler: warmup + cosine\n",
    "# ------------------------------\n",
    "def build_warmup_cosine(total_steps, warmup_ratio=0.1, min_lr_scale=0.1):\n",
    "    def lr_lambda(step):\n",
    "        warm = int(total_steps * warmup_ratio)\n",
    "        if step < warm:\n",
    "            return float(step) / max(1, warm)\n",
    "        progress = (step - warm) / max(1, total_steps - warm)\n",
    "        cosine = 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        return min_lr_scale + (1 - min_lr_scale) * cosine\n",
    "    return lr_lambda\n",
    "\n",
    "# ------------------------------\n",
    "# Train one seed for one label\n",
    "# ------------------------------\n",
    "def train_label_seed(label_name: str, j: int, seed: int,\n",
    "                     epochs_max: int = 50, patience: int = 10,\n",
    "                     lr: float = 3e-3, wd: float = 1e-2) -> Dict[str, float]:\n",
    "    seed_everything(seed)\n",
    "\n",
    "    # Loaders & stats\n",
    "    train_loader, val_loader = make_loaders_for_label(j, bs=1024)\n",
    "    pos_count = int((Ytr[~Mtr[:, j], j] == 1).sum())\n",
    "    alpha = effective_alpha(pos_count)  # per-label CB factor\n",
    "\n",
    "    model = LabelHead(in_dim=768, h1=512, h2=256, h3=128, p=0.30).to(DEVICE)\n",
    "    opt   = optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
    "    steps_per_epoch = len(train_loader)\n",
    "    sched = LambdaLR(opt, build_warmup_cosine(total_steps=steps_per_epoch*epochs_max, warmup_ratio=0.1))\n",
    "\n",
    "    criterion = BinaryASL(gamma_neg=5.0, gamma_pos=1.0, clip=0.05, alpha=alpha)\n",
    "    scaler = torch.amp.GradScaler(\"cuda\", enabled=(DEVICE.type==\"cuda\"))\n",
    "    ema_decay = 0.999\n",
    "    ema = {n: p.detach().clone() for n,p in model.named_parameters() if p.requires_grad}\n",
    "\n",
    "    def ema_update():\n",
    "        for n,p in model.named_parameters():\n",
    "            if p.requires_grad:\n",
    "                ema[n].mul_(ema_decay).add_(p.detach(), alpha=1-ema_decay)\n",
    "\n",
    "    def ema_copy_to():\n",
    "        with torch.no_grad():\n",
    "            for n,p in model.named_parameters():\n",
    "                if p.requires_grad:\n",
    "                    p.data.copy_(ema[n])\n",
    "\n",
    "    # paths\n",
    "    label_dir = ENSEMBLE_DIR / label_name / f\"seed{seed:02d}\"\n",
    "    label_dir.mkdir(parents=True, exist_ok=True)\n",
    "    log_path  = label_dir / \"train_log.jsonl\"\n",
    "    if log_path.exists(): log_path.unlink()\n",
    "\n",
    "    best_ap, best_path = -1.0, label_dir / \"best.pt\"\n",
    "    wait = patience\n",
    "    t_start = time.time()\n",
    "\n",
    "    for epoch in range(1, epochs_max + 1):\n",
    "        # ---- train\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(DEVICE, non_blocking=True); yb = yb.to(DEVICE, non_blocking=True)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            with torch.amp.autocast(\"cuda\", enabled=(DEVICE.type==\"cuda\")):\n",
    "                logits = model(xb)\n",
    "                loss = criterion(logits, yb)\n",
    "            scaler.scale(loss).backward()\n",
    "            # grad clip\n",
    "            scaler.unscale_(opt)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            scaler.step(opt); scaler.update()\n",
    "            sched.step()\n",
    "            epoch_loss += float(loss.item())\n",
    "            # EMA\n",
    "            ema_update()\n",
    "\n",
    "        # ---- validate (with EMA weights)\n",
    "        ema_copy_to()\n",
    "        model.eval()\n",
    "        preds, gts = [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in val_loader:\n",
    "                xb = xb.to(DEVICE, non_blocking=True)\n",
    "                logits = model(xb)\n",
    "                p = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "                preds.append(p); gts.append(yb.numpy())\n",
    "        preds = np.concatenate(preds); gts = np.concatenate(gts)\n",
    "        try:\n",
    "            ap = float(average_precision_score(gts, preds))\n",
    "        except Exception:\n",
    "            ap = float(\"nan\")\n",
    "\n",
    "        row = {\"epoch\": epoch, \"train_loss\": epoch_loss/max(1,len(train_loader)), \"val_ap\": ap, \"time_min\": round((time.time()-t_start)/60,2)}\n",
    "        with open(log_path, \"a\") as f: f.write(json.dumps(row) + \"\\n\")\n",
    "        print(f\"[{label_name} | seed {seed:02d}] ep {epoch:02d}  loss {row['train_loss']:.4f}  val AP {ap:.4f}\")\n",
    "\n",
    "        # save val preds for calibration (overwrite each epoch; best saved below)\n",
    "        np.savez_compressed(label_dir / \"val_preds.npz\", preds=preds, y=gts)\n",
    "\n",
    "        if np.isnan(ap):\n",
    "            continue\n",
    "        if ap > best_ap:\n",
    "            best_ap = ap; wait = patience\n",
    "            torch.save({\"model\": model.state_dict(),\n",
    "                        \"config\": {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30},\n",
    "                        \"label\": label_name, \"seed\": seed}, best_path)\n",
    "            # also save best preds snapshot\n",
    "            np.savez_compressed(label_dir / \"val_preds_best.npz\", preds=preds, y=gts)\n",
    "            print(f\"  ✅ New best AP: {best_ap:.4f} → {best_path}\")\n",
    "        else:\n",
    "            wait -= 1\n",
    "            if wait <= 0:\n",
    "                print(f\"  ⏹ Early stop (no improve for {patience} epochs). Best AP: {best_ap:.4f}\")\n",
    "                break\n",
    "\n",
    "    # write seed summary\n",
    "    with open(ENSEMBLE_DIR / label_name / f\"seed{seed:02d}\" / \"metrics.json\", \"w\") as f:\n",
    "        f.write(json.dumps({\"best_ap\": best_ap, \"epochs\": epoch}, indent=2))\n",
    "    return {\"label\": label_name, \"seed\": seed, \"best_ap\": best_ap}\n",
    "\n",
    "# ------------------------------\n",
    "# Driver: all labels, multi-seed\n",
    "# ------------------------------\n",
    "SEEDS = [13, 29, 47, 61, 83]    # 5 seeds (tweak as desired)\n",
    "EPOCHS_MAX = 50\n",
    "PATIENCE   = 10\n",
    "\n",
    "summary = []\n",
    "for j, name in enumerate(LABEL_NAMES):\n",
    "    print(\"\\n==============================\")\n",
    "    print(f\"Training specialist heads for: {name} (label {j})\")\n",
    "    print(\"==============================\")\n",
    "    for seed in SEEDS:\n",
    "        res = train_label_seed(name, j, seed, epochs_max=EPOCHS_MAX, patience=PATIENCE, lr=3e-3, wd=1e-2)\n",
    "        summary.append(res)\n",
    "\n",
    "# Aggregate best AP per label across seeds\n",
    "agg = {}\n",
    "for name in LABEL_NAMES:\n",
    "    best = max((r for r in summary if r[\"label\"] == name), key=lambda r: (r[\"best_ap\"] if not math.isnan(r[\"best_ap\"]) else -1.0))\n",
    "    agg[name] = {\"best_ap\": best[\"best_ap\"]}\n",
    "\n",
    "(ENSEMBLE_DIR / \"ensemble_summary.json\").write_text(json.dumps({\"per_seed\": summary, \"best_per_label\": agg}, indent=2))\n",
    "print(\"\\n✅ Ensemble training complete.\")\n",
    "print(json.dumps({\"best_per_label\": agg}, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bfdea1",
   "metadata": {},
   "source": [
    "## phase 4 (Calibrate/Threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e18ab8",
   "metadata": {},
   "source": [
    "### 1: Calibration (Temperature Scaling) + Thresholds (F1 / Fβ)\n",
    "\n",
    "- Pick best specialist head per label (from `ensemble_summary.json`)\n",
    "- Calibrate with per-label **temperature scaling** (on validation logits)\n",
    "- Select **F1-max** and **Fβ=1.5-max** thresholds per label\n",
    "- Save to `v7/model/calibration/` and curves to `v7/results/calibration/`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1de76889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ensemble summary for 12 labels.\n",
      "\n",
      "Calibrating: NR-AR\n",
      "  T=4.549  AP_val=0.1770  th_f1=0.573  th_fβ1.5=0.573\n",
      "\n",
      "Calibrating: NR-AR-LBD\n",
      "  T=3.660  AP_val=0.3007  th_f1=0.610  th_fβ1.5=0.610\n",
      "\n",
      "Calibrating: NR-AhR\n",
      "  T=1.500  AP_val=0.5251  th_f1=0.681  th_fβ1.5=0.631\n",
      "\n",
      "Calibrating: NR-Aromatase\n",
      "  T=2.483  AP_val=0.2754  th_f1=0.604  th_fβ1.5=0.552\n",
      "\n",
      "Calibrating: NR-ER\n",
      "  T=4.657  AP_val=0.2309  th_f1=0.542  th_fβ1.5=0.523\n",
      "\n",
      "Calibrating: NR-ER-LBD\n",
      "  T=1.857  AP_val=0.1527  th_f1=0.650  th_fβ1.5=0.616\n",
      "\n",
      "Calibrating: NR-PPAR-gamma\n",
      "  T=4.558  AP_val=0.0918  th_f1=0.521  th_fβ1.5=0.521\n",
      "\n",
      "Calibrating: SR-ARE\n",
      "  T=4.500  AP_val=0.3360  th_f1=0.532  th_fβ1.5=0.532\n",
      "\n",
      "Calibrating: SR-ATAD5\n",
      "  T=4.475  AP_val=0.2277  th_f1=0.567  th_fβ1.5=0.556\n",
      "\n",
      "Calibrating: SR-HSE\n",
      "  T=4.639  AP_val=0.2323  th_f1=0.551  th_fβ1.5=0.545\n",
      "\n",
      "Calibrating: SR-MMP\n",
      "  T=1.896  AP_val=0.4462  th_f1=0.600  th_fβ1.5=0.600\n",
      "\n",
      "Calibrating: SR-p53\n",
      "  T=4.245  AP_val=0.2346  th_f1=0.546  th_fβ1.5=0.546\n",
      "\n",
      "✅ Saved:\n",
      "  • temperatures → v7\\model\\calibration\\temps.json\n",
      "  • thresholds   → v7\\model\\calibration\\thresholds.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, Tuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# ------------- Paths -------------\n",
    "BASE_DIR    = Path(\"v7\")\n",
    "FUSED_DIR   = BASE_DIR / \"data\" / \"fused\"\n",
    "ENS_DIR     = BASE_DIR / \"model\" / \"ensembles\"\n",
    "CAL_DIR     = BASE_DIR / \"model\" / \"calibration\"\n",
    "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CAL_RES_DIR = BASE_DIR / \"results\" / \"calibration\"\n",
    "CAL_RES_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load label names from earlier phase\n",
    "with open(BASE_DIR / \"data\" / \"prepared\" / \"dataset_manifest.json\") as f:\n",
    "    ds_manifest = json.load(f)\n",
    "LABEL_NAMES = ds_manifest[\"labels\"]\n",
    "\n",
    "# Load ensemble summary (decide best seed per label)\n",
    "ens_summary = json.loads((ENS_DIR / \"ensemble_summary.json\").read_text())\n",
    "best_per_label = ens_summary[\"best_per_label\"]  # label -> {best_ap: ...}\n",
    "print(\"Loaded ensemble summary for\", len(best_per_label), \"labels.\")\n",
    "\n",
    "# Fused validation cache\n",
    "Xva = np.load(FUSED_DIR / \"val_fused.npy\")    # (N_val, 768)\n",
    "Yva = np.load(FUSED_DIR / \"val_Y.npy\")        # (N_val, 12)\n",
    "Mva = np.load(FUSED_DIR / \"val_mask.npy\")     # (N_val, 12) True where missing\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --------- Head definition (must match saved config) ---------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def load_best_seed_dir(label: str) -> Path:\n",
    "    # Find the seed folder with highest best_ap for this label\n",
    "    candidates = []\n",
    "    for seed_dir in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mpath = seed_dir / \"metrics.json\"\n",
    "        if mpath.exists():\n",
    "            try:\n",
    "                m = json.loads(mpath.read_text())\n",
    "                candidates.append((float(m.get(\"best_ap\", float(\"nan\"))), seed_dir))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No seed folders with metrics for label {label}\")\n",
    "    candidates.sort(key=lambda x: (x[0] if not math.isnan(x[0]) else -1.0), reverse=True)\n",
    "    return candidates[0][1]  # best seed dir\n",
    "\n",
    "@torch.no_grad()\n",
    "def head_logits_on_val(label: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Return logits and ground truth for valid val rows of this label.\"\"\"\n",
    "    # Filter valid rows (not missing)\n",
    "    j = LABEL_NAMES.index(label)\n",
    "    valid = ~Mva[:, j]\n",
    "    X = torch.tensor(Xva[valid], dtype=torch.float32, device=device)\n",
    "    y = Yva[valid, j].astype(np.float32)\n",
    "\n",
    "    # Load head\n",
    "    best_dir = load_best_seed_dir(label)\n",
    "    ckpt = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg  = ckpt.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    head.eval()\n",
    "\n",
    "    logits = []\n",
    "    BS = 4096  # very fast on 4070 Ti\n",
    "    for i in range(0, X.shape[0], BS):\n",
    "        l = head(X[i:i+BS])\n",
    "        logits.append(l.detach().cpu().numpy())\n",
    "    logits = np.concatenate(logits, axis=0)  # (Nv,)\n",
    "    return logits, y\n",
    "\n",
    "def fit_temperature(logits: np.ndarray, y: np.ndarray, max_iter: int = 200, lr: float = 0.05) -> float:\n",
    "    \"\"\"\n",
    "    Fit scalar temperature T>0, minimizing NLL on validation.\n",
    "    \"\"\"\n",
    "    t = torch.tensor([1.0], dtype=torch.float32, requires_grad=True, device=device)\n",
    "    x = torch.tensor(logits, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(y,      dtype=torch.float32, device=device)\n",
    "    opt = torch.optim.Adam([t], lr=lr)\n",
    "    for _ in range(max_iter):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z = x / (t.clamp(min=1e-3))\n",
    "        p = torch.sigmoid(z).clamp(1e-6, 1-1e-6)\n",
    "        loss = - (y*torch.log(p) + (1-y)*torch.log(1-p)).mean()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "    return float(t.detach().cpu().item())\n",
    "\n",
    "def best_thresholds(y_true: np.ndarray, probs: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Compute thresholds that maximize F1 and F-beta (beta=1.5) on validation.\n",
    "    \"\"\"\n",
    "    if probs.ndim != 1: probs = probs.ravel()\n",
    "    precision, recall, th = precision_recall_curve(y_true, probs)\n",
    "    # PR curve returns len(th)+1 points; align F1/Fb on thresholds\n",
    "    eps = 1e-8\n",
    "    f1 = (2*precision*recall) / np.maximum(precision+recall, eps)\n",
    "    beta = 1.5\n",
    "    fb = ((1+beta**2)*precision*recall) / np.maximum((beta**2)*precision + recall, eps)\n",
    "\n",
    "    # The first PR point has no threshold; we’ll map scores to thresholds array length\n",
    "    f1_th  = th[np.nanargmax(f1[1:])]  if th.size>0 else 0.5\n",
    "    fb_th  = th[np.nanargmax(fb[1:])]  if th.size>0 else 0.5\n",
    "    # also report AP for reference\n",
    "    try:\n",
    "        ap = float(average_precision_score(y_true, probs))\n",
    "    except Exception:\n",
    "        ap = float(\"nan\")\n",
    "    return {\"th_f1\": float(f1_th), \"th_fbeta15\": float(fb_th), \"ap_val\": ap}\n",
    "\n",
    "temps = {}\n",
    "thresholds = {}\n",
    "\n",
    "for label in LABEL_NAMES:\n",
    "    print(f\"\\nCalibrating: {label}\")\n",
    "    logits, y = head_logits_on_val(label)\n",
    "    if logits.size == 0 or np.all(y == y[0]):\n",
    "        print(\"  ⚠️ Skipping (no variance or no valid rows). Using defaults.\")\n",
    "        temps[label] = 1.0\n",
    "        thresholds[label] = {\"th_f1\": 0.5, \"th_fbeta15\": 0.5, \"ap_val\": float(\"nan\")}\n",
    "        continue\n",
    "\n",
    "    T = fit_temperature(logits, y, max_iter=200, lr=0.05)\n",
    "    probs_cal = 1.0 / (1.0 + np.exp(-logits / max(T, 1e-3)))\n",
    "\n",
    "    th_dict = best_thresholds(y, probs_cal)\n",
    "    temps[label] = T\n",
    "    thresholds[label] = th_dict\n",
    "\n",
    "    # optional: save PR curve arrays for later plotting/debug\n",
    "    np.savez_compressed(CAL_RES_DIR / f\"{label}_val_calib.npz\", logits=logits, y=y, T=T, **th_dict)\n",
    "    print(f\"  T={T:.3f}  AP_val={th_dict['ap_val']:.4f}  th_f1={th_dict['th_f1']:.3f}  th_fβ1.5={th_dict['th_fbeta15']:.3f}\")\n",
    "\n",
    "# Save calibration artifacts\n",
    "(Path(CAL_DIR / \"temps.json\")).write_text(json.dumps(temps, indent=2))\n",
    "(Path(CAL_DIR / \"thresholds.json\")).write_text(json.dumps(thresholds, indent=2))\n",
    "print(\"\\n✅ Saved:\")\n",
    "print(\"  • temperatures →\", CAL_DIR / \"temps.json\")\n",
    "print(\"  • thresholds   →\", CAL_DIR / \"thresholds.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07aaa9c9",
   "metadata": {},
   "source": [
    "## phase 5 (Inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd388517",
   "metadata": {},
   "source": [
    "### 1:  Inference (calibrated specialist ensemble) + test export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91ebbf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded shared fusion model.\n",
      "✅ Loaded specialist heads for all labels.\n",
      "✅ Inference is ready: call predict_smiles(['CCO'], threshold_mode='fbeta15' or 'f1').\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---------------- Paths & basics ----------------\n",
    "BASE       = Path(\"v7\")\n",
    "PREP_DIR   = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR   = BASE / \"data\" / \"descriptors\"\n",
    "MODEL_DIR  = BASE / \"model\"\n",
    "CKPT_BEST  = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "ENS_DIR    = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR    = MODEL_DIR / \"calibration\"\n",
    "\n",
    "assert CKPT_BEST.exists(), f\"Missing shared checkpoint: {CKPT_BEST}\"\n",
    "assert (PREP_DIR / \"dataset_manifest.json\").exists(), \"Missing dataset manifest.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------------- Labels, temps, thresholds ----------------\n",
    "ds_manifest = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABEL_NAMES = ds_manifest[\"labels\"]\n",
    "DESC_IN_DIM = ds_manifest[\"n_features\"]  # 208\n",
    "\n",
    "temps      = json.loads((CAL_DIR / \"temps.json\").read_text())\n",
    "thresholds = json.loads((CAL_DIR / \"thresholds.json\").read_text())\n",
    "\n",
    "# ---------------- Text encoder (ChemBERTa) ----------------\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class ChemBERTaEncoder(nn.Module):\n",
    "    def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "        self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "        self.pad_token_id = self.tokenizer.pad_token_id or self.tokenizer.eos_token_id\n",
    "        self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "        self.ln = nn.LayerNorm(fusion_dim)\n",
    "    def forward(self, smiles_list: List[str], max_length=256, add_special_tokens=True):\n",
    "        enc = self.tokenizer(list(smiles_list), padding=True, truncation=True,\n",
    "                             max_length=max_length, add_special_tokens=add_special_tokens,\n",
    "                             return_tensors=\"pt\")\n",
    "        input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,L,H)\n",
    "        toks = self.ln(self.proj(out))  # (B,L,256)\n",
    "        return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "# ---------------- Graph encoder (match checkpoint names) ----------------\n",
    "from rdkit import Chem as _Chem\n",
    "\n",
    "ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "\n",
    "def _one_hot(v, choices):\n",
    "    z = [0]*len(choices)\n",
    "    if v in choices:\n",
    "        z[choices.index(v)] = 1\n",
    "    return z\n",
    "\n",
    "def _bucket_oh(v, lo, hi):\n",
    "    buckets = list(range(lo, hi+1))\n",
    "    o = [0]*(len(buckets)+1)\n",
    "    idx = v - lo\n",
    "    o[idx if 0 <= idx < len(buckets) else -1] = 1\n",
    "    return o\n",
    "\n",
    "def _atom_feat(atom):\n",
    "    hybs = [\n",
    "        _Chem.rdchem.HybridizationType.S, _Chem.rdchem.HybridizationType.SP,\n",
    "        _Chem.rdchem.HybridizationType.SP2, _Chem.rdchem.HybridizationType.SP3,\n",
    "        _Chem.rdchem.HybridizationType.SP3D, _Chem.rdchem.HybridizationType.SP3D2\n",
    "    ]\n",
    "    chir = [\n",
    "        _Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "        _Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "        _Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "        _Chem.rdchem.ChiralType.CHI_OTHER\n",
    "    ]\n",
    "    sym = atom.GetSymbol()\n",
    "    feat = _one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST+[\"other\"])\n",
    "    feat += _bucket_oh(atom.GetDegree(), 0, 5)\n",
    "    feat += _bucket_oh(atom.GetFormalCharge(), -2, 2)\n",
    "    feat += (_one_hot(atom.GetHybridization(), hybs)+[0])  # +other\n",
    "    feat += [int(atom.GetIsAromatic())]\n",
    "    feat += [int(atom.IsInRing())]\n",
    "    feat += _one_hot(atom.GetChiralTag(), chir)\n",
    "    feat += _bucket_oh(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)\n",
    "    feat += _bucket_oh(atom.GetTotalValence(), 0, 5)\n",
    "    feat += [atom.GetMass()/200.0]\n",
    "    return feat  # ~51 dims\n",
    "\n",
    "def _smiles_to_graph(smi, max_nodes=128):\n",
    "    mol = _Chem.MolFromSmiles(smi)\n",
    "    if mol is None or mol.GetNumAtoms() == 0:\n",
    "        return np.zeros((0,0), dtype=np.float32), np.zeros((0,0), dtype=np.float32)\n",
    "    feats = [_atom_feat(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "    x = np.asarray(feats, dtype=np.float32)\n",
    "    N = mol.GetNumAtoms()\n",
    "    adj = np.zeros((N, N), dtype=np.float32)\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        adj[i, j] = 1.0; adj[j, i] = 1.0\n",
    "    if N > max_nodes:\n",
    "        x = x[:max_nodes]; adj = adj[:max_nodes, :max_nodes]\n",
    "    return x, adj\n",
    "\n",
    "def _collate_graphs(smiles_batch, max_nodes=128):\n",
    "    graphs = [_smiles_to_graph(s) for s in smiles_batch]\n",
    "    Nmax = max([g[0].shape[0] for g in graphs] + [1])\n",
    "    Fnode = graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "    B = len(graphs)\n",
    "    X = np.zeros((B, Nmax, Fnode), dtype=np.float32)\n",
    "    A = np.zeros((B, Nmax, Nmax), dtype=np.float32)\n",
    "    M = np.zeros((B, Nmax), dtype=np.int64)\n",
    "    for i, (x, a) in enumerate(graphs):\n",
    "        n = x.shape[0]\n",
    "        if n == 0: continue\n",
    "        X[i, :n, :] = x\n",
    "        A[i, :n, :n] = a\n",
    "        M[i, :n] = 1\n",
    "    return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, h=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "        self.mlp = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "    def forward(self, x, adj, mask):\n",
    "        out = (1.0 + self.eps) * x + torch.matmul(adj, x)\n",
    "        out = self.mlp(out)\n",
    "        return out * mask.unsqueeze(-1).to(out.dtype)\n",
    "\n",
    "class GraphGINEncoder(nn.Module):\n",
    "    def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(nn.Linear(node_in_dim, hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden_dim, p) for _ in range(n_layers)])\n",
    "        # IMPORTANT: name must be 'out_ln' to match checkpoint\n",
    "        self.out_ln = nn.LayerNorm(hidden_dim)\n",
    "    def forward(self, smiles_list: List[str], max_nodes=128):\n",
    "        X, A, M = _collate_graphs(smiles_list, max_nodes=max_nodes)\n",
    "        h = self.inp(X)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, A, M)\n",
    "        return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "# ---------------- Fusion blocks ----------------\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "    denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "    return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "        self.ln  = nn.LayerNorm(dim)\n",
    "        self.do  = nn.Dropout(p)\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "        Q = text_tokens.transpose(0,1)   # (L,B,D)\n",
    "        K = graph_nodes.transpose(0,1)   # (N,B,D)\n",
    "        V = graph_nodes.transpose(0,1)\n",
    "        kpm = (graph_mask == 0)          # (B,N) True where pad\n",
    "        attn, _ = self.mha(Q, K, V, key_padding_mask=kpm)\n",
    "        attn = attn.transpose(0,1)       # (B,L,D)\n",
    "        return self.ln(text_tokens + self.do(attn))\n",
    "\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim), nn.GELU(), nn.Dropout(p)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# IMPORTANT: name must be 'mlp' to match checkpoint ('shared_head.mlp.*')\n",
    "class FusionClassifier(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=12, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, fused_vec):\n",
    "        return self.mlp(fused_vec)\n",
    "\n",
    "class V7FusionModel(nn.Module):\n",
    "    def __init__(self, text_encoder, graph_encoder, desc_in_dim=208, dim=256, n_labels=12, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.text_encoder=text_encoder\n",
    "        self.graph_encoder=graph_encoder\n",
    "        self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "        self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "        self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "    def forward(self, smiles_list, desc_feats, return_intermediates=False):\n",
    "        tt, tm = self.text_encoder(smiles_list, max_length=256)\n",
    "        gn, gm = self.graph_encoder(smiles_list, max_nodes=128)\n",
    "        tt, tm, gn, gm, desc_feats = tt.to(device), tm.to(device), gn.to(device), gm.to(device), desc_feats.to(device)\n",
    "        tta = self.cross(tt, tm, gn, gm)\n",
    "        de  = self.desc_mlp(desc_feats)\n",
    "        text_pool  = masked_mean(tta, tm, 1)\n",
    "        graph_pool = masked_mean(gn,  gm, 1)\n",
    "        fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "        logits = self.shared_head(fused)\n",
    "        if return_intermediates:\n",
    "            return logits, fused\n",
    "        return logits\n",
    "\n",
    "# ---------------- Build & load ----------------\n",
    "text_encoder = ChemBERTaEncoder().to(device)\n",
    "graph_encoder= GraphGINEncoder().to(device)\n",
    "v7_shared    = V7FusionModel(text_encoder, graph_encoder, desc_in_dim=DESC_IN_DIM, n_labels=len(LABEL_NAMES)).to(device)\n",
    "ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "v7_shared.eval()\n",
    "print(\"✅ Loaded shared fusion model.\")\n",
    "\n",
    "# ---------------- Specialist heads (match boosted Cell 2) ----------------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _load_best_head(label: str) -> nn.Module:\n",
    "    # pick seed dir with highest best_ap\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if mfile.exists():\n",
    "            try:\n",
    "                ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "                cands.append((ap, sd))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(ck[\"model\"], strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS: Dict[str, nn.Module] = {lbl: _load_best_head(lbl) for lbl in LABEL_NAMES}\n",
    "print(\"✅ Loaded specialist heads for all labels.\")\n",
    "\n",
    "# ---------------- Descriptors for ad-hoc SMILES ----------------\n",
    "# For quick testing without the exact 208-d extractor, use standardized zero vector for descriptors.\n",
    "def prepare_desc_matrix(smiles_list: List[str]) -> torch.Tensor:\n",
    "    n = len(smiles_list)\n",
    "    Z = np.zeros((n, DESC_IN_DIM), dtype=np.float32)  # standardized zeros (mean feature)\n",
    "    return torch.tensor(Z, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---------------- Fused feature builder ----------------\n",
    "@torch.no_grad()\n",
    "def fused_from_smiles(smiles_list: List[str], desc_tensor: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
    "    if desc_tensor is None:\n",
    "        desc_tensor = prepare_desc_matrix(smiles_list)\n",
    "    tt, tm = v7_shared.text_encoder(smiles_list, max_length=256)\n",
    "    gn, gm = v7_shared.graph_encoder(smiles_list, max_nodes=128)\n",
    "    tt, tm = tt.to(device), tm.to(device)\n",
    "    gn, gm = gn.to(device), gm.to(device)\n",
    "    de = v7_shared.desc_mlp(desc_tensor.to(device))\n",
    "    # cross-attend & pool\n",
    "    tta = v7_shared.cross(tt, tm, gn, gm)\n",
    "    text_pool  = masked_mean(tta, tm, 1)\n",
    "    graph_pool = masked_mean(gn,  gm, 1)\n",
    "    return torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "\n",
    "# ---------------- Public API ----------------\n",
    "def predict_smiles(smiles_list: List[str], threshold_mode: str = \"fbeta15\"):\n",
    "    \"\"\"\n",
    "    Returns list[dict]: one per SMILES\n",
    "      label -> {logit, prob_raw, prob_cal, decision}\n",
    "    \"\"\"\n",
    "    assert threshold_mode in (\"f1\", \"fbeta15\")\n",
    "    fused = fused_from_smiles(smiles_list)  # (B,768)\n",
    "    out = []\n",
    "    for i in range(fused.size(0)):\n",
    "        row = {}\n",
    "        x = fused[i:i+1]\n",
    "        for label in LABEL_NAMES:\n",
    "            head = HEADS[label]\n",
    "            with torch.no_grad():\n",
    "                logit = head(x).item()\n",
    "            T   = max(float(temps.get(label, 1.0)), 1e-3)\n",
    "            p_r = 1.0 / (1.0 + math.e**(-logit))\n",
    "            p_c = 1.0 / (1.0 + math.e**(-logit / T))\n",
    "            th  = thresholds[label][\"th_fbeta15\"] if threshold_mode==\"fbeta15\" else thresholds[label][\"th_f1\"]\n",
    "            row[label] = {\"logit\": float(logit), \"prob_raw\": float(p_r), \"prob_cal\": float(p_c), \"decision\": bool(p_c >= float(th))}\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "print(\"✅ Inference is ready: call predict_smiles(['CCO'], threshold_mode='fbeta15' or 'f1').\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29fb6f68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12/12 label columns in the Excel.\n",
      "\n",
      "SMILES: CCOc1ccc2nc(S(N)(=O)=O)sc2c1\n",
      "  NR-AhR        prob=0.594  th=0.681  → pred=0\n",
      "  SR-ARE        prob=0.533  th=0.532  → pred=1\n",
      "  NR-ER         prob=0.529  th=0.542  → pred=0\n",
      "  SR-ATAD5      prob=0.529  th=0.567  → pred=0\n",
      "  NR-PPAR-gamma  prob=0.521  th=0.521  → pred=1\n",
      "  True positives: NR-AhR, SR-ARE\n",
      "  Pred positives (f1): NR-PPAR-gamma, SR-ARE\n",
      "\n",
      "SMILES: CCN1C(=O)NC(c2ccccc2)C1=O\n",
      "  NR-AhR        prob=0.616  th=0.681  → pred=0\n",
      "  SR-MMP        prob=0.554  th=0.600  → pred=0\n",
      "  SR-ATAD5      prob=0.537  th=0.567  → pred=0\n",
      "  NR-PPAR-gamma  prob=0.536  th=0.521  → pred=1\n",
      "  SR-p53        prob=0.534  th=0.546  → pred=0\n",
      "  True positives: —\n",
      "  Pred positives (f1): NR-PPAR-gamma\n",
      "\n",
      "SMILES: O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\n",
      "  NR-AhR        prob=0.670  th=0.681  → pred=0\n",
      "  SR-MMP        prob=0.641  th=0.600  → pred=1\n",
      "  NR-ER-LBD     prob=0.580  th=0.650  → pred=0\n",
      "  NR-Aromatase  prob=0.569  th=0.604  → pred=0\n",
      "  SR-p53        prob=0.560  th=0.546  → pred=1\n",
      "  True positives: NR-ER, SR-ARE, SR-HSE, SR-p53\n",
      "  Pred positives (f1): NR-PPAR-gamma, SR-ARE, SR-MMP, SR-p53\n",
      "\n",
      "SMILES: CC(O)CNCC(C)O\n",
      "  NR-AR         prob=0.516  th=0.573  → pred=0\n",
      "  NR-ER         prob=0.510  th=0.542  → pred=0\n",
      "  SR-HSE        prob=0.503  th=0.551  → pred=0\n",
      "  SR-ARE        prob=0.500  th=0.532  → pred=0\n",
      "  NR-PPAR-gamma  prob=0.494  th=0.521  → pred=0\n",
      "  True positives: —\n",
      "  Pred positives (f1): —\n",
      "\n",
      "SMILES: O=c1cc(-c2ccccc2)oc2cc(O)cc(O)c12\n",
      "  NR-AhR        prob=0.708  th=0.681  → pred=1\n",
      "  SR-MMP        prob=0.668  th=0.600  → pred=1\n",
      "  NR-ER-LBD     prob=0.617  th=0.650  → pred=0\n",
      "  NR-Aromatase  prob=0.576  th=0.604  → pred=0\n",
      "  SR-ARE        prob=0.567  th=0.532  → pred=1\n",
      "  True positives: NR-AhR, NR-ER, NR-ER-LBD, NR-PPAR-gamma, SR-ARE, SR-MMP\n",
      "  Pred positives (f1): NR-AhR, NR-ER, NR-PPAR-gamma, SR-ARE, SR-HSE, SR-MMP, SR-p53\n",
      "\n",
      "=== Summary (micro over labels with truth present) ===\n",
      "TP=8 FP=3 FN=4\n",
      "Precision=0.727 Recall=0.667 F1=0.696\n",
      "\n",
      "Saved detailed results → v7\\results\\inference\\f1.csv\n"
     ]
    }
   ],
   "source": [
    "# my_smiles = [\"CCOc1ccc2nc(S(N)(=O)=O)sc2c1\"]\n",
    "# mode = \"f1\"  # or \"f1\" fbeta15\n",
    "\n",
    "# results = predict_smiles(my_smiles, threshold_mode=mode)\n",
    "\n",
    "# from operator import itemgetter\n",
    "# for smi, rec in zip(my_smiles, results):\n",
    "#     print(\"\\nSMILES:\", smi)\n",
    "#     top = sorted([(lbl, d[\"prob_cal\"], d[\"decision\"]) for lbl, d in rec.items()],\n",
    "#                  key=itemgetter(1), reverse=True)[:5]\n",
    "#     for lbl, p, dec in top:\n",
    "#         th = thresholds[lbl][\"th_fbeta15\"] if mode==\"fbeta15\" else thresholds[lbl][\"th_f1\"]\n",
    "#         print(f\"  {lbl:12s}  prob={p:.3f}  th={th:.3f}  → pred={int(dec)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Ad-hoc evaluation on Excel truth labels (simple)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from operator import itemgetter\n",
    "import math, json, os\n",
    "\n",
    "# ----------- CONFIG -----------\n",
    "EXCEL_PATH = Path(\"tox21_dualenc_v1/data/raw/Truth Lables.xlsx\")\n",
    "MODE = \"f1\"            # \"f1\" or \"fbeta15\"\n",
    "N_DISPLAY = 5          # how many rows to pretty-print (set to None to print all)\n",
    "OUT_CSV = Path(\"v7/results/inference/f1.csv\")\n",
    "OUT_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------- Checks -----------\n",
    "assert 'predict_smiles' in globals(), \"predict_smiles() not found. Run the cold-start inference cell first.\"\n",
    "assert 'LABEL_NAMES' in globals(), \"LABEL_NAMES not found. Run the cold-start inference cell first.\"\n",
    "assert 'thresholds' in globals(), \"thresholds not found. Run Phase 4 calibration cell first.\"\n",
    "assert EXCEL_PATH.exists(), f\"Cannot find: {EXCEL_PATH}\"\n",
    "\n",
    "# ----------- Load Excel -----------\n",
    "df = pd.read_excel(EXCEL_PATH)\n",
    "cols_lower = {c.lower(): c for c in df.columns}\n",
    "# find smiles col (case-insensitive)\n",
    "smiles_col = None\n",
    "for key in [\"smiles\", \"smile\", \"SMILES\", \"Smiles\"]:\n",
    "    if key.lower() in cols_lower:\n",
    "        smiles_col = cols_lower[key.lower()]\n",
    "        break\n",
    "if smiles_col is None:\n",
    "    # fallback: first column named like 'smile*'\n",
    "    cand = [c for c in df.columns if c.lower().startswith(\"smiles\")]\n",
    "    smiles_col = cand[0] if cand else None\n",
    "assert smiles_col is not None, \"Could not locate a SMILES column in the Excel file.\"\n",
    "\n",
    "# ----------- Match label columns (case/spacing/hyphen-insensitive) -----------\n",
    "def _norm(s: str) -> str:\n",
    "    return \"\".join(ch for ch in str(s).lower() if ch.isalnum())\n",
    "\n",
    "label_norm = { _norm(lbl): lbl for lbl in LABEL_NAMES }\n",
    "col_for_label = {}  # label -> column name in df (if present)\n",
    "\n",
    "for col in df.columns:\n",
    "    if col == smiles_col: \n",
    "        continue\n",
    "    n = _norm(col)\n",
    "    if n in label_norm:\n",
    "        col_for_label[label_norm[n]] = col\n",
    "\n",
    "available_labels = [lbl for lbl in LABEL_NAMES if lbl in col_for_label]\n",
    "missing_labels = [lbl for lbl in LABEL_NAMES if lbl not in col_for_label]\n",
    "print(f\"Found {len(available_labels)}/{len(LABEL_NAMES)} label columns in the Excel.\")\n",
    "if missing_labels:\n",
    "    print(\"Missing label columns (will be skipped in scoring):\", \", \".join(missing_labels))\n",
    "\n",
    "# ----------- Parse truth values -----------\n",
    "def parse_truth(v):\n",
    "    if pd.isna(v): \n",
    "        return None\n",
    "    if isinstance(v, (int, np.integer)): \n",
    "        return int(v) == 1\n",
    "    if isinstance(v, float): \n",
    "        if math.isnan(v): return None\n",
    "        return int(v) == 1\n",
    "    s = str(v).strip().lower()\n",
    "    if s in (\"1\",\"y\",\"yes\",\"true\",\"t\",\"pos\",\"positive\"):\n",
    "        return True\n",
    "    if s in (\"0\",\"n\",\"no\",\"false\",\"f\",\"neg\",\"negative\"):\n",
    "        return False\n",
    "    # anything else → None (unknown)\n",
    "    return None\n",
    "\n",
    "# ----------- Run predictions -----------\n",
    "smiles_list = df[smiles_col].astype(str).tolist()\n",
    "preds = predict_smiles(smiles_list, threshold_mode=MODE)  # list[dict[label -> details]]\n",
    "\n",
    "# ----------- Build a simple evaluation table -----------\n",
    "rows = []\n",
    "micro_tp = micro_fp = micro_fn = 0\n",
    "\n",
    "for i, (smi, rec) in enumerate(zip(smiles_list, preds)):\n",
    "    # truth set (only for labels available in Excel)\n",
    "    true_pos = set()\n",
    "    true_neg = set()\n",
    "    for lbl in available_labels:\n",
    "        val = parse_truth(df.loc[i, col_for_label[lbl]])\n",
    "        if val is True:\n",
    "            true_pos.add(lbl)\n",
    "        elif val is False:\n",
    "            true_neg.add(lbl)\n",
    "        # None → skip\n",
    "\n",
    "    # predicted positives at chosen threshold\n",
    "    pred_pos = {lbl for lbl, d in rec.items() if d[\"decision\"]}\n",
    "    # accumulate micro counts only on labels where truth is known\n",
    "    for lbl in available_labels:\n",
    "        val = parse_truth(df.loc[i, col_for_label[lbl]])\n",
    "        if val is None: \n",
    "            continue\n",
    "        if lbl in pred_pos and val is True:\n",
    "            micro_tp += 1\n",
    "        elif lbl in pred_pos and val is False:\n",
    "            micro_fp += 1\n",
    "        elif lbl not in pred_pos and val is True:\n",
    "            micro_fn += 1\n",
    "\n",
    "    # top-5 by calibrated probability (for pretty print)\n",
    "    top5 = sorted([(lbl, d[\"prob_cal\"], d[\"decision\"]) for lbl, d in rec.items()],\n",
    "                  key=itemgetter(1), reverse=True)[:5]\n",
    "\n",
    "    # save a row for CSV: include probs & preds, and truths if present\n",
    "    row = {\"smiles\": smi}\n",
    "    for lbl, det in rec.items():\n",
    "        row[f\"{lbl}_prob\"] = det[\"prob_cal\"]\n",
    "        row[f\"{lbl}_pred\"] = int(det[\"decision\"])\n",
    "        if lbl in available_labels:\n",
    "            tv = parse_truth(df.loc[i, col_for_label[lbl]])\n",
    "            row[f\"{lbl}_true\"] = (None if tv is None else int(tv))\n",
    "    rows.append(row)\n",
    "\n",
    "    # pretty print a few rows\n",
    "    if N_DISPLAY is None or i < N_DISPLAY:\n",
    "        print(\"\\nSMILES:\", smi)\n",
    "        for lbl, p, dec in top5:\n",
    "            th = thresholds[lbl][\"th_fbeta15\"] if MODE==\"fbeta15\" else thresholds[lbl][\"th_f1\"]\n",
    "            print(f\"  {lbl:12s}  prob={p:.3f}  th={float(th):.3f}  → pred={int(dec)}\")\n",
    "        if available_labels:\n",
    "            print(\"  True positives:\", \", \".join(sorted(true_pos)) if true_pos else \"—\")\n",
    "            chosen = \", \".join(sorted(pred_pos)) if pred_pos else \"—\"\n",
    "            print(f\"  Pred positives ({MODE}): {chosen}\")\n",
    "\n",
    "# ----------- Micro summary -----------\n",
    "prec = micro_tp / (micro_tp + micro_fp) if (micro_tp + micro_fp) > 0 else 0.0\n",
    "rec  = micro_tp / (micro_tp + micro_fn) if (micro_tp + micro_fn) > 0 else 0.0\n",
    "f1   = (2*prec*rec)/(prec+rec) if (prec+rec) > 0 else 0.0\n",
    "\n",
    "print(\"\\n=== Summary (micro over labels with truth present) ===\")\n",
    "print(f\"TP={micro_tp} FP={micro_fp} FN={micro_fn}\")\n",
    "print(f\"Precision={prec:.3f} Recall={rec:.3f} F1={f1:.3f}\")\n",
    "\n",
    "# ----------- Save CSV -----------\n",
    "pd.DataFrame(rows).to_csv(OUT_CSV, index=False)\n",
    "print(f\"\\nSaved detailed results → {OUT_CSV}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68668ff5",
   "metadata": {},
   "source": [
    "### 2: Calibrate shared head, create blended ensemble, refit thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8c6905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrating shared head temperatures on val...\n",
      "  NR-AR: T_shared=0.134\n",
      "  NR-AR-LBD: T_shared=0.132\n",
      "  NR-AhR: T_shared=0.167\n",
      "  NR-Aromatase: T_shared=0.126\n",
      "  NR-ER: T_shared=0.134\n",
      "  NR-ER-LBD: T_shared=0.110\n",
      "  NR-PPAR-gamma: T_shared=0.167\n",
      "  SR-ARE: T_shared=0.260\n",
      "  SR-ATAD5: T_shared=0.146\n",
      "  SR-HSE: T_shared=0.100\n",
      "  SR-MMP: T_shared=0.250\n",
      "  SR-p53: T_shared=0.119\n",
      "Saved → v7\\model\\calibration\\temps_shared.json\n",
      "\n",
      "Blending probs on val with alpha=0.80 (specialist weight)\n",
      "  NR-AR: AP_val=0.171 th_f1=0.653 th_fb15=0.653\n",
      "  NR-AR-LBD: AP_val=0.253 th_f1=0.621 th_fb15=0.621\n",
      "  NR-AhR: AP_val=0.524 th_f1=0.709 th_fb15=0.642\n",
      "  NR-Aromatase: AP_val=0.295 th_f1=0.564 th_fb15=0.474\n",
      "  NR-ER: AP_val=0.253 th_f1=0.547 th_fb15=0.480\n",
      "  NR-ER-LBD: AP_val=0.139 th_f1=0.589 th_fb15=0.589\n",
      "  NR-PPAR-gamma: AP_val=0.063 th_f1=0.441 th_fb15=0.427\n",
      "  SR-ARE: AP_val=0.344 th_f1=0.528 th_fb15=0.528\n",
      "  SR-ATAD5: AP_val=0.171 th_f1=0.483 th_fb15=0.483\n",
      "  SR-HSE: AP_val=0.196 th_f1=0.472 th_fb15=0.459\n",
      "  SR-MMP: AP_val=0.444 th_f1=0.589 th_fb15=0.589\n",
      "  SR-p53: AP_val=0.210 th_f1=0.513 th_fb15=0.478\n",
      "\n",
      "Saved → v7\\model\\calibration\\thresholds_blend.json\n",
      "\n",
      "✅ Blend ready: use predict_smiles_blend([...], mode='fbeta15' or 'f1').\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "BASE      = Path(\"v7\")\n",
    "FUSED_DIR = BASE / \"data\" / \"fused\"\n",
    "CAL_DIR   = BASE / \"model\" / \"calibration\"\n",
    "ENS_DIR   = BASE / \"model\" / \"ensembles\"\n",
    "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Expect these in memory from earlier cold-start cell:\n",
    "# v7_shared (with .shared_head), HEADS (specialists), LABEL_NAMES, temps (specialist temps)\n",
    "assert 'v7_shared' in globals() and 'HEADS' in globals() and 'LABEL_NAMES' in globals() and 'temps' in globals()\n",
    "\n",
    "# ---- load val fused + labels/mask ----\n",
    "Xva = np.load(FUSED_DIR / \"val_fused.npy\")     # (N,768)\n",
    "Yva = np.load(FUSED_DIR / \"val_Y.npy\")         # (N,12)\n",
    "Mva = np.load(FUSED_DIR / \"val_mask.npy\")      # (N,12) True where missing\n",
    "\n",
    "Xva_t = torch.tensor(Xva, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---- helper: fit per-label temperature (on logits) ----\n",
    "def fit_temperature(logits: np.ndarray, y: np.ndarray, max_iter=200, lr=0.05) -> float:\n",
    "    t = torch.tensor([1.0], dtype=torch.float32, requires_grad=True, device=device)\n",
    "    x = torch.tensor(logits, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(y,      dtype=torch.float32, device=device)\n",
    "    opt = torch.optim.Adam([t], lr=lr)\n",
    "    for _ in range(max_iter):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z = x / (t.clamp(min=1e-3))\n",
    "        p = torch.sigmoid(z).clamp(1e-6, 1-1e-6)\n",
    "        loss = - (y*torch.log(p) + (1-y)*torch.log(1-p)).mean()\n",
    "        loss.backward(); opt.step()\n",
    "    return float(t.detach().cpu().item())\n",
    "\n",
    "def best_thresholds(y_true: np.ndarray, probs: np.ndarray):\n",
    "    prec, rec, th = precision_recall_curve(y_true, probs)\n",
    "    eps = 1e-8\n",
    "    f1 = (2*prec*rec) / np.maximum(prec+rec, eps)\n",
    "    beta = 1.5\n",
    "    fb = ((1+beta**2)*prec*rec) / np.maximum((beta**2)*prec + rec, eps)\n",
    "    th_f1 = th[np.nanargmax(f1[1:])] if th.size>0 else 0.5\n",
    "    th_fb = th[np.nanargmax(fb[1:])] if th.size>0 else 0.5\n",
    "    try:\n",
    "        ap = float(average_precision_score(y_true, probs))\n",
    "    except Exception:\n",
    "        ap = float(\"nan\")\n",
    "    return {\"th_f1\": float(th_f1), \"th_fbeta15\": float(th_fb), \"ap_val\": ap}\n",
    "\n",
    "# ---- 1) Calibrate SHARED head per label on val ----\n",
    "print(\"Calibrating shared head temperatures on val...\")\n",
    "logits_shared = v7_shared.shared_head(Xva_t).detach().cpu().numpy()  # (N,12)\n",
    "temps_shared = {}\n",
    "for j, lbl in enumerate(LABEL_NAMES):\n",
    "    valid = ~Mva[:, j]\n",
    "    if valid.sum() == 0 or np.all(Yva[valid, j] == Yva[valid, j][0]):\n",
    "        temps_shared[lbl] = 1.0\n",
    "        continue\n",
    "    T = fit_temperature(logits_shared[valid, j], Yva[valid, j])\n",
    "    temps_shared[lbl] = T\n",
    "    print(f\"  {lbl}: T_shared={T:.3f}\")\n",
    "(Path(CAL_DIR / \"temps_shared.json\")).write_text(json.dumps(temps_shared, indent=2))\n",
    "print(\"Saved →\", CAL_DIR / \"temps_shared.json\")\n",
    "\n",
    "# ---- 2) Build BLENDED probs on val (alpha specialist, (1-alpha) shared) ----\n",
    "ALPHA = 0.8  # weight on specialist; tweak if desired\n",
    "print(f\"\\nBlending probs on val with alpha={ALPHA:.2f} (specialist weight)\")\n",
    "\n",
    "# specialist logits on val\n",
    "spec_logits = np.zeros_like(logits_shared)\n",
    "with torch.no_grad():\n",
    "    for j, lbl in enumerate(LABEL_NAMES):\n",
    "        head = HEADS[lbl]\n",
    "        spec_logits[:, j] = head(Xva_t).detach().cpu().numpy()\n",
    "\n",
    "# calibrate both streams\n",
    "p_spec_val   = np.zeros_like(spec_logits)\n",
    "p_shared_val = np.zeros_like(logits_shared)\n",
    "for j, lbl in enumerate(LABEL_NAMES):\n",
    "    T_spec   = max(float(temps.get(lbl, 1.0)), 1e-3)\n",
    "    T_shared = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "    p_spec_val[:, j]   = 1. / (1. + np.exp(-spec_logits[:, j]   / T_spec))\n",
    "    p_shared_val[:, j] = 1. / (1. + np.exp(-logits_shared[:, j] / T_shared))\n",
    "\n",
    "p_blend_val = ALPHA * p_spec_val + (1-ALPHA) * p_shared_val\n",
    "p_blend_val = np.clip(p_blend_val, 0.0, 1.0)\n",
    "\n",
    "# ---- 3) Refit thresholds for BLEND on val ----\n",
    "thresholds_blend = {}\n",
    "for j, lbl in enumerate(LABEL_NAMES):\n",
    "    valid = ~Mva[:, j]\n",
    "    if valid.sum() == 0 or np.all(Yva[valid, j] == Yva[valid, j][0]):\n",
    "        thresholds_blend[lbl] = {\"th_f1\": 0.5, \"th_fbeta15\": 0.5, \"ap_val\": float(\"nan\")}\n",
    "        continue\n",
    "    thresholds_blend[lbl] = best_thresholds(Yva[valid, j], p_blend_val[valid, j])\n",
    "    print(f\"  {lbl}: AP_val={thresholds_blend[lbl]['ap_val']:.3f} th_f1={thresholds_blend[lbl]['th_f1']:.3f} th_fb15={thresholds_blend[lbl]['th_fbeta15']:.3f}\")\n",
    "\n",
    "(Path(CAL_DIR / \"thresholds_blend.json\")).write_text(json.dumps({\n",
    "    \"alpha\": ALPHA,\n",
    "    \"thresholds\": thresholds_blend\n",
    "}, indent=2))\n",
    "print(\"\\nSaved →\", CAL_DIR / \"thresholds_blend.json\")\n",
    "\n",
    "# ---- 4) Provide a convenience predictor using the BLEND (keep specialist predictor unchanged) ----\n",
    "def predict_smiles_blend(smiles_list, mode: str = \"fbeta15\", alpha: float = ALPHA):\n",
    "    \"\"\"\n",
    "    Returns list[dict]: per SMILES -> label -> {prob_spec, prob_shared, prob_blend, decision}\n",
    "    \"\"\"\n",
    "    assert mode in (\"f1\",\"fbeta15\")\n",
    "    # fused features from shared encoders (desc branch is already wired)\n",
    "    fused = fused_from_smiles(smiles_list)  # (B,768)\n",
    "    out = []\n",
    "    X = fused  # torch Tensor\n",
    "    with torch.no_grad():\n",
    "        logits_shared = v7_shared.shared_head(X).detach().cpu().numpy()\n",
    "    for i in range(X.size(0)):\n",
    "        row = {}\n",
    "        xi = X[i:i+1]\n",
    "        for j, lbl in enumerate(LABEL_NAMES):\n",
    "            # specialist\n",
    "            with torch.no_grad():\n",
    "                logit_spec = HEADS[lbl](xi).item()\n",
    "            T_spec   = max(float(temps.get(lbl, 1.0)), 1e-3)\n",
    "            p_spec   = 1. / (1. + math.e**(-logit_spec / T_spec))\n",
    "            # shared\n",
    "            T_shared = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "            logit_sh = logits_shared[i, j]\n",
    "            p_shared = 1. / (1. + math.e**(-logit_sh   / T_shared))\n",
    "            # blend\n",
    "            p_blend = alpha * p_spec + (1-alpha) * p_shared\n",
    "            # threshold (use blended thresholds we just computed)\n",
    "            th = thresholds_blend[lbl][\"th_fbeta15\"] if mode==\"fbeta15\" else thresholds_blend[lbl][\"th_f1\"]\n",
    "            row[lbl] = {\n",
    "                \"prob_spec\": float(p_spec),\n",
    "                \"prob_shared\": float(p_shared),\n",
    "                \"prob_blend\": float(p_blend),\n",
    "                \"decision\": bool(p_blend >= float(th)),\n",
    "            }\n",
    "        out.append(row)\n",
    "    return out\n",
    "\n",
    "print(\"\\n✅ Blend ready: use predict_smiles_blend([...], mode='fbeta15' or 'f1').\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ccd367",
   "metadata": {},
   "source": [
    "### 3: Evaluate on test set & export CSV (choose specialist or blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7bf8aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved: v7\\results\\inference\\predictions_test_blend_fbeta15.csv\n",
      "\n",
      "Summary (test):\n",
      "{\n",
      "  \"mode\": \"blend\",\n",
      "  \"threshold_mode\": \"fbeta15\",\n",
      "  \"macro_pr_auc\": 0.3208,\n",
      "  \"micro_precision\": 0.2079,\n",
      "  \"micro_recall\": 0.5734,\n",
      "  \"micro_f1\": 0.3052\n",
      "}\n",
      "Per-label AP saved in report JSON.\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import average_precision_score, precision_recall_curve\n",
    "\n",
    "BASE       = Path(\"v7\")\n",
    "PREP_DIR   = BASE / \"data\" / \"prepared\"\n",
    "FUSED_DIR  = BASE / \"data\" / \"fused\"\n",
    "RESULTS_DIR= BASE / \"results\" / \"inference\"\n",
    "RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CAL_DIR    = BASE / \"model\" / \"calibration\"\n",
    "\n",
    "# Choose which predictor to use:\n",
    "USE_BLEND = True     # True → use predict_smiles_blend; False → use specialist-only predict_smiles\n",
    "MODE      = \"fbeta15\"  # \"fbeta15\" or \"f1\"\n",
    "\n",
    "# Load test blobs\n",
    "blob = np.load(PREP_DIR / \"test.npz\", allow_pickle=True)\n",
    "smiles = [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "Yte    = blob[\"Y\"].astype(np.float32)\n",
    "Mte    = blob[\"y_missing_mask\"].astype(bool)\n",
    "\n",
    "# Also load fused for test to speed shared head for blend\n",
    "Xte_fused = np.load(FUSED_DIR / \"test_fused.npy\") if (FUSED_DIR / \"test_fused.npy\").exists() else None\n",
    "\n",
    "# Ensure thresholds for selected path\n",
    "if USE_BLEND:\n",
    "    data = json.loads((CAL_DIR / \"thresholds_blend.json\").read_text())\n",
    "    thresholds_blend = data[\"thresholds\"]\n",
    "else:\n",
    "    thresholds_spec = json.loads((CAL_DIR / \"thresholds.json\").read_text())\n",
    "\n",
    "rows = []\n",
    "probs_mat = np.zeros((len(smiles), len(LABEL_NAMES)), dtype=np.float32)\n",
    "\n",
    "if USE_BLEND:\n",
    "    # Compute via blend predictor\n",
    "    preds = predict_smiles_blend(smiles, mode=MODE)\n",
    "    for i, (smi, rec) in enumerate(zip(smiles, preds)):\n",
    "        row = {\"smiles\": smi}\n",
    "        for j, lbl in enumerate(LABEL_NAMES):\n",
    "            p = rec[lbl][\"prob_blend\"]\n",
    "            d = int(rec[lbl][\"decision\"])\n",
    "            row[f\"{lbl}_prob\"] = p\n",
    "            row[f\"{lbl}_pred\"] = d\n",
    "            probs_mat[i, j] = p\n",
    "        rows.append(row)\n",
    "    out_csv = RESULTS_DIR / f\"predictions_test_blend_{MODE}.csv\"\n",
    "else:\n",
    "    # Specialist-only\n",
    "    preds = predict_smiles(smiles, threshold_mode=MODE)\n",
    "    for i, (smi, rec) in enumerate(zip(smiles, preds)):\n",
    "        row = {\"smiles\": smi}\n",
    "        for j, lbl in enumerate(LABEL_NAMES):\n",
    "            p = rec[lbl][\"prob_cal\"]\n",
    "            d = int(rec[lbl][\"decision\"])\n",
    "            row[f\"{lbl}_prob\"] = p\n",
    "            row[f\"{lbl}_pred\"] = d\n",
    "            probs_mat[i, j] = p\n",
    "        rows.append(row)\n",
    "    out_csv = RESULTS_DIR / f\"predictions_test_specialist_{MODE}.csv\"\n",
    "\n",
    "pd.DataFrame(rows).to_csv(out_csv, index=False)\n",
    "print(\"✅ Saved:\", out_csv)\n",
    "\n",
    "# ---- Tiny metrics (test) ----\n",
    "per_label_ap = {}\n",
    "for j, lbl in enumerate(LABEL_NAMES):\n",
    "    valid = ~Mte[:, j]\n",
    "    if valid.sum() == 0 or np.all(Yte[valid, j] == Yte[valid, j][0]):\n",
    "        per_label_ap[lbl] = float(\"nan\"); continue\n",
    "    try:\n",
    "        per_label_ap[lbl] = float(average_precision_score(Yte[valid, j], probs_mat[valid, j]))\n",
    "    except Exception:\n",
    "        per_label_ap[lbl] = float(\"nan\")\n",
    "\n",
    "macro_pr = float(np.nanmean([v for v in per_label_ap.values()]))\n",
    "\n",
    "# micro P/R/F1 using chosen thresholds\n",
    "tp = fp = fn = 0\n",
    "for i in range(len(smiles)):\n",
    "    for j, lbl in enumerate(LABEL_NAMES):\n",
    "        if Mte[i, j]: \n",
    "            continue\n",
    "        truth = int(Yte[i, j])\n",
    "        pred  = rows[i][f\"{lbl}_pred\"]\n",
    "        tp += int(pred == 1 and truth == 1)\n",
    "        fp += int(pred == 1 and truth == 0)\n",
    "        fn += int(pred == 0 and truth == 1)\n",
    "\n",
    "prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "\n",
    "report = {\n",
    "    \"mode\": (\"blend\" if USE_BLEND else \"specialist\"),\n",
    "    \"threshold_mode\": MODE,\n",
    "    \"macro_pr_auc\": macro_pr,\n",
    "    \"micro_precision\": prec,\n",
    "    \"micro_recall\": rec,\n",
    "    \"micro_f1\": f1,\n",
    "    \"per_label_ap\": per_label_ap\n",
    "}\n",
    "report_path = RESULTS_DIR / f\"test_report_{'blend' if USE_BLEND else 'specialist'}_{MODE}.json\"\n",
    "report_path.write_text(json.dumps(report, indent=2))\n",
    "print(\"\\nSummary (test):\")\n",
    "print(json.dumps({k: (round(v,4) if isinstance(v, float) else v) for k,v in report.items() if k!='per_label_ap'}, indent=2))\n",
    "print(\"Per-label AP saved in report JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b5054b8",
   "metadata": {},
   "source": [
    "### 4:test reg after cell 2& 3 (gave very strong results!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09817076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Blend test rig ready. Example:\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "BASE       = Path(\"v7\")\n",
    "PREP_DIR   = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR   = BASE / \"data\" / \"descriptors\"\n",
    "MODEL_DIR  = BASE / \"model\"\n",
    "CKPT_BEST  = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "ENS_DIR    = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR    = MODEL_DIR / \"calibration\"\n",
    "\n",
    "assert CKPT_BEST.exists(), f\"Missing shared checkpoint: {CKPT_BEST}\"\n",
    "assert (PREP_DIR / \"dataset_manifest.json\").exists(), \"Missing dataset manifest.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Labels & calibration artifacts ---\n",
    "ds_manifest = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABEL_NAMES: List[str] = ds_manifest[\"labels\"]\n",
    "DESC_IN_DIM = ds_manifest[\"n_features\"]  # 208\n",
    "\n",
    "temps_spec    = json.loads((CAL_DIR / \"temps.json\").read_text())           # specialist\n",
    "temps_shared  = json.loads((CAL_DIR / \"temps_shared.json\").read_text())    # shared\n",
    "blend_payload = json.loads((CAL_DIR / \"thresholds_blend.json\").read_text())\n",
    "ALPHA         = float(blend_payload.get(\"alpha\", 0.8))\n",
    "thr_blend     = blend_payload[\"thresholds\"]  # label -> {th_f1, th_fbeta15, ap_val}\n",
    "\n",
    "# --- Text encoder (ChemBERTa) ---\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "class ChemBERTaEncoder(nn.Module):\n",
    "    def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "        self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "        self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "        self.ln = nn.LayerNorm(fusion_dim)\n",
    "    def forward(self, smiles_list: List[str], max_length=256, add_special_tokens=True):\n",
    "        enc = self.tokenizer(list(smiles_list), padding=True, truncation=True,\n",
    "                             max_length=max_length, add_special_tokens=add_special_tokens,\n",
    "                             return_tensors=\"pt\")\n",
    "        input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "        out = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,L,H)\n",
    "        toks = self.ln(self.proj(out))  # (B,L,256)\n",
    "        return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "# --- Graph encoder (names matched to checkpoint) ---\n",
    "from rdkit import Chem\n",
    "ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "\n",
    "def _one_hot(v, choices):\n",
    "    z = [0]*len(choices)\n",
    "    if v in choices: z[choices.index(v)] = 1\n",
    "    return z\n",
    "\n",
    "def _bucket_oh(v, lo, hi):\n",
    "    buckets = list(range(lo, hi+1))\n",
    "    o = [0]*(len(buckets)+1)\n",
    "    idx = v - lo\n",
    "    o[idx if 0 <= idx < len(buckets) else -1] = 1\n",
    "    return o\n",
    "\n",
    "def _atom_feat(atom):\n",
    "    hybs = [Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "    chir = [Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "    sym = atom.GetSymbol()\n",
    "    feat = _one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST+[\"other\"])\n",
    "    feat += _bucket_oh(atom.GetDegree(), 0, 5)\n",
    "    feat += _bucket_oh(atom.GetFormalCharge(), -2, 2)\n",
    "    feat += (_one_hot(atom.GetHybridization(), hybs)+[0])\n",
    "    feat += [int(atom.GetIsAromatic())]\n",
    "    feat += [int(atom.IsInRing())]\n",
    "    feat += _one_hot(atom.GetChiralTag(), chir)\n",
    "    feat += _bucket_oh(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)\n",
    "    feat += _bucket_oh(atom.GetTotalValence(), 0, 5)\n",
    "    feat += [atom.GetMass()/200.0]\n",
    "    return feat  # ~51 dims\n",
    "\n",
    "def _smiles_to_graph(smi, max_nodes=128):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None or mol.GetNumAtoms() == 0:\n",
    "        return np.zeros((0,0), dtype=np.float32), np.zeros((0,0), dtype=np.float32)\n",
    "    feats = [_atom_feat(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "    x = np.asarray(feats, dtype=np.float32)\n",
    "    N = mol.GetNumAtoms()\n",
    "    adj = np.zeros((N, N), dtype=np.float32)\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        adj[i, j] = 1.0; adj[j, i] = 1.0\n",
    "    if N > max_nodes:\n",
    "        x = x[:max_nodes]; adj = adj[:max_nodes, :max_nodes]\n",
    "    return x, adj\n",
    "\n",
    "def _collate_graphs(smiles_batch, max_nodes=128):\n",
    "    graphs = [_smiles_to_graph(s) for s in smiles_batch]\n",
    "    Nmax = max([g[0].shape[0] for g in graphs] + [1])\n",
    "    Fnode = graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "    B = len(graphs)\n",
    "    X = np.zeros((B, Nmax, Fnode), dtype=np.float32)\n",
    "    A = np.zeros((B, Nmax, Nmax), dtype=np.float32)\n",
    "    M = np.zeros((B, Nmax), dtype=np.int64)\n",
    "    for i, (x, a) in enumerate(graphs):\n",
    "        n = x.shape[0]\n",
    "        if n == 0: continue\n",
    "        X[i, :n, :] = x\n",
    "        A[i, :n, :n] = a\n",
    "        M[i, :n] = 1\n",
    "    return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, h=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "        self.mlp = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "    def forward(self, x, adj, mask):\n",
    "        out = (1.0 + self.eps) * x + torch.matmul(adj, x)\n",
    "        out = self.mlp(out)\n",
    "        return out * mask.unsqueeze(-1).to(out.dtype)\n",
    "\n",
    "class GraphGINEncoder(nn.Module):\n",
    "    def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(nn.Linear(node_in_dim, hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden_dim, p) for _ in range(n_layers)])\n",
    "        self.out_ln = nn.LayerNorm(hidden_dim)  # name matches checkpoint\n",
    "    def forward(self, smiles_list: List[str], max_nodes=128):\n",
    "        X, A, M = _collate_graphs(smiles_list, max_nodes=max_nodes)\n",
    "        h = self.inp(X)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, A, M)\n",
    "        return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "# --- Fusion & heads ---\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "    denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "    return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "        self.ln  = nn.LayerNorm(dim)\n",
    "        self.do  = nn.Dropout(p)\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "        Q = text_tokens.transpose(0,1)   # (L,B,D)\n",
    "        K = graph_nodes.transpose(0,1)   # (N,B,D)\n",
    "        V = graph_nodes.transpose(0,1)\n",
    "        kpm = (graph_mask == 0)          # (B,N)\n",
    "        attn, _ = self.mha(Q, K, V, key_padding_mask=kpm)\n",
    "        attn = attn.transpose(0,1)       # (B,L,D)\n",
    "        return self.ln(text_tokens + self.do(attn))\n",
    "\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim), nn.GELU(), nn.Dropout(p)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    # name 'mlp' matches checkpoint ('shared_head.mlp.*')\n",
    "    def __init__(self, dim=256, n_labels=12, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, fused_vec): return self.mlp(fused_vec)\n",
    "\n",
    "class V7FusionModel(nn.Module):\n",
    "    def __init__(self, text_encoder, graph_encoder, desc_in_dim=208, dim=256, n_labels=12, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.text_encoder=text_encoder\n",
    "        self.graph_encoder=graph_encoder\n",
    "        self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "        self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "        self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "    def forward(self, smiles_list, desc_feats):\n",
    "        tt, tm = self.text_encoder(smiles_list, max_length=256)\n",
    "        gn, gm = self.graph_encoder(smiles_list, max_nodes=128)\n",
    "        tta = self.cross(tt.to(device), tm.to(device), gn.to(device), gm.to(device))\n",
    "        de  = self.desc_mlp(desc_feats.to(device))\n",
    "        text_pool  = masked_mean(tta, tm.to(device), 1)\n",
    "        graph_pool = masked_mean(gn.to(device),  gm.to(device), 1)\n",
    "        fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "        logits = self.shared_head(fused)\n",
    "        return logits, fused\n",
    "\n",
    "# Build model & load checkpoint\n",
    "text_encoder = ChemBERTaEncoder().to(device)\n",
    "graph_encoder= GraphGINEncoder().to(device)\n",
    "v7_shared    = V7FusionModel(text_encoder, graph_encoder, desc_in_dim=DESC_IN_DIM, n_labels=len(LABEL_NAMES)).to(device)\n",
    "ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "v7_shared.eval()\n",
    "\n",
    "# Specialist heads (same as trained)\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _load_best_head(label: str) -> nn.Module:\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if mfile.exists():\n",
    "            try:\n",
    "                ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "                cands.append((ap, sd))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not cands: raise FileNotFoundError(f\"No trained heads for label {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(ck[\"model\"], strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS: Dict[str, nn.Module] = {lbl: _load_best_head(lbl) for lbl in LABEL_NAMES}\n",
    "\n",
    "# Descriptors for ad-hoc inputs: standardized zeros (keeps it simple & robust)\n",
    "def prepare_desc_matrix(smiles_list: List[str]) -> torch.Tensor:\n",
    "    Z = np.zeros((len(smiles_list), DESC_IN_DIM), dtype=np.float32)\n",
    "    return torch.tensor(Z, dtype=torch.float32, device=device)\n",
    "\n",
    "# Normalize SMARTS→SMILES if needed\n",
    "def normalize_smiles_or_smarts(s: str) -> str:\n",
    "    if not isinstance(s, str): s = str(s)\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol: return Chem.MolToSmiles(mol)\n",
    "    q = Chem.MolFromSmarts(s)\n",
    "    if q:\n",
    "        try:\n",
    "            smi = Chem.MolToSmiles(q)\n",
    "            return smi if smi else s\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "@torch.no_grad()\n",
    "def fused_from_smiles(smiles_list: List[str]) -> torch.Tensor:\n",
    "    smiles_list = [normalize_smiles_or_smarts(s) for s in smiles_list]\n",
    "    desc = prepare_desc_matrix(smiles_list)\n",
    "    logits_sh, fused = v7_shared(smiles_list, desc)  # logits not used here directly\n",
    "    return fused  # (B,768)\n",
    "\n",
    "def predict_one_blend(smi: str, mode: str = \"fbeta15\", topk: int = 5):\n",
    "    \"\"\"\n",
    "    Blended prediction for one SMILES/SMARTS using:\n",
    "      prob_blend = alpha*P_spec + (1-alpha)*P_shared\n",
    "    Thresholds taken from thresholds_blend.json for chosen mode (\"f1\" or \"fbeta15\").\n",
    "    Prints a clean summary and returns a dict[label]->details.\n",
    "    \"\"\"\n",
    "    assert mode in (\"f1\",\"fbeta15\")\n",
    "    fused = fused_from_smiles([smi])\n",
    "    x = fused[0:1]\n",
    "\n",
    "    # Shared logits and calibrated probs\n",
    "    with torch.no_grad():\n",
    "        logits_shared = v7_shared.shared_head(x).detach().cpu().numpy()[0]  # (12,)\n",
    "\n",
    "    rec = {}\n",
    "    for j, lbl in enumerate(LABEL_NAMES):\n",
    "        # Specialist prob (with its temperature)\n",
    "        with torch.no_grad():\n",
    "            logit_spec = HEADS[lbl](x).item()\n",
    "        T_spec   = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "        p_spec   = 1. / (1. + math.e**(-logit_spec / T_spec))\n",
    "\n",
    "        # Shared prob (with shared temperature)\n",
    "        T_shared = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "        p_shared = 1. / (1. + math.e**(-float(logits_shared[j]) / T_shared))\n",
    "\n",
    "        # Blend\n",
    "        p_blend = ALPHA * p_spec + (1.0 - ALPHA) * p_shared\n",
    "\n",
    "        # Threshold\n",
    "        th = thr_blend[lbl][\"th_fbeta15\"] if mode==\"fbeta15\" else thr_blend[lbl][\"th_f1\"]\n",
    "        rec[lbl] = {\n",
    "            \"prob_spec\": float(p_spec),\n",
    "            \"prob_shared\": float(p_shared),\n",
    "            \"prob_blend\": float(p_blend),\n",
    "            \"threshold\": float(th),\n",
    "            \"decision\": bool(p_blend >= float(th)),\n",
    "        }\n",
    "\n",
    "    # Pretty print\n",
    "    print(\"\\nSMILES/SMARTS:\", smi, f\"(alpha={ALPHA:.2f}, mode={mode})\")\n",
    "    top = sorted([(lbl, d[\"prob_blend\"], d[\"decision\"]) for lbl, d in rec.items()],\n",
    "                 key=lambda z: z[1], reverse=True)[:topk]\n",
    "    for lbl, p, dec in top:\n",
    "        th = rec[lbl][\"threshold\"]\n",
    "        print(f\"  {lbl:12s}  prob_blend={p:.3f}  th={th:.3f}  → pred={int(dec)}\")\n",
    "    pos = [lbl for lbl, d in rec.items() if d[\"decision\"]]\n",
    "    print(\"  Positives:\", (\", \".join(sorted(pos)) if pos else \"none\"))\n",
    "    return rec\n",
    "\n",
    "print(\"✅ Blend test rig ready. Example:\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8eb95f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMILES/SMARTS: O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1 (alpha=0.80, mode=fbeta15)\n",
      "  NR-AhR        prob_blend=0.700  th=0.642  → pred=1\n",
      "  SR-MMP        prob_blend=0.680  th=0.589  → pred=1\n",
      "  SR-ARE        prob_blend=0.588  th=0.528  → pred=1\n",
      "  NR-ER         prob_blend=0.556  th=0.480  → pred=1\n",
      "  SR-p53        prob_blend=0.551  th=0.478  → pred=1\n",
      "  NR-ER-LBD     prob_blend=0.499  th=0.589  → pred=0\n",
      "  NR-Aromatase  prob_blend=0.498  th=0.474  → pred=1\n",
      "  SR-ATAD5      prob_blend=0.479  th=0.483  → pred=0\n",
      "  NR-PPAR-gamma  prob_blend=0.478  th=0.427  → pred=1\n",
      "  SR-HSE        prob_blend=0.460  th=0.459  → pred=1\n",
      "  NR-AR         prob_blend=0.432  th=0.653  → pred=0\n",
      "  NR-AR-LBD     prob_blend=0.423  th=0.621  → pred=0\n",
      "  Positives: NR-AhR, NR-Aromatase, NR-ER, NR-PPAR-gamma, SR-ARE, SR-HSE, SR-MMP, SR-p53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NR-AR': {'prob_spec': 0.5297900819654926,\n",
       "  'prob_shared': 0.039244673619722704,\n",
       "  'prob_blend': 0.43168100029633866,\n",
       "  'threshold': 0.653282642364502,\n",
       "  'decision': False},\n",
       " 'NR-AR-LBD': {'prob_spec': 0.527085290472785,\n",
       "  'prob_shared': 0.007392770345504927,\n",
       "  'prob_blend': 0.42314678644732895,\n",
       "  'threshold': 0.6206690669059753,\n",
       "  'decision': False},\n",
       " 'NR-AhR': {'prob_spec': 0.6696848171077073,\n",
       "  'prob_shared': 0.8209122313828833,\n",
       "  'prob_blend': 0.6999302999627425,\n",
       "  'threshold': 0.6417197585105896,\n",
       "  'decision': True},\n",
       " 'NR-Aromatase': {'prob_spec': 0.5691290816416109,\n",
       "  'prob_shared': 0.21573499976865418,\n",
       "  'prob_blend': 0.4984502652670196,\n",
       "  'threshold': 0.4737248420715332,\n",
       "  'decision': True},\n",
       " 'NR-ER': {'prob_spec': 0.5319725845509377,\n",
       "  'prob_shared': 0.6530290641575132,\n",
       "  'prob_blend': 0.5561838804722528,\n",
       "  'threshold': 0.4802268147468567,\n",
       "  'decision': True},\n",
       " 'NR-ER-LBD': {'prob_spec': 0.5796462377733185,\n",
       "  'prob_shared': 0.17392469045136985,\n",
       "  'prob_blend': 0.49850192830892875,\n",
       "  'threshold': 0.5893745422363281,\n",
       "  'decision': False},\n",
       " 'NR-PPAR-gamma': {'prob_spec': 0.5515514734433192,\n",
       "  'prob_shared': 0.1855413765345053,\n",
       "  'prob_blend': 0.4783494540615565,\n",
       "  'threshold': 0.42748475074768066,\n",
       "  'decision': True},\n",
       " 'SR-ARE': {'prob_spec': 0.5481504678492859,\n",
       "  'prob_shared': 0.7475649174003929,\n",
       "  'prob_blend': 0.5880333577595073,\n",
       "  'threshold': 0.5278913378715515,\n",
       "  'decision': True},\n",
       " 'SR-ATAD5': {'prob_spec': 0.5547528009570648,\n",
       "  'prob_shared': 0.174235174637529,\n",
       "  'prob_blend': 0.4786492756931577,\n",
       "  'threshold': 0.4825417995452881,\n",
       "  'decision': False},\n",
       " 'SR-HSE': {'prob_spec': 0.5444436734627249,\n",
       "  'prob_shared': 0.12223593661546725,\n",
       "  'prob_blend': 0.4600021260932734,\n",
       "  'threshold': 0.4591193199157715,\n",
       "  'decision': True},\n",
       " 'SR-MMP': {'prob_spec': 0.6408550104292131,\n",
       "  'prob_shared': 0.8353312082985317,\n",
       "  'prob_blend': 0.6797502500030768,\n",
       "  'threshold': 0.589274525642395,\n",
       "  'decision': True},\n",
       " 'SR-p53': {'prob_spec': 0.560059474096727,\n",
       "  'prob_shared': 0.5128437890040615,\n",
       "  'prob_blend': 0.5506163370781939,\n",
       "  'threshold': 0.477995365858078,\n",
       "  'decision': True}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_one_blend(\"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\", mode=\"fbeta15\", topk=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8f1c687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SMILES/SMARTS: O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1 (alpha=0.80, mode=f1)\n",
      "  NR-AhR        prob_blend=0.700  th=0.709  → pred=0\n",
      "  SR-MMP        prob_blend=0.680  th=0.589  → pred=1\n",
      "  SR-ARE        prob_blend=0.588  th=0.528  → pred=1\n",
      "  NR-ER         prob_blend=0.556  th=0.547  → pred=1\n",
      "  SR-p53        prob_blend=0.551  th=0.513  → pred=1\n",
      "  NR-ER-LBD     prob_blend=0.499  th=0.589  → pred=0\n",
      "  NR-Aromatase  prob_blend=0.498  th=0.564  → pred=0\n",
      "  SR-ATAD5      prob_blend=0.479  th=0.483  → pred=0\n",
      "  NR-PPAR-gamma  prob_blend=0.478  th=0.441  → pred=1\n",
      "  SR-HSE        prob_blend=0.460  th=0.472  → pred=0\n",
      "  NR-AR         prob_blend=0.432  th=0.653  → pred=0\n",
      "  NR-AR-LBD     prob_blend=0.423  th=0.621  → pred=0\n",
      "  Positives: NR-ER, NR-PPAR-gamma, SR-ARE, SR-MMP, SR-p53\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'NR-AR': {'prob_spec': 0.5297900819654926,\n",
       "  'prob_shared': 0.039244673619722704,\n",
       "  'prob_blend': 0.43168100029633866,\n",
       "  'threshold': 0.653282642364502,\n",
       "  'decision': False},\n",
       " 'NR-AR-LBD': {'prob_spec': 0.527085290472785,\n",
       "  'prob_shared': 0.007392770345504927,\n",
       "  'prob_blend': 0.42314678644732895,\n",
       "  'threshold': 0.6206690669059753,\n",
       "  'decision': False},\n",
       " 'NR-AhR': {'prob_spec': 0.6696848171077073,\n",
       "  'prob_shared': 0.8209122313828833,\n",
       "  'prob_blend': 0.6999302999627425,\n",
       "  'threshold': 0.7087583541870117,\n",
       "  'decision': False},\n",
       " 'NR-Aromatase': {'prob_spec': 0.5691290816416109,\n",
       "  'prob_shared': 0.21573499976865418,\n",
       "  'prob_blend': 0.4984502652670196,\n",
       "  'threshold': 0.5641032457351685,\n",
       "  'decision': False},\n",
       " 'NR-ER': {'prob_spec': 0.5319725845509377,\n",
       "  'prob_shared': 0.6530290641575132,\n",
       "  'prob_blend': 0.5561838804722528,\n",
       "  'threshold': 0.547207772731781,\n",
       "  'decision': True},\n",
       " 'NR-ER-LBD': {'prob_spec': 0.5796462377733185,\n",
       "  'prob_shared': 0.17392469045136985,\n",
       "  'prob_blend': 0.49850192830892875,\n",
       "  'threshold': 0.5893745422363281,\n",
       "  'decision': False},\n",
       " 'NR-PPAR-gamma': {'prob_spec': 0.5515514734433192,\n",
       "  'prob_shared': 0.1855413765345053,\n",
       "  'prob_blend': 0.4783494540615565,\n",
       "  'threshold': 0.4406646490097046,\n",
       "  'decision': True},\n",
       " 'SR-ARE': {'prob_spec': 0.5481504678492859,\n",
       "  'prob_shared': 0.7475649174003929,\n",
       "  'prob_blend': 0.5880333577595073,\n",
       "  'threshold': 0.5278913378715515,\n",
       "  'decision': True},\n",
       " 'SR-ATAD5': {'prob_spec': 0.5547528009570648,\n",
       "  'prob_shared': 0.174235174637529,\n",
       "  'prob_blend': 0.4786492756931577,\n",
       "  'threshold': 0.4825417995452881,\n",
       "  'decision': False},\n",
       " 'SR-HSE': {'prob_spec': 0.5444436734627249,\n",
       "  'prob_shared': 0.12223593661546725,\n",
       "  'prob_blend': 0.4600021260932734,\n",
       "  'threshold': 0.47199299931526184,\n",
       "  'decision': False},\n",
       " 'SR-MMP': {'prob_spec': 0.6408550104292131,\n",
       "  'prob_shared': 0.8353312082985317,\n",
       "  'prob_blend': 0.6797502500030768,\n",
       "  'threshold': 0.589274525642395,\n",
       "  'decision': True},\n",
       " 'SR-p53': {'prob_spec': 0.560059474096727,\n",
       "  'prob_shared': 0.5128437890040615,\n",
       "  'prob_blend': 0.5506163370781939,\n",
       "  'threshold': 0.5132721066474915,\n",
       "  'decision': True}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_one_blend(\"O=C(O)Cc1cc(I)c(Oc2ccc(O)c(I)c2)c(I)c1\", mode=\"f1\", topk=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee58d75",
   "metadata": {},
   "source": [
    "## phase 6 (Evaluation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "097898e2",
   "metadata": {},
   "source": [
    "### 1: Ground truth and fused features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271078cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Rebuild] v7\\data\\fused\\test_fused.npy not found → recomputing test fused features...\n",
      "[Rebuild] Saved → v7\\data\\fused\\test_fused.npy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\1952192011.py:373: RuntimeWarning: overflow encountered in exp\n",
      "  def sigmoid(x): return 1.0/(1.0+np.exp(-x))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Evaluation complete.\n",
      " Per-label CSV  → v7\\eval\\per_label_metrics.csv\n",
      " Summary JSON   → v7\\eval\\summary.json\n",
      " PR curves      → v7\\eval\\plots\\pr\n",
      " Reliability    → v7\\eval\\plots\\reliability\n",
      "\n",
      "Global (test):\n",
      "  eval_mode: blend\n",
      "  threshold_mode: fbeta15\n",
      "  n_test: 783\n",
      "  macro_pr_auc: 0.2179368491745162\n",
      "  macro_roc_auc: 0.7544820729548566\n",
      "  micro_precision: 0.2006872852233677\n",
      "  micro_recall: 0.5793650793650794\n",
      "  micro_f1: 0.29811128126595204\n",
      "  avg_true_cardinality: 0.6436781609195402\n",
      "  avg_pred_cardinality: 1.8582375478927202\n",
      "  cardinality_error: 1.21455938697318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\1952192011.py:515: RuntimeWarning: invalid value encountered in cast\n",
      "  true_bin = Yte.copy().astype(int)\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score, roc_auc_score,\n",
    "    precision_recall_curve, roc_curve\n",
    ")\n",
    "\n",
    "# ---- Config ----\n",
    "BASE         = Path(\"v7\")\n",
    "PREP_DIR     = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR     = BASE / \"data\" / \"descriptors\"\n",
    "FUSED_DIR    = BASE / \"data\" / \"fused\"\n",
    "MODEL_DIR    = BASE / \"model\"\n",
    "CAL_DIR      = MODEL_DIR / \"calibration\"\n",
    "ENS_DIR      = MODEL_DIR / \"ensembles\"\n",
    "EVAL_DIR     = BASE / \"eval\"\n",
    "PLOT_PR_DIR  = EVAL_DIR / \"plots\" / \"pr\"\n",
    "PLOT_REL_DIR = EVAL_DIR / \"plots\" / \"reliability\"\n",
    "\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_PR_DIR.mkdir(parents=True, exist_ok=True)\n",
    "PLOT_REL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FUSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Choose path: \"specialist\" OR \"blend\"\n",
    "EVAL_MODE   = \"blend\"       # \"specialist\" or \"blend\"\n",
    "THRESH_MODE = \"fbeta15\"     # \"fbeta15\" or \"f1\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Load manifest & test blobs ----\n",
    "mani_path = PREP_DIR / \"dataset_manifest.json\"\n",
    "assert mani_path.exists(), f\"Missing manifest: {mani_path}\"\n",
    "mani      = json.loads(mani_path.read_text())\n",
    "LABELS    = mani[\"labels\"]\n",
    "N_LABELS  = len(LABELS)\n",
    "DESC_IN_DIM = int(mani[\"n_features\"])  # 208\n",
    "\n",
    "blob_path = PREP_DIR / \"test.npz\"\n",
    "assert blob_path.exists(), f\"Missing test blob: {blob_path}\"\n",
    "blob  = np.load(blob_path, allow_pickle=True)\n",
    "smiles= [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "Yte   = blob[\"Y\"].astype(np.float32)            # (N, L)\n",
    "Mte   = blob[\"y_missing_mask\"].astype(bool)     # (N, L) True where missing\n",
    "N     = Yte.shape[0]\n",
    "\n",
    "# ---- Helper: rebuild fused features if absent ----------------------------\n",
    "def ensure_fused(split: str = \"test\") -> np.ndarray:\n",
    "    \"\"\"Return fused features for split. If missing, recompute and cache.\"\"\"\n",
    "    path = FUSED_DIR / f\"{split}_fused.npy\"\n",
    "    if path.exists():\n",
    "        return np.load(path).astype(np.float32)\n",
    "\n",
    "    print(f\"[Rebuild] {path} not found → recomputing {split} fused features...\")\n",
    "\n",
    "    # 1) Load descriptor transformer\n",
    "    from joblib import load as joblib_load\n",
    "    imputer = joblib_load(DESC_DIR / \"imputer.joblib\")\n",
    "    scaler  = joblib_load(DESC_DIR / \"scaler.joblib\")\n",
    "\n",
    "    # 2) RDKit descriptor function that matches training order via feature_names.txt\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors as RDDesc\n",
    "\n",
    "    feat_list_path = DESC_DIR / \"feature_names.txt\"\n",
    "    assert feat_list_path.exists(), f\"Missing feature_names.txt at {feat_list_path}\"\n",
    "    feature_names = [ln.strip() for ln in feat_list_path.read_text().splitlines() if ln.strip()]\n",
    "    # Build callables dict for RDKit Descriptors.*\n",
    "    rd_fns = {name: getattr(RDDesc, name, None) for name in feature_names}\n",
    "\n",
    "    def compute_rdkit_descriptors_for_smiles(smiles_list: List[str]) -> np.ndarray:\n",
    "        rows = []\n",
    "        for smi in smiles_list:\n",
    "            mol = Chem.MolFromSmiles(smi)\n",
    "            if mol is None:\n",
    "                # keep row length consistent; fill with NaN\n",
    "                rows.append([np.nan]*len(feature_names))\n",
    "                continue\n",
    "            vals = []\n",
    "            for name in feature_names:\n",
    "                fn = rd_fns.get(name, None)\n",
    "                if fn is None:\n",
    "                    vals.append(np.nan)\n",
    "                    continue\n",
    "                try:\n",
    "                    v = fn(mol)\n",
    "                except Exception:\n",
    "                    v = np.nan\n",
    "                vals.append(float(v) if (v is not None and np.isfinite(v)) else np.nan)\n",
    "            rows.append(vals)\n",
    "        return np.asarray(rows, dtype=np.float32)\n",
    "\n",
    "    # 3) Build shared model (text+graph encoders + desc MLP) and load checkpoint\n",
    "    CKPT_BEST = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "    assert CKPT_BEST.exists(), f\"Missing shared checkpoint: {CKPT_BEST}\"\n",
    "\n",
    "    # -- Text encoder (ChemBERTa) --\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    class ChemBERTaEncoder(nn.Module):\n",
    "        def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "            super().__init__()\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "            self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "            self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "            self.ln = nn.LayerNorm(fusion_dim)\n",
    "        def forward(self, smiles_list: List[str], max_length=256, add_special_tokens=True):\n",
    "            enc = self.tokenizer(list(smiles_list), padding=True, truncation=True,\n",
    "                                 max_length=max_length, add_special_tokens=add_special_tokens,\n",
    "                                 return_tensors=\"pt\")\n",
    "            input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "            out = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,L,H)\n",
    "            toks = self.ln(self.proj(out))  # (B,L,256)\n",
    "            return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "    # -- Graph encoder (names match checkpoint) --\n",
    "    ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "    def _one_hot(v, choices):\n",
    "        z = [0]*len(choices)\n",
    "        if v in choices: z[choices.index(v)] = 1\n",
    "        return z\n",
    "    def _bucket_oh(v, lo, hi):\n",
    "        buckets = list(range(lo, hi+1))\n",
    "        o = [0]*(len(buckets)+1)\n",
    "        idx = v - lo\n",
    "        o[idx if 0 <= idx < len(buckets) else -1] = 1\n",
    "        return o\n",
    "    def _atom_feat(atom):\n",
    "        hybs = [Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP,\n",
    "                Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3,\n",
    "                Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "        chir = [Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "                Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "                Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "                Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "        sym = atom.GetSymbol()\n",
    "        feat = _one_hot(sym if sym in ATOM_LIST else \"other\", ATOM_LIST+[\"other\"])\n",
    "        feat += _bucket_oh(atom.GetDegree(), 0, 5)\n",
    "        feat += _bucket_oh(atom.GetFormalCharge(), -2, 2)\n",
    "        feat += (_one_hot(atom.GetHybridization(), hybs)+[0])\n",
    "        feat += [int(atom.GetIsAromatic())]\n",
    "        feat += [int(atom.IsInRing())]\n",
    "        feat += _one_hot(atom.GetChiralTag(), chir)\n",
    "        feat += _bucket_oh(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)\n",
    "        feat += _bucket_oh(atom.GetTotalValence(), 0, 5)\n",
    "        feat += [atom.GetMass()/200.0]\n",
    "        return feat  # ~51 dims\n",
    "    def _smiles_to_graph(smi, max_nodes=128):\n",
    "        mol = Chem.MolFromSmiles(smi)\n",
    "        if mol is None or mol.GetNumAtoms() == 0:\n",
    "            return np.zeros((0,0), dtype=np.float32), np.zeros((0,0), dtype=np.float32)\n",
    "        feats = [_atom_feat(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())]\n",
    "        x = np.asarray(feats, dtype=np.float32)\n",
    "        N = mol.GetNumAtoms()\n",
    "        adj = np.zeros((N, N), dtype=np.float32)\n",
    "        for b in mol.GetBonds():\n",
    "            i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "            adj[i, j] = 1.0; adj[j, i] = 1.0\n",
    "        if N > max_nodes:\n",
    "            x = x[:max_nodes]; adj = adj[:max_nodes, :max_nodes]\n",
    "        return x, adj\n",
    "    def _collate_graphs(smiles_batch, max_nodes=128):\n",
    "        graphs = [_smiles_to_graph(s) for s in smiles_batch]\n",
    "        Nmax = max([g[0].shape[0] for g in graphs] + [1])\n",
    "        Fnode = graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "        B = len(graphs)\n",
    "        X = np.zeros((B, Nmax, Fnode), dtype=np.float32)\n",
    "        A = np.zeros((B, Nmax, Nmax), dtype=np.float32)\n",
    "        M = np.zeros((B, Nmax), dtype=np.int64)\n",
    "        for i, (x, a) in enumerate(graphs):\n",
    "            n = x.shape[0]\n",
    "            if n == 0: continue\n",
    "            X[i, :n, :] = x\n",
    "            A[i, :n, :n] = a\n",
    "            M[i, :n] = 1\n",
    "        return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "    class GINLayer(nn.Module):\n",
    "        def __init__(self, h=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "            self.mlp = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "        def forward(self, x, adj, mask):\n",
    "            out = (1.0 + self.eps) * x + torch.matmul(adj, x)\n",
    "            out = self.mlp(out)\n",
    "            return out * mask.unsqueeze(-1).to(out.dtype)\n",
    "\n",
    "    class GraphGINEncoder(nn.Module):\n",
    "        def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.inp = nn.Sequential(nn.Linear(node_in_dim, hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "            self.layers = nn.ModuleList([GINLayer(hidden_dim, p) for _ in range(n_layers)])\n",
    "            self.out_ln = nn.LayerNorm(hidden_dim)  # name matches checkpoint\n",
    "        def forward(self, smiles_list: List[str], max_nodes=128):\n",
    "            X, A, M = _collate_graphs(smiles_list, max_nodes=max_nodes)\n",
    "            h = self.inp(X)\n",
    "            for layer in self.layers:\n",
    "                h = layer(h, A, M)\n",
    "            return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "    # -- Fusion parts --\n",
    "    def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "        mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "        denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "        return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "            self.ln  = nn.LayerNorm(dim)\n",
    "            self.do  = nn.Dropout(p)\n",
    "        def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "            Q = text_tokens.transpose(0,1); K = graph_nodes.transpose(0,1); V = graph_nodes.transpose(0,1)\n",
    "            kpm = (graph_mask == 0)\n",
    "            attn, _ = self.mha(Q, K, V, key_padding_mask=kpm)\n",
    "            attn = attn.transpose(0,1)\n",
    "            return self.ln(text_tokens + self.do(attn))\n",
    "    class DescriptorMLP(nn.Module):\n",
    "        def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, hidden), nn.GELU(), nn.Dropout(p),\n",
    "                nn.Linear(hidden, out_dim), nn.GELU(), nn.Dropout(p)\n",
    "            )\n",
    "        def forward(self, x): return self.net(x)\n",
    "    class FusionClassifier(nn.Module):\n",
    "        # name 'mlp' to match checkpoint\n",
    "        def __init__(self, dim=256, n_labels=N_LABELS, p=0.1):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "                nn.Linear(dim*2, n_labels)\n",
    "            )\n",
    "        def forward(self, fused_vec): return self.mlp(fused_vec)\n",
    "    class V7FusionModel(nn.Module):\n",
    "        def __init__(self, text_encoder, graph_encoder, desc_in_dim=DESC_IN_DIM, dim=256, n_labels=N_LABELS, n_heads=4, p=0.1):\n",
    "            super().__init__()\n",
    "            self.text_encoder=text_encoder\n",
    "            self.graph_encoder=graph_encoder\n",
    "            self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "            self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "            self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "        def forward(self, smiles_list, desc_feats, return_fused=False):\n",
    "            tt, tm = self.text_encoder(smiles_list, max_length=256)\n",
    "            gn, gm = self.graph_encoder(smiles_list, max_nodes=128)\n",
    "            tta = self.cross(tt.to(device), tm.to(device), gn.to(device), gm.to(device))\n",
    "            de  = self.desc_mlp(desc_feats.to(device))\n",
    "            text_pool  = masked_mean(tta, tm.to(device), 1)\n",
    "            graph_pool = masked_mean(gn.to(device),  gm.to(device), 1)\n",
    "            fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "            logits = self.shared_head(fused)\n",
    "            return (logits, fused) if return_fused else logits\n",
    "\n",
    "    text_encoder = ChemBERTaEncoder().to(device)\n",
    "    graph_encoder= GraphGINEncoder().to(device)\n",
    "    model        = V7FusionModel(text_encoder, graph_encoder).to(device)\n",
    "    ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "    model.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "    model.eval()\n",
    "\n",
    "    # 4) Compute descriptors → impute/scale → fused features\n",
    "    X_raw = compute_rdkit_descriptors_for_smiles(smiles)           # (N, 208 with NaNs)\n",
    "    X_imp = imputer.transform(X_raw)\n",
    "    X_std = scaler.transform(X_imp)\n",
    "    desc_t= torch.tensor(X_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    fused_list = []\n",
    "    B = 64\n",
    "    for i in range(0, N, B):\n",
    "        batch_smiles = smiles[i:i+B]\n",
    "        logits, fused = model(batch_smiles, desc_t[i:i+B], return_fused=True)\n",
    "        fused_list.append(fused.detach().cpu().numpy())\n",
    "    fused_all = np.concatenate(fused_list, axis=0).astype(np.float32)\n",
    "    np.save(path, fused_all)\n",
    "    print(f\"[Rebuild] Saved → {path}\")\n",
    "    return fused_all\n",
    "\n",
    "# ---- Get fused test features (rebuild if missing) ----\n",
    "X_fused = ensure_fused(\"test\")\n",
    "X_fused_t = torch.tensor(X_fused, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---- Load calibration/thresholds ----\n",
    "temps_spec = json.loads((CAL_DIR / \"temps.json\").read_text())                # specialist temps\n",
    "if EVAL_MODE == \"specialist\":\n",
    "    thresholds_spec = json.loads((CAL_DIR / \"thresholds.json\").read_text())  # specialist thresholds\n",
    "else:\n",
    "    # Blend\n",
    "    temps_shared   = json.loads((CAL_DIR / \"temps_shared.json\").read_text())\n",
    "    blend_payload  = json.loads((CAL_DIR / \"thresholds_blend.json\").read_text())\n",
    "    ALPHA          = float(blend_payload.get(\"alpha\", 0.8))\n",
    "    thresholds_blend = blend_payload[\"thresholds\"]\n",
    "\n",
    "# ---- Define heads (specialists) ----\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    # choose seed with highest best_ap\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if mfile.exists():\n",
    "            try:\n",
    "                ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "                cands.append((ap, sd))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(ck[\"model\"], strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "\n",
    "# ---- Shared head (for blend only): load just the classifier on fused\n",
    "if EVAL_MODE == \"blend\":\n",
    "    CKPT_BEST = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "    class SharedHeadOnly(nn.Module):\n",
    "        def __init__(self, dim=256, n_labels=N_LABELS, p=0.1):\n",
    "            super().__init__()\n",
    "            self.mlp = nn.Sequential(\n",
    "                nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "                nn.Linear(dim*2, n_labels)\n",
    "            )\n",
    "        def forward(self, fused):\n",
    "            return self.mlp(fused)\n",
    "    shared_head = SharedHeadOnly().to(device)\n",
    "    ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "    sh_state = {k.replace(\"shared_head.\", \"\"): v for k,v in ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "    shared_head.load_state_dict(sh_state, strict=True)\n",
    "    shared_head.eval()\n",
    "\n",
    "# ---- Specialist logits on fused test (fast) ----\n",
    "with torch.no_grad():\n",
    "    spec_logits = torch.zeros((N, N_LABELS), dtype=torch.float32, device=device)\n",
    "    for j, lbl in enumerate(LABELS):\n",
    "        spec_logits[:, j] = HEADS[lbl](X_fused_t)\n",
    "spec_logits = spec_logits.cpu().numpy()\n",
    "\n",
    "# ---- Shared logits on fused test (for blend path) ----\n",
    "if EVAL_MODE == \"blend\":\n",
    "    with torch.no_grad():\n",
    "        shared_logits = shared_head(X_fused_t).cpu().numpy()\n",
    "else:\n",
    "    shared_logits = None\n",
    "\n",
    "# ---- Build probability matrix according to EVAL_MODE ----\n",
    "def sigmoid(x): return 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "if EVAL_MODE == \"specialist\":\n",
    "    PROBS = np.zeros_like(spec_logits, dtype=np.float32)\n",
    "    for j, lbl in enumerate(LABELS):\n",
    "        T = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "        PROBS[:, j] = sigmoid(spec_logits[:, j] / T)\n",
    "else:\n",
    "    PROBS = np.zeros_like(spec_logits, dtype=np.float32)\n",
    "    for j, lbl in enumerate(LABELS):\n",
    "        T_spec   = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "        T_shared = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "        p_spec   = sigmoid(spec_logits[:, j]   / T_spec)\n",
    "        p_shared = sigmoid(shared_logits[:, j] / T_shared)\n",
    "        PROBS[:, j] = np.clip(ALPHA * p_spec + (1-ALPHA) * p_shared, 0.0, 1.0)\n",
    "\n",
    "# ---- Helper: ECE & reliability curve ----\n",
    "def reliability_and_ece(y_true, y_prob, n_bins=15):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins+1)\n",
    "    bin_ids = np.digitize(y_prob, bins) - 1\n",
    "    bin_acc, bin_conf, bin_count = [], [], []\n",
    "    ece = 0.0\n",
    "    for b in range(n_bins):\n",
    "        mask = (bin_ids == b)\n",
    "        n = mask.sum()\n",
    "        if n == 0:\n",
    "            bin_acc.append(np.nan); bin_conf.append(np.nan); bin_count.append(0)\n",
    "            continue\n",
    "        p = y_prob[mask]; t = y_true[mask]\n",
    "        acc = t.mean(); conf = p.mean()\n",
    "        bin_acc.append(acc); bin_conf.append(conf); bin_count.append(n)\n",
    "        ece += (n/len(y_true)) * abs(acc - conf)\n",
    "    return (bins, np.array(bin_acc), np.array(bin_conf), np.array(bin_count)), float(ece)\n",
    "\n",
    "# ---- Compute metrics per label & global ----\n",
    "rows = []\n",
    "tp_micro = fp_micro = fn_micro = 0\n",
    "macro_ap_vals = []\n",
    "macro_roc_vals = []\n",
    "\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    valid = ~Mte[:, j]\n",
    "    y = Yte[valid, j].astype(int)\n",
    "    p = PROBS[valid, j]\n",
    "\n",
    "    # AUCs\n",
    "    ap = float(average_precision_score(y, p)) if valid.sum() > 0 else float(\"nan\")\n",
    "    macro_ap_vals.append(ap)\n",
    "    try:\n",
    "        roc = float(roc_auc_score(y, p))\n",
    "    except Exception:\n",
    "        roc = float(\"nan\")\n",
    "    macro_roc_vals.append(roc)\n",
    "\n",
    "    # Operating thresholds (from saved calibration)\n",
    "    if EVAL_MODE == \"specialist\":\n",
    "        th_f1  = float(json.loads((CAL_DIR / \"thresholds.json\").read_text())[lbl][\"th_f1\"])\n",
    "        th_fb  = float(json.loads((CAL_DIR / \"thresholds.json\").read_text())[lbl][\"th_fbeta15\"])\n",
    "    else:\n",
    "        th_f1  = float(thresholds_blend[lbl][\"th_f1\"])\n",
    "        th_fb  = float(thresholds_blend[lbl][\"th_fbeta15\"])\n",
    "\n",
    "    def prf_at_thresh(th):\n",
    "        pred = (p >= th).astype(int)\n",
    "        tp = int(((pred==1) & (y==1)).sum())\n",
    "        fp = int(((pred==1) & (y==0)).sum())\n",
    "        fn = int(((pred==0) & (y==1)).sum())\n",
    "        prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "        rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "        f1   = (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0\n",
    "        return tp, fp, fn, prec, rec, f1\n",
    "\n",
    "    tp1, fp1, fn1, pr1, rc1, f1 = prf_at_thresh(th_f1)\n",
    "    tpb, fpb, fnb, prb, rcb, fb = prf_at_thresh(th_fb)\n",
    "\n",
    "    if THRESH_MODE == \"f1\":\n",
    "        tp_micro += tp1; fp_micro += fp1; fn_micro += fn1\n",
    "    else:\n",
    "        tp_micro += tpb; fp_micro += fpb; fn_micro += fnb\n",
    "\n",
    "    # Prevalence\n",
    "    prev = float(y.mean()) if valid.sum() > 0 else float(\"nan\")\n",
    "\n",
    "    # ECE + save reliability plot\n",
    "    (bins, acc, conf, counts), ece = reliability_and_ece(y, p, n_bins=15)\n",
    "    plt.figure()\n",
    "    mask = ~np.isnan(acc)\n",
    "    plt.plot([0,1], [0,1], linestyle=\"--\")\n",
    "    if mask.any():\n",
    "        plt.plot(conf[mask], acc[mask], marker=\"o\")\n",
    "    plt.xlabel(\"Mean predicted probability\"); plt.ylabel(\"Fraction of positives\")\n",
    "    plt.title(f\"Reliability: {lbl} (ECE={ece:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_REL_DIR / f\"{lbl}.png\", dpi=160); plt.close()\n",
    "\n",
    "    # PR curve plot\n",
    "    prec, rec, _ = precision_recall_curve(y, p)\n",
    "    plt.figure()\n",
    "    plt.step(rec, prec, where=\"post\")\n",
    "    plt.xlabel(\"Recall\"); plt.ylabel(\"Precision\")\n",
    "    plt.title(f\"PR curve: {lbl} (AP={ap:.3f})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(PLOT_PR_DIR / f\"{lbl}.png\", dpi=160); plt.close()\n",
    "\n",
    "    rows.append({\n",
    "        \"label\": lbl,\n",
    "        \"n_valid\": int(valid.sum()),\n",
    "        \"prevalence\": prev,\n",
    "        \"ap\": ap,\n",
    "        \"roc_auc\": roc,\n",
    "        \"th_f1\": th_f1,\n",
    "        \"prec@f1\": pr1, \"recall@f1\": rc1, \"f1\": f1,\n",
    "        \"tp@f1\": tp1, \"fp@f1\": fp1, \"fn@f1\": fn1,\n",
    "        \"th_fbeta15\": th_fb,\n",
    "        \"prec@fbeta15\": prb, \"recall@fbeta15\": rcb, \"f_beta15\": fb,\n",
    "        \"tp@fbeta15\": tpb, \"fp@fbeta15\": fpb, \"fn@fbeta15\": fnb,\n",
    "        \"ece\": ece\n",
    "    })\n",
    "\n",
    "# ---- Global summaries ----\n",
    "macro_pr_auc  = float(np.nanmean([r[\"ap\"] for r in rows]))\n",
    "macro_roc_auc = float(np.nanmean([r[\"roc_auc\"] for r in rows]))\n",
    "micro_prec = tp_micro/(tp_micro+fp_micro) if (tp_micro+fp_micro)>0 else 0.0\n",
    "micro_rec  = tp_micro/(tp_micro+fn_micro) if (tp_micro+fn_micro)>0 else 0.0\n",
    "micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec)>0 else 0.0\n",
    "\n",
    "# Cardinality (avg #positive labels per sample) – true vs predicted at chosen operating mode\n",
    "if THRESH_MODE == \"f1\":\n",
    "    if EVAL_MODE == \"specialist\":\n",
    "        thobj = json.loads((CAL_DIR / \"thresholds.json\").read_text())\n",
    "        THS = np.array([float(thobj[l][\"th_f1\"]) for l in LABELS], dtype=np.float32)\n",
    "    else:\n",
    "        THS = np.array([float(thresholds_blend[l][\"th_f1\"]) for l in LABELS], dtype=np.float32)\n",
    "else:\n",
    "    if EVAL_MODE == \"specialist\":\n",
    "        thobj = json.loads((CAL_DIR / \"thresholds.json\").read_text())\n",
    "        THS = np.array([float(thobj[l][\"th_fbeta15\"]) for l in LABELS], dtype=np.float32)\n",
    "    else:\n",
    "        THS = np.array([float(thresholds_blend[l][\"th_fbeta15\"]) for l in LABELS], dtype=np.float32)\n",
    "\n",
    "pred_bin = (PROBS >= THS.reshape(1, -1)).astype(int)\n",
    "pred_bin[Mte] = 0\n",
    "true_bin = Yte.copy().astype(int)\n",
    "true_bin[Mte] = 0\n",
    "\n",
    "avg_true_card = float(true_bin.sum(axis=1).mean())\n",
    "avg_pred_card = float(pred_bin.sum(axis=1).mean())\n",
    "card_err      = float(avg_pred_card - avg_true_card)\n",
    "\n",
    "# ---- Save reports ----\n",
    "per_label_df = pd.DataFrame(rows)\n",
    "per_label_csv = EVAL_DIR / \"per_label_metrics.csv\"\n",
    "per_label_df.to_csv(per_label_csv, index=False)\n",
    "\n",
    "summary = {\n",
    "    \"eval_mode\": EVAL_MODE,\n",
    "    \"threshold_mode\": THRESH_MODE,\n",
    "    \"n_test\": int(N),\n",
    "    \"macro_pr_auc\": macro_pr_auc,\n",
    "    \"macro_roc_auc\": macro_roc_auc,\n",
    "    \"micro_precision\": micro_prec,\n",
    "    \"micro_recall\": micro_rec,\n",
    "    \"micro_f1\": micro_f1,\n",
    "    \"avg_true_cardinality\": avg_true_card,\n",
    "    \"avg_pred_cardinality\": avg_pred_card,\n",
    "    \"cardinality_error\": card_err,\n",
    "    \"plots\": {\n",
    "        \"pr_curves_dir\": str(PLOT_PR_DIR),\n",
    "        \"reliability_dir\": str(PLOT_REL_DIR)\n",
    "    },\n",
    "}\n",
    "(EVAL_DIR / \"summary.json\").write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"✅ Evaluation complete.\")\n",
    "print(f\" Per-label CSV  → {per_label_csv}\")\n",
    "print(f\" Summary JSON   → {EVAL_DIR / 'summary.json'}\")\n",
    "print(f\" PR curves      → {PLOT_PR_DIR}\")\n",
    "print(f\" Reliability    → {PLOT_REL_DIR}\")\n",
    "print(\"\\nGlobal (test):\")\n",
    "for k in [\"eval_mode\",\"threshold_mode\",\"n_test\",\"macro_pr_auc\",\"macro_roc_auc\",\"micro_precision\",\"micro_recall\",\"micro_f1\",\"avg_true_cardinality\",\"avg_pred_cardinality\",\"cardinality_error\"]:\n",
    "    print(f\"  {k}: {summary[k]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a18843",
   "metadata": {},
   "source": [
    "### 2: Specialist vs Blend comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2e3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded specialist heads: 12/12\n",
      "✅ Saved:\n",
      "  • v7\\eval\\compare_table.csv\n",
      "  • v7\\eval\\compare_summary.json\n",
      "\n",
      "Quick view:\n",
      "      path    mode  macro_pr_auc  macro_roc_auc  micro_precision  micro_recall  micro_f1  avg_true_cardinality  avg_pred_cardinality  cardinality_error\n",
      "specialist fbeta15      0.149778       0.709502         0.183659      0.503968  0.269210              0.643678              1.766284           1.122605\n",
      "     blend fbeta15      0.157441       0.727127         0.200687      0.579365  0.298111              0.643678              1.858238           1.214559\n",
      "specialist      f1      0.149778       0.709502         0.199797      0.390873  0.264430              0.643678              1.259259           0.615581\n",
      "     blend      f1      0.157441       0.727127         0.230997      0.464286  0.308504              0.643678              1.293742           0.650064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:263: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  Truth = Yte.astype(int)\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  Truth = Yte.astype(int)\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  Truth = Yte.astype(int)\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_10028\\868570203.py:282: RuntimeWarning: invalid value encountered in cast\n",
      "  Truth = Yte.astype(int)\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# ---- Paths & basic setup ----\n",
    "BASE         = Path(\"v7\")\n",
    "PREP_DIR     = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR     = BASE / \"data\" / \"descriptors\"\n",
    "FUSED_DIR    = BASE / \"data\" / \"fused\"\n",
    "MODEL_DIR    = BASE / \"model\"\n",
    "CAL_DIR      = MODEL_DIR / \"calibration\"\n",
    "ENS_DIR      = MODEL_DIR / \"ensembles\"\n",
    "EVAL_DIR     = BASE / \"eval\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FUSED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---- Manifest & test blobs ----\n",
    "mani_path = PREP_DIR / \"dataset_manifest.json\"\n",
    "assert mani_path.exists(), f\"Missing manifest: {mani_path}\"\n",
    "mani = json.loads(mani_path.read_text())\n",
    "LABELS = mani[\"labels\"]\n",
    "N_LABELS = len(LABELS)\n",
    "DESC_IN_DIM = int(mani[\"n_features\"])\n",
    "\n",
    "blob_path = PREP_DIR / \"test.npz\"\n",
    "assert blob_path.exists(), f\"Missing test blob: {blob_path}\"\n",
    "blob   = np.load(blob_path, allow_pickle=True)\n",
    "smiles = [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "Yte    = blob[\"Y\"].astype(np.float32)\n",
    "Mte    = blob[\"y_missing_mask\"].astype(bool)\n",
    "N      = Yte.shape[0]\n",
    "\n",
    "# ---- Ensure fused features (rebuild if missing) ----\n",
    "def ensure_fused(split=\"test\") -> np.ndarray:\n",
    "    path = FUSED_DIR / f\"{split}_fused.npy\"\n",
    "    if path.exists():\n",
    "        return np.load(path).astype(np.float32)\n",
    "\n",
    "    print(f\"[Rebuild] {path} not found → recomputing {split} fused features...\")\n",
    "\n",
    "    # Load descriptor imputer/scaler\n",
    "    from joblib import load as joblib_load\n",
    "    imp_path = DESC_DIR / \"imputer.joblib\"\n",
    "    scl_path = DESC_DIR / \"scaler.joblib\"\n",
    "    assert imp_path.exists() and scl_path.exists(), \"Missing imputer/scaler joblib files.\"\n",
    "    imputer = joblib_load(imp_path)\n",
    "    scaler  = joblib_load(scl_path)\n",
    "\n",
    "    # Prepare RDKit 208 descriptors in the SAME ORDER as training\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors as RDDesc\n",
    "    feat_names_file = DESC_DIR / \"feature_names.txt\"\n",
    "    assert feat_names_file.exists(), f\"Missing {feat_names_file}\"\n",
    "    feat_names = [ln.strip() for ln in feat_names_file.read_text().splitlines() if ln.strip()]\n",
    "    rd_fns = {name: getattr(RDDesc, name, None) for name in feat_names}\n",
    "\n",
    "    def rdkit_feats(smis: List[str]) -> np.ndarray:\n",
    "        rows = []\n",
    "        for s in smis:\n",
    "            m = Chem.MolFromSmiles(s)\n",
    "            if m is None:\n",
    "                rows.append([np.nan]*len(feat_names)); continue\n",
    "            vals = []\n",
    "            for name in feat_names:\n",
    "                fn = rd_fns.get(name, None)\n",
    "                try:\n",
    "                    v = float(fn(m)) if fn is not None else np.nan\n",
    "                except Exception:\n",
    "                    v = np.nan\n",
    "                vals.append(v if np.isfinite(v) else np.nan)\n",
    "            rows.append(vals)\n",
    "        return np.asarray(rows, dtype=np.float32)\n",
    "\n",
    "    # Build shared fusion model (text+graph+desc) & load checkpoint\n",
    "    CKPT = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "    assert CKPT.exists(), f\"Missing shared checkpoint: {CKPT}\"\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    class ChemBERTaEncoder(nn.Module):\n",
    "        def __init__(self, ckpt=\"seyonec/ChemBERTa-zinc-base-v1\", dim=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.tok = AutoTokenizer.from_pretrained(ckpt)\n",
    "            self.m   = AutoModel.from_pretrained(ckpt)\n",
    "            self.proj= nn.Sequential(nn.Dropout(p), nn.Linear(self.m.config.hidden_size, dim))\n",
    "            self.ln  = nn.LayerNorm(dim)\n",
    "        def forward(self, smis, max_length=256):\n",
    "            enc = self.tok(list(smis), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            out = self.m(input_ids=enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device)).last_hidden_state\n",
    "            return self.ln(self.proj(out)), enc[\"attention_mask\"].to(device, dtype=torch.int32)\n",
    "\n",
    "    ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "    def _one_hot(v, C): z=[0]*len(C); z[C.index(v) if v in C else -1]=1; return z\n",
    "    def _bucket(v, lo, hi):\n",
    "        buckets=list(range(lo,hi+1)); o=[0]*(len(buckets)+1); i=v-lo; o[i if 0<=i<len(buckets) else -1]=1; return o\n",
    "    def _atom_feat(a):\n",
    "        hyb=[Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "        chir=[Chem.rdchem.ChiralType.CHI_UNSPECIFIED, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW, Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "        f=_one_hot(a.GetSymbol(), ATOM_LIST+[\"other\"])\n",
    "        f+=_bucket(a.GetDegree(),0,5)+_bucket(a.GetFormalCharge(),-2,2)+(_one_hot(a.GetHybridization(),hyb)+[0])\n",
    "        f+=[int(a.GetIsAromatic()), int(a.IsInRing())]+_one_hot(a.GetChiralTag(),chir)\n",
    "        f+=_bucket(a.GetTotalNumHs(True),0,4)+_bucket(a.GetTotalValence(),0,5)+[a.GetMass()/200.0]\n",
    "        return f\n",
    "    def _smiles_to_graph(smi, max_nodes=128):\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        if m is None or m.GetNumAtoms()==0: return np.zeros((0,0),np.float32), np.zeros((0,0),np.float32)\n",
    "        X = np.asarray([_atom_feat(m.GetAtomWithIdx(i)) for i in range(m.GetNumAtoms())], np.float32)\n",
    "        N = m.GetNumAtoms(); A = np.zeros((N,N),np.float32)\n",
    "        for b in m.GetBonds(): i,j=b.GetBeginAtomIdx(), b.GetEndAtomIdx(); A[i,j]=A[j,i]=1.0\n",
    "        if N>max_nodes: X=X[:max_nodes]; A=A[:max_nodes,:max_nodes]\n",
    "        return X,A\n",
    "    def _collate(smis, max_nodes=128):\n",
    "        G=[_smiles_to_graph(s) for s in smis]\n",
    "        Nmax=max([g[0].shape[0] for g in G]+[1]); F=G[0][0].shape[1] if G[0][0].size>0 else 51; B=len(G)\n",
    "        X=np.zeros((B,Nmax,F),np.float32); A=np.zeros((B,Nmax,Nmax),np.float32); M=np.zeros((B,Nmax),np.int64)\n",
    "        for i,(x,a) in enumerate(G):\n",
    "            n=x.shape[0]\n",
    "            if n==0: continue\n",
    "            X[i,:n,:]=x; A[i,:n,:n]=a; M[i,:n]=1\n",
    "        return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "    class GINLayer(nn.Module):\n",
    "        def __init__(self,h=256,p=0.1): super().__init__(); self.eps=nn.Parameter(torch.tensor(0.0)); self.mlp=nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "        def forward(self,x,A,M): return self.mlp((1.0+self.eps)*x + torch.matmul(A,x)) * M.unsqueeze(-1).to(x.dtype)\n",
    "    class GraphGINEncoder(nn.Module):\n",
    "        def __init__(self,node_in_dim=51,h=256,L=4,p=0.1): super().__init__(); self.inp=nn.Sequential(nn.Linear(node_in_dim,h), nn.GELU(), nn.Dropout(p)); self.layers=nn.ModuleList([GINLayer(h,p) for _ in range(L)]); self.out_ln=nn.LayerNorm(h)\n",
    "        def forward(self,smis,max_nodes=128):\n",
    "            X,A,M=_collate(smis,max_nodes); h=self.inp(X)\n",
    "            for L in self.layers: h=L(h,A,M)\n",
    "            return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "    def masked_mean(x, m, dim): m=m.to(dtype=x.dtype, device=x.device); denom=m.sum(dim=dim,keepdim=True).clamp(min=1.0); return (x*m.unsqueeze(-1)).sum(dim=dim)/denom\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self,dim=256,heads=4,p=0.1): super().__init__(); self.mha=nn.MultiheadAttention(dim, heads, dropout=p, batch_first=False); self.ln=nn.LayerNorm(dim); self.do=nn.Dropout(p)\n",
    "        def forward(self,T,TM,G,GM): Q=T.transpose(0,1); K=G.transpose(0,1); V=G.transpose(0,1); kpm=(GM==0); A,_=self.mha(Q,K,V,key_padding_mask=kpm); return self.ln(T+self.do(A.transpose(0,1)))\n",
    "    class DescriptorMLP(nn.Module):\n",
    "        def __init__(self,inp,dim=256,p=0.1): super().__init__(); self.net=nn.Sequential(nn.Linear(inp,256), nn.GELU(), nn.Dropout(p), nn.Linear(256,dim), nn.GELU(), nn.Dropout(p))\n",
    "        def forward(self,x): return self.net(x)\n",
    "    class FusionClassifier(nn.Module):\n",
    "        def __init__(self,dim=256,L=N_LABELS,p=0.1): super().__init__(); self.mlp=nn.Sequential(nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p), nn.Linear(dim*2, L))\n",
    "        def forward(self,z): return self.mlp(z)\n",
    "    class V7Shared(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.text=ChemBERTaEncoder()\n",
    "            self.graph=GraphGINEncoder()\n",
    "            self.cross=CrossAttentionBlock()\n",
    "            self.desc=DescriptorMLP(DESC_IN_DIM)\n",
    "            self.head=FusionClassifier()\n",
    "        def forward(self,smis,desc,return_fused=False):\n",
    "            T,TM=self.text(smis,256); G,GM=self.graph(smis,128); Ta=self.cross(T,TM,G,GM); Dz=self.desc(desc)\n",
    "            Tp=masked_mean(Ta,TM,1); Gp=masked_mean(G,GM,1); fused=torch.cat([Tp,Gp,Dz],-1)\n",
    "            logits=self.head(fused); \n",
    "            return (logits, fused) if return_fused else logits\n",
    "\n",
    "    v7 = V7Shared().to(device)\n",
    "    v7.load_state_dict(torch.load(CKPT, map_location=device)[\"model\"], strict=True)\n",
    "    v7.eval()\n",
    "\n",
    "    # descriptors → impute → scale\n",
    "    X_raw = rdkit_feats(smiles)\n",
    "    X_imp = imputer.transform(X_raw)\n",
    "    X_std = scaler.transform(X_imp)\n",
    "    desc_t = torch.tensor(X_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    fused = []\n",
    "    B=64\n",
    "    for i in range(0, N, B):\n",
    "        _, f = v7(smiles[i:i+B], desc_t[i:i+B], return_fused=True)\n",
    "        fused.append(f.detach().cpu().numpy())\n",
    "    fused = np.concatenate(fused, 0).astype(np.float32)\n",
    "    np.save(path, fused)\n",
    "    print(f\"[Rebuild] Saved → {path}\")\n",
    "    return fused\n",
    "\n",
    "X_fused = ensure_fused(\"test\")\n",
    "X_fused_t = torch.tensor(X_fused, dtype=torch.float32, device=device)\n",
    "\n",
    "# ---- Load calibration artifacts ----\n",
    "temps_spec = json.loads((CAL_DIR / \"temps.json\").read_text())            # specialist temps\n",
    "thr_spec   = json.loads((CAL_DIR / \"thresholds.json\").read_text())       # specialist thresholds\n",
    "temps_sh   = json.loads((CAL_DIR / \"temps_shared.json\").read_text())     # shared temps\n",
    "blend      = json.loads((CAL_DIR / \"thresholds_blend.json\").read_text()) # blend thresholds + alpha\n",
    "ALPHA      = float(blend.get(\"alpha\", 0.8))\n",
    "thr_blend  = blend[\"thresholds\"]\n",
    "\n",
    "# ---- Robust specialist head loader (patch handles b1/b2/b3 vs block1/2/3) ----\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _remap_keys_if_needed(state_dict: dict) -> dict:\n",
    "    needs = any(k.startswith((\"b1.\", \"b2.\", \"b3.\")) for k in state_dict.keys())\n",
    "    if not needs: return state_dict\n",
    "    remap = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k2 = k.replace(\"b1.\", \"block1.\").replace(\"b2.\", \"block2.\").replace(\"b3.\", \"block3.\")\n",
    "        remap[k2] = v\n",
    "    return remap\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\"), key=lambda p: p.name):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if not mfile.exists(): continue\n",
    "        try:\n",
    "            ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "            cands.append((ap, sd))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label} under {ENS_DIR/label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    state = _remap_keys_if_needed(ck[\"model\"])\n",
    "    head.load_state_dict(state, strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "print(f\"✅ Loaded specialist heads: {len(HEADS)}/{len(LABELS)}\")\n",
    "\n",
    "# ---- Shared head (MLP on fused) for blend path ----\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "sh = SharedHeadMLP().to(device)\n",
    "ckpt = torch.load(MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "sh_state = {k.replace(\"shared_head.\", \"\"): v for k,v in ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(sh_state, strict=True)\n",
    "sh.eval()\n",
    "\n",
    "# ---- Build probability matrices for specialist & blend ----\n",
    "with torch.no_grad():\n",
    "    spec_logits = torch.stack([HEADS[lbl](X_fused_t) for lbl in LABELS], dim=1).cpu().numpy()  # (N,L)\n",
    "    sh_logits   = sh(X_fused_t).cpu().numpy()                                                  # (N,L)\n",
    "\n",
    "sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "P_spec = np.zeros_like(spec_logits, np.float32)\n",
    "P_blnd = np.zeros_like(spec_logits, np.float32)\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    Ts = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "    Th = max(float(temps_sh.get(lbl, 1.0)),   1e-3)\n",
    "    ps = sigmoid(spec_logits[:, j] / Ts)\n",
    "    ph = sigmoid(sh_logits[:, j]   / Th)\n",
    "    P_spec[:, j] = ps\n",
    "    P_blnd[:, j] = np.clip(ALPHA*ps + (1-ALPHA)*ph, 0.0, 1.0)\n",
    "\n",
    "# ---- Scoring helper ----\n",
    "def score(PROBS: np.ndarray, thresholds_obj: Dict[str, dict], mode: str = \"fbeta15\") -> dict:\n",
    "    tp=fp=fn=0\n",
    "    ap_list=[]; roc_list=[]\n",
    "    TH = np.array([float(thresholds_obj[l][\"th_f1\" if mode==\"f1\" else \"th_fbeta15\"]) for l in LABELS], np.float32)\n",
    "    Pred = (PROBS >= TH.reshape(1, -1)).astype(int)\n",
    "    Pred[Mte] = 0\n",
    "    Truth = Yte.astype(int)\n",
    "    Truth[Mte] = 0\n",
    "\n",
    "    from sklearn.metrics import average_precision_score, roc_auc_score\n",
    "    for j in range(N_LABELS):\n",
    "        y = Truth[:, j]; p = PROBS[:, j]\n",
    "        if y.max() != y.min():\n",
    "            try: ap_list.append(float(average_precision_score(y, p)))\n",
    "            except Exception: pass\n",
    "            try: roc_list.append(float(roc_auc_score(y, p)))\n",
    "            except Exception: pass\n",
    "        tp += int(((Pred[:, j]==1) & (y==1)).sum())\n",
    "        fp += int(((Pred[:, j]==1) & (y==0)).sum())\n",
    "        fn += int(((Pred[:, j]==0) & (y==1)).sum())\n",
    "    micro_prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    micro_rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec)>0 else 0.0\n",
    "    macro_pr   = float(np.nanmean(ap_list)) if ap_list else float(\"nan\")\n",
    "    macro_roc  = float(np.nanmean(roc_list)) if roc_list else float(\"nan\")\n",
    "    avg_true   = float(Truth.sum(1).mean())\n",
    "    avg_pred   = float(Pred.sum(1).mean())\n",
    "    return {\n",
    "        \"macro_pr_auc\": macro_pr,\n",
    "        \"macro_roc_auc\": macro_roc,\n",
    "        \"micro_precision\": micro_prec,\n",
    "        \"micro_recall\": micro_rec,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"avg_true_cardinality\": avg_true,\n",
    "        \"avg_pred_cardinality\": avg_pred,\n",
    "        \"cardinality_error\": avg_pred - avg_true\n",
    "    }\n",
    "\n",
    "# ---- Evaluate all four configurations ----\n",
    "res_spec_fb = score(P_spec, thr_spec,  mode=\"fbeta15\")\n",
    "res_blnd_fb = score(P_blnd, thr_blend, mode=\"fbeta15\")\n",
    "res_spec_f1 = score(P_spec, thr_spec,  mode=\"f1\")\n",
    "res_blnd_f1 = score(P_blnd, thr_blend, mode=\"f1\")\n",
    "\n",
    "table = pd.DataFrame([\n",
    "    {\"path\":\"specialist\",\"mode\":\"fbeta15\", **res_spec_fb},\n",
    "    {\"path\":\"blend\",     \"mode\":\"fbeta15\", **res_blnd_fb},\n",
    "    {\"path\":\"specialist\",\"mode\":\"f1\",      **res_spec_f1},\n",
    "    {\"path\":\"blend\",     \"mode\":\"f1\",      **res_blnd_f1},\n",
    "])\n",
    "table_path = EVAL_DIR / \"compare_table.csv\"\n",
    "table.to_csv(table_path, index=False)\n",
    "comp_json = {\"fbeta15\": {\"specialist\": res_spec_fb, \"blend\": res_blnd_fb},\n",
    "             \"f1\":      {\"specialist\": res_spec_f1, \"blend\": res_blnd_f1}}\n",
    "(EVAL_DIR / \"compare_summary.json\").write_text(json.dumps(comp_json, indent=2))\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\"  •\", table_path)\n",
    "print(\"  •\", EVAL_DIR / \"compare_summary.json\")\n",
    "print(\"\\nQuick view:\")\n",
    "print(table.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f018ce3",
   "metadata": {},
   "source": [
    "### 3: Per-label diagnostics & policy proposal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4be4c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Per-label policy proposed and saved.\n",
      "  • Policy JSON → v7\\model\\policy\\policy.json\n",
      "  • Table CSV   → v7\\eval\\policy_table.csv\n",
      "Decision counts: {'fbeta15': 5, 'precision_floor': 4, 'f1': 3}\n",
      "\n",
      "Examples:\n",
      "  NR-AR: precision_floor (th=0.653) ← val_precision_floor_0.55\n",
      "  NR-AR-LBD: precision_floor (th=0.723) ← val_precision_floor_0.55\n",
      "  NR-AhR: precision_floor (th=0.709) ← val_precision_floor_0.55\n",
      "  NR-Aromatase: fbeta15 (th=0.474) ← default_fbeta15\n"
     ]
    }
   ],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "# ------- Config -------\n",
    "BASE         = Path(\"v7\")\n",
    "PREP_DIR     = BASE / \"data\" / \"prepared\"\n",
    "DESC_DIR     = BASE / \"data\" / \"descriptors\"\n",
    "FUSED_DIR    = BASE / \"data\" / \"fused\"\n",
    "MODEL_DIR    = BASE / \"model\"\n",
    "ENS_DIR      = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR      = MODEL_DIR / \"calibration\"\n",
    "EVAL_DIR     = BASE / \"eval\"\n",
    "POL_DIR      = MODEL_DIR / \"policy\"\n",
    "for p in [EVAL_DIR, POL_DIR, FUSED_DIR]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# How strict should we be about precision?\n",
    "PREC_FLOOR = 0.55   # try 0.55–0.60; higher => fewer FPs, lower recall\n",
    "DEFAULT_ALPHA = 0.8 # specialist weight used in your saved blend calibration\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ------- Load manifests and artifacts -------\n",
    "mani = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABELS = mani[\"labels\"]; N_LABELS = len(LABELS)\n",
    "DESC_IN_DIM = int(mani[\"n_features\"])\n",
    "\n",
    "# Test per-label metrics (from Phase 6 Cell 1 with EVAL_MODE=\"blend\")\n",
    "test_metrics_csv = EVAL_DIR / \"per_label_metrics.csv\"\n",
    "assert test_metrics_csv.exists(), f\"Missing {test_metrics_csv}. Please run Phase 6 Cell 1 first.\"\n",
    "test_df = pd.read_csv(test_metrics_csv)\n",
    "\n",
    "# Blended calibration\n",
    "temps_spec   = json.loads((CAL_DIR / \"temps.json\").read_text())            # specialist temps\n",
    "temps_shared = json.loads((CAL_DIR / \"temps_shared.json\").read_text())     # shared temps\n",
    "blend_payload= json.loads((CAL_DIR / \"thresholds_blend.json\").read_text()) # has alpha + thresholds\n",
    "ALPHA        = float(blend_payload.get(\"alpha\", DEFAULT_ALPHA))\n",
    "thr_blend    = blend_payload[\"thresholds\"]  # per-label th_f1 / th_fbeta15\n",
    "\n",
    "# ------- Ensure fused VAL features (rebuild if missing) -------\n",
    "def ensure_fused(split: str) -> np.ndarray:\n",
    "    out = FUSED_DIR / f\"{split}_fused.npy\"\n",
    "    if out.exists():\n",
    "        return np.load(out).astype(np.float32)\n",
    "\n",
    "    print(f\"[Rebuild] {out} not found → recomputing {split} fused features...\")\n",
    "\n",
    "    # Load split blob\n",
    "    blob = np.load(PREP_DIR / f\"{split}.npz\", allow_pickle=True)\n",
    "    smi  = [str(s) for s in blob[\"smiles\"].tolist()]\n",
    "    N    = len(smi)\n",
    "\n",
    "    # Descriptor transformers\n",
    "    from joblib import load as joblib_load\n",
    "    imputer = joblib_load(DESC_DIR / \"imputer.joblib\")\n",
    "    scaler  = joblib_load(DESC_DIR / \"scaler.joblib\")\n",
    "\n",
    "    # RDKit descriptor list (same order as training)\n",
    "    from rdkit import Chem\n",
    "    from rdkit.Chem import Descriptors as RDDesc\n",
    "    feat_names = [ln.strip() for ln in (DESC_DIR / \"feature_names.txt\").read_text().splitlines() if ln.strip()]\n",
    "    rd_fns = {name: getattr(RDDesc, name, None) for name in feat_names}\n",
    "\n",
    "    def rdkit_feats(smiles_list: List[str]) -> np.ndarray:\n",
    "        rows = []\n",
    "        for s in smiles_list:\n",
    "            m = Chem.MolFromSmiles(s)\n",
    "            if m is None:\n",
    "                rows.append([np.nan]*len(feat_names)); continue\n",
    "            vals=[]\n",
    "            for name in feat_names:\n",
    "                fn = rd_fns.get(name, None)\n",
    "                try: v = float(fn(m)) if fn is not None else np.nan\n",
    "                except Exception: v = np.nan\n",
    "                vals.append(v if np.isfinite(v) else np.nan)\n",
    "            rows.append(vals)\n",
    "        return np.asarray(rows, dtype=np.float32)\n",
    "\n",
    "    # Rebuild shared fusion model (text+graph+desc) → fused\n",
    "    CKPT = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "    assert CKPT.exists(), f\"Missing shared checkpoint: {CKPT}\"\n",
    "\n",
    "    from transformers import AutoTokenizer, AutoModel\n",
    "    class ChemBERTaEncoder(nn.Module):\n",
    "        def __init__(self, ckpt=\"seyonec/ChemBERTa-zinc-base-v1\", dim=256, p=0.1):\n",
    "            super().__init__()\n",
    "            self.tok = AutoTokenizer.from_pretrained(ckpt)\n",
    "            self.m   = AutoModel.from_pretrained(ckpt)\n",
    "            self.proj= nn.Sequential(nn.Dropout(p), nn.Linear(self.m.config.hidden_size, dim))\n",
    "            self.ln  = nn.LayerNorm(dim)\n",
    "        def forward(self, smis, max_length=256):\n",
    "            enc = self.tok(list(smis), padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\")\n",
    "            out = self.m(input_ids=enc[\"input_ids\"].to(device), attention_mask=enc[\"attention_mask\"].to(device)).last_hidden_state\n",
    "            return self.ln(self.proj(out)), enc[\"attention_mask\"].to(device, dtype=torch.int32)\n",
    "\n",
    "    from rdkit import Chem\n",
    "    ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "    def _one_hot(v, C): z=[0]*len(C); z[C.index(v) if v in C else -1]=1; return z\n",
    "    def _bucket(v, lo, hi):\n",
    "        buckets=list(range(lo,hi+1)); o=[0]*(len(buckets)+1); i=v-lo; o[i if 0<=i<len(buckets) else -1]=1; return o\n",
    "    def _atom_feat(a):\n",
    "        hyb=[Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP, Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3, Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "        chir=[Chem.rdchem.ChiralType.CHI_UNSPECIFIED, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW, Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW, Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "        f=_one_hot(a.GetSymbol(), ATOM_LIST+[\"other\"])\n",
    "        f+=_bucket(a.GetDegree(),0,5)+_bucket(a.GetFormalCharge(),-2,2)+(_one_hot(a.GetHybridization(),hyb)+[0])\n",
    "        f+=[int(a.GetIsAromatic()), int(a.IsInRing())]+_one_hot(a.GetChiralTag(),chir)\n",
    "        f+=_bucket(a.GetTotalNumHs(True),0,4)+_bucket(a.GetTotalValence(),0,5)+[a.GetMass()/200.0]\n",
    "        return f\n",
    "    def _smiles_to_graph(smi, max_nodes=128):\n",
    "        m = Chem.MolFromSmiles(smi)\n",
    "        if m is None or m.GetNumAtoms()==0: return np.zeros((0,0),np.float32), np.zeros((0,0),np.float32)\n",
    "        X = np.asarray([_atom_feat(m.GetAtomWithIdx(i)) for i in range(m.GetNumAtoms())], np.float32)\n",
    "        N = m.GetNumAtoms(); A = np.zeros((N,N),np.float32)\n",
    "        for b in m.GetBonds(): i,j=b.GetBeginAtomIdx(), b.GetEndAtomIdx(); A[i,j]=A[j,i]=1.0\n",
    "        if N>max_nodes: X=X[:max_nodes]; A=A[:max_nodes,:max_nodes]\n",
    "        return X,A\n",
    "    def _collate(smis, max_nodes=128):\n",
    "        G=[_smiles_to_graph(s) for s in smis]\n",
    "        Nmax=max([g[0].shape[0] for g in G]+[1]); F=G[0][0].shape[1] if G[0][0].size>0 else 51; B=len(G)\n",
    "        X=np.zeros((B,Nmax,F),np.float32); A=np.zeros((B,Nmax,Nmax),np.float32); M=np.zeros((B,Nmax),np.int64)\n",
    "        for i,(x,a) in enumerate(G):\n",
    "            n=x.shape[0]\n",
    "            if n==0: continue\n",
    "            X[i,:n,:]=x; A[i,:n,:n]=a; M[i,:n]=1\n",
    "        return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "    class GINLayer(nn.Module):\n",
    "        def __init__(self,h=256,p=0.1): super().__init__(); self.eps=nn.Parameter(torch.tensor(0.0)); self.mlp=nn.Sequential(nn.Linear(h,h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "        def forward(self,x,A,M): return self.mlp((1.0+self.eps)*x + torch.matmul(A,x)) * M.unsqueeze(-1).to(x.dtype)\n",
    "    class GraphGINEncoder(nn.Module):\n",
    "        def __init__(self,node_in_dim=51,h=256,L=4,p=0.1): super().__init__(); self.inp=nn.Sequential(nn.Linear(node_in_dim,h), nn.GELU(), nn.Dropout(p)); self.layers=nn.ModuleList([GINLayer(h,p) for _ in range(L)]); self.out_ln=nn.LayerNorm(h)\n",
    "        def forward(self,smis,max_nodes=128):\n",
    "            X,A,M=_collate(smis,max_nodes); h=self.inp(X)\n",
    "            for L in self.layers: h=L(h,A,M)\n",
    "            return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "    def masked_mean(x, m, dim): m=m.to(dtype=x.dtype, device=x.device); denom=m.sum(dim=dim,keepdim=True).clamp(min=1.0); return (x*m.unsqueeze(-1)).sum(dim=dim)/denom\n",
    "    class CrossAttentionBlock(nn.Module):\n",
    "        def __init__(self,dim=256,heads=4,p=0.1): super().__init__(); self.mha=nn.MultiheadAttention(dim, heads, dropout=p, batch_first=False); self.ln=nn.LayerNorm(dim); self.do=nn.Dropout(p)\n",
    "        def forward(self,T,TM,G,GM): Q=T.transpose(0,1); K=G.transpose(0,1); V=G.transpose(0,1); kpm=(GM==0); A,_=self.mha(Q,K,V,key_padding_mask=kpm); return self.ln(T+self.do(A.transpose(0,1)))\n",
    "    class DescriptorMLP(nn.Module):\n",
    "        def __init__(self,inp,dim=256,p=0.1): super().__init__(); self.net=nn.Sequential(nn.Linear(inp,256), nn.GELU(), nn.Dropout(p), nn.Linear(256,dim), nn.GELU(), nn.Dropout(p))\n",
    "        def forward(self,x): return self.net(x)\n",
    "    class FusionClassifier(nn.Module):\n",
    "        def __init__(self,dim=256,L=N_LABELS,p=0.1): super().__init__(); self.mlp=nn.Sequential(nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p), nn.Linear(dim*2, L))\n",
    "        def forward(self,z): return self.mlp(z)\n",
    "    class V7Shared(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "            self.text=ChemBERTaEncoder()\n",
    "            self.graph=GraphGINEncoder()\n",
    "            self.cross=CrossAttentionBlock()\n",
    "            self.desc=DescriptorMLP(DESC_IN_DIM)\n",
    "            self.head=FusionClassifier()\n",
    "        def forward(self,smis,desc,return_fused=False):\n",
    "            T,TM=self.text(smis,256); G,GM=self.graph(smis,128); Ta=self.cross(T,TM,G,GM); Dz=self.desc(desc)\n",
    "            Tp=masked_mean(Ta,TM,1); Gp=masked_mean(G,GM,1); fused=torch.cat([Tp,Gp,Dz],-1)\n",
    "            logits=self.head(fused)\n",
    "            return (logits, fused) if return_fused else logits\n",
    "\n",
    "    # Build model & fused features\n",
    "    v7 = V7Shared().to(device)\n",
    "    v7.load_state_dict(torch.load(MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)[\"model\"], strict=True)\n",
    "    v7.eval()\n",
    "\n",
    "    # descriptors → impute → scale\n",
    "    X_raw = rdkit_feats(smi)\n",
    "    X_imp = imputer.transform(X_raw); X_std = scaler.transform(X_imp)\n",
    "    desc_t = torch.tensor(X_std, dtype=torch.float32, device=device)\n",
    "\n",
    "    fused_chunks=[]\n",
    "    B=64\n",
    "    for i in range(0, N, B):\n",
    "        _, f = v7(smi[i:i+B], desc_t[i:i+B], return_fused=True)\n",
    "        fused_chunks.append(f.detach().cpu().numpy())\n",
    "    fused = np.concatenate(fused_chunks, 0).astype(np.float32)\n",
    "    np.save(out, fused)\n",
    "    print(f\"[Rebuild] Saved → {out}\")\n",
    "    return fused\n",
    "\n",
    "# Get VAL fused and labels\n",
    "Xva = ensure_fused(\"val\")\n",
    "val_blob = np.load(PREP_DIR / \"val.npz\", allow_pickle=True)\n",
    "Yva = val_blob[\"Y\"].astype(np.float32)                      # (Nv, L)\n",
    "Mva = val_blob[\"y_missing_mask\"].astype(bool)               # (Nv, L)\n",
    "\n",
    "# ------- Heads: specialist + shared (MLP on fused) -------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _remap_keys_if_needed(state_dict: dict) -> dict:\n",
    "    needs = any(k.startswith((\"b1.\", \"b2.\", \"b3.\")) for k in state_dict.keys())\n",
    "    if not needs: return state_dict\n",
    "    remap = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k2 = k.replace(\"b1.\", \"block1.\").replace(\"b2.\", \"block2.\").replace(\"b3.\", \"block3.\")\n",
    "        remap[k2] = v\n",
    "    return remap\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\"), key=lambda p: p.name):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if not mfile.exists(): continue\n",
    "        try:\n",
    "            ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "            cands.append((ap, sd))\n",
    "        except Exception:\n",
    "            pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label} under {ENS_DIR/label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    state = _remap_keys_if_needed(ck[\"model\"])\n",
    "    head.load_state_dict(state, strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "sh = SharedHeadMLP().to(device)\n",
    "ckpt = torch.load(MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "sh_state = {k.replace(\"shared_head.\", \"\"): v for k,v in ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(sh_state, strict=True)\n",
    "sh.eval()\n",
    "\n",
    "# ------- Build blended probabilities on VAL -------\n",
    "sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
    "Xva_t = torch.tensor(Xva, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    spec_logits_val = torch.stack([HEADS[l](Xva_t) for l in LABELS], dim=1).cpu().numpy()\n",
    "    sh_logits_val   = sh(Xva_t).cpu().numpy()\n",
    "\n",
    "P_spec_val = np.zeros_like(spec_logits_val, np.float32)\n",
    "P_sh_val   = np.zeros_like(sh_logits_val,   np.float32)\n",
    "P_blend_val= np.zeros_like(sh_logits_val,   np.float32)\n",
    "\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    Ts = max(float(temps_spec.get(lbl, 1.0)), 1e-3)\n",
    "    Th = max(float(temps_shared.get(lbl, 1.0)), 1e-3)\n",
    "    ps = sigmoid(spec_logits_val[:, j] / Ts)\n",
    "    ph = sigmoid(sh_logits_val[:, j]   / Th)\n",
    "    P_spec_val[:, j] = ps\n",
    "    P_sh_val[:, j]   = ph\n",
    "    P_blend_val[:, j]= np.clip(ALPHA*ps + (1-ALPHA)*ph, 0.0, 1.0)\n",
    "\n",
    "# ------- Helper: precision-floor threshold on VAL -------\n",
    "def precision_floor_threshold(y_true: np.ndarray, probs: np.ndarray, floor: float) -> Tuple[float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns (threshold, precision_at_th, recall_at_th) selecting the *highest recall*\n",
    "    point on the PR curve with precision >= floor. Falls back to best F1 if none.\n",
    "    \"\"\"\n",
    "    prec, rec, th = precision_recall_curve(y_true, probs)\n",
    "    # Align thresholds with prec/rec arrays (sklearn: th has len-1)\n",
    "    th_aligned = np.concatenate([th, [1.0]]) if th.size > 0 else np.array([0.5])\n",
    "    mask = prec >= floor\n",
    "    if mask.any():\n",
    "        # Among points with precision>=floor, pick the one with highest recall\n",
    "        idx = np.argmax(rec[mask])\n",
    "        # position within masked array → original index\n",
    "        candidates = np.where(mask)[0]\n",
    "        i = candidates[idx]\n",
    "        return float(th_aligned[i]), float(prec[i]), float(rec[i])\n",
    "    # Fallback: best F1\n",
    "    eps = 1e-8\n",
    "    f1 = (2*prec*rec)/np.maximum(prec+rec, eps)\n",
    "    i = int(np.nanargmax(f1))\n",
    "    return float(th_aligned[i]), float(prec[i]), float(rec[i])\n",
    "\n",
    "# ------- Build policy per label -------\n",
    "rows = []\n",
    "policy = {\n",
    "    \"alpha\": ALPHA,\n",
    "    \"precision_floor\": PREC_FLOOR,\n",
    "    \"default_mode\": \"fbeta15\",\n",
    "    \"labels\": {}\n",
    "}\n",
    "\n",
    "# Map test metrics into a dict for easy access\n",
    "test_by_label = {r[\"label\"]: r for _, r in test_df.iterrows()}\n",
    "\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    # Use only VAL rows with non-missing labels\n",
    "    valid = ~Mva[:, j]\n",
    "    yv = Yva[valid, j].astype(int)\n",
    "    pv = P_blend_val[valid, j]\n",
    "    # Degenerate label on val?\n",
    "    degenerate = (yv.max() == yv.min())\n",
    "    # Test metrics (to judge FP-ness)\n",
    "    td = test_by_label.get(lbl, {})\n",
    "    prec_f1  = float(td.get(\"prec@f1\", np.nan))\n",
    "    rec_f1   = float(td.get(\"recall@f1\", np.nan))\n",
    "    f1_test  = float(td.get(\"f1\", np.nan))\n",
    "    prec_fb  = float(td.get(\"prec@fbeta15\", np.nan))\n",
    "    rec_fb   = float(td.get(\"recall@fbeta15\", np.nan))\n",
    "    fbeta_t  = float(td.get(\"f_beta15\", np.nan))\n",
    "\n",
    "    # Start with default: keep fbeta15 threshold from blend calibration\n",
    "    th_f1  = float(thr_blend[lbl][\"th_f1\"])\n",
    "    th_fb  = float(thr_blend[lbl][\"th_fbeta15\"])\n",
    "    decision = \"fbeta15\"\n",
    "    chosen_th = th_fb\n",
    "    reason = \"default_fbeta15\"\n",
    "\n",
    "    # Heuristic 1: if F1 better than Fβ on test *and* precision improves, prefer F1\n",
    "    if (not math.isnan(f1_test) and not math.isnan(fbeta_t)) and (f1_test >= fbeta_t) and (not math.isnan(prec_f1) and not math.isnan(prec_fb)) and (prec_f1 > prec_fb):\n",
    "        decision = \"f1\"\n",
    "        chosen_th = th_f1\n",
    "        reason = \"test_f1_better_and_more_precise\"\n",
    "\n",
    "    # Heuristic 2: if precision at Fβ is below floor, try precision-floor on VAL\n",
    "    if (not math.isnan(prec_fb)) and (prec_fb < PREC_FLOOR) and (not degenerate):\n",
    "        th_pf, pr_pf, rc_pf = precision_floor_threshold(yv, pv, PREC_FLOOR)\n",
    "        # Only adopt if recall doesn't collapse completely\n",
    "        if rc_pf >= max(0.5*rec_fb, 0.10):  # keep at least 50% of recall@Fβ, or ≥0.10 absolute\n",
    "            decision = \"precision_floor\"\n",
    "            chosen_th = th_pf\n",
    "            reason = f\"val_precision_floor_{PREC_FLOOR:.2f}\"\n",
    "\n",
    "    policy[\"labels\"][lbl] = {\n",
    "        \"mode\": decision,            # \"fbeta15\" | \"f1\" | \"precision_floor\"\n",
    "        \"threshold\": float(chosen_th),\n",
    "        \"diag\": {\n",
    "            \"test_prec@f1\": prec_f1, \"test_rec@f1\": rec_f1, \"test_f1\": f1_test,\n",
    "            \"test_prec@fb\": prec_fb, \"test_rec@fb\": rec_fb, \"test_fbeta15\": fbeta_t\n",
    "        }\n",
    "    }\n",
    "\n",
    "    rows.append({\n",
    "        \"label\": lbl,\n",
    "        \"decision\": decision,\n",
    "        \"chosen_threshold\": chosen_th,\n",
    "        \"reason\": reason,\n",
    "        \"test_prec@f1\": prec_f1, \"test_rec@f1\": rec_f1, \"test_f1\": f1_test,\n",
    "        \"test_prec@fb\": prec_fb, \"test_rec@fb\": rec_fb, \"test_fbeta15\": fbeta_t\n",
    "    })\n",
    "\n",
    "# Save artifacts\n",
    "policy_path = POL_DIR / \"policy.json\"\n",
    "policy_path.write_text(json.dumps(policy, indent=2))\n",
    "pd.DataFrame(rows).to_csv(EVAL_DIR / \"policy_table.csv\", index=False)\n",
    "\n",
    "# Console summary\n",
    "dec_counts = pd.Series([r[\"decision\"] for r in rows]).value_counts().to_dict()\n",
    "print(\"✅ Per-label policy proposed and saved.\")\n",
    "print(\"  • Policy JSON →\", policy_path)\n",
    "print(\"  • Table CSV   →\", EVAL_DIR / \"policy_table.csv\")\n",
    "print(\"Decision counts:\", dec_counts)\n",
    "print(\"\\nExamples:\")\n",
    "for r in rows[:4]:\n",
    "    print(f\"  {r['label']}: {r['decision']} (th={r['chosen_threshold']:.3f}) ← {r['reason']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b09ba94",
   "metadata": {},
   "source": [
    "## phase 7 (extra squeezes and testing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a36df3c",
   "metadata": {},
   "source": [
    "### New reg to comapre f1, fbeta15 and the new policy (6,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38860232",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Policy-aware tester ready.\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from rdkit import Chem\n",
    "\n",
    "BASE       = Path(\"v7\")\n",
    "PREP_DIR   = BASE / \"data\" / \"prepared\"\n",
    "MODEL_DIR  = BASE / \"model\"\n",
    "ENS_DIR    = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR    = MODEL_DIR / \"calibration\"\n",
    "POL_DIR    = MODEL_DIR / \"policy\"\n",
    "CKPT_BEST  = MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\"\n",
    "\n",
    "assert CKPT_BEST.exists(), f\"Missing shared checkpoint: {CKPT_BEST}\"\n",
    "assert (PREP_DIR / \"dataset_manifest.json\").exists(), \"Missing dataset manifest.\"\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Labels & dims ---\n",
    "ds_manifest = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABEL_NAMES: List[str] = ds_manifest[\"labels\"]\n",
    "DESC_IN_DIM = int(ds_manifest[\"n_features\"])  # 208\n",
    "\n",
    "# --- Calibration + policy ---\n",
    "temps_spec   = json.loads((CAL_DIR / \"temps.json\").read_text())            # specialist temps\n",
    "temps_shared = json.loads((CAL_DIR / \"temps_shared.json\").read_text())     # shared temps\n",
    "blend_payload= json.loads((CAL_DIR / \"thresholds_blend.json\").read_text()) # alpha + thresholds\n",
    "ALPHA        = float(blend_payload.get(\"alpha\", 0.8))\n",
    "thr_blend    = blend_payload[\"thresholds\"]                                  # per label\n",
    "POL_PATH     = POL_DIR / \"policy.json\"\n",
    "policy       = json.loads(POL_PATH.read_text()) if POL_PATH.exists() else None\n",
    "\n",
    "# --- Text encoder (ChemBERTa) ---\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "class ChemBERTaEncoder(nn.Module):\n",
    "    def __init__(self, ckpt_name=\"seyonec/ChemBERTa-zinc-base-v1\", fusion_dim=256, dropout_p=0.1):\n",
    "        super().__init__()\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(ckpt_name)\n",
    "        self.backbone  = AutoModel.from_pretrained(ckpt_name)\n",
    "        self.proj = nn.Sequential(nn.Dropout(dropout_p), nn.Linear(self.backbone.config.hidden_size, fusion_dim))\n",
    "        self.ln   = nn.LayerNorm(fusion_dim)\n",
    "    def forward(self, smiles_list: List[str], max_length=256, add_special_tokens=True):\n",
    "        enc = self.tokenizer(list(smiles_list), padding=True, truncation=True,\n",
    "                             max_length=max_length, add_special_tokens=add_special_tokens,\n",
    "                             return_tensors=\"pt\")\n",
    "        input_ids, attention_mask = enc[\"input_ids\"].to(device), enc[\"attention_mask\"].to(device)\n",
    "        out  = self.backbone(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state  # (B,L,H)\n",
    "        toks = self.ln(self.proj(out))  # (B,L,256)\n",
    "        return toks, attention_mask.to(dtype=torch.int32)\n",
    "\n",
    "# --- Graph encoder (names aligned to checkpoint) ---\n",
    "ATOM_LIST = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"Br\",\"I\"]\n",
    "def _one_hot(v, choices):\n",
    "    z = [0]*len(choices); z[choices.index(v) if v in choices else -1] = 1; return z\n",
    "def _bucket_oh(v, lo, hi):\n",
    "    buckets = list(range(lo, hi+1)); o = [0]*(len(buckets)+1); idx = v - lo\n",
    "    o[idx if 0<=idx<len(buckets) else -1] = 1; return o\n",
    "def _atom_feat(atom):\n",
    "    hybs = [Chem.rdchem.HybridizationType.S, Chem.rdchem.HybridizationType.SP,\n",
    "            Chem.rdchem.HybridizationType.SP2, Chem.rdchem.HybridizationType.SP3,\n",
    "            Chem.rdchem.HybridizationType.SP3D, Chem.rdchem.HybridizationType.SP3D2]\n",
    "    chir = [Chem.rdchem.ChiralType.CHI_UNSPECIFIED,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CW,\n",
    "            Chem.rdchem.ChiralType.CHI_TETRAHEDRAL_CCW,\n",
    "            Chem.rdchem.ChiralType.CHI_OTHER]\n",
    "    feat  = _one_hot(atom.GetSymbol(), ATOM_LIST+[\"other\"])\n",
    "    feat += _bucket_oh(atom.GetDegree(), 0, 5)\n",
    "    feat += _bucket_oh(atom.GetFormalCharge(), -2, 2)\n",
    "    feat += (_one_hot(atom.GetHybridization(), hybs)+[0])\n",
    "    feat += [int(atom.GetIsAromatic()), int(atom.IsInRing())]\n",
    "    feat += _one_hot(atom.GetChiralTag(), chir)\n",
    "    feat += _bucket_oh(atom.GetTotalNumHs(includeNeighbors=True), 0, 4)\n",
    "    feat += _bucket_oh(atom.GetTotalValence(), 0, 5)\n",
    "    feat += [atom.GetMass()/200.0]\n",
    "    return feat  # ~51 dims\n",
    "def _smiles_to_graph(smi, max_nodes=128):\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None or mol.GetNumAtoms()==0: return np.zeros((0,0),np.float32), np.zeros((0,0),np.float32)\n",
    "    X = np.asarray([_atom_feat(mol.GetAtomWithIdx(i)) for i in range(mol.GetNumAtoms())], np.float32)\n",
    "    N = mol.GetNumAtoms(); A = np.zeros((N,N),np.float32)\n",
    "    for b in mol.GetBonds():\n",
    "        i, j = b.GetBeginAtomIdx(), b.GetEndAtomIdx()\n",
    "        A[i,j] = 1.0; A[j,i] = 1.0\n",
    "    if N>max_nodes: X=X[:max_nodes]; A=A[:max_nodes,:max_nodes]\n",
    "    return X, A\n",
    "def _collate_graphs(smiles_batch, max_nodes=128):\n",
    "    graphs = [_smiles_to_graph(s) for s in smiles_batch]\n",
    "    Nmax = max([g[0].shape[0] for g in graphs] + [1])\n",
    "    Fnode = graphs[0][0].shape[1] if graphs[0][0].size>0 else 51\n",
    "    B = len(graphs)\n",
    "    X = np.zeros((B, Nmax, Fnode), np.float32)\n",
    "    A = np.zeros((B, Nmax, Nmax), np.float32)\n",
    "    M = np.zeros((B, Nmax), np.int64)\n",
    "    for i, (x, a) in enumerate(graphs):\n",
    "        n = x.shape[0]\n",
    "        if n==0: continue\n",
    "        X[i,:n,:] = x; A[i,:n,:n] = a; M[i,:n] = 1\n",
    "    return torch.from_numpy(X).to(device), torch.from_numpy(A).to(device), torch.from_numpy(M).to(device)\n",
    "\n",
    "class GINLayer(nn.Module):\n",
    "    def __init__(self, h=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.eps = nn.Parameter(torch.tensor(0.0))\n",
    "        self.mlp = nn.Sequential(nn.Linear(h, h), nn.GELU(), nn.LayerNorm(h), nn.Dropout(p))\n",
    "    def forward(self, x, adj, mask):\n",
    "        out = (1.0 + self.eps) * x + torch.matmul(adj, x)\n",
    "        out = self.mlp(out)\n",
    "        return out * mask.unsqueeze(-1).to(out.dtype)\n",
    "\n",
    "class GraphGINEncoder(nn.Module):\n",
    "    def __init__(self, node_in_dim=51, hidden_dim=256, n_layers=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.inp = nn.Sequential(nn.Linear(node_in_dim, hidden_dim), nn.GELU(), nn.Dropout(p))\n",
    "        self.layers = nn.ModuleList([GINLayer(hidden_dim, p) for _ in range(n_layers)])\n",
    "        self.out_ln = nn.LayerNorm(hidden_dim)  # name matches checkpoint\n",
    "    def forward(self, smiles_list: List[str], max_nodes=128):\n",
    "        X, A, M = _collate_graphs(smiles_list, max_nodes=max_nodes)\n",
    "        h = self.inp(X)\n",
    "        for layer in self.layers:\n",
    "            h = layer(h, A, M)\n",
    "        return self.out_ln(h), M.to(dtype=torch.int32)\n",
    "\n",
    "# --- Fusion & shared head ---\n",
    "def masked_mean(x: torch.Tensor, mask: torch.Tensor, dim: int) -> torch.Tensor:\n",
    "    mask = mask.to(dtype=x.dtype, device=x.device)\n",
    "    denom = mask.sum(dim=dim, keepdim=True).clamp(min=1.0)\n",
    "    return (x * mask.unsqueeze(-1)).sum(dim=dim) / denom\n",
    "\n",
    "class CrossAttentionBlock(nn.Module):\n",
    "    def __init__(self, dim=256, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mha = nn.MultiheadAttention(dim, n_heads, dropout=p, batch_first=False)\n",
    "        self.ln  = nn.LayerNorm(dim)\n",
    "        self.do  = nn.Dropout(p)\n",
    "    def forward(self, text_tokens, text_mask, graph_nodes, graph_mask):\n",
    "        Q = text_tokens.transpose(0,1)   # (L,B,D)\n",
    "        K = graph_nodes.transpose(0,1)   # (N,B,D)\n",
    "        V = graph_nodes.transpose(0,1)\n",
    "        kpm = (graph_mask == 0)          # (B,N) 1=pad\n",
    "        attn, _ = self.mha(Q, K, V, key_padding_mask=kpm)\n",
    "        attn = attn.transpose(0,1)       # (B,L,D)\n",
    "        return self.ln(text_tokens + self.do(attn))\n",
    "\n",
    "class DescriptorMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim=256, hidden=256, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(hidden, out_dim), nn.GELU(), nn.Dropout(p)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "class FusionClassifier(nn.Module):\n",
    "    # name 'mlp' matches checkpoint ('shared_head.mlp.*')\n",
    "    def __init__(self, dim=256, n_labels=12, p=0.1):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(p),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, fused_vec): return self.mlp(fused_vec)\n",
    "\n",
    "class V7FusionModel(nn.Module):\n",
    "    def __init__(self, text_encoder, graph_encoder, desc_in_dim=208, dim=256, n_labels=12, n_heads=4, p=0.1):\n",
    "        super().__init__()\n",
    "        self.text_encoder=text_encoder\n",
    "        self.graph_encoder=graph_encoder\n",
    "        self.cross=CrossAttentionBlock(dim, n_heads, p)\n",
    "        self.desc_mlp=DescriptorMLP(desc_in_dim, out_dim=dim, hidden=256, p=p)\n",
    "        self.shared_head=FusionClassifier(dim, n_labels, p)\n",
    "    def forward(self, smiles_list, desc_feats):\n",
    "        tt, tm = self.text_encoder(smiles_list, max_length=256)\n",
    "        gn, gm = self.graph_encoder(smiles_list, max_nodes=128)\n",
    "        tta = self.cross(tt.to(device), tm.to(device), gn.to(device), gm.to(device))\n",
    "        de  = self.desc_mlp(desc_feats.to(device))\n",
    "        text_pool  = masked_mean(tta, tm.to(device), 1)\n",
    "        graph_pool = masked_mean(gn.to(device),  gm.to(device), 1)\n",
    "        fused = torch.cat([text_pool, graph_pool, de], dim=-1)  # (B,768)\n",
    "        logits = self.shared_head(fused)\n",
    "        return logits, fused\n",
    "\n",
    "# Build shared model\n",
    "text_encoder = ChemBERTaEncoder().to(device)\n",
    "graph_encoder= GraphGINEncoder().to(device)\n",
    "v7_shared    = V7FusionModel(text_encoder, graph_encoder, desc_in_dim=DESC_IN_DIM, n_labels=len(LABEL_NAMES)).to(device)\n",
    "ckpt = torch.load(CKPT_BEST, map_location=device)\n",
    "v7_shared.load_state_dict(ckpt[\"model\"], strict=True)\n",
    "v7_shared.eval()\n",
    "\n",
    "# --- Specialist heads (robust loader) ---\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1 = nn.Sequential(nn.Linear(in_dim, h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2 = nn.Sequential(nn.Linear(h1, h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3 = nn.Sequential(nn.Linear(h2, h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out    = nn.Linear(h3, 1)\n",
    "        self.short  = nn.Linear(in_dim, h3)\n",
    "    def forward(self, x):\n",
    "        z1 = self.block1(x); z2 = self.block2(z1); z3 = self.block3(z2)\n",
    "        z  = z3 + self.short(x)\n",
    "        return self.out(z).squeeze(-1)\n",
    "\n",
    "def _remap_keys_if_needed(state_dict: dict) -> dict:\n",
    "    if not any(k.startswith((\"b1.\",\"b2.\",\"b3.\")) for k in state_dict.keys()):\n",
    "        return state_dict\n",
    "    remap = {}\n",
    "    for k, v in state_dict.items():\n",
    "        k2 = k.replace(\"b1.\", \"block1.\").replace(\"b2.\", \"block2.\").replace(\"b3.\", \"block3.\")\n",
    "        remap[k2] = v\n",
    "    return remap\n",
    "\n",
    "def _load_best_head(label: str) -> nn.Module:\n",
    "    cands = []\n",
    "    for sd in sorted((ENS_DIR / label).glob(\"seed*/\")):\n",
    "        mfile = sd / \"metrics.json\"\n",
    "        if mfile.exists():\n",
    "            try:\n",
    "                ap = float(json.loads(mfile.read_text()).get(\"best_ap\", float(\"nan\")))\n",
    "                cands.append((ap, sd))\n",
    "            except Exception:\n",
    "                pass\n",
    "    if not cands:\n",
    "        raise FileNotFoundError(f\"No trained heads for label {label} under {ENS_DIR/label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best_dir = cands[0][1]\n",
    "    ck = torch.load(best_dir / \"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(in_dim=cfg[\"in_dim\"], h1=cfg[\"h1\"], h2=cfg[\"h2\"], h3=cfg[\"h3\"], p=cfg.get(\"dropout\",0.30)).to(device)\n",
    "    state = _remap_keys_if_needed(ck[\"model\"])\n",
    "    head.load_state_dict(state, strict=True)\n",
    "    head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS: Dict[str, nn.Module] = {lbl: _load_best_head(lbl) for lbl in LABEL_NAMES}\n",
    "\n",
    "# --- Descriptor prep for ad-hoc inputs: standardized zeros (robust & simple) ---\n",
    "def prepare_desc_matrix(smiles_list: List[str]) -> torch.Tensor:\n",
    "    Z = np.zeros((len(smiles_list), DESC_IN_DIM), dtype=np.float32)\n",
    "    return torch.tensor(Z, dtype=torch.float32, device=device)\n",
    "\n",
    "# --- Normalize SMARTS→SMILES if needed ---\n",
    "def normalize_smiles_or_smarts(s: str) -> str:\n",
    "    if not isinstance(s, str): s = str(s)\n",
    "    mol = Chem.MolFromSmiles(s)\n",
    "    if mol: return Chem.MolToSmiles(mol)\n",
    "    q = Chem.MolFromSmarts(s)\n",
    "    if q:\n",
    "        try:\n",
    "            smi = Chem.MolToSmiles(q)\n",
    "            return smi if smi else s\n",
    "        except Exception:\n",
    "            return s\n",
    "    return s\n",
    "\n",
    "@torch.no_grad()\n",
    "def fused_from_smiles(smiles_list: List[str]) -> torch.Tensor:\n",
    "    smiles_list = [normalize_smiles_or_smarts(s) for s in smiles_list]\n",
    "    desc = prepare_desc_matrix(smiles_list)\n",
    "    _, fused = v7_shared(smiles_list, desc)\n",
    "    return fused  # (B,768)\n",
    "\n",
    "# --- Inference helpers ---\n",
    "sigmoid = lambda x: 1.0/(1.0+math.e**(-x))\n",
    "\n",
    "@torch.no_grad()\n",
    "def _probs_for_one(smi: str) -> Dict[str, Dict[str, float]]:\n",
    "    \"\"\"Return per-label {prob_spec, prob_shared, prob_blend} for one SMILES using alpha & temps.\"\"\"\n",
    "    x = fused_from_smiles([smi])  # (1,768)\n",
    "    logits_shared = v7_shared.shared_head(x).detach().cpu().numpy()[0]  # (L,)\n",
    "    rec = {}\n",
    "    for j, lbl in enumerate(LABEL_NAMES):\n",
    "        logit_spec = HEADS[lbl](x).item()\n",
    "        p_spec   = sigmoid(logit_spec / max(float(temps_spec.get(lbl, 1.0)), 1e-3))\n",
    "        p_shared = sigmoid(float(logits_shared[j]) / max(float(temps_shared.get(lbl, 1.0)), 1e-3))\n",
    "        p_blend  = ALPHA * p_spec + (1.0 - ALPHA) * p_shared\n",
    "        rec[lbl] = {\"prob_spec\": float(p_spec), \"prob_shared\": float(p_shared), \"prob_blend\": float(p_blend)}\n",
    "    return rec\n",
    "\n",
    "def _threshold_for(lbl: str, mode: str) -> float:\n",
    "    \"\"\"Return threshold for a label under 'f1', 'fbeta15', or 'policy'.\"\"\"\n",
    "    if mode in (\"f1\", \"fbeta15\"):\n",
    "        key = \"th_f1\" if mode == \"f1\" else \"th_fbeta15\"\n",
    "        return float(thr_blend[lbl][key])\n",
    "    elif mode == \"policy\":\n",
    "        if policy is None:\n",
    "            raise RuntimeError(\"policy.json not found; run Phase 6 — Cell 3 to create it.\")\n",
    "        return float(policy[\"labels\"][lbl][\"threshold\"])\n",
    "    else:\n",
    "        raise ValueError(\"mode must be 'f1', 'fbeta15', or 'policy'\")\n",
    "\n",
    "def predict_one(smi: str, mode: str = \"fbeta15\", topk: int = 5, show_parts: bool = False):\n",
    "    \"\"\"\n",
    "    mode: 'f1' | 'fbeta15' | 'policy'\n",
    "    Prints top-k by blended prob and the positive set at chosen thresholds.\n",
    "    If show_parts=True, also shows p_spec / p_shared next to p_blend.\n",
    "    \"\"\"\n",
    "    assert mode in (\"f1\",\"fbeta15\",\"policy\")\n",
    "    rec = _probs_for_one(smi)\n",
    "    print(f\"\\nSMILES/SMARTS: {smi}\\n mode={mode}, alpha={ALPHA:.2f}\")\n",
    "    rows = []\n",
    "    for lbl, d in rec.items():\n",
    "        th = _threshold_for(lbl, mode)\n",
    "        rows.append((lbl, d[\"prob_blend\"], th, d[\"prob_blend\"] >= th, d[\"prob_spec\"], d[\"prob_shared\"]))\n",
    "    rows.sort(key=lambda z: z[1], reverse=True)\n",
    "    for lbl, p, th, dec, ps, ph in rows[:topk]:\n",
    "        if show_parts:\n",
    "            print(f\"  {lbl:12s}  p_spec={ps:.3f}  p_shared={ph:.3f}  p_blend={p:.3f}  th={th:.3f}  → pred={int(dec)}\")\n",
    "        else:\n",
    "            print(f\"  {lbl:12s}  p_blend={p:.3f}  th={th:.3f}  → pred={int(dec)}\")\n",
    "    positives = [lbl for lbl, p, th, dec, *_ in rows if dec]\n",
    "    print(\"  Positives:\", \", \".join(positives) if positives else \"none\")\n",
    "    return {lbl: {\"prob_spec\": float(ps), \"prob_shared\": float(ph), \"prob_blend\": float(p),\n",
    "                  \"threshold\": float(th), \"decision\": bool(dec)}\n",
    "            for (lbl, p, th, dec, ps, ph) in rows}\n",
    "\n",
    "def compare_modes(smi: str, modes: List[str] = (\"fbeta15\",\"f1\",\"policy\"), topk: int = 5):\n",
    "    \"\"\"Side-by-side comparison for the same SMILES.\"\"\"\n",
    "    print(\"=\"*72)\n",
    "    for m in modes:\n",
    "        predict_one(smi, mode=m, topk=topk, show_parts=False)\n",
    "        print(\"-\"*72)\n",
    "\n",
    "print(\"✅ Policy-aware tester ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6673a81",
   "metadata": {},
   "source": [
    "#### test on the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10652b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved:\n",
      "  • Detailed predictions → v7\\results\\inference\\Truth_labels_detailed.csv\n",
      "  • Summary metrics      → v7\\results\\inference\\Truth_labels_summary.json\n",
      "\n",
      "=== Micro metrics by mode ===\n",
      "  fbeta15   P=0.625  R=0.833  F1=0.714\n",
      "  f1        P=0.750  R=0.750  F1=0.750\n",
      "  policy    P=0.667  R=0.667  F1=0.667\n",
      "\n",
      "=== Macro F1 by mode ===\n",
      "  fbeta15   macro-F1=0.783\n",
      "  f1        macro-F1=0.829\n",
      "  policy    macro-F1=0.772\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "need = ['v7_shared','HEADS','LABEL_NAMES','temps_spec','temps_shared','ALPHA','thr_blend','_probs_for_one','_threshold_for']\n",
    "for n in need:\n",
    "    assert n in globals(), f\"Missing '{n}'. Please run the self-contained test rig cell first.\"\n",
    "\n",
    "# ---- paths ----\n",
    "BASE    = Path(\"v7\")\n",
    "DATA_XL = BASE / \"data\" / \"Truth Lables.xlsx\"\n",
    "OUT_DIR = BASE / \"results\" / \"inference\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---- load excel ----\n",
    "assert DATA_XL.exists(), f\"Missing file: {DATA_XL}\"\n",
    "df = pd.read_excel(DATA_XL)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# smiles column\n",
    "smiles_col = next((c for c in [\"smiles\",\"SMILES\",\"Smile\",\"smile\",\"SMILE\"] if c in df.columns), None)\n",
    "assert smiles_col is not None, f\"Could not find a SMILES column in: {list(df.columns)}\"\n",
    "\n",
    "# label columns must match training label names exactly\n",
    "LABELS = list(LABEL_NAMES)\n",
    "label_cols = [c for c in df.columns if c in LABELS]\n",
    "assert len(label_cols) == len(LABELS), \\\n",
    "    f\"Expected 12 label columns matching training names.\\nFound {len(label_cols)}: {label_cols}\\nWanted: {LABELS}\"\n",
    "\n",
    "def _to01(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    if isinstance(x, str):\n",
    "        xs = x.strip().lower()\n",
    "        if xs in {\"1\",\"true\",\"yes\",\"y\"}: return 1\n",
    "        if xs in {\"0\",\"false\",\"no\",\"n\"}: return 0\n",
    "        try:\n",
    "            xv = float(xs)\n",
    "            if np.isnan(xv): return np.nan\n",
    "            return 1 if xv >= 0.5 else 0\n",
    "        except Exception:\n",
    "            return np.nan\n",
    "    try:\n",
    "        xv = float(x)\n",
    "        if np.isnan(xv): return np.nan\n",
    "        return 1 if xv >= 0.5 else 0\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "MODES = (\"fbeta15\",\"f1\",\"policy\")\n",
    "\n",
    "# ---- iterate molecules, compute predictions once, then threshold by mode ----\n",
    "rows = []\n",
    "for idx, r in df.iterrows():\n",
    "    smi = str(r[smiles_col])\n",
    "    probs = _probs_for_one(smi)  # {lbl: {prob_spec, prob_shared, prob_blend}}\n",
    "    for lbl in LABELS:\n",
    "        prob = float(probs[lbl][\"prob_blend\"])\n",
    "        truth = _to01(r.get(lbl, np.nan))\n",
    "        for mode in MODES:\n",
    "            # skip policy if policy.json wasn’t created\n",
    "            try:\n",
    "                th = _threshold_for(lbl, mode)\n",
    "            except Exception:\n",
    "                if mode == \"policy\":\n",
    "                    continue\n",
    "                raise\n",
    "            pred = int(prob >= th)\n",
    "            rows.append({\n",
    "                \"row_id\": int(idx),\n",
    "                \"smiles\": smi,\n",
    "                \"label\": lbl,\n",
    "                \"mode\": mode,\n",
    "                \"prob_blend\": prob,\n",
    "                \"threshold\": float(th),\n",
    "                \"prediction\": pred,\n",
    "                \"truth\": (np.nan if pd.isna(truth) else int(truth))\n",
    "            })\n",
    "\n",
    "detailed = pd.DataFrame(rows)\n",
    "\n",
    "# ---- metrics helpers (robust to empty/degenerate cases) ----\n",
    "def _safe_div(n, d):\n",
    "    return (n / d) if d > 0 else np.nan\n",
    "\n",
    "def _prf(tp, fp, fn):\n",
    "    prec = _safe_div(tp, tp+fp)\n",
    "    rec  = _safe_div(tp, tp+fn)\n",
    "    if np.isnan(prec) or np.isnan(rec) or (prec+rec) == 0:\n",
    "        f1 = np.nan\n",
    "    else:\n",
    "        f1 = 2 * prec * rec / (prec + rec)\n",
    "    return prec, rec, f1\n",
    "\n",
    "def _metrics(df_long: pd.DataFrame, mode: str) -> Dict:\n",
    "    dd = df_long[df_long[\"mode\"] == mode].copy()\n",
    "    dd = dd.dropna(subset=[\"truth\"])  # drop rows with unknown truth\n",
    "    if dd.empty:\n",
    "        return {\n",
    "            \"mode\": mode,\n",
    "            \"micro\": {\"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan},\n",
    "            \"macro_f1\": np.nan,\n",
    "            \"per_label\": [{ \"label\": lbl, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan } for lbl in LABELS]\n",
    "        }\n",
    "\n",
    "    y = dd[\"truth\"].astype(int).to_numpy()\n",
    "    p = dd[\"prediction\"].astype(int).to_numpy()\n",
    "    tp = int(((p==1)&(y==1)).sum())\n",
    "    fp = int(((p==1)&(y==0)).sum())\n",
    "    fn = int(((p==0)&(y==1)).sum())\n",
    "    m_prec, m_rec, m_f1 = _prf(tp, fp, fn)\n",
    "\n",
    "    per_label = []\n",
    "    for lbl in LABELS:\n",
    "        d = dd[dd[\"label\"] == lbl]\n",
    "        if d.empty:\n",
    "            per_label.append({\"label\": lbl, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan})\n",
    "            continue\n",
    "        yj = d[\"truth\"].astype(int).to_numpy()\n",
    "        pj = d[\"prediction\"].astype(int).to_numpy()\n",
    "        tpj = int(((pj==1)&(yj==1)).sum())\n",
    "        fpj = int(((pj==1)&(yj==0)).sum())\n",
    "        fnj = int(((pj==0)&(yj==1)).sum())\n",
    "        prec, rec, f1 = _prf(tpj, fpj, fnj)\n",
    "        per_label.append({\n",
    "            \"label\": lbl,\n",
    "            \"precision\": float(prec) if not np.isnan(prec) else np.nan,\n",
    "            \"recall\":    float(rec)  if not np.isnan(rec)  else np.nan,\n",
    "            \"f1\":        float(f1)   if not np.isnan(f1)   else np.nan\n",
    "        })\n",
    "\n",
    "    # macro F1 across labels (ignore NaNs)\n",
    "    macro_f1 = float(np.nanmean([pl[\"f1\"] for pl in per_label])) if len(per_label) else np.nan\n",
    "\n",
    "    return {\n",
    "        \"mode\": mode,\n",
    "        \"micro\": {\"precision\": float(m_prec) if not np.isnan(m_prec) else np.nan,\n",
    "                  \"recall\":    float(m_rec)  if not np.isnan(m_rec)  else np.nan,\n",
    "                  \"f1\":        float(m_f1)   if not np.isnan(m_f1)   else np.nan},\n",
    "        \"macro_f1\": macro_f1,\n",
    "        \"per_label\": per_label\n",
    "    }\n",
    "\n",
    "# Only compute 'policy' summary if any policy rows exist\n",
    "modes_present = detailed[\"mode\"].unique().tolist()\n",
    "summary = {m: _metrics(detailed, m) for m in MODES if m in modes_present}\n",
    "\n",
    "# ---- save outputs ----\n",
    "csv_path  = OUT_DIR / \"Truth_labels_detailed.csv\"\n",
    "json_path = OUT_DIR / \"Truth_labels_summary.json\"\n",
    "detailed.to_csv(csv_path, index=False)\n",
    "json_path.write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\"  • Detailed predictions →\", csv_path)\n",
    "print(\"  • Summary metrics      →\", json_path)\n",
    "\n",
    "# quick leaderboard\n",
    "print(\"\\n=== Micro metrics by mode ===\")\n",
    "for m, rep in summary.items():\n",
    "    pr = rep[\"micro\"][\"precision\"]; rc = rep[\"micro\"][\"recall\"]; f1 = rep[\"micro\"][\"f1\"]\n",
    "    print(f\"  {m:8s}  P={np.nan if pr is None else pr:.3f}  R={np.nan if rc is None else rc:.3f}  F1={np.nan if f1 is None else f1:.3f}\")\n",
    "print(\"\\n=== Macro F1 by mode ===\")\n",
    "for m, rep in summary.items():\n",
    "    print(f\"  {m:8s}  macro-F1={rep['macro_f1']:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7665d1a9",
   "metadata": {},
   "source": [
    "### Recompute the F1 Recompute F1 thresholds (VAL) and save as thresholds_blend_v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7691d1b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomputing F1 thresholds on VAL...\n",
      "  NR-AR         old_f1=0.653 → new_f1=0.660  (Δ=+0.007)  AP=0.171\n",
      "  NR-AR-LBD     old_f1=0.621 → new_f1=0.621  (Δ=+0.001)  AP=0.253\n",
      "  NR-AhR        old_f1=0.709 → new_f1=0.709  (Δ=+0.001)  AP=0.524\n",
      "  NR-Aromatase  old_f1=0.564 → new_f1=0.565  (Δ=+0.001)  AP=0.295\n",
      "  NR-ER         old_f1=0.547 → new_f1=0.547  (Δ=+0.000)  AP=0.253\n",
      "  NR-ER-LBD     old_f1=0.589 → new_f1=0.594  (Δ=+0.004)  AP=0.139\n",
      "  NR-PPAR-gamma  old_f1=0.441 → new_f1=0.441  (Δ=+0.000)  AP=0.063\n",
      "  SR-ARE        old_f1=0.528 → new_f1=0.528  (Δ=+0.001)  AP=0.344\n",
      "  SR-ATAD5      old_f1=0.483 → new_f1=0.483  (Δ=+0.001)  AP=0.171\n",
      "  SR-HSE        old_f1=0.472 → new_f1=0.472  (Δ=+0.000)  AP=0.196\n",
      "  SR-MMP        old_f1=0.589 → new_f1=0.591  (Δ=+0.002)  AP=0.444\n",
      "  SR-p53        old_f1=0.513 → new_f1=0.513  (Δ=+0.000)  AP=0.210\n",
      "\n",
      "✅ Saved updated thresholds → v7\\model\\calibration\\thresholds_blend_v2.json\n"
     ]
    }
   ],
   "source": [
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "BASE      = Path(\"v7\")\n",
    "PREP_DIR  = BASE / \"data\" / \"prepared\"\n",
    "FUSED_DIR = BASE / \"data\" / \"fused\"\n",
    "MODEL_DIR = BASE / \"model\"\n",
    "CAL_DIR   = MODEL_DIR / \"calibration\"\n",
    "ENS_DIR   = MODEL_DIR / \"ensembles\"\n",
    "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Load manifest, temps, old thresholds (keep fb15 as-is) ---\n",
    "mani = json.loads((PREP_DIR / \"dataset_manifest.json\").read_text())\n",
    "LABELS = mani[\"labels\"]; N_LABELS = len(LABELS)\n",
    "DESC_IN_DIM = int(mani[\"n_features\"])\n",
    "\n",
    "temps_spec   = json.loads((CAL_DIR / \"temps.json\").read_text())\n",
    "temps_shared = json.loads((CAL_DIR / \"temps_shared.json\").read_text())\n",
    "blend_payload= json.loads((CAL_DIR / \"thresholds_blend.json\").read_text())\n",
    "ALPHA        = float(blend_payload.get(\"alpha\", 0.8))\n",
    "thr_old      = blend_payload[\"thresholds\"]  # dict[label]->{th_f1, th_fbeta15, ap_val}\n",
    "\n",
    "# --- Ensure VAL fused features ---\n",
    "Xva = np.load(FUSED_DIR / \"val_fused.npy\").astype(np.float32)\n",
    "val_blob = np.load(PREP_DIR / \"val.npz\", allow_pickle=True)\n",
    "Yva = val_blob[\"Y\"].astype(np.float32)\n",
    "Mva = val_blob[\"y_missing_mask\"].astype(bool)\n",
    "Xva_t = torch.tensor(Xva, dtype=torch.float32, device=device)\n",
    "\n",
    "# --- Shared head MLP (for logits) ---\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "sh = SharedHeadMLP().to(device)\n",
    "ckpt = torch.load(MODEL_DIR / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "sh_state = {k.replace(\"shared_head.\", \"\"): v for k,v in ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(sh_state, strict=True)\n",
    "sh.eval()\n",
    "\n",
    "# --- Specialist heads loader (best seed per label) ---\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(nn.Linear(in_dim,h1),nn.GELU(),nn.LayerNorm(h1),nn.Dropout(p))\n",
    "        self.block2=nn.Sequential(nn.Linear(h1,h2),nn.GELU(),nn.LayerNorm(h2),nn.Dropout(p))\n",
    "        self.block3=nn.Sequential(nn.Linear(h2,h3),nn.GELU(),nn.LayerNorm(h3),nn.Dropout(p))\n",
    "        self.out=nn.Linear(h3,1); self.short=nn.Linear(in_dim,h3)\n",
    "    def forward(self,x):\n",
    "        z1=self.block1(x); z2=self.block2(z1); z3=self.block3(z2)\n",
    "        return self.out(z3+self.short(x)).squeeze(-1)\n",
    "\n",
    "def _remap_keys_if_needed(sd: dict)->dict:\n",
    "    if not any(k.startswith((\"b1.\",\"b2.\",\"b3.\")) for k in sd.keys()): return sd\n",
    "    out={}; \n",
    "    for k,v in sd.items():\n",
    "        out[k.replace(\"b1.\",\"block1.\").replace(\"b2.\",\"block2.\").replace(\"b3.\",\"block3.\")] = v\n",
    "    return out\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    import math, json\n",
    "    cands=[]\n",
    "    for sd in sorted((ENS_DIR/label).glob(\"seed*/\")):\n",
    "        m=sd/\"metrics.json\"\n",
    "        if m.exists():\n",
    "            try: cands.append((float(json.loads(m.read_text()).get(\"best_ap\", float(\"nan\"))), sd))\n",
    "            except: pass\n",
    "    if not cands: raise FileNotFoundError(f\"No heads for {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best = cands[0][1]\n",
    "    ck = torch.load(best/\"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head=LabelHead(cfg[\"in_dim\"],cfg[\"h1\"],cfg[\"h2\"],cfg[\"h3\"],cfg.get(\"dropout\",0.30)).to(device)\n",
    "    state=_remap_keys_if_needed(ck[\"model\"]); head.load_state_dict(state, strict=True); head.eval(); return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "\n",
    "# --- Build blended probabilities on VAL ---\n",
    "sigmoid = lambda x: 1/(1+np.exp(-x))\n",
    "with torch.no_grad():\n",
    "    spec_logits = torch.stack([HEADS[l](Xva_t) for l in LABELS], dim=1).cpu().numpy()  # (Nv,L)\n",
    "    sh_logits   = sh(Xva_t).cpu().numpy()\n",
    "\n",
    "P_blend = np.zeros_like(sh_logits, np.float32)\n",
    "for j,lbl in enumerate(LABELS):\n",
    "    Ts=max(float(temps_spec.get(lbl,1.0)),1e-3); Th=max(float(temps_shared.get(lbl,1.0)),1e-3)\n",
    "    ps=sigmoid(spec_logits[:,j]/Ts); ph=sigmoid(sh_logits[:,j]/Th)\n",
    "    P_blend[:,j]=np.clip(ALPHA*ps + (1-ALPHA)*ph, 0, 1)\n",
    "\n",
    "# --- Recompute th_f1 per label (keep old th_fbeta15) ---\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def best_f1_threshold(y_true, probs):\n",
    "    prec, rec, th = precision_recall_curve(y_true, probs)\n",
    "    eps=1e-8; f1=(2*prec*rec)/np.maximum(prec+rec, eps)\n",
    "    if th.size==0: return 0.5\n",
    "    # f1 is len = len(th)+1; pick argmax ignoring the first point\n",
    "    idx = int(np.nanargmax(f1[1:]))+1\n",
    "    # align threshold array with f1: append 1.0 at end\n",
    "    th_aligned = np.concatenate([th, [1.0]])\n",
    "    return float(th_aligned[idx])\n",
    "\n",
    "new = {\"alpha\": ALPHA, \"thresholds\": {}}\n",
    "print(\"Recomputing F1 thresholds on VAL...\")\n",
    "for j,lbl in enumerate(LABELS):\n",
    "    valid = ~Mva[:,j]\n",
    "    if valid.sum()==0 or np.all(Yva[valid,j]==Yva[valid,j][0]):\n",
    "        new[\"thresholds\"][lbl] = {\n",
    "            \"th_f1\": float(thr_old[lbl][\"th_f1\"]),\n",
    "            \"th_fbeta15\": float(thr_old[lbl][\"th_fbeta15\"]),\n",
    "            \"ap_val\": float(thr_old[lbl].get(\"ap_val\", float(\"nan\")))\n",
    "        }\n",
    "        continue\n",
    "    th_f1_new = best_f1_threshold(Yva[valid,j].astype(int), P_blend[valid,j])\n",
    "    ap = float(average_precision_score(Yva[valid,j].astype(int), P_blend[valid,j]))\n",
    "    th_f1_old = float(thr_old[lbl][\"th_f1\"])\n",
    "    th_fb = float(thr_old[lbl][\"th_fbeta15\"])\n",
    "    print(f\"  {lbl:12s}  old_f1={th_f1_old:.3f} → new_f1={th_f1_new:.3f}  (Δ={th_f1_new-th_f1_old:+.3f})  AP={ap:.3f}\")\n",
    "    new[\"thresholds\"][lbl] = {\"th_f1\": th_f1_new, \"th_fbeta15\": th_fb, \"ap_val\": ap}\n",
    "\n",
    "out_path = CAL_DIR / \"thresholds_blend_v2.json\"\n",
    "out_path.write_text(json.dumps(new, indent=2))\n",
    "print(\"\\n✅ Saved updated thresholds →\", out_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f8feb",
   "metadata": {},
   "source": [
    "#### Test of CSV "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1fc1c35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved:\n",
      "  • Detailed predictions → v7\\results\\inference\\Truth_labels_detailed_v2.csv\n",
      "  • Summary metrics      → v7\\results\\inference\\Truth_labels_summary_v2.json\n",
      "  • f1_soft delta = 0.040\n",
      "\n",
      "=== Micro metrics by mode ===\n",
      "  fbeta15   P=0.703  R=0.818  F1=0.756\n",
      "  f1        P=0.741  R=0.727  F1=0.734\n",
      "  f1_soft   P=0.705  R=0.782  F1=0.741\n",
      "  policy    P=0.691  R=0.691  F1=0.691\n",
      "\n",
      "=== Macro F1 by mode ===\n",
      "  fbeta15   macro-F1=0.705\n",
      "  f1        macro-F1=0.760\n",
      "  f1_soft   macro-F1=0.703\n",
      "  policy    macro-F1=0.739\n"
     ]
    }
   ],
   "source": [
    "# === Phase 6 — Cell 4b: Truth test with new f1_soft mode (uses thresholds_blend_v2.json if present) ===\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "BASE     = Path(\"v7\")\n",
    "DATA_XL  = BASE / \"data\" / \"Truth Lables.xlsx\"\n",
    "OUT_DIR  = BASE / \"results\" / \"inference\"\n",
    "CAL_DIR  = BASE / \"model\" / \"calibration\"\n",
    "POL_DIR  = BASE / \"model\" / \"policy\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Prereqs from the self-contained rig\n",
    "need = ['v7_shared','HEADS','LABEL_NAMES','temps_spec','temps_shared','ALPHA','_probs_for_one']\n",
    "for n in need: assert n in globals(), f\"Missing {n}; run the policy-aware test rig cell first.\"\n",
    "\n",
    "# Load thresholds: prefer v2 if exists\n",
    "tb_v2 = CAL_DIR / \"thresholds_blend_v2.json\"\n",
    "tb_v1 = CAL_DIR / \"thresholds_blend.json\"\n",
    "assert tb_v1.exists(), f\"Missing {tb_v1}\"\n",
    "thr_payload = json.loads((tb_v2 if tb_v2.exists() else tb_v1).read_text())\n",
    "thr = thr_payload[\"thresholds\"]   # label -> {th_f1, th_fbeta15}\n",
    "ALPHA = float(thr_payload.get(\"alpha\", ALPHA))  # keep same alpha\n",
    "\n",
    "# Optional policy\n",
    "POL_PATH = POL_DIR / \"policy.json\"\n",
    "policy = json.loads(POL_PATH.read_text()) if POL_PATH.exists() else None\n",
    "\n",
    "# New soft band (delta)\n",
    "DELTA = 0.04\n",
    "\n",
    "def _threshold_for(lbl: str, mode: str, p_blend: float = None) -> float:\n",
    "    \"\"\"For 'f1_soft' we return the f1 threshold (decision rule adds softness outside).\"\"\"\n",
    "    if mode == \"f1\":\n",
    "        return float(thr[lbl][\"th_f1\"])\n",
    "    if mode == \"fbeta15\":\n",
    "        return float(thr[lbl][\"th_fbeta15\"])\n",
    "    if mode == \"policy\":\n",
    "        if policy is None: raise RuntimeError(\"policy.json not found\")\n",
    "        return float(policy[\"labels\"][lbl][\"threshold\"])\n",
    "    if mode == \"f1_soft\":\n",
    "        return float(thr[lbl][\"th_f1\"])\n",
    "    raise ValueError(\"mode must be one of: 'f1','fbeta15','policy','f1_soft'\")\n",
    "\n",
    "def _decide(lbl: str, mode: str, p: float) -> int:\n",
    "    if mode in (\"f1\",\"fbeta15\",\"policy\"):\n",
    "        th = _threshold_for(lbl, mode)\n",
    "        return int(p >= th)\n",
    "    if mode == \"f1_soft\":\n",
    "        th_f1 = float(thr[lbl][\"th_f1\"])\n",
    "        th_fb = float(thr[lbl][\"th_fbeta15\"])\n",
    "        if p >= th_f1: \n",
    "            return 1\n",
    "        if (p >= th_fb) and ((th_f1 - p) <= DELTA):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "# --- Load excel truth set ---\n",
    "assert DATA_XL.exists(), f\"Missing {DATA_XL}\"\n",
    "df = pd.read_excel(DATA_XL)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "smiles_col = next((c for c in [\"smiles\",\"SMILES\",\"Smile\",\"smile\",\"SMILE\"] if c in df.columns), None)\n",
    "assert smiles_col is not None, f\"No SMILES column found. Got: {list(df.columns)}\"\n",
    "LABELS = list(LABEL_NAMES)\n",
    "label_cols = [c for c in df.columns if c in LABELS]\n",
    "assert len(label_cols) == len(LABELS), f\"Label columns mismatch. Found {label_cols}, expected {LABELS}\"\n",
    "\n",
    "def _to01(x):\n",
    "    if pd.isna(x): return np.nan\n",
    "    try:\n",
    "        if isinstance(x,str):\n",
    "            xs=x.strip().lower()\n",
    "            if xs in {\"1\",\"true\",\"yes\",\"y\"}: return 1\n",
    "            if xs in {\"0\",\"false\",\"no\",\"n\"}: return 0\n",
    "            xv=float(xs); \n",
    "            if np.isnan(xv): return np.nan\n",
    "            return 1 if xv>=0.5 else 0\n",
    "        xv=float(x); \n",
    "        if np.isnan(xv): return np.nan\n",
    "        return 1 if xv>=0.5 else 0\n",
    "    except: \n",
    "        return np.nan\n",
    "\n",
    "MODES = [\"fbeta15\",\"f1\",\"f1_soft\"]\n",
    "if policy is not None: MODES.append(\"policy\")\n",
    "\n",
    "# --- Compute predictions ---\n",
    "rows=[]\n",
    "for idx, r in df.iterrows():\n",
    "    smi = str(r[smiles_col])\n",
    "    per_label = _probs_for_one(smi)  # {lbl:{prob_spec,prob_shared,prob_blend}}\n",
    "    for lbl in LABELS:\n",
    "        p = float(per_label[lbl][\"prob_blend\"])\n",
    "        truth = _to01(r.get(lbl, np.nan))\n",
    "        for m in MODES:\n",
    "            th = _threshold_for(lbl, m, p)\n",
    "            pred = _decide(lbl, m, p)\n",
    "            rows.append({\n",
    "                \"row_id\": int(idx),\n",
    "                \"smiles\": smi,\n",
    "                \"label\": lbl,\n",
    "                \"mode\": m,\n",
    "                \"prob_blend\": p,\n",
    "                \"threshold\": float(th),\n",
    "                \"prediction\": int(pred),\n",
    "                \"truth\": (np.nan if pd.isna(truth) else int(truth))\n",
    "            })\n",
    "detailed = pd.DataFrame(rows)\n",
    "\n",
    "# --- Metrics (robust) ---\n",
    "def _safe_div(n,d): return (n/d) if d>0 else np.nan\n",
    "def _prf(tp,fp,fn):\n",
    "    prec=_safe_div(tp,tp+fp); rec=_safe_div(tp,tp+fn)\n",
    "    f1=np.nan if (np.isnan(prec) or np.isnan(rec) or (prec+rec)==0) else (2*prec*rec/(prec+rec))\n",
    "    return prec,rec,f1\n",
    "\n",
    "def _metrics(df_long, mode):\n",
    "    dd=df_long[df_long[\"mode\"]==mode].dropna(subset=[\"truth\"]).copy()\n",
    "    if dd.empty:\n",
    "        return {\"mode\":mode,\"micro\":{\"precision\":np.nan,\"recall\":np.nan,\"f1\":np.nan},\"macro_f1\":np.nan,\"per_label\":[]}\n",
    "    y=dd[\"truth\"].astype(int).to_numpy(); p=dd[\"prediction\"].astype(int).to_numpy()\n",
    "    tp=int(((p==1)&(y==1)).sum()); fp=int(((p==1)&(y==0)).sum()); fn=int(((p==0)&(y==1)).sum())\n",
    "    M=_prf(tp,fp,fn)\n",
    "    per=[]\n",
    "    for lbl in LABELS:\n",
    "        d=dd[dd[\"label\"]==lbl]; \n",
    "        if d.empty: per.append({\"label\":lbl,\"precision\":np.nan,\"recall\":np.nan,\"f1\":np.nan}); continue\n",
    "        yj=d[\"truth\"].astype(int).to_numpy(); pj=d[\"prediction\"].astype(int).to_numpy()\n",
    "        tpj=int(((pj==1)&(yj==1)).sum()); fpj=int(((pj==1)&(yj==0)).sum()); fnj=int(((pj==0)&(yj==1)).sum())\n",
    "        pr,rc,f1=_prf(tpj,fpj,fnj)\n",
    "        per.append({\"label\":lbl,\"precision\":float(pr) if not np.isnan(pr) else np.nan,\n",
    "                           \"recall\":float(rc) if not np.isnan(rc) else np.nan,\n",
    "                           \"f1\":float(f1) if not np.isnan(f1) else np.nan})\n",
    "    macro_f1=float(np.nanmean([pl[\"f1\"] for pl in per])) if per else np.nan\n",
    "    return {\"mode\":mode,\"micro\":{\"precision\":float(M[0]) if not np.isnan(M[0]) else np.nan,\n",
    "                                 \"recall\":float(M[1]) if not np.isnan(M[1]) else np.nan,\n",
    "                                 \"f1\":float(M[2]) if not np.isnan(M[2]) else np.nan},\n",
    "            \"macro_f1\":macro_f1,\"per_label\":per}\n",
    "\n",
    "summary = {m:_metrics(detailed,m) for m in MODES}\n",
    "\n",
    "# --- Save v2 outputs ---\n",
    "csv_path  = OUT_DIR / \"Truth_labels_detailed_v2.csv\"\n",
    "json_path = OUT_DIR / \"Truth_labels_summary_v2.json\"\n",
    "detailed.to_csv(csv_path, index=False)\n",
    "json_path.write_text(json.dumps(summary, indent=2))\n",
    "\n",
    "print(\"✅ Saved:\")\n",
    "print(\"  • Detailed predictions →\", csv_path)\n",
    "print(\"  • Summary metrics      →\", json_path)\n",
    "print(f\"  • f1_soft delta = {DELTA:.3f}\")\n",
    "\n",
    "print(\"\\n=== Micro metrics by mode ===\")\n",
    "for m, rep in summary.items():\n",
    "    pr, rc, f1 = rep[\"micro\"][\"precision\"], rep[\"micro\"][\"recall\"], rep[\"micro\"][\"f1\"]\n",
    "    print(f\"  {m:8s}  P={np.nan if pr is None else pr:.3f}  R={np.nan if rc is None else rc:.3f}  F1={np.nan if f1 is None else f1:.3f}\")\n",
    "print(\"\\n=== Macro F1 by mode ===\")\n",
    "for m, rep in summary.items():\n",
    "    print(f\"  {m:8s}  macro-F1={rep['macro_f1']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db90845",
   "metadata": {},
   "source": [
    "### Per-label stacking combiner (VAL), calibrate & threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfa23c7",
   "metadata": {},
   "source": [
    "#### A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4598421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting per-label logistic stackers on VAL...\n",
      "  NR-AR       : T=0.826  AP_val=0.173  th_f1=0.966  th_fb=0.966\n",
      "  NR-AR-LBD   : T=0.602  AP_val=0.261  th_f1=0.945  th_fb=0.945\n",
      "  NR-AhR      : T=1.009  AP_val=0.518  th_f1=0.818  th_fb=0.713\n",
      "  NR-Aromatase: T=0.969  AP_val=0.293  th_f1=0.784  th_fb=0.622\n",
      "  NR-ER       : T=0.703  AP_val=0.251  th_f1=0.672  th_fb=0.504\n",
      "  NR-ER-LBD   : T=0.591  AP_val=0.116  th_f1=0.714  th_fb=0.714\n",
      "  NR-PPAR-gamma: T=1.134  AP_val=0.062  th_f1=0.513  th_fb=0.513\n",
      "  SR-ARE      : T=0.988  AP_val=0.344  th_f1=0.531  th_fb=0.524\n",
      "  SR-ATAD5    : T=0.946  AP_val=0.167  th_f1=0.709  th_fb=0.709\n",
      "  SR-HSE      : T=0.952  AP_val=0.216  th_f1=0.651  th_fb=0.651\n",
      "  SR-MMP      : T=0.953  AP_val=0.445  th_f1=0.647  th_fb=0.612\n",
      "  SR-p53      : T=1.015  AP_val=0.217  th_f1=0.625  th_fb=0.625\n",
      "\n",
      "✅ Stacking complete.\n",
      "  • Models         → v7_exp\\stacking_asl_v1\\model\\stacking_lr\n",
      "  • Temps (stack)  → v7_exp\\stacking_asl_v1\\calibration\\temps_stack.json\n",
      "  • Thresholds     → v7_exp\\stacking_asl_v1\\calibration\\thresholds_stack.json\n"
     ]
    }
   ],
   "source": [
    "# === Cell A: Per-label stacking on VAL (logistic combiner of [spec_logit, shared_logit])\n",
    "# Saves under v7_exp/stacking_asl_v1/* without touching v7/*\n",
    "import json, math, os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from joblib import dump as joblib_dump\n",
    "\n",
    "# ---------- Paths (new experiment root) ----------\n",
    "ROOT_OLD = Path(\"v7\")\n",
    "ROOT_NEW = Path(\"v7_exp\") / \"stacking_asl_v1\"\n",
    "ROOT_NEW.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_OLD = ROOT_OLD / \"model\"\n",
    "CAL_OLD   = MODEL_OLD / \"calibration\"\n",
    "ENS_OLD   = MODEL_OLD / \"ensembles\"\n",
    "FUSED     = ROOT_OLD / \"data\" / \"fused\"\n",
    "PREP      = ROOT_OLD / \"data\" / \"prepared\"\n",
    "\n",
    "MODEL_NEW = ROOT_NEW / \"model\"\n",
    "STACK_DIR = MODEL_NEW / \"stacking_lr\"\n",
    "CAL_NEW   = ROOT_NEW / \"calibration\"\n",
    "EVAL_NEW  = ROOT_NEW / \"eval\"\n",
    "for p in [STACK_DIR, CAL_NEW, EVAL_NEW]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Load manifests / artifacts ----------\n",
    "mani = json.loads((PREP / \"dataset_manifest.json\").read_text())\n",
    "LABELS: List[str] = mani[\"labels\"]; N_LABELS = len(LABELS)\n",
    "\n",
    "# fused VAL features (B,768)\n",
    "Xv = np.load(FUSED / \"val_fused.npy\").astype(np.float32)\n",
    "val_blob = np.load(PREP / \"val.npz\", allow_pickle=True)\n",
    "Yv = val_blob[\"Y\"].astype(np.float32)                # (Nv, L)\n",
    "Mv = val_blob[\"y_missing_mask\"].astype(bool)         # True where missing\n",
    "\n",
    "# specialist temperatures and shared temperatures\n",
    "temps_spec   = json.loads((CAL_OLD / \"temps.json\").read_text())\n",
    "temps_shared = json.loads((CAL_OLD / \"temps_shared.json\").read_text())\n",
    "\n",
    "# shared-head MLP: reuse weights from v7/model/checkpoints/shared/best.pt\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sh = SharedHeadMLP().to(device)\n",
    "ck = torch.load(MODEL_OLD / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "state = {k.replace(\"shared_head.\",\"\"): v for k,v in ck[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(state, strict=True); sh.eval()\n",
    "\n",
    "# specialist heads: load best per label from v7/ensembles/*\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(nn.Linear(in_dim,h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2=nn.Sequential(nn.Linear(h1,h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3=nn.Sequential(nn.Linear(h2,h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out=nn.Linear(h3,1); self.short=nn.Linear(in_dim,h3)\n",
    "    def forward(self, x):\n",
    "        z1=self.block1(x); z2=self.block2(z1); z3=self.block3(z2)\n",
    "        return self.out(z3 + self.short(x)).squeeze(-1)\n",
    "\n",
    "def _remap(sd: dict)->dict:\n",
    "    if not any(k.startswith((\"b1.\",\"b2.\",\"b3.\")) for k in sd): return sd\n",
    "    m={}\n",
    "    for k,v in sd.items():\n",
    "        m[k.replace(\"b1.\",\"block1.\").replace(\"b2.\",\"block2.\").replace(\"b3.\",\"block3.\")] = v\n",
    "    return m\n",
    "\n",
    "def load_best_head(label: str) -> nn.Module:\n",
    "    cands=[]\n",
    "    for sd in sorted((ENS_OLD/label).glob(\"seed*/\")):\n",
    "        m = sd/\"metrics.json\"\n",
    "        if m.exists():\n",
    "            try: cands.append((float(json.loads(m.read_text()).get(\"best_ap\", float(\"nan\"))), sd))\n",
    "            except: pass\n",
    "    if not cands: raise FileNotFoundError(f\"No heads for {label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best = cands[0][1]\n",
    "    ckpt = torch.load(best/\"best.pt\", map_location=device)\n",
    "    cfg = ckpt.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(cfg[\"in_dim\"],cfg[\"h1\"],cfg[\"h2\"],cfg[\"h3\"],cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(_remap(ckpt[\"model\"]), strict=True); head.eval(); return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl) for lbl in LABELS}\n",
    "\n",
    "# ---------- Build VAL logits: shared & specialist ----------\n",
    "Xv_t = torch.tensor(Xv, dtype=torch.float32, device=device)\n",
    "with torch.no_grad():\n",
    "    sh_logits = sh(Xv_t).cpu().numpy()                           # (Nv, L)\n",
    "    spec_logits = torch.stack([HEADS[l](Xv_t) for l in LABELS],  # -> (L, Nv)\n",
    "                               dim=1).cpu().numpy()              # -> (Nv, L)\n",
    "\n",
    "# ---------- Train per-label logistic combiner on VAL ----------\n",
    "def fit_temperature(logits: np.ndarray, y: np.ndarray, max_iter=200, lr=0.05) -> float:\n",
    "    t = torch.tensor([1.0], dtype=torch.float32, requires_grad=True, device=device)\n",
    "    x = torch.tensor(logits, dtype=torch.float32, device=device)\n",
    "    y = torch.tensor(y,      dtype=torch.float32, device=device)\n",
    "    opt = torch.optim.Adam([t], lr=lr)\n",
    "    for _ in range(max_iter):\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "        z = x / (t.clamp(min=1e-3))\n",
    "        p = torch.sigmoid(z).clamp(1e-6, 1-1e-6)\n",
    "        loss = - (y*torch.log(p) + (1-y)*torch.log(1-p)).mean()\n",
    "        loss.backward(); opt.step()\n",
    "    return float(t.detach().cpu().item())\n",
    "\n",
    "def best_thresholds(y_true: np.ndarray, probs: np.ndarray) -> Dict[str,float]:\n",
    "    prec, rec, th = precision_recall_curve(y_true, probs)\n",
    "    eps=1e-8\n",
    "    f1  = (2*prec*rec)/np.maximum(prec+rec, eps)\n",
    "    beta=1.5\n",
    "    fb  = ((1+beta**2)*prec*rec)/np.maximum((beta**2)*prec+rec, eps)\n",
    "    th_f1 = th[np.nanargmax(f1[1:])] if th.size>0 else 0.5\n",
    "    th_fb = th[np.nanargmax(fb[1:])] if th.size>0 else 0.5\n",
    "    ap = float(average_precision_score(y_true, probs)) if (~np.isnan(y_true)).any() else float(\"nan\")\n",
    "    return {\"th_f1\": float(th_f1), \"th_fbeta15\": float(th_fb), \"ap_val\": ap}\n",
    "\n",
    "comb_meta = {}\n",
    "temps_stack = {}\n",
    "thr_stack   = {}\n",
    "\n",
    "print(\"Fitting per-label logistic stackers on VAL...\")\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    valid = ~Mv[:, j]\n",
    "    x = np.stack([spec_logits[valid, j], sh_logits[valid, j]], axis=1)   # (Nv_valid, 2)\n",
    "    y = Yv[valid, j].astype(int)\n",
    "    if x.shape[0] == 0 or y.max()==y.min():\n",
    "        print(f\"  {lbl}: degenerate label on VAL → skipping (copying old thresholds).\")\n",
    "        continue\n",
    "    # Logistic combiner with class_weight='balanced' to help rare positives\n",
    "    lr = LogisticRegression(\n",
    "        penalty=\"l2\", C=1.0, solver=\"lbfgs\", max_iter=500,\n",
    "        class_weight=\"balanced\", fit_intercept=True\n",
    "    )\n",
    "    lr.fit(x, y)\n",
    "    # raw combiner logits on VAL\n",
    "    z_val = lr.decision_function(x)  # shape (Nv_valid,)\n",
    "    # temperature on combiner logits\n",
    "    T = fit_temperature(z_val, y, max_iter=300, lr=0.05)\n",
    "    temps_stack[lbl] = T\n",
    "    # calibrated probs\n",
    "    p_cal = 1.0 / (1.0 + np.exp(-(z_val / max(T,1e-3))))\n",
    "    thr = best_thresholds(y, p_cal)\n",
    "    thr_stack[lbl] = thr\n",
    "    # save model per label\n",
    "    lbl_dir = STACK_DIR / lbl\n",
    "    lbl_dir.mkdir(parents=True, exist_ok=True)\n",
    "    joblib_dump(lr, lbl_dir / \"stack_lr.joblib\")\n",
    "    json.dump({\"coef\": lr.coef_.tolist(), \"intercept\": lr.intercept_.tolist(),\n",
    "               \"n\": int(x.shape[0]), \"pos_rate\": float(y.mean())},\n",
    "              open(lbl_dir / \"meta.json\",\"w\"), indent=2)\n",
    "    print(f\"  {lbl:12s}: T={T:.3f}  AP_val={thr['ap_val']:.3f}  th_f1={thr['th_f1']:.3f}  th_fb={thr['th_fbeta15']:.3f}\")\n",
    "\n",
    "# Save global calibration/thresholds for stackers\n",
    "json.dump(temps_stack, open(CAL_NEW / \"temps_stack.json\",\"w\"), indent=2)\n",
    "json.dump({\"thresholds\": thr_stack, \"note\": \"Per-label logistic stacker on VAL\"},\n",
    "          open(CAL_NEW / \"thresholds_stack.json\",\"w\"), indent=2)\n",
    "\n",
    "print(\"\\n✅ Stacking complete.\")\n",
    "print(\"  • Models         →\", STACK_DIR)\n",
    "print(\"  • Temps (stack)  →\", CAL_NEW / \"temps_stack.json\")\n",
    "print(\"  • Thresholds     →\", CAL_NEW / \"thresholds_stack.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b650fdc1",
   "metadata": {},
   "source": [
    "#### B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e68333a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels selected for ASL tweak: ['NR-AR', 'NR-AR-LBD', 'NR-AhR', 'NR-Aromatase', 'NR-ER-LBD', 'NR-PPAR-gamma', 'SR-ATAD5', 'SR-HSE', 'SR-p53']\n",
      "[NR-AR] epoch 1/6  train_loss=0.0878  val_AP=0.1491\n",
      "[NR-AR] epoch 2/6  train_loss=0.0643  val_AP=0.1636\n",
      "[NR-AR] epoch 3/6  train_loss=0.0616  val_AP=0.1550\n",
      "[NR-AR] epoch 4/6  train_loss=0.0575  val_AP=0.1657\n",
      "[NR-AR] epoch 5/6  train_loss=0.0562  val_AP=0.1765\n",
      "[NR-AR] epoch 6/6  train_loss=0.0555  val_AP=0.1734\n",
      "[NR-AR-LBD] epoch 1/6  train_loss=0.0808  val_AP=0.1397\n",
      "[NR-AR-LBD] epoch 2/6  train_loss=0.0486  val_AP=0.2891\n",
      "[NR-AR-LBD] epoch 3/6  train_loss=0.0467  val_AP=0.3058\n",
      "[NR-AR-LBD] epoch 4/6  train_loss=0.0472  val_AP=0.2826\n",
      "[NR-AR-LBD] epoch 5/6  train_loss=0.0460  val_AP=0.2834\n",
      "[NR-AR-LBD] epoch 6/6  train_loss=0.0447  val_AP=0.2944\n",
      "[NR-AhR] epoch 1/6  train_loss=0.2295  val_AP=0.5028\n",
      "[NR-AhR] epoch 2/6  train_loss=0.1584  val_AP=0.5323\n",
      "[NR-AhR] epoch 3/6  train_loss=0.1501  val_AP=0.5355\n",
      "[NR-AhR] epoch 4/6  train_loss=0.1470  val_AP=0.5383\n",
      "[NR-AhR] epoch 5/6  train_loss=0.1447  val_AP=0.5444\n",
      "[NR-AhR] epoch 6/6  train_loss=0.1428  val_AP=0.5274\n",
      "[NR-Aromatase] epoch 1/6  train_loss=0.1224  val_AP=0.2222\n",
      "[NR-Aromatase] epoch 2/6  train_loss=0.0850  val_AP=0.3375\n",
      "[NR-Aromatase] epoch 3/6  train_loss=0.0733  val_AP=0.3394\n",
      "[NR-Aromatase] epoch 4/6  train_loss=0.0769  val_AP=0.3256\n",
      "[NR-Aromatase] epoch 5/6  train_loss=0.0737  val_AP=0.3545\n",
      "[NR-Aromatase] epoch 6/6  train_loss=0.0707  val_AP=0.3700\n",
      "[NR-ER] epoch 1/6  train_loss=0.1365  val_AP=0.1821\n",
      "[NR-ER] epoch 2/6  train_loss=0.1085  val_AP=0.1917\n",
      "[NR-ER] epoch 3/6  train_loss=0.1035  val_AP=0.2563\n",
      "[NR-ER] epoch 4/6  train_loss=0.1026  val_AP=0.2616\n",
      "[NR-ER] epoch 5/6  train_loss=0.1029  val_AP=0.2713\n",
      "[NR-ER] epoch 6/6  train_loss=0.1009  val_AP=0.2565\n",
      "[NR-ER-LBD] epoch 1/6  train_loss=0.1267  val_AP=0.1324\n",
      "[NR-ER-LBD] epoch 2/6  train_loss=0.0871  val_AP=0.1263\n",
      "[NR-ER-LBD] epoch 3/6  train_loss=0.0831  val_AP=0.1034\n",
      "[NR-ER-LBD] epoch 4/6  train_loss=0.0794  val_AP=0.1152\n",
      "[NR-PPAR-gamma] epoch 1/6  train_loss=0.0780  val_AP=0.0804\n",
      "[NR-PPAR-gamma] epoch 2/6  train_loss=0.0479  val_AP=0.1734\n",
      "[NR-PPAR-gamma] epoch 3/6  train_loss=0.0481  val_AP=0.1624\n",
      "[NR-PPAR-gamma] epoch 4/6  train_loss=0.0473  val_AP=0.1636\n",
      "[NR-PPAR-gamma] epoch 5/6  train_loss=0.0423  val_AP=0.1663\n",
      "[SR-ARE] epoch 1/6  train_loss=0.1906  val_AP=0.3257\n",
      "[SR-ARE] epoch 2/6  train_loss=0.1229  val_AP=0.3453\n",
      "[SR-ARE] epoch 3/6  train_loss=0.1170  val_AP=0.3515\n",
      "[SR-ARE] epoch 4/6  train_loss=0.1172  val_AP=0.3627\n",
      "[SR-ARE] epoch 5/6  train_loss=0.1153  val_AP=0.3727\n",
      "[SR-ARE] epoch 6/6  train_loss=0.1141  val_AP=0.3727\n",
      "[SR-ATAD5] epoch 1/6  train_loss=0.1121  val_AP=0.1740\n",
      "[SR-ATAD5] epoch 2/6  train_loss=0.0638  val_AP=0.1723\n",
      "[SR-ATAD5] epoch 3/6  train_loss=0.0612  val_AP=0.1667\n",
      "[SR-ATAD5] epoch 4/6  train_loss=0.0599  val_AP=0.1734\n",
      "[SR-HSE] epoch 1/6  train_loss=0.1698  val_AP=0.2902\n",
      "[SR-HSE] epoch 2/6  train_loss=0.0966  val_AP=0.3549\n",
      "[SR-HSE] epoch 3/6  train_loss=0.0908  val_AP=0.4094\n",
      "[SR-HSE] epoch 4/6  train_loss=0.0908  val_AP=0.4642\n",
      "[SR-HSE] epoch 5/6  train_loss=0.0907  val_AP=0.4333\n",
      "[SR-HSE] epoch 6/6  train_loss=0.0883  val_AP=0.4870\n",
      "[SR-MMP] epoch 1/6  train_loss=0.1579  val_AP=0.4647\n",
      "[SR-MMP] epoch 2/6  train_loss=0.1096  val_AP=0.5024\n",
      "[SR-MMP] epoch 3/6  train_loss=0.1031  val_AP=0.5449\n",
      "[SR-MMP] epoch 4/6  train_loss=0.1031  val_AP=0.5558\n",
      "[SR-MMP] epoch 5/6  train_loss=0.0989  val_AP=0.5564\n",
      "[SR-MMP] epoch 6/6  train_loss=0.0956  val_AP=0.5629\n",
      "[SR-p53] epoch 1/6  train_loss=0.1320  val_AP=0.1801\n",
      "[SR-p53] epoch 2/6  train_loss=0.0974  val_AP=0.1873\n",
      "[SR-p53] epoch 3/6  train_loss=0.0940  val_AP=0.1963\n",
      "[SR-p53] epoch 4/6  train_loss=0.0918  val_AP=0.1950\n",
      "[SR-p53] epoch 5/6  train_loss=0.0931  val_AP=0.2379\n",
      "[SR-p53] epoch 6/6  train_loss=0.0918  val_AP=0.2207\n",
      "\n",
      "✅ ASL retrain complete.\n",
      "  • New heads → v7_exp\\stacking_asl_v1\\model\\ensembles_v2\n"
     ]
    }
   ],
   "source": [
    "# === Cell B: Fast specialist-head retrain with ASL tweaks (heads only, fused inputs)\n",
    "import json, math, os\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "# ---------- Paths ----------\n",
    "ROOT_OLD = Path(\"v7\")\n",
    "FUSED    = ROOT_OLD / \"data\" / \"fused\"\n",
    "PREP     = ROOT_OLD / \"data\" / \"prepared\"\n",
    "\n",
    "ROOT_NEW = Path(\"v7_exp\") / \"stacking_asl_v1\"\n",
    "MODEL_NEW= ROOT_NEW / \"model\"\n",
    "ENS_NEW  = MODEL_NEW / \"ensembles_v2\"\n",
    "LOG_NEW  = ROOT_NEW / \"logs\"\n",
    "for p in [ENS_NEW, LOG_NEW]:\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ---------- Data ----------\n",
    "# fused train/val + labels/masks\n",
    "Xtr = np.load(FUSED / \"train_fused.npy\").astype(np.float32)\n",
    "Xva = np.load(FUSED / \"val_fused.npy\").astype(np.float32)\n",
    "tr = np.load(PREP / \"train.npz\", allow_pickle=True)\n",
    "va = np.load(PREP / \"val.npz\", allow_pickle=True)\n",
    "Ytr = tr[\"Y\"].astype(np.float32); Mtr = tr[\"y_missing_mask\"].astype(bool)\n",
    "Yva = va[\"Y\"].astype(np.float32); Mva = va[\"y_missing_mask\"].astype(bool)\n",
    "\n",
    "mani = json.loads((PREP / \"dataset_manifest.json\").read_text())\n",
    "LABELS: List[str] = mani[\"labels\"]; N_LABELS=len(LABELS)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Simple dataset ----------\n",
    "class FusedDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, i): return self.X[i], self.y[i]\n",
    "\n",
    "# ---------- Label head (same architecture as before) ----------\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(nn.Linear(in_dim,h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2=nn.Sequential(nn.Linear(h1,h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3=nn.Sequential(nn.Linear(h2,h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out=nn.Linear(h3,1); self.short=nn.Linear(in_dim,h3)\n",
    "    def forward(self, x):\n",
    "        z1=self.block1(x); z2=self.block2(z1); z3=self.block3(z2)\n",
    "        return self.out(z3 + self.short(x)).squeeze(-1)\n",
    "\n",
    "# ---------- Asymmetric Loss (recall-leaning) ----------\n",
    "class AsymmetricLoss(nn.Module):\n",
    "    def __init__(self, gamma_pos=0.0, gamma_neg=4.0, clip=0.05, eps=1e-8, alpha_pos=1.0, alpha_neg=1.0):\n",
    "        super().__init__()\n",
    "        self.gamma_pos=gamma_pos; self.gamma_neg=gamma_neg\n",
    "        self.clip=clip; self.eps=eps; self.alpha_pos=alpha_pos; self.alpha_neg=alpha_neg\n",
    "    def forward(self, logits, y):\n",
    "        x_sigmoid = torch.sigmoid(logits)\n",
    "        xs_pos = x_sigmoid; xs_neg = 1.0 - x_sigmoid\n",
    "        if self.clip is not None and self.clip>0:\n",
    "            xs_neg = (xs_neg + self.clip).clamp(max=1.0)\n",
    "        los_pos = y * torch.log(xs_pos.clamp(min=self.eps))\n",
    "        los_neg = (1.0 - y) * torch.log(xs_neg.clamp(min=self.eps))\n",
    "        if self.gamma_pos>0 or self.gamma_neg>0:\n",
    "            pt = torch.where(y>=0.5, xs_pos, xs_neg)\n",
    "            gamma = torch.where(y>=0.5, torch.tensor(self.gamma_pos, device=logits.device),\n",
    "                                           torch.tensor(self.gamma_neg, device=logits.device))\n",
    "            los_pos = los_pos * torch.pow(1.0 - xs_pos, self.gamma_pos)\n",
    "            los_neg = los_neg * torch.pow(1.0 - xs_neg, self.gamma_neg)\n",
    "        # class weighting\n",
    "        los = -(self.alpha_pos*los_pos + self.alpha_neg*los_neg)\n",
    "        return los.mean()\n",
    "\n",
    "# ---------- Auto-pick labels to tweak ----------\n",
    "# Use VAL prevalence and (if present) stack AP to identify \"hard/rare\" labels\n",
    "thr_stack_path = ROOT_NEW / \"calibration\" / \"thresholds_stack.json\"\n",
    "ap_hint = {}\n",
    "if thr_stack_path.exists():\n",
    "    pay = json.loads(thr_stack_path.read_text())\n",
    "    for lbl, d in pay[\"thresholds\"].items():\n",
    "        ap_hint[lbl] = float(d.get(\"ap_val\", np.nan))\n",
    "\n",
    "# prevalence on VAL\n",
    "prev_val = {LABELS[j]: float(Yva[~Mva[:,j], j].mean()) if (~Mva[:,j]).any() else np.nan for j in range(N_LABELS)}\n",
    "\n",
    "# selection rule (editable): low prevalence (<8%) OR AP hint < 0.20\n",
    "TO_TWEAK = sorted([lbl for lbl in LABELS if\n",
    "                   (not np.isnan(prev_val.get(lbl, np.nan)) and prev_val[lbl] < 0.08) or\n",
    "                   (not np.isnan(ap_hint.get(lbl, np.nan)) and ap_hint[lbl] < 0.20)])\n",
    "\n",
    "print(\"Labels selected for ASL tweak:\", TO_TWEAK)\n",
    "\n",
    "# ---------- Training hyperparams ----------\n",
    "BS = 256\n",
    "EPOCHS = 6\n",
    "LR = 1e-3\n",
    "WD = 1e-4\n",
    "PATIENCE = 3\n",
    "\n",
    "# Recall-leaning ASL defaults for tweaked labels (you can adjust)\n",
    "ASL_TWEAK = dict(gamma_neg=2.0, gamma_pos=0.0, alpha_pos=1.3, alpha_neg=1.0, clip=0.05)\n",
    "ASL_BASE  = dict(gamma_neg=4.0, gamma_pos=0.0, alpha_pos=1.0, alpha_neg=1.0, clip=0.05)\n",
    "\n",
    "# ---------- Train per label ----------\n",
    "def _val_ap(head: nn.Module, X: np.ndarray, y: np.ndarray) -> float:\n",
    "    head.eval()\n",
    "    with torch.no_grad():\n",
    "        z = head(torch.tensor(X, dtype=torch.float32, device=device)).cpu().numpy()\n",
    "        p = 1.0/(1.0+np.exp(-z))\n",
    "    try:\n",
    "        return float(average_precision_score(y.astype(int), p))\n",
    "    except Exception:\n",
    "        return float(\"nan\")\n",
    "\n",
    "for j, lbl in enumerate(LABELS):\n",
    "    # slice train/val rows with known labels\n",
    "    tr_mask = ~Mtr[:, j]; va_mask = ~Mva[:, j]\n",
    "    Xtr_j, ytr_j = Xtr[tr_mask], Ytr[tr_mask, j]\n",
    "    Xva_j, yva_j = Xva[va_mask], Yva[va_mask, j]\n",
    "    if Xtr_j.shape[0] == 0 or Xva_j.shape[0] == 0:\n",
    "        print(f\"[{lbl}] no data → skip.\")\n",
    "        continue\n",
    "\n",
    "    # choose ASL config\n",
    "    cfg = ASL_TWEAK if lbl in TO_TWEAK else ASL_BASE\n",
    "    loss_fn = AsymmetricLoss(**cfg)\n",
    "\n",
    "    head = LabelHead().to(device)\n",
    "    opt = torch.optim.AdamW(head.parameters(), lr=LR, weight_decay=WD)\n",
    "\n",
    "    ds_tr = FusedDataset(Xtr_j, ytr_j)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=BS, shuffle=True, num_workers=0, pin_memory=True)\n",
    "\n",
    "    best_ap = -1.0; best_state=None; bad=0\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        head.train()\n",
    "        losses=[]\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "            logits = head(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward(); opt.step()\n",
    "            losses.append(float(loss.item()))\n",
    "        ap = _val_ap(head, Xva_j, yva_j)\n",
    "        print(f\"[{lbl}] epoch {epoch}/{EPOCHS}  train_loss={np.mean(losses):.4f}  val_AP={ap:.4f}\")\n",
    "        if ap > best_ap + 1e-4:\n",
    "            best_ap = ap; best_state = {k: v.detach().cpu().clone() for k, v in head.state_dict().items()}\n",
    "            bad = 0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= PATIENCE: break\n",
    "\n",
    "    # save best\n",
    "    out_dir = ENS_NEW / lbl / \"seed00\"\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    torch.save({\"model\": best_state if best_state is not None else head.state_dict(),\n",
    "                \"config\": {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30},\n",
    "                \"best_ap\": best_ap},\n",
    "               out_dir / \"best.pt\")\n",
    "    json.dump({\"label\": lbl, \"best_ap\": best_ap, \"asl_cfg\": cfg, \"epochs\": epoch},\n",
    "              open(out_dir / \"metrics.json\",\"w\"), indent=2)\n",
    "\n",
    "print(\"\\n✅ ASL retrain complete.\")\n",
    "print(\"  • New heads →\", ENS_NEW)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684806d4",
   "metadata": {},
   "source": [
    "#### C) test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b729bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating on VAL and TEST...\n",
      "\n",
      "=== VAL ===\n",
      " Old F1   : P=0.244  R=0.474  F1=0.322  | macro-F1=0.311\n",
      " Stack F1 : P=0.221  R=0.506  F1=0.308  | macro-F1=0.307\n",
      "\n",
      "=== TEST ===\n",
      " Old F1   : P=0.232  R=0.462  F1=0.309  | macro-F1=0.278\n",
      " Stack F1 : P=0.212  R=0.490  F1=0.296  | macro-F1=0.261\n",
      "\n",
      "✅ Saved:\n",
      "  • v7_exp\\stacking_asl_v1\\eval\\compare_val.json\n",
      "  • v7_exp\\stacking_asl_v1\\eval\\compare_val.csv\n",
      "  • v7_exp\\stacking_asl_v1\\eval\\compare_test.json\n",
      "  • v7_exp\\stacking_asl_v1\\eval\\compare_test.csv\n",
      "\n",
      "(use_new_heads=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
      "C:\\Users\\Amir\\AppData\\Local\\Temp\\ipykernel_16956\\2871748339.py:109: RuntimeWarning: overflow encountered in exp\n",
      "  sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n"
     ]
    }
   ],
   "source": [
    "# === Compare old F1 blend vs NEW stacked F1 on VAL & TEST (self-contained) ===\n",
    "# Saves under: v7_exp/stacking_asl_v1/eval/*\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from joblib import load as joblib_load\n",
    "\n",
    "# -------------------- Paths --------------------\n",
    "ROOT_OLD   = Path(\"v7\")\n",
    "PREP       = ROOT_OLD / \"data\" / \"prepared\"\n",
    "FUSED      = ROOT_OLD / \"data\" / \"fused\"\n",
    "MODEL_OLD  = ROOT_OLD / \"model\"\n",
    "ENS_OLD    = MODEL_OLD / \"ensembles\"\n",
    "CAL_OLD    = MODEL_OLD / \"calibration\"\n",
    "\n",
    "ROOT_NEW   = Path(\"v7_exp\") / \"stacking_asl_v1\"\n",
    "MODEL_NEW  = ROOT_NEW / \"model\"\n",
    "ENS_NEW    = MODEL_NEW / \"ensembles_v2\"          # optional: ASL-tweaked heads\n",
    "STACK_DIR  = MODEL_NEW / \"stacking_lr\"\n",
    "CAL_NEW    = ROOT_NEW / \"calibration\"\n",
    "EVAL_DIR   = ROOT_NEW / \"eval\"\n",
    "EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Toggle: use ASL-tweaked heads for *both* methods (may mismatch the stackers' training)\n",
    "USE_NEW_HEADS = False   # default False (recommended)\n",
    "\n",
    "# -------------------- Manifests --------------------\n",
    "mani = json.loads((PREP / \"dataset_manifest.json\").read_text())\n",
    "LABELS: List[str] = mani[\"labels\"]; N_LABELS = len(LABELS)\n",
    "\n",
    "# Baseline (old) calibration\n",
    "thr_blend_v = CAL_OLD / \"thresholds_blend_v2.json\"\n",
    "thr_blend_1 = CAL_OLD / \"thresholds_blend.json\"\n",
    "assert thr_blend_v.exists() or thr_blend_1.exists(), \"Missing thresholds_blend file(s)\"\n",
    "blend_payload = json.loads((thr_blend_v if thr_blend_v.exists() else thr_blend_1).read_text())\n",
    "ALPHA   = float(blend_payload.get(\"alpha\", 0.8))\n",
    "THR_OLD = blend_payload[\"thresholds\"]  # label -> {th_f1, th_fbeta15, ap_val}\n",
    "temps_spec   = json.loads((CAL_OLD / \"temps.json\").read_text())\n",
    "temps_shared = json.loads((CAL_OLD / \"temps_shared.json\").read_text())\n",
    "\n",
    "# Stacked calibration\n",
    "temps_stack  = json.loads((CAL_NEW / \"temps_stack.json\").read_text())\n",
    "thr_stack    = json.loads((CAL_NEW / \"thresholds_stack.json\").read_text())[\"thresholds\"]\n",
    "\n",
    "# -------------------- Models (shared head + label heads) --------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class SharedHeadMLP(nn.Module):\n",
    "    def __init__(self, dim=256, n_labels=N_LABELS):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim*3, dim*2), nn.GELU(), nn.Dropout(0.1),\n",
    "            nn.Linear(dim*2, n_labels)\n",
    "        )\n",
    "    def forward(self, z): return self.mlp(z)\n",
    "\n",
    "# load shared head weights from v7 (unchanged)\n",
    "shared_ckpt = torch.load(MODEL_OLD / \"checkpoints\" / \"shared\" / \"best.pt\", map_location=device)\n",
    "sh = SharedHeadMLP().to(device)\n",
    "sh_state = {k.replace(\"shared_head.\",\"\"): v for k,v in shared_ckpt[\"model\"].items() if k.startswith(\"shared_head.\")}\n",
    "sh.load_state_dict(sh_state, strict=True); sh.eval()\n",
    "\n",
    "class LabelHead(nn.Module):\n",
    "    def __init__(self, in_dim=768, h1=512, h2=256, h3=128, p=0.30):\n",
    "        super().__init__()\n",
    "        self.block1=nn.Sequential(nn.Linear(in_dim,h1), nn.GELU(), nn.LayerNorm(h1), nn.Dropout(p))\n",
    "        self.block2=nn.Sequential(nn.Linear(h1,h2), nn.GELU(), nn.LayerNorm(h2), nn.Dropout(p))\n",
    "        self.block3=nn.Sequential(nn.Linear(h2,h3), nn.GELU(), nn.LayerNorm(h3), nn.Dropout(p))\n",
    "        self.out=nn.Linear(h3,1); self.short=nn.Linear(in_dim,h3)\n",
    "    def forward(self, x):\n",
    "        z1=self.block1(x); z2=self.block2(z1); z3=self.block3(z2)\n",
    "        return self.out(z3 + self.short(x)).squeeze(-1)\n",
    "\n",
    "def _remap(sd: dict)->dict:\n",
    "    if not any(k.startswith((\"b1.\",\"b2.\",\"b3.\")) for k in sd): return sd\n",
    "    m={}; \n",
    "    for k,v in sd.items():\n",
    "        m[k.replace(\"b1.\",\"block1.\").replace(\"b2.\",\"block2.\").replace(\"b3.\",\"block3.\")] = v\n",
    "    return m\n",
    "\n",
    "def load_best_head(label: str, use_new: bool=False) -> nn.Module:\n",
    "    base = ENS_NEW if use_new else ENS_OLD\n",
    "    cands=[]\n",
    "    for sd in sorted((base/label).glob(\"seed*/\")):\n",
    "        m = sd/\"metrics.json\"\n",
    "        if m.exists():\n",
    "            try: cands.append((float(json.loads(m.read_text()).get(\"best_ap\", float(\"nan\"))), sd))\n",
    "            except: pass\n",
    "    if not cands: raise FileNotFoundError(f\"No heads for {label} under {base/label}\")\n",
    "    cands.sort(key=lambda x: (-1.0 if math.isnan(x[0]) else x[0]), reverse=True)\n",
    "    best = cands[0][1]\n",
    "    ck = torch.load(best/\"best.pt\", map_location=device)\n",
    "    cfg = ck.get(\"config\", {\"in_dim\":768,\"h1\":512,\"h2\":256,\"h3\":128,\"dropout\":0.30})\n",
    "    head = LabelHead(cfg[\"in_dim\"],cfg[\"h1\"],cfg[\"h2\"],cfg[\"h3\"],cfg.get(\"dropout\",0.30)).to(device)\n",
    "    head.load_state_dict(_remap(ck[\"model\"]), strict=True); head.eval()\n",
    "    return head\n",
    "\n",
    "HEADS = {lbl: load_best_head(lbl, use_new=USE_NEW_HEADS) for lbl in LABELS}\n",
    "\n",
    "# Load per-label stackers (trained on OLD heads; if USE_NEW_HEADS=True, results may shift)\n",
    "STACKERS = {lbl: joblib_load((STACK_DIR/lbl/\"stack_lr.joblib\")) for lbl in LABELS}\n",
    "\n",
    "# -------------------- Helpers --------------------\n",
    "sigmoid = lambda x: 1.0/(1.0+np.exp(-x))\n",
    "\n",
    "def build_logits(X: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"Compute shared and specialist logits on fused features.\"\"\"\n",
    "    X_t = torch.tensor(X, dtype=torch.float32, device=device)\n",
    "    with torch.no_grad():\n",
    "        sh_logits   = sh(X_t).cpu().numpy()                          # (N, L)\n",
    "        spec_logits = torch.stack([HEADS[l](X_t) for l in LABELS],   # (L, N)\n",
    "                                  dim=1).cpu().numpy()               # -> (N, L)\n",
    "    return spec_logits, sh_logits\n",
    "\n",
    "def old_blend_probs(spec_logits: np.ndarray, sh_logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Calibrated per-stream → blended probs with ALPHA.\"\"\"\n",
    "    N, L = spec_logits.shape\n",
    "    P = np.zeros_like(spec_logits, dtype=np.float32)\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        Ts = max(float(temps_spec.get(lbl,1.0)), 1e-3)\n",
    "        Th = max(float(temps_shared.get(lbl,1.0)), 1e-3)\n",
    "        ps = sigmoid(spec_logits[:,j]/Ts)\n",
    "        ph = sigmoid(sh_logits[:,j]/Th)\n",
    "        P[:,j] = np.clip(ALPHA*ps + (1-ALPHA)*ph, 0, 1)\n",
    "    return P  # (N,L)\n",
    "\n",
    "def stack_probs(spec_logits: np.ndarray, sh_logits: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"Per-label logistic combiner [spec,shared] → logit → temp → prob.\"\"\"\n",
    "    N, L = spec_logits.shape\n",
    "    P = np.zeros_like(spec_logits, dtype=np.float32)\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        lr = STACKERS[lbl]\n",
    "        z  = lr.decision_function(np.stack([spec_logits[:,j], sh_logits[:,j]], axis=1))\n",
    "        T  = max(float(temps_stack.get(lbl,1.0)), 1e-3)\n",
    "        P[:,j] = sigmoid(z / T)\n",
    "    return P  # (N,L)\n",
    "\n",
    "def metric_micro_macro(Y_true: np.ndarray, M_missing: np.ndarray,\n",
    "                       P: np.ndarray, thresholds: Dict[str, Dict[str, float]], mode=\"f1\") -> Dict:\n",
    "    \"\"\"Compute micro/macro F1 given probs and per-label thresholds.\"\"\"\n",
    "    assert mode in (\"f1\",\"fbeta15\")\n",
    "    L = Y_true.shape[1]\n",
    "    valid_mask = ~M_missing\n",
    "    # predictions\n",
    "    preds = np.zeros_like(P, dtype=np.int32)\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        th = thresholds[lbl][\"th_f1\"] if mode==\"f1\" else thresholds[lbl][\"th_fbeta15\"]\n",
    "        preds[:,j] = (P[:,j] >= float(th)).astype(np.int32)\n",
    "    # micro\n",
    "    y = Y_true[valid_mask].astype(int); p = preds[valid_mask]\n",
    "    tp = int(((p==1)&(y==1)).sum()); fp = int(((p==1)&(y==0)).sum()); fn = int(((p==0)&(y==1)).sum())\n",
    "    micro_prec = tp/(tp+fp) if (tp+fp)>0 else 0.0\n",
    "    micro_rec  = tp/(tp+fn) if (tp+fn)>0 else 0.0\n",
    "    micro_f1   = (2*micro_prec*micro_rec)/(micro_prec+micro_rec) if (micro_prec+micro_rec)>0 else 0.0\n",
    "    # per-label\n",
    "    per=[]\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        mask = valid_mask[:,j]\n",
    "        if not mask.any():\n",
    "            per.append({\"label\": lbl, \"precision\": np.nan, \"recall\": np.nan, \"f1\": np.nan}); continue\n",
    "        yj = Y_true[mask, j].astype(int); pj = preds[mask, j]\n",
    "        tpj = int(((pj==1)&(yj==1)).sum()); fpj = int(((pj==1)&(yj==0)).sum()); fnj = int(((pj==0)&(yj==1)).sum())\n",
    "        prec = (tpj/(tpj+fpj)) if (tpj+fpj)>0 else np.nan\n",
    "        rec  = (tpj/(tpj+fnj)) if (tpj+fnj)>0 else np.nan\n",
    "        f1   = (2*prec*rec/(prec+rec)) if (not np.isnan(prec) and not np.isnan(rec) and (prec+rec)>0) else np.nan\n",
    "        per.append({\"label\": lbl, \"precision\": prec, \"recall\": rec, \"f1\": f1})\n",
    "    macro_f1 = float(np.nanmean([d[\"f1\"] for d in per]))\n",
    "    return {\"micro\": {\"precision\": micro_prec, \"recall\": micro_rec, \"f1\": micro_f1},\n",
    "            \"macro_f1\": macro_f1, \"per_label\": per, \"n_samples\": int(Y_true.shape[0])}\n",
    "\n",
    "def evaluate_split(split: str) -> Dict:\n",
    "    assert split in (\"val\",\"test\")\n",
    "    blob = np.load(PREP / f\"{split}.npz\", allow_pickle=True)\n",
    "    Y = blob[\"Y\"].astype(np.float32)\n",
    "    M = blob[\"y_missing_mask\"].astype(bool)\n",
    "    X = np.load(FUSED / f\"{split}_fused.npy\").astype(np.float32)\n",
    "\n",
    "    spec_z, sh_z = build_logits(X)\n",
    "\n",
    "    # OLD blend (F1)\n",
    "    P_old = old_blend_probs(spec_z, sh_z)\n",
    "    m_old = metric_micro_macro(Y, M, P_old, THR_OLD, mode=\"f1\")\n",
    "\n",
    "    # NEW stacked (F1 thresholds from stack)\n",
    "    P_stk = stack_probs(spec_z, sh_z)\n",
    "    m_stk = metric_micro_macro(Y, M, P_stk, thr_stack, mode=\"f1\")\n",
    "\n",
    "    # Save detailed per-label CSV\n",
    "    per_rows=[]\n",
    "    for j,lbl in enumerate(LABELS):\n",
    "        per_rows.append({\n",
    "            \"label\": lbl,\n",
    "            \"old_precision\": m_old[\"per_label\"][j][\"precision\"],\n",
    "            \"old_recall\":    m_old[\"per_label\"][j][\"recall\"],\n",
    "            \"old_f1\":        m_old[\"per_label\"][j][\"f1\"],\n",
    "            \"stack_precision\": m_stk[\"per_label\"][j][\"precision\"],\n",
    "            \"stack_recall\":    m_stk[\"per_label\"][j][\"recall\"],\n",
    "            \"stack_f1\":        m_stk[\"per_label\"][j][\"f1\"],\n",
    "        })\n",
    "    df = pd.DataFrame(per_rows)\n",
    "    df.to_csv(EVAL_DIR / f\"compare_{split}.csv\", index=False)\n",
    "\n",
    "    # Save summary JSON\n",
    "    out = {\n",
    "        \"split\": split,\n",
    "        \"use_new_heads\": USE_NEW_HEADS,\n",
    "        \"old_f1\": m_old,\n",
    "        \"stack_f1\": m_stk,\n",
    "        \"alpha_old\": ALPHA\n",
    "    }\n",
    "    (EVAL_DIR / f\"compare_{split}.json\").write_text(json.dumps(out, indent=2))\n",
    "    return out\n",
    "\n",
    "print(\"Evaluating on VAL and TEST...\")\n",
    "rep_val  = evaluate_split(\"val\")\n",
    "rep_test = evaluate_split(\"test\")\n",
    "\n",
    "def _fmt(m): \n",
    "    return f\"P={m['micro']['precision']:.3f}  R={m['micro']['recall']:.3f}  F1={m['micro']['f1']:.3f}  | macro-F1={m['macro_f1']:.3f}\"\n",
    "\n",
    "print(\"\\n=== VAL ===\")\n",
    "print(\" Old F1   :\", _fmt(rep_val[\"old_f1\"]))\n",
    "print(\" Stack F1 :\", _fmt(rep_val[\"stack_f1\"]))\n",
    "print(\"\\n=== TEST ===\")\n",
    "print(\" Old F1   :\", _fmt(rep_test[\"old_f1\"]))\n",
    "print(\" Stack F1 :\", _fmt(rep_test[\"stack_f1\"]))\n",
    "\n",
    "print(\"\\n✅ Saved:\")\n",
    "print(\"  •\", EVAL_DIR / \"compare_val.json\")\n",
    "print(\"  •\", EVAL_DIR / \"compare_val.csv\")\n",
    "print(\"  •\", EVAL_DIR / \"compare_test.json\")\n",
    "print(\"  •\", EVAL_DIR / \"compare_test.csv\")\n",
    "print(f\"\\n(use_new_heads={USE_NEW_HEADS})\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lord",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
