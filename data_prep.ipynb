{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98719ccb",
   "metadata": {},
   "source": [
    "# objective is to prepare our data from step 0 to step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e09ea38",
   "metadata": {},
   "source": [
    "## step 0 \n",
    "\n",
    "**AIM:** \n",
    "* Multilabel classification with 3 specific ADE targets.\n",
    "* Input = SMILES; Output = 3 binary labels.\n",
    "\n",
    "**Specifying our label columns:**\n",
    "* label_Gastrointestinal disorders\n",
    "* label_Infections and infestations\n",
    "* label_Nervous system disorders\n",
    "\n",
    "**we need to Ensure input column (smiles) exists and is clean, thus:**\n",
    "\n",
    "* Remove rows with missing or malformed SMILES.\n",
    "\n",
    "\n",
    "**Inspect label distributions (class balance preview):**\n",
    "* helps later with class weighting and threshold tuning.\n",
    "\n",
    "\n",
    "**NOTE:** we are dropping everything else from the DataFrame, as they are not needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f2e2d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Processing TRAIN set...\n",
      "‚úÖ Rows after cleaning: 12419\n",
      "Label distribution:\n",
      " label_Gastrointestinal disorders     4683\n",
      "label_Infections and infestations    3303\n",
      "label_Nervous system disorders       4206\n",
      "dtype: uint64\n",
      "üìÅ Saved to: Data\\clean\\clean_train.csv\n",
      "\n",
      "üîπ Processing VAL set...\n",
      "‚úÖ Rows after cleaning: 1518\n",
      "Label distribution:\n",
      " label_Gastrointestinal disorders     629\n",
      "label_Infections and infestations    410\n",
      "label_Nervous system disorders       514\n",
      "dtype: uint64\n",
      "üìÅ Saved to: Data\\clean\\clean_val.csv\n",
      "\n",
      "üîπ Processing TEST set...\n",
      "‚úÖ Rows after cleaning: 1260\n",
      "Label distribution:\n",
      " label_Gastrointestinal disorders     513\n",
      "label_Infections and infestations    367\n",
      "label_Nervous system disorders       522\n",
      "dtype: uint64\n",
      "üìÅ Saved to: Data\\clean\\clean_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define input/output paths\n",
    "input_dir = Path(\"Data/CT-ADE-SOC\")\n",
    "output_dir = Path(\"Data/clean\")\n",
    "\n",
    "# Target label columns to keep\n",
    "target_cols = [\n",
    "    \"label_Gastrointestinal disorders\",\n",
    "    \"label_Infections and infestations\",\n",
    "    \"label_Nervous system disorders\"\n",
    "]\n",
    "\n",
    "# Input and output mapping\n",
    "file_map = {\n",
    "    \"train\": (\"train.csv\", \"clean_train.csv\"),\n",
    "    \"val\": (\"val.csv\", \"clean_val.csv\"),\n",
    "    \"test\": (\"test.csv\", \"clean_test.csv\")\n",
    "}\n",
    "\n",
    "# Process each file\n",
    "for split_name, (in_file, out_file) in file_map.items():\n",
    "    print(f\"\\nüîπ Processing {split_name.upper()} set...\")\n",
    "    \n",
    "    # Load and filter\n",
    "    df = pd.read_csv(input_dir / in_file)\n",
    "    df = df[['smiles'] + target_cols]\n",
    "    df = df.dropna(subset=['smiles']).reset_index(drop=True)\n",
    "    df[target_cols] = df[target_cols].astype('uint8')\n",
    "\n",
    "    # Basic stats\n",
    "    print(f\"‚úÖ Rows after cleaning: {len(df)}\")\n",
    "    print(\"Label distribution:\\n\", df[target_cols].sum())\n",
    "\n",
    "    # Save cleaned file\n",
    "    df.to_csv(output_dir / out_file, index=False)\n",
    "    print(f\"üìÅ Saved to: {output_dir / out_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9e3fe1",
   "metadata": {},
   "source": [
    "## Step 1:\n",
    "\n",
    "* Load every cleaned split from Data/clean/.\n",
    "\n",
    "* Drop ‚Äúall-zero‚Äù rows:, why? \n",
    "If a compound has none of the three labels, it doesn't help the classifier unless you need extra negatives. We‚Äôll drop them now for a more balanced training set.\n",
    "\n",
    "\n",
    "* Compute label counts per split ‚Äî needed later for pos_weight in the loss.\n",
    "\n",
    "* Persist curated files to Data/interim/ for downstream tokenisation.\n",
    "\n",
    "* Save a YAML or JSON summary of the label statistics so you never lose track.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79f0ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TRAIN saved to Data\\interim\\ade_3lbl_train.csv  (6542 rows)\n",
      "‚úÖ VAL saved to Data\\interim\\ade_3lbl_val.csv  (803 rows)\n",
      "‚úÖ TEST saved to Data\\interim\\ade_3lbl_test.csv  (710 rows)\n",
      "\n",
      "üìä Label stats written to Data\\interim\\label_stats.yaml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Paths\n",
    "clean_dir = Path(\"Data/clean\")          # input from Step 0\n",
    "interim_dir = Path(\"Data/interim\")\n",
    "interim_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "splits = {\n",
    "    \"train\": \"clean_train.csv\",\n",
    "    \"val\":   \"clean_val.csv\",\n",
    "    \"test\":  \"clean_test.csv\"\n",
    "}\n",
    "\n",
    "target_cols = [\n",
    "    \"label_Gastrointestinal disorders\",\n",
    "    \"label_Infections and infestations\",\n",
    "    \"label_Nervous system disorders\"\n",
    "]\n",
    "\n",
    "stats = {}  # hold label counts for YAML summary\n",
    "\n",
    "for split, fname in splits.items():\n",
    "    df = pd.read_csv(clean_dir / fname)\n",
    "\n",
    "    # 1Ô∏è‚É£  Remove rows where all three labels are zero\n",
    "    mask_nonzero = df[target_cols].sum(axis=1) > 0\n",
    "    dropped = len(df) - mask_nonzero.sum()\n",
    "    df = df[mask_nonzero].reset_index(drop=True)\n",
    "\n",
    "    # 2Ô∏è‚É£  Capture label counts\n",
    "    label_counts = df[target_cols].sum().to_dict()\n",
    "    stats[split] = {\n",
    "        \"rows_after_drop\": len(df),\n",
    "        \"rows_dropped_all_zero\": int(dropped),\n",
    "        \"label_counts\": {k: int(v) for k, v in label_counts.items()}\n",
    "    }\n",
    "\n",
    "    # 3Ô∏è‚É£  Save curated split\n",
    "    out_path = interim_dir / f\"ade_3lbl_{split}.csv\"\n",
    "    df.to_csv(out_path, index=False)\n",
    "    print(f\"‚úÖ {split.upper()} saved to {out_path}  ({len(df)} rows)\")\n",
    "\n",
    "# 4Ô∏è‚É£  Persist stats for future reference\n",
    "with open(interim_dir / \"label_stats.yaml\", \"w\") as fp:\n",
    "    yaml.dump(stats, fp, default_flow_style=False)\n",
    "\n",
    "print(\"\\nüìä Label stats written to\", interim_dir / \"label_stats.yaml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c93040",
   "metadata": {},
   "source": [
    "## Step 2: \n",
    "\n",
    "* Load curated CSVs from Data/interim/ade_3lbl_{train,val,test}.csv.\n",
    "\n",
    "* Initialise ChemBERTa tokenizer\n",
    "\n",
    "* Model: seyonec/ChemBERTa-zinc-250k-pubchem-1m.\n",
    "\n",
    "* Parameters: max_length = 128, padding='max_length', truncation=True.\n",
    "\n",
    "* Convert SMILES to token IDs (input_ids, optional attention_mask if desired).\n",
    "\n",
    "* Extract label matrix (y = N √ó 3, dtype uint8).\n",
    "\n",
    "* Save arrays to Data/processed/\n",
    "\n",
    "* X_train.npy, y_train.npy, X_val.npy, y_val.npy, X_test.npy, y_test.npy.\n",
    "\n",
    "* Optionally save attn_train.npy, etc. for attention masks.\n",
    "\n",
    "* Log shapes so you can sanity-check later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9a58e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\god\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Amir\\.cache\\huggingface\\hub\\models--seyonec--ChemBERTa_zinc250k_v2_40k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîπ Tokenising TRAIN set ‚Ä¶\n",
      "‚úÖ Saved: X_train.npy   (6542, 128)\n",
      "‚úÖ Saved: y_train.npy   (6542, 3)\n",
      "‚úÖ Saved: attn_train.npy (6542, 128)\n",
      "\n",
      "üîπ Tokenising VAL set ‚Ä¶\n",
      "‚úÖ Saved: X_val.npy   (803, 128)\n",
      "‚úÖ Saved: y_val.npy   (803, 3)\n",
      "‚úÖ Saved: attn_val.npy (803, 128)\n",
      "\n",
      "üîπ Tokenising TEST set ‚Ä¶\n",
      "‚úÖ Saved: X_test.npy   (710, 128)\n",
      "‚úÖ Saved: y_test.npy   (710, 3)\n",
      "‚úÖ Saved: attn_test.npy (710, 128)\n",
      "\n",
      "üìÅ All processed tensors now reside in D:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\Data\\processed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# --- Config ---\n",
    "interim_dir   = Path(\"Data/interim\")\n",
    "processed_dir = Path(\"Data/processed\")\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "target_cols = [\n",
    "    \"label_Gastrointestinal disorders\",\n",
    "    \"label_Infections and infestations\",\n",
    "    \"label_Nervous system disorders\"\n",
    "]\n",
    "\n",
    "# üîÑ  Using the open-access checkpoint\n",
    "model_name = \"seyonec/ChemBERTa_zinc250k_v2_40k\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name, trust_remote_code=False)\n",
    "\n",
    "max_len = 128  # keep sequences short for speed\n",
    "\n",
    "for split in splits:\n",
    "    print(f\"\\nüîπ Tokenising {split.upper()} set ‚Ä¶\")\n",
    "\n",
    "    # 1) Load curated CSV\n",
    "    df = pd.read_csv(interim_dir / f\"ade_3lbl_{split}.csv\")\n",
    "\n",
    "    # 2) Tokenise SMILES strings\n",
    "    toks = tokenizer(\n",
    "        df[\"smiles\"].tolist(),\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"np\"\n",
    "    )\n",
    "\n",
    "    X     = toks[\"input_ids\"].astype(np.int32)     # (N, 128)\n",
    "    attn  = toks[\"attention_mask\"].astype(np.int8) # (N, 128) OPTIONAL\n",
    "    y     = df[target_cols].values.astype(np.uint8)# (N, 3)\n",
    "\n",
    "    # 3) Save arrays\n",
    "    np.save(processed_dir / f\"X_{split}.npy\",     X)\n",
    "    np.save(processed_dir / f\"y_{split}.npy\",     y)\n",
    "    np.save(processed_dir / f\"attn_{split}.npy\",  attn)  # drop if not used\n",
    "\n",
    "    print(f\"‚úÖ Saved: X_{split}.npy   {X.shape}\")\n",
    "    print(f\"‚úÖ Saved: y_{split}.npy   {y.shape}\")\n",
    "    print(f\"‚úÖ Saved: attn_{split}.npy {attn.shape}\")\n",
    "\n",
    "print(\"\\nüìÅ All processed tensors now reside in\", processed_dir.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd449283",
   "metadata": {},
   "source": [
    "## step 3: \n",
    "\n",
    "**Load tensors**\n",
    "\n",
    "* X_train.npy, y_train.npy, attn_train.npy (and corresponding val).\n",
    "* Shapes: X = (N, 128) token IDs, attn = (N, 128) masks, y = (N, 3). (it's in the code above)\n",
    "\n",
    "**Compute class weights (to fight imbalance)**\n",
    "\n",
    "* For each label: pos_weight = (N ‚àí pos) / pos.\n",
    "* Feed into BCEWithLogitsLoss.\n",
    "\n",
    "**Build model**\n",
    "\n",
    "* Base: seyonec/ChemBERTa_zinc250k_v2_40k. it's our base line, could use other ones too\n",
    "* Replace head with nn.Linear(hidden_size, 3), followed by sigmoid at inference.\n",
    "\n",
    "**Training hyper-params**\n",
    "\n",
    "* LR = 3 e-5, batch = 32, epochs = 5‚Äì8, weight-decay = 0.01.\n",
    "* Scheduler: cosine with 10 % warm-up steps.\n",
    "* Metric: per-label F1 + ‚Äúnone-positive‚Äù accuracy (all logits < thr).\n",
    "\n",
    "**Validation each epoch**\n",
    "\n",
    "* Track loss + macro F1; early-stop if no improvement ‚â• 2 epochs.\n",
    "\n",
    "**Save artefacts**\n",
    "\n",
    "* Best checkpoint ‚Üí models/chemberta_3lbl/.\n",
    "* Store config.json, pytorch_model.bin, tokenizer files.\n",
    "\n",
    "**(Optional) Threshold sweep on validation set (Step 4 later).**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3e531d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "317a8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üñ•Ô∏è  Using device: cuda\n",
      "Epoch 1 | val macro-F1=0.5534\n",
      "  ‚úÖ  New best, model saved.\n",
      "Epoch 2 | val macro-F1=0.6410\n",
      "  ‚úÖ  New best, model saved.\n",
      "Epoch 3 | val macro-F1=0.5882\n",
      "Epoch 4 | val macro-F1=0.6233\n",
      "  ‚èπÔ∏è  Early stop.\n",
      "\n",
      "üéØ  Best val macro-F1: 0.6410\n"
     ]
    }
   ],
   "source": [
    "import torch, torch.nn as nn\n",
    "from transformers import AutoModel, AutoTokenizer, get_cosine_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# ---------- Device ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üñ•Ô∏è  Using device:\", device)\n",
    "\n",
    "# ---------- Data ----------\n",
    "proc_dir = Path(\"Data/processed\")\n",
    "X_train = np.load(proc_dir / \"X_train.npy\")\n",
    "y_train = np.load(proc_dir / \"y_train.npy\")\n",
    "attn_train = np.load(proc_dir / \"attn_train.npy\")\n",
    "\n",
    "X_val = np.load(proc_dir / \"X_val.npy\")\n",
    "y_val = np.load(proc_dir / \"y_val.npy\")\n",
    "attn_val = np.load(proc_dir / \"attn_val.npy\")\n",
    "\n",
    "to_t = lambda arr, dtype: torch.tensor(arr, dtype=dtype)\n",
    "\n",
    "train_ds = TensorDataset(\n",
    "    to_t(X_train, torch.long),  to_t(attn_train, torch.long),  to_t(y_train, torch.float32)\n",
    ")\n",
    "val_ds = TensorDataset(\n",
    "    to_t(X_val, torch.long),    to_t(attn_val, torch.long),    to_t(y_val, torch.float32)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=64, shuffle=False)\n",
    "\n",
    "# ---------- Model ----------\n",
    "model_name = \"seyonec/ChemBERTa_zinc250k_v2_40k\"\n",
    "base = AutoModel.from_pretrained(model_name)\n",
    "hidden = base.config.hidden_size\n",
    "classifier = nn.Linear(hidden, 3)\n",
    "\n",
    "class ChemClassifier(nn.Module):\n",
    "    def __init__(self, base, head):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.head = head\n",
    "    def forward(self, ids, attn):\n",
    "        out = self.base(input_ids=ids, attention_mask=attn)\n",
    "        cls = out.last_hidden_state[:, 0, :]          # CLS token\n",
    "        return self.head(cls)\n",
    "\n",
    "model = ChemClassifier(base, classifier).to(device)\n",
    "\n",
    "# ---------- Loss (with class weights) ----------\n",
    "pos = y_train.sum(axis=0)\n",
    "neg = len(y_train) - pos\n",
    "pos_weight = torch.tensor(neg / pos, dtype=torch.float32, device=device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "# ---------- Optimizer & scheduler ----------\n",
    "optim = AdamW(model.parameters(), lr=3e-5, weight_decay=0.01)\n",
    "total_steps = len(train_loader) * 6\n",
    "sched = get_cosine_schedule_with_warmup(\n",
    "    optim, num_warmup_steps=int(0.1 * total_steps), num_training_steps=total_steps\n",
    ")\n",
    "\n",
    "# ---------- Training ----------\n",
    "best_f1, patience = 0, 0\n",
    "for epoch in range(1, 10):\n",
    "    model.train()\n",
    "    for ids, attn, labels in train_loader:\n",
    "        ids, attn, labels = ids.to(device), attn.to(device), labels.to(device)\n",
    "        loss = criterion(model(ids, attn), labels)\n",
    "        loss.backward()\n",
    "        optim.step(); sched.step(); optim.zero_grad()\n",
    "\n",
    "    # ---- Validation ----\n",
    "    model.eval(); p_all, y_all = [], []\n",
    "    with torch.no_grad():\n",
    "        for ids, attn, labels in val_loader:\n",
    "            ids, attn = ids.to(device), attn.to(device)\n",
    "            p_all.append(torch.sigmoid(model(ids, attn)).cpu())\n",
    "            y_all.append(labels)\n",
    "    preds = torch.cat(p_all).numpy()\n",
    "    gts   = torch.cat(y_all).numpy()\n",
    "    f1 = f1_score(gts, preds > 0.5, average=\"macro\")\n",
    "    print(f\"Epoch {epoch} | val macro-F1={f1:.4f}\")\n",
    "\n",
    "    # ---- Early-stopping ----\n",
    "    if f1 > best_f1:\n",
    "        best_f1, patience = f1, 0\n",
    "        torch.save(model.state_dict(), \"models/chemberta_3lbl/pytorch_model.bin\")\n",
    "        print(\"  ‚úÖ  New best, model saved.\")\n",
    "    else:\n",
    "        patience += 1\n",
    "        if patience == 2:\n",
    "            print(\"  ‚èπÔ∏è  Early stop.\")\n",
    "            break\n",
    "\n",
    "print(f\"\\nüéØ  Best val macro-F1: {best_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717f26e0",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "\n",
    "* Reload the best checkpoint (weights + tokenizer).\n",
    "\n",
    "* Run inference on the validation tensors to collect raw logits (before sigmoid).\n",
    "\n",
    "* Convert logits ‚Üí probabilities with sigmoid.\n",
    "\n",
    "* Sweep thresholds from 0.05 to 0.50 (step 0.01) for each label independently.\n",
    "    * Compute balanced accuracy = ¬Ω ( TPR + TNR ).\n",
    "    * Keep the threshold giving the highest balanced accuracy.\n",
    "\n",
    "* Persist thresholds to models/chemberta_3lbl/thresholds.json.\n",
    "\n",
    "* Quick report: print the chosen threshold and val metrics (F1, bal-acc) for each label.\n",
    "\n",
    "* These thresholds will be used at inference; if all three probs < their thresholds we return ‚Äúnone-of-three‚Äù."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fda41e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gastrointestinal  thr=0.48 | bal-acc=0.584 | F1=0.775\n",
      "Infections       thr=0.33 | bal-acc=0.593 | F1=0.673\n",
      "NervousSystem    thr=0.50 | bal-acc=0.523 | F1=0.685\n",
      "\n",
      "üíæ Thresholds saved to models\\chemberta_3lbl\\thresholds.json\n"
     ]
    }
   ],
   "source": [
    "import torch, json, numpy as np, torch.nn as nn\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score\n",
    "from transformers import AutoModel\n",
    "from pathlib import Path\n",
    "\n",
    "# --- paths ---\n",
    "proc_dir  = Path(\"Data/processed\")\n",
    "model_dir = Path(\"models/chemberta_3lbl\")\n",
    "ckpt_path = model_dir / \"pytorch_model.bin\"\n",
    "thresh_out = model_dir / \"thresholds.json\"\n",
    "\n",
    "# --- data ---\n",
    "X_val   = np.load(proc_dir / \"X_val.npy\")\n",
    "attn_val = np.load(proc_dir / \"attn_val.npy\")\n",
    "y_val   = np.load(proc_dir / \"y_val.npy\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- rebuild architecture exactly as during training ---\n",
    "base_name = \"seyonec/ChemBERTa_zinc250k_v2_40k\"\n",
    "base = AutoModel.from_pretrained(base_name)\n",
    "hidden = base.config.hidden_size\n",
    "classifier = nn.Linear(hidden, 3)\n",
    "class ChemClassifier(nn.Module):\n",
    "    def __init__(self, base, classifier):\n",
    "        super().__init__()\n",
    "        self.base = base\n",
    "        self.classifier = classifier\n",
    "    def forward(self, ids, attn):\n",
    "        cls = self.base(input_ids=ids, attention_mask=attn).last_hidden_state[:,0,:]\n",
    "        return self.classifier(cls)\n",
    "\n",
    "model = ChemClassifier(base, classifier).to(device)\n",
    "\n",
    "# --- load state dict, renaming head.* -> classifier.* ---\n",
    "state = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "if \"head.weight\" in state:\n",
    "    state[\"classifier.weight\"] = state.pop(\"head.weight\")\n",
    "    state[\"classifier.bias\"]   = state.pop(\"head.bias\")\n",
    "model.load_state_dict(state, strict=True)\n",
    "model.eval()\n",
    "\n",
    "# --- collect logits on val ---\n",
    "batch = 256\n",
    "logits = []\n",
    "with torch.no_grad():\n",
    "    for i in range(0, len(X_val), batch):\n",
    "        ids  = torch.tensor(X_val[i:i+batch], dtype=torch.long, device=device)\n",
    "        att  = torch.tensor(attn_val[i:i+batch], dtype=torch.long, device=device)\n",
    "        logits.append(model(ids, att).cpu())\n",
    "logits = torch.cat(logits).numpy()\n",
    "probs  = 1/(1+np.exp(-logits))\n",
    "\n",
    "# --- sweep thresholds ---\n",
    "thr_range = np.arange(0.05,0.51,0.01)\n",
    "best_thr, best_bal, best_f1 = [], [], []\n",
    "for c in range(3):\n",
    "    bal = [balanced_accuracy_score(y_val[:,c], probs[:,c]>=t) for t in thr_range]\n",
    "    f1  = [f1_score(y_val[:,c], probs[:,c]>=t)               for t in thr_range]\n",
    "    idx = int(np.argmax(bal))\n",
    "    best_thr.append(float(thr_range[idx])); best_bal.append(float(bal[idx])); best_f1.append(float(f1[idx]))\n",
    "\n",
    "labels = [\"Gastrointestinal\",\"Infections\",\"NervousSystem\"]\n",
    "json.dump({k:v for k,v in zip(labels,best_thr)}, open(thresh_out,\"w\"), indent=2)\n",
    "\n",
    "for l,t,ba,f in zip(labels,best_thr,best_bal,best_f1):\n",
    "    print(f\"{l:15s}  thr={t:.2f} | bal-acc={ba:.3f} | F1={f:.3f}\")\n",
    "print(\"\\nüíæ Thresholds saved to\", thresh_out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a29825",
   "metadata": {},
   "source": [
    "## Step 5: \n",
    "\n",
    "* Reload the frozen ChemBERTa classifier (weights + classifier renaming trick).\n",
    "\n",
    "* Pick a compact background set for SHAP DeepExplainer\n",
    "    * Randomly select 100 diverse SMILES from train (stratified by label).\n",
    "\n",
    "* Build the SHAP explainer\n",
    "    * shap.DeepExplainer(model, background_embeddings) where\n",
    "      background_embeddings = base(**tokenised_bg).last_hidden_state[:,0,:].\n",
    "    * We pass embeddings instead of token IDs to cut compute time.\n",
    "\n",
    "* Batch-compute SHAP values for each split\n",
    "    * Loop over X_* tensors (batch 128).\n",
    "    * For each batch:\n",
    "        * Feed token IDs & masks ‚Üí logits.\n",
    "        * explainer.shap_values((ids, attn)) returns 3 √ó B √ó 128 arrays.\n",
    "    * Store alongside y_* and token IDs into .npz files: data/shap/shap_train.npz, shap_val.npz, shap_test.npz.\n",
    "\n",
    "**Memory tips** \n",
    "* Process on GPU but move finished SHAP arrays to CPU before saving.\n",
    "* Save in float16 (astype(np.float16)) to shrink disk usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf337302",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 59\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚úÖ cached \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshap_all.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sp \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mval\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     \u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43msp\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 49\u001b[39m, in \u001b[36mcache\u001b[39m\u001b[34m(split)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(ids), batch):\n\u001b[32m     47\u001b[39m     emb = embed(torch.tensor(ids[i:i+batch],  dtype=torch.long, device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     48\u001b[39m                 torch.tensor(att[i:i+batch], dtype=torch.long, device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     sv  = \u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43memb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m              \u001b[38;5;66;03m# list of 3 (B√ó768)\u001b[39;00m\n\u001b[32m     50\u001b[39m     rows.append(np.stack(sv, axis=\u001b[32m0\u001b[39m).astype(np.float16))\n\u001b[32m     52\u001b[39m shap_all = np.concatenate(rows, axis=\u001b[32m1\u001b[39m)          \u001b[38;5;66;03m# 3 √ó N √ó 768\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\god\\Lib\\site-packages\\shap\\explainers\\_gradient.py:144\u001b[39m, in \u001b[36mGradient.shap_values\u001b[39m\u001b[34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mshap_values\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, nsamples=\u001b[32m200\u001b[39m, ranked_outputs=\u001b[38;5;28;01mNone\u001b[39;00m, output_rank_order=\u001b[33m\"\u001b[39m\u001b[33mmax\u001b[39m\u001b[33m\"\u001b[39m, rseed=\u001b[38;5;28;01mNone\u001b[39;00m, return_variances=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    107\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\" Return the values for the model applied to X.\u001b[39;00m\n\u001b[32m    108\u001b[39m \n\u001b[32m    109\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    142\u001b[39m \u001b[33;03m        were chosen as \"top\".\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexplainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshap_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnsamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mranked_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_rank_order\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_variances\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\god\\Lib\\site-packages\\shap\\explainers\\_gradient.py:535\u001b[39m, in \u001b[36m_PyTorchGradient.shap_values\u001b[39m\u001b[34m(self, X, nsamples, ranked_outputs, output_rank_order, rseed, return_variances)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    534\u001b[39m     x = X[l][j].clone().detach()\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m samples_input[l][k] = (t * x + (\u001b[32m1\u001b[39m - t) * \u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m.detach()).\\\n\u001b[32m    536\u001b[39m     clone().detach()\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.input_handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    538\u001b[39m     samples_delta[l][k] = (x - (\u001b[38;5;28mself\u001b[39m.data[l][rind]).clone().detach()).cpu().numpy()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import shap, torch, numpy as np, random\n",
    "from transformers import AutoModel\n",
    "from pathlib import Path\n",
    "\n",
    "# ----- 1.  Build wrapper that matches the checkpoint keys -----\n",
    "base_name = \"seyonec/ChemBERTa_zinc250k_v2_40k\"\n",
    "base = AutoModel.from_pretrained(base_name)\n",
    "hidden = base.config.hidden_size\n",
    "head  = torch.nn.Linear(hidden, 3)        # call it *head* to match ckpt\n",
    "\n",
    "class ChemClassifier(torch.nn.Module):\n",
    "    def __init__(self, base, head):\n",
    "        super().__init__(); self.base, self.head = base, head\n",
    "    def forward(self, ids, att):\n",
    "        cls = self.base(input_ids=ids, attention_mask=att).last_hidden_state[:,0,:]\n",
    "        return self.head(cls)\n",
    "\n",
    "model = ChemClassifier(base, head)\n",
    "state = torch.load(\"models/chemberta_3lbl/pytorch_model.bin\", map_location=\"cpu\")\n",
    "model.load_state_dict(state, strict=True)   # names now line-up\n",
    "model.eval().cuda()\n",
    "\n",
    "# ----- 2.  Helper to turn token IDs into CLS embeddings -----\n",
    "@torch.no_grad()\n",
    "def embed(ids, att):\n",
    "    return base(input_ids=ids, attention_mask=att).last_hidden_state[:,0,:]\n",
    "\n",
    "# ----- 3.  Prep background CLS embeddings (100 random rows) -----\n",
    "proc = Path(\"Data/processed\")\n",
    "Xtr  = np.load(proc/\"X_train.npy\")\n",
    "Attr = np.load(proc/\"attn_train.npy\")\n",
    "bg_i = random.sample(range(len(Xtr)), 100)\n",
    "bg_emb = embed(torch.tensor(Xtr[bg_i],  dtype=torch.long, device=\"cuda\"),\n",
    "               torch.tensor(Attr[bg_i], dtype=torch.long, device=\"cuda\"))\n",
    "\n",
    "# Explain only the linear head w.r.t. these embeddings\n",
    "explainer = shap.GradientExplainer(model.head, [bg_emb])\n",
    "\n",
    "# ----- 4.  Cache SHAP per split (CLS-emb dim = 768) -----\n",
    "def cache(split):\n",
    "    ids = np.load(proc/f\"X_{split}.npy\")\n",
    "    att = np.load(proc/f\"attn_{split}.npy\")\n",
    "    y   = np.load(proc/f\"y_{split}.npy\")\n",
    "\n",
    "    rows, batch = [], 128\n",
    "    for i in range(0, len(ids), batch):\n",
    "        emb = embed(torch.tensor(ids[i:i+batch],  dtype=torch.long, device=\"cuda\"),\n",
    "                    torch.tensor(att[i:i+batch], dtype=torch.long, device=\"cuda\"))\n",
    "        sv  = explainer.shap_values([emb])              # list of 3 (B√ó768)\n",
    "        rows.append(np.stack(sv, axis=0).astype(np.float16))\n",
    "\n",
    "    shap_all = np.concatenate(rows, axis=1)          # 3 √ó N √ó 768\n",
    "    Path(\"Data/shap\").mkdir(exist_ok=True, parents=True)\n",
    "    np.savez(f\"Data/shap/shap_{split}.npz\",\n",
    "             shap=shap_all, y=y)                     # CLS-level SHAP\n",
    "    print(f\"‚úÖ cached {split}: {shap_all.shape}\")\n",
    "\n",
    "for sp in [\"train\", \"val\", \"test\"]:\n",
    "    cache(sp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0143a836",
   "metadata": {},
   "source": [
    "## Step 6: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "god",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
