{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8edec91",
   "metadata": {},
   "source": [
    "# Meta explainer network for Tox21 dataset   version 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb4c3f",
   "metadata": {},
   "source": [
    "## 1: load & clean Tox21 CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12509b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3079, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"Data_v3/original/tox21.csv\"  \n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "label_cols = [c for c in df.columns if c.startswith((\"NR-\",\"SR-\"))]\n",
    "df = df.dropna(subset=[\"smiles\"] + label_cols).reset_index(drop=True)\n",
    "print(\"Data shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b01f6a",
   "metadata": {},
   "source": [
    "## 2: compute RDKit Descriptors & Toxicophore Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c63ea97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor frame: (3079, 13)\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import numpy as np\n",
    "\n",
    "rows = []\n",
    "for smi in df.smiles:\n",
    "    m = Chem.MolFromSmiles(smi)\n",
    "    rows.append({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\":  Descriptors.MolLogP(m),\n",
    "        \"TPSA\":  Descriptors.TPSA(m),\n",
    "        \"HDon\":  Descriptors.NumHDonors(m),\n",
    "        \"HAcc\":  Descriptors.NumHAcceptors(m),\n",
    "        \"RotB\":  Descriptors.NumRotatableBonds(m),\n",
    "        \"RingC\": Descriptors.RingCount(m),\n",
    "        \"AromR\": Descriptors.NumAromaticRings(m),\n",
    "        \"nitro\": int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[N+](=O)[O-]\"))),\n",
    "        \"phenol\":int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[OX2H]\"))),\n",
    "        \"carbonyl\":int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[CX3]=O\"))),\n",
    "        \"amine\": int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[NX3;H2,H1]\"))),\n",
    "        \"halogen\":int(any(a.GetSymbol() in (\"Cl\",\"Br\",\"F\",\"I\") for a in m.GetAtoms()))\n",
    "    })\n",
    "df_desc = pd.DataFrame(rows)\n",
    "print(\"Descriptor frame:\", df_desc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a3ad0",
   "metadata": {},
   "source": [
    "## 3: Train / Validation Split & Tokenise SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06eef5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\god\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: torch.Size([2463, 267])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "X_train, X_val, y_train, y_val, desc_tr, desc_val = train_test_split(\n",
    "    df.smiles, df[label_cols], df_desc, test_size=0.2, random_state=42)\n",
    "\n",
    "def tokenize(smiles_list):\n",
    "    return tokenizer(smiles_list, padding=True, truncation=True,\n",
    "                     return_tensors=\"pt\")\n",
    "\n",
    "enc_train = tokenize(X_train.tolist())\n",
    "enc_val   = tokenize(X_val.tolist())\n",
    "\n",
    "print(\"Train tokens:\", enc_train.input_ids.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503b4bd",
   "metadata": {},
   "source": [
    "## 4: Define ChemBERTa Multi-Label Classier    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44bdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class ChemBERTaClassifier(nn.Module):\n",
    "    def __init__(self, n_labels=12):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.bert.config.hidden_size, n_labels)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled = self.bert(input_ids, attention_mask).pooler_output\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = ChemBERTaClassifier(len(label_cols)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf08ff",
   "metadata": {},
   "source": [
    "## 5: Fine‑Tune ChemBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c989f5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n",
      "GPU name: NVIDIA GeForce RTX 4070 Ti\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d099726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved:\n",
      "  • Weights        → models/v4\\model.pt\n",
      "  • Full checkpoint→ models/v4\\checkpoints\\full_checkpoint.pt\n",
      "  • Tokenizer files→ models/v4\n",
      "  • Metrics        → models/v4\\metrics\n",
      "  • Config         → models/v4\\train_config.json\n"
     ]
    }
   ],
   "source": [
    "# ---- Simple training loop: 20 epochs, early stopping, metrics printed ----\n",
    "import torch, torch.nn.functional as F, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "import numpy as np, tqdm, time\n",
    "\n",
    "EPOCHS      = 20\n",
    "BATCH_SIZE  = 16\n",
    "LR          = 2e-5\n",
    "PATIENCE    = 3          # stop if macro AUC doesn't improve for PATIENCE epochs\n",
    "THRESH      = 0.5        # for F1/accuracy\n",
    "use_amp     = (device.type == \"cuda\")\n",
    "\n",
    "# Data loaders\n",
    "train_ds = TensorDataset(enc_train.input_ids,\n",
    "                         enc_train.attention_mask,\n",
    "                         torch.FloatTensor(y_train.values))\n",
    "val_ds   = TensorDataset(enc_val.input_ids,\n",
    "                         enc_val.attention_mask,\n",
    "                         torch.FloatTensor(y_val.values))\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE)\n",
    "\n",
    "# Optimizer\n",
    "opt = optim.AdamW(model.parameters(), lr=LR)\n",
    "\n",
    "# AMP helpers\n",
    "try:\n",
    "    scaler  = torch.amp.GradScaler(\"cuda\") if use_amp else None\n",
    "    autocast_ctx = lambda: torch.amp.autocast(\"cuda\", enabled=use_amp)\n",
    "except AttributeError:\n",
    "    from torch.cuda.amp import GradScaler, autocast\n",
    "    scaler  = GradScaler(enabled=use_amp)\n",
    "    autocast_ctx = lambda: autocast(enabled=use_amp)\n",
    "\n",
    "def eval_loop(loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad(), autocast_ctx():\n",
    "        for ids, attn, yb in loader:\n",
    "            ids, attn, yb = ids.to(device), attn.to(device), yb.to(device)\n",
    "            logits = model(ids, attn)\n",
    "            probs  = torch.sigmoid(logits).cpu().numpy()\n",
    "            ys.append(yb.cpu().numpy())\n",
    "            ps.append(probs)\n",
    "    ys, ps = np.vstack(ys), np.vstack(ps)\n",
    "    # metrics\n",
    "    try:\n",
    "        macro_auc = np.nanmean([roc_auc_score(ys[:,i], ps[:,i]) \n",
    "                                if len(np.unique(ys[:,i]))>1 else np.nan\n",
    "                                for i in range(ps.shape[1])])\n",
    "    except ValueError:\n",
    "        macro_auc = np.nan\n",
    "    f1_macro = f1_score(ys, (ps>THRESH).astype(int), average='macro', zero_division=0)\n",
    "    acc = ( (ps>THRESH) == ys ).mean()\n",
    "    return macro_auc, f1_macro, acc\n",
    "\n",
    "best_auc = -np.inf\n",
    "no_improve = 0\n",
    "model.train()\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    t0 = time.time()\n",
    "    epoch_loss = 0.0\n",
    "    seen = 0\n",
    "    pbar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    opt.zero_grad(set_to_none=True)\n",
    "\n",
    "    for ids, attn, yb in pbar:\n",
    "        ids, attn, yb = ids.to(device), attn.to(device), yb.to(device)\n",
    "        with autocast_ctx():\n",
    "            logits = model(ids, attn)\n",
    "            loss   = F.binary_cross_entropy_with_logits(logits, yb)\n",
    "        if use_amp:\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(opt); scaler.update()\n",
    "        else:\n",
    "            loss.backward(); opt.step()\n",
    "        opt.zero_grad(set_to_none=True)\n",
    "\n",
    "        epoch_loss += loss.item() * ids.size(0)\n",
    "        seen += ids.size(0)\n",
    "        pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
    "\n",
    "    # ---- validation ----\n",
    "    val_auc, val_f1, val_acc = eval_loop(val_loader)\n",
    "    print(f\"Epoch {epoch} done in {time.time()-t0:.1f}s | \"\n",
    "          f\"train_loss={epoch_loss/seen:.4f} | \"\n",
    "          f\"valAUC={val_auc:.3f} | valF1={val_f1:.3f} | valAcc={val_acc:.3f}\")\n",
    "\n",
    "    # early stopping check\n",
    "    if val_auc > best_auc + 1e-4:\n",
    "        best_auc = val_auc\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(\"⛔ Early stopping (no AUC improvement).\")\n",
    "            break\n",
    "\n",
    "model.eval()\n",
    "print(\"Finished training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45a958",
   "metadata": {},
   "source": [
    "## 6: Save Model & Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dffd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model & tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SAVE_DIR = \"models/v4\"\n",
    "model.eval()\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"{SAVE_DIR}/model.pt\")\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"✅ Model & tokenizer saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f2de1",
   "metadata": {},
   "source": [
    "## 7:  Compute SHAP Mean‑Abs Features (All 12 Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f0d9bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 1/12 (NR-AR): 392.9s\n",
      "Class 2/12 (NR-AR-LBD): 390.5s\n",
      "Class 3/12 (NR-AhR): 390.3s\n",
      "Class 4/12 (NR-Aromatase): 389.5s\n",
      "Class 5/12 (NR-ER): 389.3s\n",
      "Class 6/12 (NR-ER-LBD): 388.8s\n",
      "Class 7/12 (NR-PPAR-gamma): 391.0s\n",
      "Class 8/12 (SR-ARE): 391.0s\n",
      "Class 9/12 (SR-ATAD5): 390.7s\n",
      "Class 10/12 (SR-HSE): 389.7s\n",
      "Class 11/12 (SR-MMP): 387.8s\n",
      "Class 12/12 (SR-p53): 390.5s\n",
      "⏱ Total SHAP time: 78.0 min for N=616\n",
      "✅ Saved: Data_v3/SHAP_val_full\\shap_means.npy \n"
     ]
    }
   ],
   "source": [
    "# Cell 7 – Wide SHAP (GPU, chunked, timed)\n",
    "import shap, torch, numpy as np, torch.nn as nn, os, time\n",
    "\n",
    "# ----------------- config -----------------\n",
    "SUB_N     = None      # None = all validation mols; or int like 2000\n",
    "BG_N      = 32        # background size (trade-off speed/quality)\n",
    "CHUNK     = 64        # how many mols at once through SHAP\n",
    "SAVE_DIR  = \"Data_v3/SHAP_val_full\"\n",
    "SAVE_TOKENS = False   # True → also save per-token SHAP (big files!)\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "model.to(device).eval()\n",
    "\n",
    "# 1) Slice validation set & build embeddings on GPU\n",
    "ids_full = enc_val.input_ids.to(device)\n",
    "if SUB_N: ids_full = ids_full[:SUB_N]\n",
    "with torch.no_grad():\n",
    "    embed_full = model.bert.embeddings(ids_full).float()     # [N, S, 768]\n",
    "\n",
    "N, S, E = embed_full.shape\n",
    "bg      = embed_full[:BG_N]                                  # background\n",
    "n_cls   = len(label_cols)\n",
    "\n",
    "# outputs\n",
    "shap_means = torch.zeros((N, n_cls), device=device)\n",
    "if SAVE_TOKENS:\n",
    "    token_means = np.zeros((N, n_cls, S), dtype=np.float32)   # large!\n",
    "\n",
    "t_all = time.time()\n",
    "for cls in range(n_cls):\n",
    "    head = nn.Sequential(nn.Identity(), nn.Linear(E, 1)).to(device)\n",
    "    head[1].weight.data = model.classifier[1].weight.data[cls:cls+1]\n",
    "    head[1].bias.data   = model.classifier[1].bias.data[cls:cls+1]\n",
    "\n",
    "    expl = shap.DeepExplainer(head, bg)   # runs gradients on GPU internally\n",
    "\n",
    "    per_class_vals = []\n",
    "    t0 = time.time()\n",
    "    for start in range(0, N, CHUNK):\n",
    "        chunk = embed_full[start:start+CHUNK]\n",
    "        vals  = expl.shap_values(chunk)[0]             # numpy [chunk,S,768]\n",
    "        # mean over embedding dim (768) ⇒ token-level importance\n",
    "        tok_imp = np.abs(vals).mean(axis=2)            # [chunk,S]\n",
    "        if SAVE_TOKENS:\n",
    "            token_means[start:start+CHUNK, cls, :tok_imp.shape[1]] = tok_imp\n",
    "        # mean over tokens & embed ⇒ single scalar per mol\n",
    "        mol_imp = tok_imp.mean(axis=1)                 # [chunk]\n",
    "        per_class_vals.append(mol_imp)\n",
    "\n",
    "    shap_means[:, cls] = torch.from_numpy(np.concatenate(per_class_vals)).to(device)\n",
    "\n",
    "    print(f\"Class {cls+1}/{n_cls} ({label_cols[cls]}): {time.time()-t0:.1f}s\")\n",
    "\n",
    "print(f\"⏱ Total SHAP time: {(time.time()-t_all)/60:.1f} min for N={N}\")\n",
    "\n",
    "# 2) Save\n",
    "np.save(os.path.join(SAVE_DIR, \"shap_means.npy\"), shap_means.cpu().numpy())\n",
    "if SAVE_TOKENS:\n",
    "    np.save(os.path.join(SAVE_DIR, \"token_means.npy\"), token_means)\n",
    "\n",
    "print(\"✅ Saved:\",\n",
    "      os.path.join(SAVE_DIR, \"shap_means.npy\"),\n",
    "      \"(and token_means.npy)\" if SAVE_TOKENS else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2952f05",
   "metadata": {},
   "source": [
    "## 8: Build Meta‑Explainer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6be6041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Meta-Explainer tensors ready — X_tr: (492, 25), X_te: (124, 25)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1️⃣ Load SHAP-means (produced in Cell 7-lite)\n",
    "shap_means = np.load(\"Data_v3/SHAP_val_full/shap_means.npy\")           # shape (SUB_N, 12)\n",
    "N_sub = shap_means.shape[0]\n",
    "\n",
    "# 2️⃣ Take the matching slice of descriptors & labels\n",
    "desc_val_sub = desc_val.iloc[:N_sub].reset_index(drop=True)       # (SUB_N, d)\n",
    "y_val_sub    = y_val.iloc[:N_sub].reset_index(drop=True)          # (SUB_N, 12)\n",
    "\n",
    "# 3️⃣ Concatenate descriptor features + SHAP features\n",
    "meta_X = np.hstack([desc_val_sub.values, shap_means])             # (SUB_N, d+12)\n",
    "meta_y = y_val_sub.values                                         # (SUB_N, 12)\n",
    "\n",
    "# 4️⃣ Train / test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    meta_X, meta_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 5️⃣ Build DataLoaders\n",
    "tr_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr))\n",
    "te_ds = TensorDataset(torch.FloatTensor(X_te), torch.FloatTensor(y_te))\n",
    "tr_loader = DataLoader(tr_ds, batch_size=16, shuffle=True)\n",
    "te_loader = DataLoader(te_ds, batch_size=16)\n",
    "\n",
    "print(f\"✅ Meta-Explainer tensors ready — X_tr: {X_tr.shape}, X_te: {X_te.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb8d69",
   "metadata": {},
   "source": [
    "## 9: Train Meta‑Explainer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0d42b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement iterstrat (from versions: none)\n",
      "ERROR: No matching distribution found for iterstrat\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install iterstrat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9a44c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stratified shapes -> train: (604, 25), test: (12, 25)\n",
      "Ep 001 | loss=9.7579 | vPR=0.254 vAUC=0.423 vF1=0.117 vAcc=0.701 | 0.1s\n",
      "Ep 002 | loss=6.1086 | vPR=0.267 vAUC=0.420 vF1=0.167 vAcc=0.597 | 0.1s\n",
      "Ep 003 | loss=6.0653 | vPR=0.273 vAUC=0.442 vF1=0.202 vAcc=0.444 | 0.0s\n",
      "Ep 004 | loss=3.8568 | vPR=0.259 vAUC=0.410 vF1=0.173 vAcc=0.500 | 0.0s\n",
      "Ep 005 | loss=3.0642 | vPR=0.250 vAUC=0.395 vF1=0.173 vAcc=0.431 | 0.1s\n",
      "Ep 006 | loss=3.9062 | vPR=0.261 vAUC=0.411 vF1=0.150 vAcc=0.493 | 0.0s\n",
      "Ep 007 | loss=3.1602 | vPR=0.281 vAUC=0.451 vF1=0.257 vAcc=0.444 | 0.0s\n",
      "Ep 008 | loss=3.5525 | vPR=0.278 vAUC=0.467 vF1=0.160 vAcc=0.333 | 0.0s\n",
      "Ep 009 | loss=2.7341 | vPR=0.272 vAUC=0.463 vF1=0.170 vAcc=0.306 | 0.0s\n",
      "Ep 010 | loss=2.9084 | vPR=0.301 vAUC=0.499 vF1=0.206 vAcc=0.410 | 0.0s\n",
      "Ep 011 | loss=2.2476 | vPR=0.280 vAUC=0.494 vF1=0.230 vAcc=0.403 | 0.0s\n",
      "Ep 012 | loss=2.5586 | vPR=0.291 vAUC=0.510 vF1=0.242 vAcc=0.424 | 0.0s\n",
      "Ep 013 | loss=2.1041 | vPR=0.326 vAUC=0.517 vF1=0.241 vAcc=0.431 | 0.0s\n",
      "Ep 014 | loss=2.1153 | vPR=0.322 vAUC=0.513 vF1=0.180 vAcc=0.431 | 0.0s\n",
      "Ep 015 | loss=2.1781 | vPR=0.328 vAUC=0.522 vF1=0.149 vAcc=0.618 | 0.0s\n",
      "Ep 016 | loss=2.1719 | vPR=0.336 vAUC=0.565 vF1=0.262 vAcc=0.458 | 0.0s\n",
      "Ep 017 | loss=1.9390 | vPR=0.339 vAUC=0.541 vF1=0.247 vAcc=0.458 | 0.0s\n",
      "Ep 018 | loss=1.7776 | vPR=0.330 vAUC=0.533 vF1=0.211 vAcc=0.375 | 0.0s\n",
      "Ep 019 | loss=1.6385 | vPR=0.349 vAUC=0.560 vF1=0.266 vAcc=0.424 | 0.0s\n",
      "Ep 020 | loss=1.6698 | vPR=0.362 vAUC=0.580 vF1=0.274 vAcc=0.542 | 0.0s\n",
      "Ep 021 | loss=1.7142 | vPR=0.311 vAUC=0.525 vF1=0.225 vAcc=0.375 | 0.0s\n",
      "Ep 022 | loss=1.4618 | vPR=0.313 vAUC=0.529 vF1=0.270 vAcc=0.424 | 0.0s\n",
      "Ep 023 | loss=1.2989 | vPR=0.320 vAUC=0.524 vF1=0.265 vAcc=0.424 | 0.0s\n",
      "Ep 024 | loss=1.4843 | vPR=0.340 vAUC=0.541 vF1=0.220 vAcc=0.472 | 0.0s\n",
      "Ep 025 | loss=1.3032 | vPR=0.314 vAUC=0.516 vF1=0.261 vAcc=0.500 | 0.0s\n",
      "Ep 026 | loss=1.2920 | vPR=0.321 vAUC=0.544 vF1=0.253 vAcc=0.528 | 0.0s\n",
      "Ep 027 | loss=1.3594 | vPR=0.331 vAUC=0.533 vF1=0.248 vAcc=0.458 | 0.1s\n",
      "Ep 028 | loss=1.2810 | vPR=0.372 vAUC=0.550 vF1=0.279 vAcc=0.514 | 0.0s\n",
      "Ep 029 | loss=1.2854 | vPR=0.380 vAUC=0.575 vF1=0.267 vAcc=0.465 | 0.0s\n",
      "Ep 030 | loss=1.2635 | vPR=0.351 vAUC=0.568 vF1=0.264 vAcc=0.486 | 0.0s\n",
      "Ep 031 | loss=1.1863 | vPR=0.328 vAUC=0.562 vF1=0.276 vAcc=0.472 | 0.0s\n",
      "Ep 032 | loss=1.1531 | vPR=0.347 vAUC=0.550 vF1=0.303 vAcc=0.549 | 0.0s\n",
      "Ep 033 | loss=1.1556 | vPR=0.359 vAUC=0.580 vF1=0.299 vAcc=0.528 | 0.0s\n",
      "Ep 034 | loss=1.2076 | vPR=0.372 vAUC=0.588 vF1=0.286 vAcc=0.528 | 0.0s\n",
      "Ep 035 | loss=1.1407 | vPR=0.370 vAUC=0.600 vF1=0.290 vAcc=0.583 | 0.0s\n",
      "Ep 036 | loss=1.2031 | vPR=0.337 vAUC=0.583 vF1=0.272 vAcc=0.472 | 0.0s\n",
      "Ep 037 | loss=1.1549 | vPR=0.427 vAUC=0.656 vF1=0.285 vAcc=0.535 | 0.0s\n",
      "Ep 038 | loss=1.1416 | vPR=0.413 vAUC=0.614 vF1=0.330 vAcc=0.597 | 0.0s\n",
      "Ep 039 | loss=1.0741 | vPR=0.439 vAUC=0.644 vF1=0.313 vAcc=0.583 | 0.0s\n",
      "Ep 040 | loss=1.4272 | vPR=0.417 vAUC=0.651 vF1=0.300 vAcc=0.542 | 0.0s\n",
      "Ep 041 | loss=1.1465 | vPR=0.404 vAUC=0.651 vF1=0.278 vAcc=0.514 | 0.0s\n",
      "Ep 042 | loss=1.1872 | vPR=0.389 vAUC=0.634 vF1=0.291 vAcc=0.535 | 0.0s\n",
      "Ep 043 | loss=1.1648 | vPR=0.406 vAUC=0.646 vF1=0.282 vAcc=0.528 | 0.0s\n",
      "Ep 044 | loss=1.0819 | vPR=0.409 vAUC=0.624 vF1=0.283 vAcc=0.535 | 0.0s\n",
      "Ep 045 | loss=1.0780 | vPR=0.401 vAUC=0.638 vF1=0.273 vAcc=0.535 | 0.0s\n",
      "Ep 046 | loss=1.0616 | vPR=0.403 vAUC=0.644 vF1=0.311 vAcc=0.625 | 0.0s\n",
      "Ep 047 | loss=1.0896 | vPR=0.403 vAUC=0.653 vF1=0.310 vAcc=0.597 | 0.0s\n",
      "Ep 048 | loss=1.0549 | vPR=0.436 vAUC=0.665 vF1=0.296 vAcc=0.569 | 0.0s\n",
      "Ep 049 | loss=1.0806 | vPR=0.411 vAUC=0.660 vF1=0.298 vAcc=0.576 | 0.0s\n",
      "Ep 050 | loss=1.0763 | vPR=0.453 vAUC=0.655 vF1=0.314 vAcc=0.576 | 0.0s\n",
      "Ep 051 | loss=1.0506 | vPR=0.426 vAUC=0.667 vF1=0.294 vAcc=0.556 | 0.0s\n",
      "Ep 052 | loss=1.0906 | vPR=0.421 vAUC=0.654 vF1=0.304 vAcc=0.569 | 0.0s\n",
      "Ep 053 | loss=1.0483 | vPR=0.398 vAUC=0.657 vF1=0.333 vAcc=0.625 | 0.0s\n",
      "Ep 054 | loss=1.0555 | vPR=0.398 vAUC=0.657 vF1=0.336 vAcc=0.625 | 0.0s\n",
      "Ep 055 | loss=1.0435 | vPR=0.438 vAUC=0.666 vF1=0.330 vAcc=0.604 | 0.0s\n",
      "Ep 056 | loss=1.0354 | vPR=0.453 vAUC=0.678 vF1=0.340 vAcc=0.632 | 0.0s\n",
      "Ep 057 | loss=1.0622 | vPR=0.432 vAUC=0.679 vF1=0.315 vAcc=0.597 | 0.0s\n",
      "Ep 058 | loss=1.0185 | vPR=0.452 vAUC=0.677 vF1=0.323 vAcc=0.625 | 0.0s\n",
      "Ep 059 | loss=1.0593 | vPR=0.434 vAUC=0.691 vF1=0.311 vAcc=0.604 | 0.0s\n",
      "Ep 060 | loss=1.0280 | vPR=0.454 vAUC=0.670 vF1=0.328 vAcc=0.604 | 0.0s\n",
      "Ep 061 | loss=1.0205 | vPR=0.451 vAUC=0.676 vF1=0.329 vAcc=0.646 | 0.0s\n",
      "Ep 062 | loss=1.0222 | vPR=0.450 vAUC=0.683 vF1=0.343 vAcc=0.632 | 0.0s\n",
      "Ep 063 | loss=1.0668 | vPR=0.395 vAUC=0.673 vF1=0.316 vAcc=0.604 | 0.0s\n",
      "Ep 064 | loss=1.0341 | vPR=0.432 vAUC=0.670 vF1=0.331 vAcc=0.625 | 0.0s\n",
      "Ep 065 | loss=1.0553 | vPR=0.434 vAUC=0.667 vF1=0.316 vAcc=0.597 | 0.0s\n",
      "Ep 066 | loss=1.0272 | vPR=0.431 vAUC=0.681 vF1=0.307 vAcc=0.597 | 0.0s\n",
      "Ep 067 | loss=1.0152 | vPR=0.460 vAUC=0.693 vF1=0.339 vAcc=0.639 | 0.0s\n",
      "Ep 068 | loss=1.0147 | vPR=0.459 vAUC=0.685 vF1=0.336 vAcc=0.667 | 0.0s\n",
      "Ep 069 | loss=1.0176 | vPR=0.480 vAUC=0.692 vF1=0.341 vAcc=0.639 | 0.0s\n",
      "Ep 070 | loss=0.9919 | vPR=0.453 vAUC=0.694 vF1=0.330 vAcc=0.639 | 0.0s\n",
      "Ep 071 | loss=1.0211 | vPR=0.451 vAUC=0.697 vF1=0.319 vAcc=0.618 | 0.0s\n",
      "Ep 072 | loss=1.0409 | vPR=0.462 vAUC=0.712 vF1=0.322 vAcc=0.639 | 0.0s\n",
      "Ep 073 | loss=1.0177 | vPR=0.462 vAUC=0.720 vF1=0.348 vAcc=0.660 | 0.0s\n",
      "Ep 074 | loss=1.0273 | vPR=0.445 vAUC=0.690 vF1=0.327 vAcc=0.639 | 0.0s\n",
      "Ep 075 | loss=1.0156 | vPR=0.461 vAUC=0.711 vF1=0.328 vAcc=0.639 | 0.0s\n",
      "Ep 076 | loss=1.0051 | vPR=0.485 vAUC=0.738 vF1=0.338 vAcc=0.660 | 0.0s\n",
      "Ep 077 | loss=0.9992 | vPR=0.489 vAUC=0.739 vF1=0.330 vAcc=0.604 | 0.0s\n",
      "Ep 078 | loss=1.0226 | vPR=0.448 vAUC=0.698 vF1=0.319 vAcc=0.611 | 0.0s\n",
      "Ep 079 | loss=0.9955 | vPR=0.443 vAUC=0.688 vF1=0.327 vAcc=0.618 | 0.0s\n",
      "Ep 080 | loss=1.0344 | vPR=0.443 vAUC=0.679 vF1=0.336 vAcc=0.653 | 0.0s\n",
      "Ep 081 | loss=0.9765 | vPR=0.459 vAUC=0.712 vF1=0.333 vAcc=0.632 | 0.0s\n",
      "Ep 082 | loss=0.9910 | vPR=0.481 vAUC=0.719 vF1=0.330 vAcc=0.653 | 0.0s\n",
      "Ep 083 | loss=0.9968 | vPR=0.457 vAUC=0.718 vF1=0.318 vAcc=0.639 | 0.0s\n",
      "Ep 084 | loss=0.9740 | vPR=0.467 vAUC=0.718 vF1=0.335 vAcc=0.632 | 0.0s\n",
      "Ep 085 | loss=0.9791 | vPR=0.525 vAUC=0.736 vF1=0.332 vAcc=0.653 | 0.0s\n",
      "Ep 086 | loss=1.0094 | vPR=0.517 vAUC=0.737 vF1=0.341 vAcc=0.653 | 0.0s\n",
      "Ep 087 | loss=0.9696 | vPR=0.535 vAUC=0.750 vF1=0.371 vAcc=0.694 | 0.0s\n",
      "Ep 088 | loss=1.0369 | vPR=0.473 vAUC=0.735 vF1=0.328 vAcc=0.674 | 0.0s\n",
      "Ep 089 | loss=1.0151 | vPR=0.448 vAUC=0.709 vF1=0.349 vAcc=0.681 | 0.0s\n",
      "Ep 090 | loss=0.9809 | vPR=0.489 vAUC=0.725 vF1=0.363 vAcc=0.674 | 0.0s\n",
      "Ep 091 | loss=1.0184 | vPR=0.495 vAUC=0.726 vF1=0.366 vAcc=0.688 | 0.0s\n",
      "Ep 092 | loss=1.0361 | vPR=0.484 vAUC=0.715 vF1=0.335 vAcc=0.674 | 0.0s\n",
      "Ep 093 | loss=0.9850 | vPR=0.464 vAUC=0.707 vF1=0.339 vAcc=0.653 | 0.0s\n",
      "Ep 094 | loss=0.9732 | vPR=0.479 vAUC=0.723 vF1=0.321 vAcc=0.632 | 0.0s\n",
      "Ep 095 | loss=0.9614 | vPR=0.474 vAUC=0.724 vF1=0.364 vAcc=0.674 | 0.0s\n",
      "Ep 096 | loss=0.9738 | vPR=0.469 vAUC=0.728 vF1=0.369 vAcc=0.674 | 0.0s\n",
      "Ep 097 | loss=0.9995 | vPR=0.480 vAUC=0.726 vF1=0.359 vAcc=0.639 | 0.0s\n",
      "Ep 098 | loss=0.9549 | vPR=0.478 vAUC=0.726 vF1=0.363 vAcc=0.674 | 0.0s\n",
      "Ep 099 | loss=0.9593 | vPR=0.504 vAUC=0.741 vF1=0.370 vAcc=0.708 | 0.0s\n",
      "⛔ Early stopping (PR stalled).\n",
      "Total meta training: 0.1 min\n",
      "\n",
      "Best macro metrics: AUC=0.750 PR=0.535 F1=0.371 Acc=0.694\n",
      "NR-AR           AUC=0.818  PR=0.333\n",
      "NR-AR-LBD       AUC=0.455  PR=0.143\n",
      "NR-AhR          AUC=0.875  PR=0.679\n",
      "NR-Aromatase    AUC=0.800  PR=0.667\n",
      "NR-ER           AUC=0.500  PR=0.236\n",
      "NR-ER-LBD       AUC=1.000  PR=1.000\n",
      "NR-PPAR-gamma   AUC=1.000  PR=1.000\n",
      "SR-ARE          AUC=0.543  PR=0.655\n",
      "SR-ATAD5        AUC=0.818  PR=0.333\n",
      "SR-HSE          AUC=0.818  PR=0.333\n",
      "SR-MMP          AUC=0.914  PR=0.903\n",
      "SR-p53          AUC=0.455  PR=0.143\n"
     ]
    }
   ],
   "source": [
    "# Cell 9 – Meta‑Explainer with multilabel stratified split, class weighting, robust metrics\n",
    "import numpy as np, torch, torch.nn as nn, torch.nn.functional as F, time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, f1_score\n",
    "\n",
    "# ---------- 1) Stratified split (iterative multilabel); fallback if not installed ----------\n",
    "try:\n",
    "    from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "    mskf = MultilabelStratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    idx_tr, idx_te = next(mskf.split(meta_X, meta_y))\n",
    "except ImportError:\n",
    "    # very simple fallback: try to keep at least one positive per label in test\n",
    "    rng = np.random.default_rng(42)\n",
    "    idx = np.arange(len(meta_X)); rng.shuffle(idx)\n",
    "    idx_tr, idx_te = [], []\n",
    "    need = set(np.where(meta_y.sum(axis=0) > 0)[0])\n",
    "    target_te = int(len(idx) * 0.2)\n",
    "    for i in idx:\n",
    "        if len(idx_te) < target_te and any(meta_y[i, j] == 1 for j in need):\n",
    "            idx_te.append(i)\n",
    "            need -= {j for j in np.where(meta_y[i] == 1)[0]}\n",
    "        else:\n",
    "            idx_tr.append(i)\n",
    "    idx_te += list(need)\n",
    "    idx_te = np.unique(idx_te).tolist()\n",
    "    idx_tr = [i for i in idx if i not in idx_te]\n",
    "\n",
    "X_tr, X_te = meta_X[idx_tr], meta_X[idx_te]\n",
    "y_tr, y_te = meta_y[idx_tr], meta_y[idx_te]\n",
    "print(f\"Stratified shapes -> train: {X_tr.shape}, test: {X_te.shape}\")\n",
    "\n",
    "# ---------- 2) Drop labels with zero positives in whole split (optional but avoids NaNs) ----------\n",
    "pos_counts = y_tr.sum(axis=0) + y_te.sum(axis=0)\n",
    "keep_cols  = np.where(pos_counts > 0)[0]\n",
    "if len(keep_cols) < y_tr.shape[1]:\n",
    "    dropped = [label_cols[i] for i in range(len(label_cols)) if i not in keep_cols]\n",
    "    print(\"⚠️ Dropping labels with no positives:\", dropped)\n",
    "    y_tr = y_tr[:, keep_cols]; y_te = y_te[:, keep_cols]\n",
    "    label_cols_kept = [label_cols[i] for i in keep_cols]\n",
    "else:\n",
    "    label_cols_kept = label_cols\n",
    "\n",
    "IN_DIM  = X_tr.shape[1]\n",
    "OUT_DIM = y_tr.shape[1]\n",
    "\n",
    "# ---------- 3) DataLoaders ----------\n",
    "BATCH = 32\n",
    "tr_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr))\n",
    "te_ds = TensorDataset(torch.FloatTensor(X_te), torch.FloatTensor(y_te))\n",
    "tr_loader = DataLoader(tr_ds, batch_size=BATCH, shuffle=True)\n",
    "te_loader = DataLoader(te_ds, batch_size=BATCH)\n",
    "\n",
    "# ---------- 4) Model ----------\n",
    "class MetaMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(64, out_dim)\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "meta = MetaMLP(IN_DIM, OUT_DIM).to(device)\n",
    "\n",
    "# ---------- 5) Loss with class imbalance handling ----------\n",
    "pos = y_tr.sum(axis=0)\n",
    "neg = y_tr.shape[0] - pos\n",
    "pos_weight = torch.tensor((neg / np.clip(pos, 1, None)), dtype=torch.float32).to(device)\n",
    "crit = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "opt      = torch.optim.Adam(meta.parameters(), lr=1e-3)\n",
    "EPOCHS   = 200\n",
    "PATIENCE = 12\n",
    "THRESH   = 0.5\n",
    "\n",
    "best_pr  = -np.inf\n",
    "stalls   = 0\n",
    "best_state   = None\n",
    "best_metrics = None\n",
    "\n",
    "# ---------- eval helper ----------\n",
    "def eval_meta(loader):\n",
    "    meta.eval()\n",
    "    Ys, Ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            probs = torch.sigmoid(meta(xb)).cpu().numpy()\n",
    "            Ys.append(yb.cpu().numpy()); Ps.append(probs)\n",
    "    Ys, Ps = np.vstack(Ys), np.vstack(Ps)\n",
    "\n",
    "    aucs, prs = [], []\n",
    "    for i in range(OUT_DIM):\n",
    "        y_i, p_i = Ys[:, i], Ps[:, i]\n",
    "        if len(np.unique(y_i)) < 2:\n",
    "            aucs.append(np.nan); prs.append(np.nan); continue\n",
    "        aucs.append(roc_auc_score(y_i, p_i))\n",
    "        prs.append(average_precision_score(y_i, p_i))\n",
    "    macro_auc = np.nanmean(aucs)\n",
    "    macro_pr  = np.nanmean(prs)\n",
    "    f1_macro  = f1_score(Ys, (Ps > THRESH).astype(int), average='macro', zero_division=0)\n",
    "    acc       = ((Ps > THRESH) == Ys).mean()\n",
    "    return macro_auc, macro_pr, f1_macro, acc, aucs, prs\n",
    "\n",
    "# ---------- training loop ----------\n",
    "t0_all = time.time()\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    meta.train()\n",
    "    loss_sum = 0.0; n = 0\n",
    "    t0 = time.time()\n",
    "\n",
    "    for xb, yb in tr_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        logits = meta(xb)\n",
    "        loss   = crit(logits, yb)\n",
    "        opt.zero_grad(); loss.backward(); opt.step()\n",
    "        loss_sum += loss.item() * xb.size(0); n += xb.size(0)\n",
    "\n",
    "    val_auc, val_pr, val_f1, val_acc, aucs, prs = eval_meta(te_loader)\n",
    "    print(f\"Ep {ep:03d} | loss={loss_sum/n:.4f} | vPR={val_pr:.3f} vAUC={val_auc:.3f} \"\n",
    "          f\"vF1={val_f1:.3f} vAcc={val_acc:.3f} | {time.time()-t0:.1f}s\")\n",
    "\n",
    "    if val_pr > best_pr + 1e-4:\n",
    "        best_pr = val_pr; stalls = 0\n",
    "        best_state   = meta.state_dict()\n",
    "        best_metrics = (val_auc, val_pr, val_f1, val_acc, aucs, prs)\n",
    "    else:\n",
    "        stalls += 1\n",
    "        if stalls >= PATIENCE:\n",
    "            print(\"⛔ Early stopping (PR stalled).\")\n",
    "            break\n",
    "\n",
    "print(f\"Total meta training: {(time.time()-t0_all)/60:.1f} min\")\n",
    "\n",
    "# load best\n",
    "if best_state is not None:\n",
    "    meta.load_state_dict(best_state)\n",
    "    meta.eval()\n",
    "    val_auc, val_pr, val_f1, val_acc, aucs, prs = best_metrics\n",
    "    print(\"\\nBest macro metrics:\",\n",
    "          f\"AUC={val_auc:.3f} PR={val_pr:.3f} F1={val_f1:.3f} Acc={val_acc:.3f}\")\n",
    "    for i, cls in enumerate(label_cols_kept):\n",
    "        print(f\"{cls:15s} AUC={aucs[i]:.3f}  PR={prs[i]:.3f}\")\n",
    "else:\n",
    "    print(\"No improvement recorded; using last epoch weights.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdaf00ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Meta-MLP saved to: models/v4/MLP\n"
     ]
    }
   ],
   "source": [
    "## save the MLP \n",
    "# Cell 10 – Save Meta‑Explainer (MLP) weights, metrics, and config\n",
    "import os, json, numpy as np, torch\n",
    "from datetime import datetime\n",
    "\n",
    "SAVE_DIR = \"models/v4/MLP\"\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Weights\n",
    "torch.save(meta.state_dict(), os.path.join(SAVE_DIR, \"meta_mlp.pt\"))\n",
    "\n",
    "# 2) Metrics (from best_metrics defined in Cell 9)\n",
    "val_auc, val_pr, val_f1, val_acc, aucs, prs = best_metrics\n",
    "metrics = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"macro_auc\": float(val_auc),\n",
    "    \"macro_pr\":  float(val_pr),\n",
    "    \"macro_f1\":  float(val_f1),\n",
    "    \"macro_acc\": float(val_acc),\n",
    "    \"per_class_auc\": [float(x) if x==x else None for x in aucs],  # NaN→None\n",
    "    \"per_class_pr\":  [float(x) if x==x else None for x in prs],\n",
    "    \"labels\": label_cols_kept,\n",
    "}\n",
    "\n",
    "with open(os.path.join(SAVE_DIR, \"metrics.json\"), \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2)\n",
    "\n",
    "# 3) Config\n",
    "config = {\n",
    "    \"input_dim\": int(IN_DIM),\n",
    "    \"output_dim\": int(OUT_DIM),\n",
    "    \"batch_size\": BATCH,\n",
    "    \"epochs_trained\": ep,\n",
    "    \"learning_rate\": 1e-3,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"threshold\": 0.5,\n",
    "    \"device\": str(device),\n",
    "}\n",
    "with open(os.path.join(SAVE_DIR, \"config.json\"), \"w\") as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# 4) Per-class arrays as .npy (optional)\n",
    "np.save(os.path.join(SAVE_DIR, \"per_class_auc.npy\"), np.array(aucs))\n",
    "np.save(os.path.join(SAVE_DIR, \"per_class_pr.npy\"),  np.array(prs))\n",
    "\n",
    "print(\"✅ Meta-MLP saved to:\", SAVE_DIR)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9fdc6",
   "metadata": {},
   "source": [
    "## 10: Generate an Explanation for a New SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "93ac8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model predicts toxicity for NR-AhR, NR-Aromatase, NR-ER, SR-ARE because of strong model signal for SR-p53 and strong model signal for SR-MMP.\n",
      "     Endpoint  Prob\n",
      "        NR-AR 0.452\n",
      "    NR-AR-LBD 0.327\n",
      "       NR-AhR 0.553\n",
      " NR-Aromatase 0.538\n",
      "        NR-ER 0.519\n",
      "    NR-ER-LBD 0.244\n",
      "NR-PPAR-gamma 0.096\n",
      "       SR-ARE 0.544\n",
      "     SR-ATAD5 0.000\n",
      "       SR-HSE 0.283\n",
      "       SR-MMP 0.452\n",
      "       SR-p53 0.062\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 – Interactive SMILES → PubChem lookup → Meta‑explanation (uses NEW models)\n",
    "# ----------------------------------------------------------------------------------\n",
    "import os, json, requests, shap, torch, numpy as np, torch.nn as nn\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors, Lipinski, Crippen, rdMolDescriptors\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- 0. Paths ----------\n",
    "CHEMBERTA_DIR = \"models/v4\"   # where model.pt + tokenizer/ vocab.json etc. live\n",
    "META_DIR      = \"models/v4/MLP\"               # where meta_mlp.pt & metrics.json live\n",
    "\n",
    "# ---------- 1. Rebuild & load ChemBERTa classifier ----------\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "class ChemBERTaClassifier(nn.Module):\n",
    "    def __init__(self, n_labels):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.bert.config.hidden_size, n_labels)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        out = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        pooled = out.pooler_output\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CHEMBERTA_DIR)\n",
    "\n",
    "# Use the kept labels if you dropped some in Cell 9, otherwise original\n",
    "labels_used = label_cols_kept if 'label_cols_kept' in globals() else label_cols\n",
    "\n",
    "model = ChemBERTaClassifier(n_labels=len(labels_used)).to(device)\n",
    "state = torch.load(os.path.join(CHEMBERTA_DIR, \"model.pt\"), map_location=device)\n",
    "model.load_state_dict(state)\n",
    "model.eval()\n",
    "E = model.bert.config.hidden_size\n",
    "\n",
    "# ---------- 2. Rebuild & load Meta-MLP ----------\n",
    "class MetaMLP(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(64, out_dim), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "# 13 descriptors + 12 SHAP means = 25 (adjust if you changed)\n",
    "meta_in_dim = 25\n",
    "meta = MetaMLP(meta_in_dim, len(labels_used)).to(device)\n",
    "meta.load_state_dict(torch.load(os.path.join(META_DIR, \"meta_mlp.pt\"), map_location=device))\n",
    "meta.eval()\n",
    "\n",
    "# ---------- 3. Helpers ----------\n",
    "def pubchem_name_cid(smiles: str):\n",
    "    url = (\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/smiles/\"\n",
    "           f\"{smiles}/property/Title,IUPACName,CID/JSON\")\n",
    "    try:\n",
    "        js = requests.get(url, timeout=10).json()\n",
    "        props = js[\"PropertyTable\"][\"Properties\"][0]\n",
    "        return (props.get(\"Title\") or props.get(\"IUPACName\")), props.get(\"CID\")\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def calc_desc_flags(mol):\n",
    "    return np.array([\n",
    "        Descriptors.MolWt(mol),                       # 0\n",
    "        Descriptors.MolLogP(mol),                     # 1\n",
    "        Descriptors.TPSA(mol),                        # 2\n",
    "        Lipinski.NumHDonors(mol),                     # 3\n",
    "        Lipinski.NumHAcceptors(mol),                  # 4\n",
    "        Lipinski.NumRotatableBonds(mol),              # 5\n",
    "        Descriptors.RingCount(mol),                   # 6\n",
    "        Descriptors.NumAromaticRings(mol),            # 7\n",
    "        int(mol.HasSubstructMatch(Chem.MolFromSmarts(\"[N+](=O)[O-]\"))),  # 8  nitro\n",
    "        int(mol.HasSubstructMatch(Chem.MolFromSmarts(\"[OX2H]\"))),        # 9  phenolic OH\n",
    "        int(mol.HasSubstructMatch(Chem.MolFromSmarts(\"[CX3]=O\"))),       # 10 carbonyl\n",
    "        int(mol.HasSubstructMatch(Chem.MolFromSmarts(\"[NX3;H2,H1]\"))),   # 11 amine\n",
    "        int(any(a.GetSymbol() in (\"Cl\",\"Br\",\"F\",\"I\") for a in mol.GetAtoms()))  # 12 halogen\n",
    "    ], dtype=float)\n",
    "\n",
    "def explain_smiles(smiles: str, *, top_k=2, p_thresh=0.5):\n",
    "    # 0) metadata\n",
    "    name, cid = pubchem_name_cid(smiles)\n",
    "\n",
    "    # 1) descriptors\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None:\n",
    "        return \"Invalid SMILES.\", None\n",
    "    d = calc_desc_flags(mol)\n",
    "\n",
    "    # 2) per-class mean |SHAP| via DeepExplainer (fast for a single mol)\n",
    "    enc = tokenizer(smiles, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model.bert.embeddings(enc.input_ids)\n",
    "    shap_vec = []\n",
    "    for i in range(len(labels_used)):\n",
    "        head = nn.Sequential(nn.Identity(), nn.Linear(E, 1)).to(device)\n",
    "        head[1].weight.data = model.classifier[1].weight.data[i:i+1]\n",
    "        head[1].bias.data   = model.classifier[1].bias.data[i:i+1]\n",
    "        v = shap.DeepExplainer(head, emb).shap_values(emb)[0]  # [1,S,768]\n",
    "        shap_vec.append(np.abs(v).mean())\n",
    "    shap_vec = np.array(shap_vec)\n",
    "\n",
    "    # 3) meta prediction\n",
    "    feats = np.hstack([d, shap_vec])  # length must match meta_in_dim\n",
    "    with torch.no_grad():\n",
    "        probs = meta(torch.FloatTensor(feats).unsqueeze(0).to(device)).cpu().numpy()[0]\n",
    "\n",
    "    # 4) textual justification\n",
    "    positives = [labels_used[i] for i, p in enumerate(probs) if p > p_thresh]\n",
    "    reasons = []\n",
    "    if d[3] > 2:   reasons.append(\"high H‑bond donor count\")\n",
    "    if d[0] > 500: reasons.append(\"large MolWt\")\n",
    "    if d[8]:       reasons.append(\"nitro group\")\n",
    "    if d[9]:       reasons.append(\"phenolic hydroxyl\")\n",
    "    if d[12]:      reasons.append(\"halogen substituent\")\n",
    "    if not reasons:\n",
    "        idxs = shap_vec.argsort()[-top_k:][::-1]\n",
    "        reasons = [f\"strong model signal for {labels_used[i]}\" for i in idxs]\n",
    "\n",
    "    header = \"\"\n",
    "    if name: header += f\"**{name}** \"\n",
    "    if cid:  header += f\"(CID {cid}) \"\n",
    "    txt = f\"{header}Model predicts toxicity for {', '.join(positives) if positives else 'no endpoints'} because of {' and '.join(reasons) if reasons else 'model signal patterns'}.\"\n",
    "    return txt, probs\n",
    "\n",
    "# ---------- 4. Run interactively ----------\n",
    "smiles_in = input(\"Enter your drug SMILES: \").strip()\n",
    "explanation, prob_vec = explain_smiles(smiles_in)\n",
    "\n",
    "print(\"\\n\" + explanation)\n",
    "if prob_vec is not None:\n",
    "    import pandas as pd\n",
    "    df_probs = pd.DataFrame({\"Endpoint\": labels_used, \"Prob\": np.round(prob_vec, 3)})\n",
    "    print(df_probs.to_string(index=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "god",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
