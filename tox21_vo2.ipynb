{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8edec91",
   "metadata": {},
   "source": [
    "# Meta explainer network for Tox21 dataset   version 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabb4c3f",
   "metadata": {},
   "source": [
    "## 1: load & clean Tox21 CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12509b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (3079, 14)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "DATA_PATH = \"Data_v3/original/tox21.csv\"  \n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "label_cols = [c for c in df.columns if c.startswith((\"NR-\",\"SR-\"))]\n",
    "df = df.dropna(subset=[\"smiles\"] + label_cols).reset_index(drop=True)\n",
    "print(\"Data shape:\", df.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b01f6a",
   "metadata": {},
   "source": [
    "## 2: compute RDKit Descriptors & Toxicophore Flags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c63ea97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptor frame: (3079, 13)\n"
     ]
    }
   ],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "import numpy as np\n",
    "\n",
    "rows = []\n",
    "for smi in df.smiles:\n",
    "    m = Chem.MolFromSmiles(smi)\n",
    "    rows.append({\n",
    "        \"MolWt\": Descriptors.MolWt(m),\n",
    "        \"LogP\":  Descriptors.MolLogP(m),\n",
    "        \"TPSA\":  Descriptors.TPSA(m),\n",
    "        \"HDon\":  Descriptors.NumHDonors(m),\n",
    "        \"HAcc\":  Descriptors.NumHAcceptors(m),\n",
    "        \"RotB\":  Descriptors.NumRotatableBonds(m),\n",
    "        \"RingC\": Descriptors.RingCount(m),\n",
    "        \"AromR\": Descriptors.NumAromaticRings(m),\n",
    "        \"nitro\": int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[N+](=O)[O-]\"))),\n",
    "        \"phenol\":int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[OX2H]\"))),\n",
    "        \"carbonyl\":int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[CX3]=O\"))),\n",
    "        \"amine\": int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[NX3;H2,H1]\"))),\n",
    "        \"halogen\":int(any(a.GetSymbol() in (\"Cl\",\"Br\",\"F\",\"I\") for a in m.GetAtoms()))\n",
    "    })\n",
    "df_desc = pd.DataFrame(rows)\n",
    "print(\"Descriptor frame:\", df_desc.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426a3ad0",
   "metadata": {},
   "source": [
    "## 3: Train / Validation Split & Tokenise SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06eef5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding Projects\\Predicting-Drug-Response-Using-Multi-Omics-Data-with-XAI\\god\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train tokens: torch.Size([2463, 267])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "\n",
    "X_train, X_val, y_train, y_val, desc_tr, desc_val = train_test_split(\n",
    "    df.smiles, df[label_cols], df_desc, test_size=0.2, random_state=42)\n",
    "\n",
    "def tokenize(smiles_list):\n",
    "    return tokenizer(smiles_list, padding=True, truncation=True,\n",
    "                     return_tensors=\"pt\")\n",
    "\n",
    "enc_train = tokenize(X_train.tolist())\n",
    "enc_val   = tokenize(X_val.tolist())\n",
    "\n",
    "print(\"Train tokens:\", enc_train.input_ids.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3503b4bd",
   "metadata": {},
   "source": [
    "## 4: Define ChemBERTa Multi-Label Classier    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44bdff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from transformers import AutoModel\n",
    "\n",
    "class ChemBERTaClassifier(nn.Module):\n",
    "    def __init__(self, n_labels=12):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(\"seyonec/ChemBERTa-zinc-base-v1\")\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(self.bert.config.hidden_size, n_labels)\n",
    "        )\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        pooled = self.bert(input_ids, attention_mask).pooler_output\n",
    "        return self.classifier(pooled)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model  = ChemBERTaClassifier(len(label_cols)).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bf08ff",
   "metadata": {},
   "source": [
    "## 5: Fine‑Tune ChemBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d099726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 308/308 [00:13<00:00, 22.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 1 epoch, loss: 0.059265751391649246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F, torch.optim as optim, tqdm\n",
    "\n",
    "train_ds = TensorDataset(enc_train.input_ids, enc_train.attention_mask,\n",
    "                         torch.FloatTensor(y_train.values))\n",
    "loader   = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "\n",
    "opt = optim.AdamW(model.parameters(), lr=2e-5)\n",
    "model.train()\n",
    "for input_ids, attn, labels in tqdm.tqdm(loader):\n",
    "    input_ids, attn, labels = input_ids.to(device), attn.to(device), labels.to(device)\n",
    "    logits = model(input_ids, attn)\n",
    "    loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "print(\"Finished 1 epoch, loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c45a958",
   "metadata": {},
   "source": [
    "## 6: Save Model & Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "45dffd6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Model & tokenizer saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SAVE_DIR = \"models/v3/chemberta_tox21\"\n",
    "model.eval()\n",
    "os.makedirs(SAVE_DIR, exist_ok=True)\n",
    "torch.save(model.state_dict(), f\"{SAVE_DIR}/model.pt\")\n",
    "tokenizer.save_pretrained(SAVE_DIR)\n",
    "print(\"✅ Model & tokenizer saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7f2de1",
   "metadata": {},
   "source": [
    "## 7:  Compute SHAP Mean‑Abs Features (All 12 Classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d9bc9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏱️ SHAP for 32 molecules finished in 126.5s\n",
      "✅ Saved shap_means: torch.Size([32, 12]) → Data_v3/SHAP_val/shap_means.npy\n"
     ]
    }
   ],
   "source": [
    "import shap, torch, numpy as np, torch.nn as nn, os, time\n",
    "\n",
    "# ▶︎  choose a very small subset for prototyping\n",
    "SUB_N = 32                 # ← change to 256 or None later\n",
    "BG_N  = 8                  # background size (must be < SUB_N)\n",
    "SAVE  = \"Data_v3/SHAP_val\"\n",
    "os.makedirs(SAVE, exist_ok=True)\n",
    "\n",
    "model.to(device).eval()\n",
    "\n",
    "# 1️⃣  Slice validation set\n",
    "ids_sub = enc_val.input_ids[:SUB_N].to(device)          # [SUB_N, S]\n",
    "\n",
    "# 2️⃣  Get embeddings (GPU)\n",
    "with torch.no_grad():\n",
    "    embed_val = model.bert.embeddings(ids_sub).float()  # FP32 to save RAM\n",
    "N, S, E = embed_val.shape\n",
    "\n",
    "shap_means = torch.zeros((N, len(label_cols)), device=device)\n",
    "\n",
    "# 3️⃣  SHAP per class (tiny background, chunked)\n",
    "start = time.time()\n",
    "bg = embed_val[:BG_N]                                   # background tensor\n",
    "\n",
    "for cls in range(len(label_cols)):\n",
    "    head = nn.Sequential(nn.Identity(), nn.Linear(E, 1)).to(device)\n",
    "    head[1].weight.data = model.classifier[1].weight.data[cls:cls+1]\n",
    "    head[1].bias.data   = model.classifier[1].bias.data[cls:cls+1]\n",
    "    explainer = shap.DeepExplainer(head, bg)\n",
    "\n",
    "    # run SHAP on this tiny batch\n",
    "    vals = explainer.shap_values(embed_val)[0]          # NumPy [N, S, 768]\n",
    "    shap_means[:, cls] = torch.from_numpy(np.abs(vals).mean(axis=(1, 2))).to(device)\n",
    "\n",
    "print(f\"⏱️ SHAP for {N} molecules finished in {(time.time()-start):.1f}s\")\n",
    "\n",
    "# 4️⃣  Save\n",
    "np.save(f\"{SAVE}/shap_means.npy\", shap_means.cpu().numpy())\n",
    "print(\"✅ Saved shap_means:\", shap_means.shape, \"→\", f'{SAVE}/shap_means.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2952f05",
   "metadata": {},
   "source": [
    "## 8: Build Meta‑Explainer Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6be6041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Meta-Explainer tensors ready — X_tr: (25, 25), X_te: (7, 25)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1️⃣ Load SHAP-means (produced in Cell 7-lite)\n",
    "shap_means = np.load(\"Data_v3/SHAP_val/shap_means.npy\")           # shape (SUB_N, 12)\n",
    "N_sub = shap_means.shape[0]\n",
    "\n",
    "# 2️⃣ Take the matching slice of descriptors & labels\n",
    "desc_val_sub = desc_val.iloc[:N_sub].reset_index(drop=True)       # (SUB_N, d)\n",
    "y_val_sub    = y_val.iloc[:N_sub].reset_index(drop=True)          # (SUB_N, 12)\n",
    "\n",
    "# 3️⃣ Concatenate descriptor features + SHAP features\n",
    "meta_X = np.hstack([desc_val_sub.values, shap_means])             # (SUB_N, d+12)\n",
    "meta_y = y_val_sub.values                                         # (SUB_N, 12)\n",
    "\n",
    "# 4️⃣ Train / test split\n",
    "X_tr, X_te, y_tr, y_te = train_test_split(\n",
    "    meta_X, meta_y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 5️⃣ Build DataLoaders\n",
    "tr_ds = TensorDataset(torch.FloatTensor(X_tr), torch.FloatTensor(y_tr))\n",
    "te_ds = TensorDataset(torch.FloatTensor(X_te), torch.FloatTensor(y_te))\n",
    "tr_loader = DataLoader(tr_ds, batch_size=16, shuffle=True)\n",
    "te_loader = DataLoader(te_ds, batch_size=16)\n",
    "\n",
    "print(f\"✅ Meta-Explainer tensors ready — X_tr: {X_tr.shape}, X_te: {X_te.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42eb8d69",
   "metadata": {},
   "source": [
    "## 9: Train Meta‑Explainer MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c9a44c9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Meta‑Explainer trained 1 epoch, loss: 2.5703611373901367\n"
     ]
    }
   ],
   "source": [
    "class MetaExplainer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 64), nn.ReLU(), nn.Dropout(0.3),\n",
    "            nn.Linear(64, out_dim), nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, x): return self.net(x)\n",
    "\n",
    "meta = MetaExplainer(in_dim=X_tr.shape[1], out_dim=y_tr.shape[1]).to(device)\n",
    "opt = torch.optim.Adam(meta.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCELoss()\n",
    "\n",
    "meta.train()\n",
    "for xb, yb in tr_loader:\n",
    "    xb, yb = xb.to(device), yb.to(device)\n",
    "    pred = meta(xb)\n",
    "    loss = loss_fn(pred, yb)\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "print(\"✅ Meta‑Explainer trained 1 epoch, loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db9fdc6",
   "metadata": {},
   "source": [
    "## 10: Generate an Explanation for a New SMILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93ac8717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      ": Model predicts toxicity for NR-AR, NR-AhR, SR-HSE, SR-p53 because of strong model signal for SR-p53 and strong model signal for SR-MMP.\n",
      "Probabilities: [0.79  0.408 0.887 0.    0.    0.085 0.185 0.    0.003 0.998 0.    1.   ]\n"
     ]
    }
   ],
   "source": [
    "# Cell 10 – Interactive SMILES → Name / CID lookup → Explanation\n",
    "# --------------------------------------------------------------\n",
    "import requests, shap, torch, numpy as np, torch.nn as nn\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Descriptors\n",
    "\n",
    "E = model.bert.config.hidden_size  # 768\n",
    "\n",
    "def pubchem_name_cid(smiles: str):\n",
    "    \"\"\"Return (name, cid) via PubChem PUG REST; (None, None) if not found.\"\"\"\n",
    "    url = (\"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/smiles/\"\n",
    "           f\"{smiles}/property/Title,IUPACName,CID/JSON\")\n",
    "    try:\n",
    "        js = requests.get(url, timeout=10).json()\n",
    "        props = js[\"PropertyTable\"][\"Properties\"][0]\n",
    "        name = props.get(\"Title\") or props.get(\"IUPACName\")\n",
    "        cid  = props.get(\"CID\")\n",
    "        return name, cid\n",
    "    except Exception:\n",
    "        return None, None\n",
    "\n",
    "def explain_smiles(smiles: str, *, top_k=2, p_thresh=0.5):\n",
    "    # 0) metadata\n",
    "    name, cid = pubchem_name_cid(smiles)\n",
    "\n",
    "    # 1) descriptors / flags\n",
    "    m = Chem.MolFromSmiles(smiles)\n",
    "    d = np.array([\n",
    "        Descriptors.MolWt(m), Descriptors.MolLogP(m), Descriptors.TPSA(m),\n",
    "        Descriptors.NumHDonors(m), Descriptors.NumHAcceptors(m),\n",
    "        Descriptors.NumRotatableBonds(m), Descriptors.RingCount(m),\n",
    "        Descriptors.NumAromaticRings(m),\n",
    "        int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[N+](=O)[O-]\"))),\n",
    "        int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[OX2H]\"))),\n",
    "        int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[CX3]=O\"))),\n",
    "        int(m.HasSubstructMatch(Chem.MolFromSmarts(\"[NX3;H2,H1]\"))),\n",
    "        int(any(a.GetSymbol() in (\"Cl\",\"Br\",\"F\",\"I\") for a in m.GetAtoms()))\n",
    "    ], dtype=float)\n",
    "\n",
    "    # 2) per‑class mean‑abs SHAP (single mol → tiny tensors, runs fast)\n",
    "    enc = tokenizer(smiles, return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        emb = model.bert.embeddings(enc.input_ids)\n",
    "    shap_vec = []\n",
    "    for i in range(len(label_cols)):\n",
    "        head = nn.Sequential(nn.Identity(), nn.Linear(E, 1)).to(device)\n",
    "        head[1].weight.data = model.classifier[1].weight.data[i:i+1]\n",
    "        head[1].bias.data   = model.classifier[1].bias.data[i:i+1]\n",
    "        v = shap.DeepExplainer(head, emb).shap_values(emb)[0]\n",
    "        shap_vec.append(np.abs(v).mean())\n",
    "    shap_vec = np.array(shap_vec)\n",
    "\n",
    "    # 3) meta‑explainer prediction\n",
    "    feats = np.hstack([d, shap_vec])\n",
    "    with torch.no_grad():\n",
    "        probs = meta(torch.FloatTensor(feats).unsqueeze(0).to(device)).cpu().numpy()[0]\n",
    "\n",
    "    # 4) text explanation\n",
    "    positives = [label_cols[i] for i,p in enumerate(probs) if p > p_thresh]\n",
    "    reasons = []\n",
    "    if d[3] > 2:   reasons.append(\"high H‑bond donor count\")\n",
    "    if d[0] > 500: reasons.append(\"large MolWt\")\n",
    "    if d[8]:       reasons.append(\"nitro group\")\n",
    "    if d[9]:       reasons.append(\"phenolic hydroxyl\")\n",
    "    if d[12]:      reasons.append(\"halogen substituent\")\n",
    "    if not reasons:\n",
    "        idxs = shap_vec.argsort()[-top_k:][::-1]\n",
    "        reasons = [f\"strong model signal for {label_cols[i]}\" for i in idxs]\n",
    "\n",
    "    header = \"\"\n",
    "    if name: header += f\"**{name}** \"\n",
    "    if cid:  header += f\"(CID {cid}) \"\n",
    "    text = (f\"{header}: Model predicts toxicity for {', '.join(positives)} \"\n",
    "            f\"because of {' and '.join(reasons)}.\")\n",
    "    return text, probs\n",
    "\n",
    "# ── Interactive prompt ────────────────────────────────────\n",
    "smiles_in = input(\"Enter your drug SMILES: \").strip()\n",
    "explanation, prob_vec = explain_smiles(smiles_in)\n",
    "print(\"\\n\" + explanation)\n",
    "print(\"Probabilities:\", np.round(prob_vec, 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c639a1c",
   "metadata": {},
   "source": [
    "## Final output!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "god",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
