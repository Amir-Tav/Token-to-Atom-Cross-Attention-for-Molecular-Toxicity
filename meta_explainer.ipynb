{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c951ffd4",
   "metadata": {},
   "source": [
    "# we will be using this notebook to generate our meta knowledge to extract our dictionary for the rule based meta explainer "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e807d8",
   "metadata": {},
   "source": [
    "## Meta explainer  \n",
    "\n",
    "we will take in the features.txt, then use our deature_mask.npy to select our features \n",
    "\n",
    "collect info about our selected variable \n",
    "\n",
    "Map each feature to an explanation \n",
    "\n",
    "save jason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa4069e",
   "metadata": {},
   "source": [
    "### Meta_explnation_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d115f955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ 1613 features total â€“ 1086 occur in any label-specific model\n",
      "ğŸ” Collected doc-strings for 682 Mordred descriptors\n",
      "ğŸ§¬ Generated explanations for 1086 descriptors\n",
      "âœ… Saved â†’ tox21_lightgb_pipeline\\Data_v6\\meta_explainer\\meta_explanations.json\n",
      " - nAcid: acidic group count descriptor.\n",
      " - nBase: basic group count descriptor.\n",
      " - SpMax_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - SpDiam_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - SpMAD_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - VE1_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - VE2_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - VE3_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - VR1_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - VR2_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - VR3_A: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - nAromBond: aromatic bonds count descriptor.\n",
      " - nBridgehead: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - nHetero: no information about this feature and looked at these sources: Mordred, RDKit, PubChem, Wikipedia\n",
      " - nH: NH or Nh may refer to\n"
     ]
    }
   ],
   "source": [
    "############################################\n",
    "# Imports & Paths\n",
    "############################################\n",
    "import os, json, joblib, re, time, functools, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import requests\n",
    "\n",
    "# ---------- chemistry toolkits ----------\n",
    "from mordred import Calculator, descriptors\n",
    "try:\n",
    "    from rdkit.Chem import Descriptors as RD_DESC\n",
    "except ImportError:\n",
    "    RD_DESC = None\n",
    "    warnings.warn(\"RDKit not found â€“ RDKit fallback disabled\")\n",
    "\n",
    "############################################\n",
    "# Repo-relative paths\n",
    "############################################\n",
    "DATA_DIR    = Path(\"tox21_lightgb_pipeline\")\n",
    "FEATURE_TXT = DATA_DIR / \"Data_v6/processed/feature_names.txt\"\n",
    "MASK_PKL    = DATA_DIR / \"models/v7/feature_masks.pkl\"\n",
    "MASK_NPY    = DATA_DIR / \"Data_v6/processed/label_mask.npy\"\n",
    "SAVE_JSON   = DATA_DIR / \"Data_v6/meta_explainer/meta_explanations.json\"\n",
    "SAVE_JSON.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "############################################\n",
    "# Load feature names & mask\n",
    "############################################\n",
    "with open(FEATURE_TXT) as f:\n",
    "    feature_names = [ln.strip() for ln in f]\n",
    "num_features = len(feature_names)\n",
    "\n",
    "if MASK_PKL.exists():\n",
    "    masks    = joblib.load(MASK_PKL)               # dict[label â†’ list(indices)]\n",
    "    all_idx  = {i for idx_list in masks.values() for i in idx_list}\n",
    "elif MASK_NPY.exists():\n",
    "    mask_arr = np.load(MASK_NPY, allow_pickle=True)\n",
    "    all_idx  = set(np.where(mask_arr.any(axis=0))[0])\n",
    "else:\n",
    "    raise FileNotFoundError(\"No feature-mask file found.\")\n",
    "\n",
    "print(f\"ğŸ“ {num_features} features total â€“ {len(all_idx)} occur in any label-specific model\")\n",
    "\n",
    "############################################\n",
    "# Build a Mordred â†’ docstring map\n",
    "############################################\n",
    "MORDRED_DOCS: dict[str, str] = {}\n",
    "\n",
    "def _first_sentence(txt: str, max_len: int = 160) -> str:\n",
    "    \"\"\"Return first sentence, optionally truncated to max_len.\"\"\"\n",
    "    sent = re.split(r\"(?<=\\.)\\s\", txt.strip())[0]\n",
    "    return (sent if len(sent) <= max_len\n",
    "            else sent[: max_len - 1].rstrip() + \"â€¦\")\n",
    "\n",
    "calc_all = Calculator(descriptors, ignore_3D=True)\n",
    "\n",
    "for desc_obj in calc_all.descriptors:\n",
    "    raw_name = str(desc_obj)            # e.g. 'piPC10', 'JGI9', 'ATS8m'\n",
    "    base_cls = type(desc_obj)\n",
    "    doc      = (base_cls.__doc__ or \"\").strip()\n",
    "    if not doc:\n",
    "        continue\n",
    "\n",
    "    line = _first_sentence(doc)\n",
    "    if len(line.split()) < 4:           # ignore one-word junk\n",
    "        continue\n",
    "\n",
    "    # Store the exact name (piPC10) and, if applicable, its base prefix (piPC)\n",
    "    MORDRED_DOCS[raw_name] = line\n",
    "\n",
    "    m = re.match(r\"^([A-Za-z_]+)\\d+$\", raw_name)\n",
    "    if m:                               # 'piPC10' â†’ 'piPC'\n",
    "        base = m.group(1)\n",
    "        MORDRED_DOCS.setdefault(base, line)\n",
    "\n",
    "print(f\"ğŸ” Collected doc-strings for {len(MORDRED_DOCS):,} Mordred descriptors\")\n",
    "\n",
    "############################################\n",
    "# Helper look-ups\n",
    "############################################\n",
    "def mordred_lookup(term: str) -> str | None:\n",
    "    \"\"\"Exact or numeric-suffix match against Mordred docs.\"\"\"\n",
    "    if term in MORDRED_DOCS:\n",
    "        return MORDRED_DOCS[term]\n",
    "\n",
    "    m = re.match(r\"^([A-Za-z_]+)(\\d+)$\", term)\n",
    "    if m and m.group(1) in MORDRED_DOCS:\n",
    "        base, order = m.groups()\n",
    "        return f\"{MORDRED_DOCS[base]} â€” order {order}\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def rdkit_lookup(term: str) -> str | None:\n",
    "    if RD_DESC is None:\n",
    "        return None\n",
    "    term_low = term.lower()\n",
    "    for name, fn in RD_DESC.descList:\n",
    "        if name.lower() == term_low or term_low.startswith(name.lower()):\n",
    "            doc = (fn.__doc__ or \"\").strip()\n",
    "            sent = _first_sentence(doc)\n",
    "            return sent if len(sent.split()) > 4 else None\n",
    "    return None\n",
    "\n",
    "\n",
    "PUBCHEM_API = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/descriptor/JSON?descriptor={}\"\n",
    "\n",
    "@functools.lru_cache(maxsize=1024)\n",
    "def pubchem_lookup(term: str, retries: int = 2) -> str | None:\n",
    "    safe = requests.utils.quote(term)\n",
    "    url  = PUBCHEM_API.format(safe)\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=4)\n",
    "            if r.status_code == 200:\n",
    "                js   = r.json()\n",
    "                desc = js.get(\"Information\", [{}])[0].get(\"Description\", \"\")\n",
    "                sent = _first_sentence(desc)\n",
    "                return sent if len(sent.split()) > 4 else None\n",
    "            if r.status_code == 404:\n",
    "                return None\n",
    "        except requests.exceptions.RequestException:\n",
    "            time.sleep(0.5)\n",
    "    return None\n",
    "\n",
    "\n",
    "WIKI_API = \"https://en.wikipedia.org/api/rest_v1/page/summary/{}\"\n",
    "\n",
    "@functools.lru_cache(maxsize=1024)\n",
    "def wikipedia_lookup(term: str, retries: int = 2) -> str | None:\n",
    "    safe = requests.utils.quote(term.replace(\" \", \"_\"))\n",
    "    url  = WIKI_API.format(safe)\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            r = requests.get(url, timeout=4)\n",
    "            if r.status_code == 200:\n",
    "                summary = r.json().get(\"extract\", \"\")\n",
    "                sent    = _first_sentence(summary)\n",
    "                return sent if len(sent.split()) > 4 else None\n",
    "            if r.status_code == 404:\n",
    "                return None\n",
    "        except requests.exceptions.RequestException:\n",
    "            time.sleep(0.5)\n",
    "    return None\n",
    "\n",
    "############################################\n",
    "# Build the meta-explanation map\n",
    "############################################\n",
    "meta_expl     = {}\n",
    "SOURCE_ORDER  = (\n",
    "    (\"Mordred\",   mordred_lookup),\n",
    "    (\"RDKit\",     rdkit_lookup),\n",
    "    (\"PubChem\",   pubchem_lookup),\n",
    "    (\"Wikipedia\", wikipedia_lookup),\n",
    ")\n",
    "\n",
    "SOURCE_LIST = \", \".join(name for name, _ in SOURCE_ORDER)\n",
    "\n",
    "for idx in sorted(all_idx):\n",
    "    feat   = feature_names[idx]\n",
    "    expl   = None\n",
    "\n",
    "    for src_name, fn in SOURCE_ORDER:\n",
    "        expl = fn(feat)\n",
    "        if expl:\n",
    "            meta_expl[feat] = expl\n",
    "            break\n",
    "\n",
    "    if expl is None:\n",
    "        meta_expl[feat] = (\n",
    "            f\"no information about this feature and looked at these sources: {SOURCE_LIST}\"\n",
    "        )\n",
    "\n",
    "print(f\"ğŸ§¬ Generated explanations for {len(meta_expl)} descriptors\")\n",
    "\n",
    "############################################\n",
    "# Save & quick preview\n",
    "############################################\n",
    "with open(SAVE_JSON, \"w\") as f:\n",
    "    json.dump(meta_expl, f, indent=2)\n",
    "\n",
    "print(f\"âœ… Saved â†’ {SAVE_JSON}\")\n",
    "for k, v in list(meta_expl.items())[:15]:\n",
    "    print(f\" - {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6ba793",
   "metadata": {},
   "source": [
    "### Check script\n",
    "\n",
    "so for our initial output, we got 67% of the description (missing 32%), so we are going to save them adn try to find another way to get thoses descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "67430a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing descriptions: 734 / 1086  (67.6%)\n",
      "first few missing: SpMax_A, SpDiam_A, SpMAD_A, VE1_A, VE2_A, VE3_A, VR1_A, VR2_A, VR3_A, nBridgehead\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the explanations file you just generated\n",
    "JSON_PATH = Path(\"tox21_lightgb_pipeline/Data_v6/meta_explainer/meta_explanations.json\")\n",
    "\n",
    "with open(JSON_PATH) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "# Detect the fallback phrase (prefix-match is enough)\n",
    "missing_keys = [k for k, v in meta.items()\n",
    "                if v.startswith(\"no information about this feature\")]\n",
    "\n",
    "total   = len(meta)\n",
    "missing = len(missing_keys)\n",
    "print(f\"Missing descriptions: {missing} / {total}  ({missing/total:.1%})\")\n",
    "\n",
    "# Optional: show a few example names\n",
    "if missing_keys:\n",
    "    print(\"first few missing:\", \", \".join(missing_keys[:10]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e590c677",
   "metadata": {},
   "source": [
    "### to find the feature for the missing ones "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4f928efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”  pass-2 will retry 734 still-missing descriptors\n",
      "â³  downloading Mordred descriptor list ...\n",
      "ğŸ“œ  scraped 1,453 descriptor rows from online doc\n",
      "âœ…  recovered 635 additional descriptions; 99 still missing\n",
      "ğŸ’¾  wrote updated file   â†’  tox21_lightgb_pipeline\\Data_v6\\meta_explainer\\meta_explanations_v2.json\n"
     ]
    }
   ],
   "source": [
    "import json, re, html, requests, time\n",
    "from pathlib import Path\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 1. load existing explanations\n",
    "# --------------------------------------------------\n",
    "JSON_IN  = Path(\"tox21_lightgb_pipeline/Data_v6/meta_explainer/meta_explanations.json\")\n",
    "JSON_OUT = JSON_IN.with_stem(JSON_IN.stem + \"_v2\")   # meta_explanations_v2.json\n",
    "\n",
    "with open(JSON_IN) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "missing = [k for k, v in meta.items()\n",
    "           if v.startswith(\"no information about this feature\")]\n",
    "\n",
    "print(f\"ğŸ”  pass-2 will retry {len(missing)} still-missing descriptors\")\n",
    "\n",
    "if not missing:\n",
    "    print(\"âœ… nothing to do â€“ everything already annotated\")\n",
    "    raise SystemExit\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 2. pull the master Mordred descriptor table once\n",
    "# --------------------------------------------------\n",
    "DOC_URL = \"https://mordred-descriptor.github.io/documentation/master/descriptors.html\"\n",
    "print(\"â³  downloading Mordred descriptor list ...\")\n",
    "html_text = requests.get(DOC_URL, timeout=15).text\n",
    "\n",
    "# strip all HTML tags (quick-and-dirty is fine here)\n",
    "plain = re.sub(\"<[^>]+>\", \"\", html_text)           # keep only text\n",
    "plain = html.unescape(plain)                       # un-escape &nbsp; etc.\n",
    "lines = [ln.strip() for ln in plain.splitlines() if ln.strip()]\n",
    "\n",
    "# build {descriptor â†’ description} map from the flat text table\n",
    "mordred_table = {}\n",
    "for i, ln in enumerate(lines[:-1]):\n",
    "    # descriptor names have no spaces and are followed two lines later\n",
    "    if re.fullmatch(r\"[A-Za-z0-9_]+\", ln) and not ln[0].isdigit():\n",
    "        desc = lines[i + 1]                        # the next line is the description\n",
    "        # heuristic: accept descriptions containing â‰¥3 words\n",
    "        if len(desc.split()) >= 3:\n",
    "            mordred_table[ln] = desc\n",
    "\n",
    "print(f\"ğŸ“œ  scraped {len(mordred_table):,} descriptor rows from online doc\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 3. fill any gaps we can\n",
    "# --------------------------------------------------\n",
    "fixed = 0\n",
    "for feat in missing:\n",
    "    if feat in mordred_table:\n",
    "        meta[feat] = mordred_table[feat] + \" â€” scraped from Mordred docs\"\n",
    "        fixed += 1\n",
    "    else:\n",
    "        # try base-name match for variants like \"SpMax_A\" -> \"SpMax\"\n",
    "        base = feat.split(\"_\", 1)[0]\n",
    "        if base in mordred_table:\n",
    "            meta[feat] = (mordred_table[base] +\n",
    "                          f\" â€” weighting variant ({feat[len(base)+1:]})\")\n",
    "            fixed += 1\n",
    "\n",
    "print(f\"âœ…  recovered {fixed} additional descriptions; \"\n",
    "      f\"{len(missing) - fixed} still missing\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# 4. save the augmented JSON\n",
    "# --------------------------------------------------\n",
    "with open(JSON_OUT, \"w\") as f:\n",
    "    json.dump(meta, f, indent=2)\n",
    "\n",
    "print(f\"ğŸ’¾  wrote updated file   â†’  {JSON_OUT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4005204f",
   "metadata": {},
   "source": [
    "### diagnostics \n",
    "\n",
    "So first we need to understand the style of our descriptions, then we can post process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a250b9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
      "â”‚  Missing completely :   99 â”‚\n",
      "â”‚  Generic â€˜scrapedâ€™ :  615 â”‚\n",
      "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
      "\n",
      "âš™ï¸  Property-code letters *not* in PROPERTY_CODE:\n",
      "   â€¢ z   (52 features)\n",
      "   â€¢ m   (51 features)\n",
      "   â€¢ v   (51 features)\n",
      "   â€¢ d   (49 features)\n",
      "   â€¢ c   (34 features)\n",
      "\n",
      "âœ… No obvious new OVERRIDE families needed.\n",
      "\n",
      "ğŸ“ Frequent jargon words not in JARGON_MAP (count â‰¥ 3):\n",
      "mordred, scraped, numeric, property, information, feature, sources, pubchem,\n",
      "    wikipedia, structural, pattern, similarity, structure, surface, descriptors,\n",
      "    contribution, content, baryszmatrix, distance, ringcount\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json, re, textwrap\n",
    "from collections import Counter, defaultdict\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 0.  paths  (edit if you keep the file elsewhere)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "META = Path(\"tox21_lightgb_pipeline/Data_v6/meta_explainer/meta_explanations_plain.json\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1.  existing maps you already have\n",
    "#     (copy-paste the latest versions here â†“)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PROPERTY_CODE = {\n",
    "    \"a\": \"atomic mass\",\n",
    "    \"p\": \"polarizability\",\n",
    "    \"i\": \"ionisation potential\",\n",
    "    \"s\": \"sigma electronegativity\",\n",
    "    \"e\": \"electronegativity\",\n",
    "    \"u\": \"van-der-Waals volume\",\n",
    "}\n",
    "\n",
    "OVERRIDES = {\"piPC\", \"JGI\", \"BIC\", \"RPCG\"}     # just the *prefixes* youâ€™ve handled\n",
    "\n",
    "JARGON_MAP = {\n",
    "    r\"\\bautocorrelation\\b\": \"similarity pattern\",\n",
    "    r\"\\blipophilicity\\b\":   \"greasiness\",\n",
    "    r\"\\bpolar\\b\":           \"water-attracting\",\n",
    "    r\"\\bhydrophobicity\\b\":  \"water-repelling\",\n",
    "    r\"\\btopological\\b\":     \"structural\",\n",
    "    r\"\\beigenvalue\\b\":      \"mathematical value\",\n",
    "    r\"\\bpermeability\\b\":    \"ability to pass through membranes\",\n",
    "    r\"\\bÏ€\\b\":               \"pi-bond\",\n",
    "    r\"\\bconjugated\\b\":      \"alternating-double-bond\",\n",
    "    r\"\\bdescriptor\\b\":      \"numeric property\",\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2.  load explanations\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(META) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3.  basic counts\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "missing = {k for k, v in meta.items()\n",
    "           if v.lower().startswith(\"no information about this feature\")}\n",
    "scraped_generic = {k for k, v in meta.items()\n",
    "                   if \"scraped from mordred docs\" in v.lower()}\n",
    "\n",
    "print(\"â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\")\n",
    "print(f\"â”‚  Missing completely : {len(missing):>4} â”‚\")\n",
    "print(f\"â”‚  Generic â€˜scrapedâ€™ : {len(scraped_generic):>4} â”‚\")\n",
    "print(\"â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4.  property-code letters not yet mapped\n",
    "#     (AATS4p â†’ code â€˜pâ€™; if â€˜pâ€™ absent from PROPERTY_CODE, flag it)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prop_pat = re.compile(r\"^(?:AATS|ATS|AATSC|ATSC|MATS|GATS)(\\d+)([a-z])$\", re.I)\n",
    "missing_props = Counter()\n",
    "\n",
    "for feat in meta:\n",
    "    m = prop_pat.match(feat)\n",
    "    if m:\n",
    "        code = m.group(2).lower()\n",
    "        if code not in PROPERTY_CODE:\n",
    "            missing_props[code] += 1\n",
    "\n",
    "if missing_props:\n",
    "    print(\"âš™ï¸  Property-code letters *not* in PROPERTY_CODE:\")\n",
    "    for code, n in missing_props.most_common():\n",
    "        print(f\"   â€¢ {code!s:<2}  ({n} features)\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âœ… All AATS/ATS property codes are covered.\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5.  descriptor families worth an OVERRIDE\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "family_pat = re.compile(r\"^([A-Z]{3,})(\\d*)$\")\n",
    "candidates = Counter()\n",
    "\n",
    "for feat, expl in meta.items():\n",
    "    m = family_pat.match(feat)\n",
    "    if not m:\n",
    "        continue\n",
    "    base = m.group(1)\n",
    "    if base not in OVERRIDES and base.isupper():\n",
    "        # heuristic: if explanation still contains the bare family name, flag it\n",
    "        if base.lower() in expl.lower():\n",
    "            candidates[base] += 1\n",
    "\n",
    "if candidates:\n",
    "    print(\"ğŸ’¡ Families you may want to add to OVERRIDES:\")\n",
    "    for base, n in candidates.most_common(10):\n",
    "        print(f\"   â€¢ {base:<6}  ({n} variants)\")\n",
    "    print()\n",
    "else:\n",
    "    print(\"âœ… No obvious new OVERRIDE families needed.\\n\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6.  frequent jargon not yet in JARGON_MAP\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "word_counter = Counter()\n",
    "token_pat = re.compile(r\"[A-Za-z]{7,}\")\n",
    "\n",
    "for expl in meta.values():\n",
    "    for w in token_pat.findall(expl):\n",
    "        wl = w.lower()\n",
    "        # ignore if already translated\n",
    "        if any(re.search(pat, wl, flags=re.I) for pat in JARGON_MAP):\n",
    "            continue\n",
    "        word_counter[wl] += 1\n",
    "\n",
    "common_jargon = [w for w, c in word_counter.most_common(20) if c >= 3]\n",
    "\n",
    "if common_jargon:\n",
    "    print(\"ğŸ“ Frequent jargon words not in JARGON_MAP (count â‰¥ 3):\")\n",
    "    print(textwrap.fill(\", \".join(common_jargon), width=80, subsequent_indent=\"    \"))\n",
    "else:\n",
    "    print(\"âœ… No high-frequency jargon left un-mapped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2596e5",
   "metadata": {},
   "source": [
    "### Post-processing\n",
    "\n",
    "making sure it is in plain english and easy to follow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ebcdfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ…  Plain-English explanations written â†’ tox21_lightgb_pipeline\\Data_v6\\meta_explainer\\meta_explanations_plain.json\n"
     ]
    }
   ],
   "source": [
    "import json, re\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. paths\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT = Path(\"tox21_lightgb_pipeline/Data_v6/meta_explainer\")\n",
    "IN   = ROOT / \"meta_explanations_v2.json\"\n",
    "OUT  = ROOT / \"meta_explanations_plain.json\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. load source JSON\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(IN) as f:\n",
    "    meta = json.load(f)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. helper maps\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "PROPERTY_CODE = {\n",
    "    # original\n",
    "    \"a\": \"atomic mass\",\n",
    "    \"p\": \"polarizability\",\n",
    "    \"i\": \"ionisation potential\",\n",
    "    \"s\": \"sigma electronegativity\",\n",
    "    \"e\": \"electronegativity\",\n",
    "    \"u\": \"van der Waals volume\",\n",
    "    # newly added\n",
    "    \"z\": \"atomic number\",\n",
    "    \"m\": \"atomic mass\",\n",
    "    \"v\": \"van der Waals volume\",\n",
    "    \"d\": \"electron density\",\n",
    "    \"c\": \"atomic charge\",\n",
    "}\n",
    "\n",
    "JARGON_MAP = {\n",
    "    # scientific jargon\n",
    "    r\"\\bautocorrelation\\b\": \"similarity pattern\",\n",
    "    r\"\\blipophilicity\\b\":   \"greasiness\",\n",
    "    r\"\\bpolar\\b\":           \"water-attracting\",\n",
    "    r\"\\bhydrophobicity\\b\":  \"water-repelling\",\n",
    "    r\"\\btopological\\b\":     \"structural\",\n",
    "    r\"\\beigenvalue\\b\":      \"mathematical value\",\n",
    "    r\"\\bpermeability\\b\":    \"ability to pass membranes\",\n",
    "    r\"\\bÏ€\\b\":               \"pi-bond (double-bond)\",\n",
    "    r\"\\bconjugated\\b\":      \"alternating-double-bond\",\n",
    "    r\"\\bdescriptor\\b\":      \"numeric property\",\n",
    "    # frequent filler / boiler-plate â†’ delete\n",
    "    r\"\\bmordred\\b\":         \"\",\n",
    "    r\"\\bscraped\\b\":         \"\",\n",
    "    r\"\\bnumeric\\b\":         \"\",\n",
    "    r\"\\bproperty\\b\":        \"\",\n",
    "    r\"\\binformation\\b\":     \"\",\n",
    "    r\"\\bfeature\\b\":         \"\",\n",
    "    r\"\\bsources?\\b\":        \"\",\n",
    "    r\"\\bpubchem\\b\":         \"\",\n",
    "    r\"\\bwikipedia\\b\":       \"\",\n",
    "    # misc often-repeated words (keep or shorten)\n",
    "    r\"\\bpattern\\b\":         \"pattern\",\n",
    "    r\"\\bsimilarity\\b\":      \"similarity\",\n",
    "    r\"\\bstructure\\b\":       \"molecule\",\n",
    "    r\"\\bsurface\\b\":         \"surface\",\n",
    "    r\"\\bdescriptors?\\b\":    \"descriptors\",\n",
    "    r\"\\bcontribution\\b\":    \"contribution\",\n",
    "    r\"\\bcontent\\b\":         \"content\",\n",
    "    r\"\\bbaryszmatrix\\b\":    \"Barysz matrix\",\n",
    "    r\"\\bdistance\\b\":        \"distance\",\n",
    "    r\"\\bringcount\\b\":       \"ring count\",\n",
    "}\n",
    "\n",
    "OVERRIDES = {\n",
    "    \"piPC\": lambda n: (f\"Count of Ï€-electron paths exactly {n} bonds long \"\n",
    "                       \"(log-scaled). High values flag long conjugated chains.\"),\n",
    "    \"JGI\":  lambda n: (f\"Average structural charge difference between atoms \"\n",
    "                       f\"{n} bonds apart. Gauges charge distribution.\"),\n",
    "    \"BIC\":  lambda n: (f\"Bonding-information content at distance {n}. \"\n",
    "                       \"Higher = more complex bonding pattern.\"),\n",
    "    \"RPCG\": lambda _: (\"Relative positive charge: most positive atomâ€™s charge \"\n",
    "                       \"divided by total positive charge in the molecule.\"),\n",
    "}\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. family-specific decoders  (AATS / ATS â€¦)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "AATS_PAT = re.compile(r\"^(AATS|ATS|AATSC|ATSC|MATS|GATS)(\\d+)([a-z])$\", re.I)\n",
    "\n",
    "def decode_aats(name: str) -> str | None:\n",
    "    \"\"\"\n",
    "    Turn â€˜AATS4pâ€™ into 'Average similarity pattern of polarizability at lag 4'.\n",
    "    Returns None if the name does not match the regex.\n",
    "    \"\"\"\n",
    "    m = AATS_PAT.match(name)\n",
    "    if not m:\n",
    "        return None\n",
    "    base, lag, code = m.groups()\n",
    "    prop = PROPERTY_CODE.get(code.lower(), f\"property {code}\")\n",
    "    family = \"Average\" if base[0] == \"A\" else \"Centered\"\n",
    "    return (f\"{family} similarity pattern of {prop} at lag {lag}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. misc cleaning helpers\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "SCRAPE_PAT = re.compile(r\"â€”\\s*(?:scraped|from)\\s+(?:mordred|pubchem|wikipedia).*\",\n",
    "                        flags=re.I)\n",
    "\n",
    "def apply_overrides(name: str) -> str | None:\n",
    "    m = re.match(r\"^([A-Za-z]+)(\\d*)$\", name)\n",
    "    if not m:\n",
    "        return None\n",
    "    base, order = m.groups()\n",
    "    if base in OVERRIDES:\n",
    "        return OVERRIDES[base](order) if order else OVERRIDES[base](\"\")\n",
    "    return None\n",
    "\n",
    "def simplify(text: str) -> str:\n",
    "    # drop scrape notes & parameter blobs\n",
    "    text = SCRAPE_PAT.sub(\"\", text)\n",
    "    text = re.sub(r\"\\([^)]*\\)\", \"\", text)            # (â€¦) blocks\n",
    "    text = re.sub(r\"â€”\\s*order\\s*\\d+\", \"\", text)      # â€” order N\n",
    "    # jargon replacements\n",
    "    for pat, repl in JARGON_MAP.items():\n",
    "        text = re.sub(pat, repl, text, flags=re.I)\n",
    "    # tidy whitespace / dashes\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text).strip(\" â€”\")\n",
    "    return text[0].upper() + text[1:] if text else text\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. build the plain-English dict\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plain_meta = {}\n",
    "for name, sentence in meta.items():\n",
    "    # 1) family-specific decoder\n",
    "    decoded = decode_aats(name)\n",
    "    if decoded:\n",
    "        plain_meta[name] = decoded\n",
    "        continue\n",
    "    # 2) hard overrides\n",
    "    override = apply_overrides(name)\n",
    "    if override:\n",
    "        plain_meta[name] = override\n",
    "        continue\n",
    "    # 3) general simplifier\n",
    "    plain_meta[name] = simplify(sentence)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 7. save\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(OUT, \"w\") as f:\n",
    "    json.dump(plain_meta, f, indent=2)\n",
    "\n",
    "print(f\"âœ…  Plain-English explanations written â†’ {OUT}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aeb454f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AATS â€” scraped from Mordred docs\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path, PurePath\n",
    "import json\n",
    "\n",
    "plain_path = Path(\"tox21_lightgb_pipeline/Data_v6/meta_explainer/meta_explanations_plain.json\")\n",
    "with open(plain_path) as f:\n",
    "    plain_meta = json.load(f)\n",
    "\n",
    "print(plain_meta[\"AATS5i\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06659979",
   "metadata": {},
   "source": [
    "## SMART explainer  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95d38edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“  Updated 57 explanations; saving enriched file â€¦\n",
      "âœ…  Wrote â†’  tox21_lightgb_pipeline\\Data_v6\\meta_explainer\\smarts_rules_final_v2.json\n"
     ]
    }
   ],
   "source": [
    "import json, re, time, functools, requests\n",
    "from pathlib import Path\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. paths\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "ROOT = Path(\"tox21_lightgb_pipeline/Data_v6/meta_explainer\")\n",
    "IN   = ROOT / \"smarts_rules_final.json\"\n",
    "OUT  = ROOT / \"smarts_rules_final_v2.json\"\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. load existing rules\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(IN) as f:\n",
    "    rules_by_label: dict = json.load(f)\n",
    "\n",
    "label_cols = [\n",
    "    \"NR-AR\", \"NR-AR-LBD\", \"NR-AhR\", \"NR-Aromatase\", \"NR-ER\",\n",
    "    \"NR-ER-LBD\", \"NR-PPAR-gamma\", \"SR-ARE\", \"SR-ATAD5\",\n",
    "    \"SR-HSE\", \"SR-MMP\", \"SR-p53\",\n",
    "]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. add starter kits for empty endpoints\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "GENERIC_RULES = [\n",
    "    {\"name\": \"Nitro group\",        \"smarts\": \"[NX3](=O)=O\",\n",
    "     \"explanation\": \"\",            \"severity\": \"high\"},\n",
    "    {\"name\": \"Epoxide\",            \"smarts\": \"[C;r3]1[O;r3][C;r3]1\",\n",
    "     \"explanation\": \"\",            \"severity\": \"high\"},\n",
    "    {\"name\": \"Alkyl halide\",       \"smarts\": \"[CX4][F,Cl,Br,I]\",\n",
    "     \"explanation\": \"\",            \"severity\": \"medium\"},\n",
    "    {\"name\": \"Aldehyde\",           \"smarts\": \"[CX3H1](=O)[#6]\",\n",
    "     \"explanation\": \"\",            \"severity\": \"medium\"},\n",
    "]\n",
    "\n",
    "for lab in label_cols:\n",
    "    rules_by_label.setdefault(lab, []).extend([])   # ensure key exists\n",
    "    if not rules_by_label[lab]:                     # insert generic set\n",
    "        rules_by_label[lab] = [r.copy() for r in GENERIC_RULES]\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. helpers to fetch a one-liner\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "WIKI_API = \"https://en.wikipedia.org/api/rest_v1/page/summary/{}\"\n",
    "PUBCHEM  = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/{}/description/JSON\"\n",
    "\n",
    "@functools.lru_cache(maxsize=256)\n",
    "def wiki_sentence(term: str) -> tuple[str | None, str | None]:\n",
    "    safe = requests.utils.quote(term.replace(\" \", \"_\"))\n",
    "    url  = WIKI_API.format(safe)\n",
    "    try:\n",
    "        r = requests.get(url, timeout=5)\n",
    "        if r.status_code == 200:\n",
    "            text = r.json().get(\"extract\", \"\")\n",
    "            sent = re.split(r\"(?<=\\.)\\s\", text.strip())[0]\n",
    "            return (sent if len(sent.split()) > 4 else None, url)\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "    return (None, None)\n",
    "\n",
    "@functools.lru_cache(maxsize=256)\n",
    "def pubchem_sentence(term: str) -> tuple[str | None, str | None]:\n",
    "    safe = requests.utils.quote(term)\n",
    "    url  = PUBCHEM.format(safe)\n",
    "    try:\n",
    "        r = requests.get(url, timeout=5)\n",
    "        if r.status_code == 200:\n",
    "            tree = r.json()\n",
    "            desc = tree[\"InformationList\"][\"Information\"][0].get(\"Description\", \"\")\n",
    "            sent = re.split(r\"(?<=\\.)\\s\", desc.strip())[0]\n",
    "            return (sent if len(sent.split()) > 4 else None, url)\n",
    "    except requests.exceptions.RequestException:\n",
    "        pass\n",
    "    return (None, None)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 5. iterate and enrich\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "updated = 0\n",
    "for lab, rules in rules_by_label.items():\n",
    "    for rule in rules:\n",
    "        # add severity if missing\n",
    "        rule.setdefault(\"severity\", \"medium\")\n",
    "\n",
    "        # skip if explanation already >10 words\n",
    "        if rule.get(\"explanation\") and len(rule[\"explanation\"].split()) > 10:\n",
    "            continue\n",
    "\n",
    "        sent, ref = wiki_sentence(rule[\"name\"])\n",
    "        if not sent:\n",
    "            sent, ref = pubchem_sentence(rule[\"name\"])\n",
    "\n",
    "        if sent:\n",
    "            rule[\"explanation\"] = sent\n",
    "            rule[\"ref\"] = ref\n",
    "            updated += 1\n",
    "        else:\n",
    "            rule.setdefault(\n",
    "                \"explanation\",\n",
    "                \"No concise public description found (checked Wikipedia & PubChem).\",\n",
    "            )\n",
    "\n",
    "print(f\"ğŸ“  Updated {updated} explanations; saving enriched file â€¦\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 6. write out\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "with open(OUT, \"w\") as f:\n",
    "    json.dump(rules_by_label, f, indent=2)\n",
    "\n",
    "print(f\"âœ…  Wrote â†’  {OUT}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lord",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
